{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAffuMD0Jx24"
      },
      "outputs": [],
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==1.13.0 torchvision==0.13.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.13-cp38-cp38-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thGUsQBBYLfw",
        "outputId": "7aa72030-b0ab-4861-9dec-39f8b186308e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_xla\n",
            "  Downloading torch_xla-1.0-py3-none-any.whl (1.4 kB)\n",
            "Installing collected packages: torch_xla\n",
            "Successfully installed torch_xla-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_xla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qV3oD8gZD6H",
        "outputId": "625adb3f-9970-4ca8-8532-900798c5bd16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-neuronx\n",
            "  Downloading tensorflow_neuronx-1.0-py3-none-any.whl (1.5 kB)\n",
            "Installing collected packages: tensorflow-neuronx\n",
            "Successfully installed tensorflow-neuronx-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-neuronx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_3l7S9eJx28"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning bert_score datasets rouge_score evaluate transformers wandb accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfzo81ZqJx29",
        "outputId": "21adbb1d-b32f-45dc-b0d6-ff56883b7a5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'CQA_RLHF'...\n",
            "remote: Enumerating objects: 177, done.\u001b[K\n",
            "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
            "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
            "remote: Total 177 (delta 113), reused 119 (delta 58), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (177/177), 55.48 KiB | 2.22 MiB/s, done.\n",
            "Resolving deltas: 100% (113/113), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Myashka/CQA_RLHF.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar7l-FvZJx29",
        "outputId": "51c6066a-dfb6-4b78-93a8-bf627665b1b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K34s1lAzJx2-"
      },
      "outputs": [],
      "source": [
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YXCMjOSDJx2-"
      },
      "outputs": [],
      "source": [
        "trainer_config = dict(\n",
        "    seed=42,\n",
        "    model_name=\"EleutherAI/gpt-neo-125M\",\n",
        "    wandb=dict(\n",
        "        api=\"60fce56bfaec85b8d6bc78bfac2086891f5afe54\",\n",
        "        project_name=\"CQA_RLHF\",\n",
        "        args=dict(job_type=\"train\", group=\"sft\", name=\"test-run\"),\n",
        "    ),\n",
        "    data=dict(\n",
        "        path_to_data=r\"/content/drive/MyDrive/Diploma/data/1.0-data-div-ans-sep.json\",\n",
        "        max_length=512,\n",
        "        batch_size=2,\n",
        "    ),\n",
        "    model_params=dict(\n",
        "        lr=5e-5,\n",
        "        do_freeze=True,\n",
        "        use_cache=False,\n",
        "        warmup_steps=100,\n",
        "        adam_betas=[0.9, 0.95],\n",
        "        weight_decay=0.001,\n",
        "        do_compute_metrics = False\n",
        "    ),\n",
        "    trainer=dict(\n",
        "        checkpoint=dict(\n",
        "            every_n_train_steps=2000,\n",
        "            dirpath=r\"/content/drive/MyDrive/Diploma/Checkpoints\",\n",
        "        ),\n",
        "        params=dict(\n",
        "            accelerator=\"tpu\",\n",
        "            max_epochs=3,\n",
        "            auto_scale_batch_size=True,\n",
        "            accumulate_grad_batches=1,\n",
        "            gradient_clip_val=1,\n",
        "            precision=\"16\",\n",
        "            ckpt_path=None,\n",
        "            val_check_interval=1000,\n",
        "            overfit_batches=0,  # 0 for train\n",
        "            num_sanity_val_steps=2,\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "\n",
        "with open(\"trainer_config.yaml\", \"w\") as outfile:\n",
        "    yaml.dump(trainer_config, outfile, default_flow_style=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPcAvJaFJx3A",
        "outputId": "4eb73c17-47f0-4fcb-af6b-457a3f87169e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-02-22 18:31:01.828354: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-22 18:31:03.078716: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-22 18:31:03.078837: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-22 18:31:03.078855: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/utils/import_utils.py\", line 1110, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/__init__.py\", line 49, in <module>\n",
            "    from .audio_classification import AudioClassificationPipeline\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/audio_classification.py\", line 22, in <module>\n",
            "    from .base import PIPELINE_INIT_ARGS, Pipeline\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\", line 34, in <module>\n",
            "    from ..modelcard import ModelCard\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/modelcard.py\", line 47, in <module>\n",
            "    from .training_args import ParallelMode\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/training_args.py\", line 62, in <module>\n",
            "    import torch_xla.core.xla_model as xm\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/__init__.py\", line 1, in <module>\n",
            "    raise ImportError(\"WRONG PACKAGE. Please install the package from Neuron Repository - pip.repos.neuron.amazonaws.com\")\n",
            "ImportError: WRONG PACKAGE. Please install the package from Neuron Repository - pip.repos.neuron.amazonaws.com\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/CQA_RLHF/sft/lightning/pl_train.py\", line 9, in <module>\n",
            "    from model import LitLM\n",
            "  File \"/content/CQA_RLHF/sft/lightning/model.py\", line 9, in <module>\n",
            "    from evaluate import load\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/evaluate/__init__.py\", line 29, in <module>\n",
            "    from .evaluation_suite import EvaluationSuite\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/evaluate/evaluation_suite/__init__.py\", line 10, in <module>\n",
            "    from ..evaluator import evaluator\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/evaluate/evaluator/__init__.py\", line 27, in <module>\n",
            "    from .automatic_speech_recognition import AutomaticSpeechRecognitionEvaluator\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/evaluate/evaluator/automatic_speech_recognition.py\", line 22, in <module>\n",
            "    from .base import EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/evaluate/evaluator/base.py\", line 34, in <module>\n",
            "    from transformers import Pipeline, pipeline\n",
            "  File \"<frozen importlib._bootstrap>\", line 1039, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/utils/import_utils.py\", line 1100, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/utils/import_utils.py\", line 1112, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n",
            "WRONG PACKAGE. Please install the package from Neuron Repository - pip.repos.neuron.amazonaws.com\n"
          ]
        }
      ],
      "source": [
        "!python /content/CQA_RLHF/sft/lightning/pl_train.py --config_file /content/trainer_config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvFCZx-UK8Da"
      },
      "outputs": [],
      "source": [
        "!python /content/CQA_RLHF/sft/lightning/pl_train.py --config_file /content/trainer_config.yaml"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

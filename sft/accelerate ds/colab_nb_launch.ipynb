{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdOa0NC4iStA",
        "outputId": "f5e6b433-da28-4d02-b599-113028d7e1d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score datasets rouge_score evaluate transformers wandb accelerate"
      ],
      "metadata": {
        "id": "NIst2OkpiU8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Myashka/CQA_RLHF.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xef1x7uSiWHi",
        "outputId": "0b446240-1917-4da4-e0ed-ddcf6c08a780"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CQA_RLHF'...\n",
            "remote: Enumerating objects: 157, done.\u001b[K\n",
            "remote: Counting objects: 100% (157/157), done.\u001b[K\n",
            "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
            "remote: Total 157 (delta 100), reused 103 (delta 49), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (157/157), 42.03 KiB | 1.83 MiB/s, done.\n",
            "Resolving deltas: 100% (100/100), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==1.13.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.13-cp38-cp38-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-9NfKyBiXFp",
        "outputId": "7d9ad91c-26c4-429e-f9fa-82afb9dde157"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-xla==1.13\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.13-cp38-cp38-linux_x86_64.whl (151.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.3/151.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting torch==1.13.0\n",
            "  Downloading torch-1.13.0-cp38-cp38-manylinux1_x86_64.whl (890.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.2/890.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.8/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0) (4.5.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uritemplate<4dev,>=3.0.0\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Collecting google-api-core<2dev,>=1.13.0\n",
            "  Downloading google_api_core-1.34.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.1.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (57.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.38.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from torch-xla==1.13) (1.4.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.9)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.8/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.58.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.25.1)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.19.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (5.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Installing collected packages: uritemplate, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, google-api-core, google-api-python-client, cloud-tpu-client, torch-xla\n",
            "  Attempting uninstall: uritemplate\n",
            "    Found existing installation: uritemplate 4.1.1\n",
            "    Uninstalling uritemplate-4.1.1:\n",
            "      Successfully uninstalled uritemplate-4.1.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.11.0\n",
            "    Uninstalling google-api-core-2.11.0:\n",
            "      Successfully uninstalled google-api-core-2.11.0\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.70.0\n",
            "    Uninstalling google-api-python-client-2.70.0:\n",
            "      Successfully uninstalled google-api-python-client-2.70.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.13.0 which is incompatible.\n",
            "earthengine-api 0.1.341 requires google-api-python-client>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloud-tpu-client-0.10 google-api-core-1.34.0 google-api-python-client-1.8.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0 torch-xla-1.13 uritemplate-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate config default --mixed_precision bf16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr3WYizQiY5l",
        "outputId": "608731f7-da49-48aa-9766-220c8c72ebc8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.13\n",
            "2023-02-21 11:24:31.976323: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-21 11:24:31.977199: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-21 11:24:31.977238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obsRM0m_iRwH",
        "outputId": "7bb3d253-e325-438f-81f9-d9c3df867926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.13\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"/content/CQA_RLHF/sft/dataset\")\n",
        "from tqdm.auto import tqdm\n",
        "from dataset import create_dataloaders\n",
        "from accelerate import Accelerator, notebook_launcher\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from accelerate.utils import set_seed\n",
        "from evaluate import load\n",
        "import wandb\n",
        "import yaml\n",
        "from yaml import CLoader\n",
        "import numpy as np\n",
        "import torch\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yB06D-k6iRwK"
      },
      "outputs": [],
      "source": [
        "trainer_config = dict(\n",
        "    model_name=\"EleutherAI/gpt-neo-125M\",\n",
        "    data=dict(\n",
        "        data_path=r'/content/drive/MyDrive/Diploma/data/1.0-data-div-ans-sep.json',\n",
        "        batch_size=16,\n",
        "        max_length=512,\n",
        "    ),\n",
        "    train=dict(\n",
        "        n_epoches=3,\n",
        "        seed=42,\n",
        "        learning_rate=5e-5,\n",
        "        mixed_precision=\"bf16\",\n",
        "        freeze=True,\n",
        "        gradient_accumulation_steps=1,\n",
        "        max_grad_norm=None,\n",
        "        warmup_steps=100,\n",
        "        resume_from_checkpoint=False,\n",
        "        eval_every=1000,\n",
        "        output_dir=r\"/content/drive/MyDrive/Diploma/Checkpoints\",\n",
        "        log_with=\"wandb\",\n",
        "    ),\n",
        "    wandb_kwargs=dict(entity=\"myashka\", job_type=\"train\", group=\"sft\"),\n",
        "    use_cache=False,\n",
        "    is_tpu=True,\n",
        "    wandb_api=\"60fce56bfaec85b8d6bc78bfac2086891f5afe54\",\n",
        ")\n",
        "\n",
        "with open(\"trainer_config.yaml\", \"w\") as outfile:\n",
        "    yaml.dump(trainer_config, outfile, default_flow_style=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKiWDJc3iRwM"
      },
      "outputs": [],
      "source": [
        "ds_config = {\n",
        "    \"bf16\": {\"enabled\": True},\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": False,\n",
        "        \"offload_optimizer\": {\"device\": None},\n",
        "        \"offload_param\": {\"device\": None},\n",
        "    },\n",
        "    \"gradient_clipping\": 1.0,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"steps_per_print\": 2000000,\n",
        "}\n",
        "\n",
        "with open('ds-config.json', 'w') as f:\n",
        "    json.dump(ds_config, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sky_DN2CiRwN"
      },
      "outputs": [],
      "source": [
        "accelerate_config = dict(\n",
        "    compute_environment = 'LOCAL_MACHINE',\n",
        "    deepspeed_config = dict(\n",
        "        # deepspeed_config_file = 'c',\n",
        "        # zero3_init_flag = True\n",
        "    ),\n",
        "    distributed_type = 'TPU',\n",
        "    fsdp_config = dict(),\n",
        "    machine_rank = 0,\n",
        "    main_process_ip = None,\n",
        "    main_process_port = None,\n",
        "    main_training_function = 'main',\n",
        "    mixed_precision = 'bf16',\n",
        "    downcast_bf16 = True,\n",
        "    num_machines = 1,\n",
        "    num_processes = 8,\n",
        "    use_cpu = False\n",
        "\n",
        ")\n",
        "\n",
        "with open('default_config.yaml', 'w') as outfile:\n",
        "    yaml.dump(accelerate_config, outfile, default_flow_style=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xcHx7I29iRwO"
      },
      "outputs": [],
      "source": [
        "!mv default_config.yaml /root/.cache/huggingface/accelerate/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ECNT1QXLiRwP"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(\n",
        "    model, accelerator, optimizer, scheduler, output_dir, epoch, global_step\n",
        "):\n",
        "    accelerator.wait_for_everyone()\n",
        "    if accelerator.state.deepspeed_plugin is not None:\n",
        "        if accelerator.is_main_process:\n",
        "            ckpt_path = str(output_dir) + f\"/step_{global_step}.ckpt\"\n",
        "            checkpoint_state_dict = {\n",
        "                \"epoch\": epoch,\n",
        "                \"last_global_step\": global_step,\n",
        "            }\n",
        "            success = model.save_checkpoint(ckpt_path, epoch, checkpoint_state_dict)\n",
        "            accelerator.print(f\"Saved checkpoint to: {ckpt_path}: {success}\")\n",
        "    if accelerator.is_main_process:\n",
        "        unwrapped_model = accelerator.unwrap_model(model)\n",
        "        accelerator.save(save_obj, ckpt_path)\n",
        "        accelerator.print(f\"Saved checkpoint to: {ckpt_path}\")\n",
        "    else:\n",
        "        if accelerator.is_main_process:\n",
        "            unwrapped_model = accelerator.unwrap_model(model)\n",
        "            ckpt_path = str(output_dir) + f\"/step_{global_step}.ckpt\"\n",
        "            save_obj = {\n",
        "                \"model\": unwrapped_model.state_dict(),\n",
        "                \"global_step\": global_step,\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"scheduler\": scheduler.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "            }\n",
        "            accelerator.save(save_obj, ckpt_path)\n",
        "            accelerator.print(f\"Saved checkpoint to: {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SItxmDiwiRwQ"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(\n",
        "    ckpt_path,\n",
        "    accelerator,\n",
        "    model,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    global_step,\n",
        "    strict=True,\n",
        "    model_only=False,\n",
        "    resume_global_step=True,\n",
        "    **kwargs,\n",
        "):\n",
        "    if accelerator.state.deepspeed_plugin is not None:\n",
        "        _, checkpoint_state_dict = model.load_checkpoint(ckpt_path, **kwargs)\n",
        "        epoch = checkpoint_state_dict[\"epoch\"]\n",
        "        global_step = checkpoint_state_dict[\"last_global_step\"]\n",
        "\n",
        "        del checkpoint_state_dict\n",
        "        accelerator.print(f\"Loaded checkpoint {ckpt_path}\")\n",
        "        return global_step, epoch\n",
        "    else:\n",
        "        loaded_obj = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "\n",
        "        model.load_state_dict(loaded_obj[\"model\"], strict=strict)\n",
        "\n",
        "        if not model_only:\n",
        "            optimizer.load_state_dict(loaded_obj[\"optimizer\"])\n",
        "            scheduler.load_state_dict(loaded_obj[\"scheduler\"])\n",
        "            global_step = (\n",
        "                loaded_obj[\"global_step\"] if resume_global_step else global_step\n",
        "            )\n",
        "            epoch = loaded_obj[\"epoch\"]\n",
        "\n",
        "        accelerator.print(f\"Loaded checkpoint {ckpt_path}\")\n",
        "        return global_step, epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HOuTflZqiRwR"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, args):\n",
        "\n",
        "    accelerator = Accelerator(\n",
        "        log_with=args[\"train\"][\"log_with\"],\n",
        "        mixed_precision=\"bf16\"\n",
        "    )\n",
        "\n",
        "    set_seed(args[\"train\"][\"seed\"])\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.end_token_id = tokenizer.eos_token_id\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "    model.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    accelerator.print(\"Creating dataloaders...\")\n",
        "\n",
        "    train_loader, val_loader = create_dataloaders(\n",
        "        args[\"data\"][\"data_path\"],\n",
        "        tokenizer,\n",
        "        splits=[\"train\", \"val\"],\n",
        "        batch_sizes=[args[\"data\"][\"batch_size\"], args[\"data\"][\"batch_size\"]],\n",
        "        max_length=args[\"data\"][\"max_length\"],\n",
        "        all_max_length=args[\"is_tpu\"],\n",
        "    )\n",
        "\n",
        "    accelerator.print(\"Defining metrics...\")\n",
        "\n",
        "    rouge = load(\"rouge\")\n",
        "    bertscore = load(\"bertscore\")\n",
        "    bleu = load(\"bleu\")\n",
        "\n",
        "    def compute_metrics(predictions, references):\n",
        "        labels_ids = references\n",
        "        pred_ids = predictions\n",
        "        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "        label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "        result_dict = rouge.compute(predictions=pred_str, references=label_str)\n",
        "        bertscore_dict = bertscore.compute(\n",
        "            predictions=pred_str, references=label_str, lang=\"en\"\n",
        "        )\n",
        "        bleu_metric = bleu.compute(predictions=pred_str, references=label_str)[\"bleu\"]\n",
        "\n",
        "        result_dict[\"bert_precision\"] = np.mean(bertscore_dict[\"precision\"])\n",
        "        result_dict[\"bert_recall\"] = np.mean(bertscore_dict[\"recall\"])\n",
        "        result_dict[\"bert_f1\"] = np.mean(bertscore_dict[\"f1\"])\n",
        "\n",
        "        result_dict[\"bleu\"] = bleu_metric\n",
        "\n",
        "        return result_dict\n",
        "\n",
        "    def evaluate(model, val_loader, accelerator, epoch, global_step):\n",
        "        accelerator.print(\"\\nEvaluating...\")\n",
        "        losses = []\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        for batch in val_loader:\n",
        "            with torch.no_grad():\n",
        "                output = model(**batch)\n",
        "\n",
        "            loss = output.loss\n",
        "            predictions = output.logits.argmax(dim=-1)\n",
        "            all_predictions.append(accelerator.gather(predictions))\n",
        "            all_labels.append(accelerator.gather(batch[\"labels\"]))\n",
        "\n",
        "            losses.append(\n",
        "                accelerator.gather_for_metrics(loss.repeat(len(batch[\"input_ids\"])))\n",
        "            )\n",
        "\n",
        "        losses = torch.cat(losses)\n",
        "        accelerator.print(\"Concatenating predictions and labels...\")\n",
        "        all_predictions = torch.cat(all_predictions)[\n",
        "            : int(len(val_loader) * len(batch[\"input_ids\"]))\n",
        "        ]\n",
        "        all_labels = torch.cat(all_labels)[\n",
        "            : int(len(val_loader) * len(batch[\"input_ids\"]))\n",
        "        ]\n",
        "\n",
        "        eval_loss = torch.mean(losses)\n",
        "        accelerator.log({\"val_loss\": eval_loss.item()}, step=global_step)\n",
        "        eval_metric = compute_metrics(\n",
        "            predictions=all_predictions, references=all_labels\n",
        "        )\n",
        "        accelerator.print(f\"Metrics computed\\n{eval_metric}\")\n",
        "\n",
        "        accelerator.log(\n",
        "            {\n",
        "                \"bleu\": eval_metric[\"bleu\"],\n",
        "                \"bert_f1\": eval_metric[\"bert_f1\"],\n",
        "                \"rouge1\": eval_metric[\"rouge1\"],\n",
        "                \"rougeL\": eval_metric[\"rougeL\"],\n",
        "                \"epoch\": epoch,\n",
        "            },\n",
        "            step=global_step,\n",
        "        )\n",
        "        accelerator.print(\"Metrics loged\")\n",
        "\n",
        "    global_step = 0\n",
        "    n_epoches = args[\"train\"][\"n_epoches\"]\n",
        "    gradient_accumulation_steps = args[\"train\"][\"gradient_accumulation_steps\"]\n",
        "    learning_rate = args[\"train\"][\"learning_rate\"]\n",
        "    max_grad_norm = args[\"train\"][\"max_grad_norm\"]\n",
        "    resume_from_checkpoint = args[\"train\"][\"resume_from_checkpoint\"]\n",
        "    eval_every = args[\"train\"][\"eval_every\"]\n",
        "\n",
        "    starting_epoch = 0\n",
        "    max_steps = int(n_epoches * len(train_loader) // gradient_accumulation_steps)\n",
        "\n",
        "    accelerator.print(\"Define optimizer...\")\n",
        "\n",
        "    if (\n",
        "        accelerator.state.deepspeed_plugin is None\n",
        "        or \"optimizer\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n",
        "    ):\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    else:\n",
        "        optimizer = DummyOptim(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    if (\n",
        "        accelerator.state.deepspeed_plugin is None\n",
        "        or \"scheduler\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n",
        "    ):\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer=optimizer,\n",
        "            num_warmup_steps=max_steps,\n",
        "            num_training_steps=(max_steps) // gradient_accumulation_steps,\n",
        "        )\n",
        "    else:\n",
        "        scheduler = DummyScheduler(\n",
        "            optimizer,\n",
        "            total_num_steps=(max_steps) // gradient_accumulation_steps,\n",
        "            warmup_num_steps=max_steps,\n",
        "        )\n",
        "\n",
        "    if resume_from_checkpoint:\n",
        "        global_step, epoch = load_checkpoint(\n",
        "            resume_from_checkpoint,\n",
        "            accelerator,\n",
        "            model,\n",
        "            optimizer,\n",
        "            scheduler,\n",
        "            global_step,\n",
        "            **{\"load_optimizer_states\": True, \"load_lr_scheduler_states\": True},\n",
        "        )\n",
        "\n",
        "        resume_step = global_step\n",
        "        starting_epoch = global_step // len(train_loader)\n",
        "        resume_step -= starting_epoch * len(train_loader)\n",
        "\n",
        "    accelerator.print(\"Prepare model...\")\n",
        "\n",
        "    (model, optimizer, scheduler, train_loader, val_loader) = accelerator.prepare(\n",
        "        model, optimizer, scheduler, train_loader, val_loader\n",
        "    )\n",
        "\n",
        "    accelerator.print(\"Model prepared...\")\n",
        "\n",
        "    if accelerator.is_main_process:\n",
        "        accelerator.init_trackers(\n",
        "            \"CQA_RLHF\",\n",
        "            config=args,\n",
        "            init_kwargs=args[\"wandb_kwargs\"] or {},\n",
        "        )\n",
        "    progress_bar = tqdm(\n",
        "        initial=global_step,\n",
        "        total=int(max_steps),\n",
        "        disable=not accelerator.is_main_process,\n",
        "    )\n",
        "\n",
        "    for epoch in range(starting_epoch, n_epoches):\n",
        "        for step, batch in enumerate(train_loader):\n",
        "            if resume_from_checkpoint and epoch == starting_epoch:\n",
        "                if resume_step is not None and step < resume_step:\n",
        "                    global_step += 1\n",
        "                    continue\n",
        "            with accelerator.accumulate(model):\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(**batch)\n",
        "                loss = outputs.loss\n",
        "                accelerator.backward(loss)\n",
        "                if max_grad_norm and accelerator.sync_gradients:\n",
        "                    accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                if not accelerator.optimizer_step_was_skipped:\n",
        "                    scheduler.step()\n",
        "\n",
        "            accelerator.log(\n",
        "                {\n",
        "                    \"train_loss\": loss.item(),\n",
        "                    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "                    \"global_step\": global_step,\n",
        "                    \"epoch\": epoch,\n",
        "                },\n",
        "                step=global_step,\n",
        "            )\n",
        "\n",
        "            global_step += 1\n",
        "            if accelerator.is_main_process:\n",
        "                progress_bar.update(1)\n",
        "                progress_bar.set_description(f\"loss {loss.item():.4f}\")\n",
        "\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                evaluate(model, val_loader, accelerator, epoch, global_step)\n",
        "                save_checkpoint(\n",
        "                    model,\n",
        "                    accelerator,\n",
        "                    optimizer,\n",
        "                    scheduler,\n",
        "                    args[\"train\"][\"output_dir\"],\n",
        "                    epoch,\n",
        "                    global_step,\n",
        "                )\n",
        "                model.train()\n",
        "\n",
        "    save_checkpoint(\n",
        "        model,\n",
        "        accelerator,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        args[\"train\"][\"output_dir\"],\n",
        "        epoch,\n",
        "        global_step,\n",
        "    )\n",
        "    accelerator.end_training()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zXa5enJriRwT"
      },
      "outputs": [],
      "source": [
        "config_file = r'/content/trainer_config.yaml'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtl1vsx5iRwU",
        "outputId": "5d7aa38a-f675-4106-c2ae-f9444fb0d285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model freezed\n"
          ]
        }
      ],
      "source": [
        "with open(config_file, \"r\") as f:\n",
        "    config = yaml.load(f, Loader=CLoader)\n",
        "\n",
        "wandb.login(key=config['wandb_api'])\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(config['model_name'], use_cache=config['use_cache'])\n",
        "\n",
        "if config['train']['freeze']:\n",
        "    for n, p in model.named_parameters():\n",
        "        if \"transformer.h\" in n:\n",
        "            layer_num = int(n.split(\".\")[2])\n",
        "            if \"ln_\" not in n and layer_num > 0 and layer_num < 23:\n",
        "                p.requires_grad = False\n",
        "    print('Model freezed')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export TPU_NUM_DEVICES=8"
      ],
      "metadata": {
        "id": "uBjPqAAUqGTJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539,
          "referenced_widgets": [
            "df97aadaca4d4daa90350c6d82a7d1c5",
            "eeeea0147bcf43478ba24d1b69bb6a7d",
            "5a29ffdf336e4152836f6a2cc789c1b3",
            "fe6222dd4a604afd98dfff8b6d6cb780",
            "3b7182cb286d4554805a77c4c5b75d39",
            "59b5015695544cfcbf086b0015a74f50",
            "9f781ccecd374b9782d882191853414f",
            "4b41180c0f9748eea8b6fefd90637713",
            "2a37a9e5ec4041b8b7828a195b3ededf",
            "a61bb65154cb4c8583e0859f09c6570d",
            "a79d6072568149bfa6f6a93183e7fffd"
          ]
        },
        "id": "L24p7XjkiRwV",
        "outputId": "68d6c1c2-9061-4404-b539-e208e0c9597c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching a training on 8 TPU cores.\n",
            "Creating dataloaders...\n",
            "Defining metrics...\n",
            "Define optimizer...\n",
            "Prepare model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230221_113925-ndmkpy6c</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/myashka/CQA_RLHF/runs/ndmkpy6c' target=\"_blank\">tough-eon-32</a></strong> to <a href='https://wandb.ai/myashka/CQA_RLHF' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/myashka/CQA_RLHF' target=\"_blank\">https://wandb.ai/myashka/CQA_RLHF</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/myashka/CQA_RLHF/runs/ndmkpy6c' target=\"_blank\">https://wandb.ai/myashka/CQA_RLHF/runs/ndmkpy6c</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/11787 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df97aadaca4d4daa90350c6d82a7d1c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b4c153822099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnotebook_launcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/accelerate/launchers.py\u001b[0m in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mlauncher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrepareForLaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Launching a training on {num_processes} TPU cores.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fork\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0min_colab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# No need for a distributed launch otherwise as it's either CPU or one GPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0m_start_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpf_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m     return torch.multiprocessing.start_processes(\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0m_mp_start_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpf_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Wait for any process to fail or all of them to succeed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         ready = multiprocessing.connection.wait(\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "notebook_launcher(training_loop, (model, config))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pwZYbhohlW0d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "TPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "df97aadaca4d4daa90350c6d82a7d1c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eeeea0147bcf43478ba24d1b69bb6a7d",
              "IPY_MODEL_5a29ffdf336e4152836f6a2cc789c1b3",
              "IPY_MODEL_fe6222dd4a604afd98dfff8b6d6cb780"
            ],
            "layout": "IPY_MODEL_3b7182cb286d4554805a77c4c5b75d39"
          }
        },
        "eeeea0147bcf43478ba24d1b69bb6a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59b5015695544cfcbf086b0015a74f50",
            "placeholder": "​",
            "style": "IPY_MODEL_9f781ccecd374b9782d882191853414f",
            "value": "loss nan:   1%"
          }
        },
        "5a29ffdf336e4152836f6a2cc789c1b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b41180c0f9748eea8b6fefd90637713",
            "max": 11787,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a37a9e5ec4041b8b7828a195b3ededf",
            "value": 89
          }
        },
        "fe6222dd4a604afd98dfff8b6d6cb780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a61bb65154cb4c8583e0859f09c6570d",
            "placeholder": "​",
            "style": "IPY_MODEL_a79d6072568149bfa6f6a93183e7fffd",
            "value": " 89/11787 [02:40&lt;4:58:06,  1.53s/it]"
          }
        },
        "3b7182cb286d4554805a77c4c5b75d39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59b5015695544cfcbf086b0015a74f50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f781ccecd374b9782d882191853414f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b41180c0f9748eea8b6fefd90637713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a37a9e5ec4041b8b7828a195b3ededf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a61bb65154cb4c8583e0859f09c6570d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a79d6072568149bfa6f6a93183e7fffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
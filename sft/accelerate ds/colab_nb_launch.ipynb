{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/content/CQA_RLHF/sft/dataset\")\n",
    "from tqdm.auto import tqdm\n",
    "from dataset import create_dataloaders\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from accelerate.utils import set_seed, DummyOptim, DummyScheduler\n",
    "from evaluate import load\n",
    "import wandb\n",
    "import yaml\n",
    "from yaml import CLoader\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = dict(\n",
    "    model_name=\"EleutherAI/gpt-neo-125M\",\n",
    "    data=dict(\n",
    "        data_path=\"\",\n",
    "        batch_size=16,\n",
    "        max_length=512,\n",
    "    ),\n",
    "    train=dict(\n",
    "        n_epoches=3,\n",
    "        seed=42,\n",
    "        learning_rate=5e-5,\n",
    "        mixed_precision=\"bf16\",\n",
    "        freeze=True,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_grad_norm=None,\n",
    "        warmup_steps=100,\n",
    "        resume_from_checkpoint=\"\",\n",
    "        eval_every=1000,\n",
    "        output_dir=\"\",\n",
    "        log_with=\"wandb\",\n",
    "    ),\n",
    "    wandb_kwargs=dict(entity=\"myashka\", job_type=\"train\", group=\"sft\"),\n",
    "    use_cache=False,\n",
    "    is_tpu=True,\n",
    "    wandb_api=\"text\",\n",
    ")\n",
    "\n",
    "with open(\"trainer_config.yaml\", \"w\") as outfile:\n",
    "    yaml.dump(trainer_config, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model, accelerator, optimizer, scheduler, output_dir, epoch, global_step\n",
    "):\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.state.deepspeed_plugin is not None:\n",
    "        if accelerator.is_main_process:\n",
    "            ckpt_path = str(output_dir) + f\"/step_{global_step}.ckpt\"\n",
    "            checkpoint_state_dict = {\n",
    "                \"epoch\": epoch,\n",
    "                \"last_global_step\": global_step,\n",
    "            }\n",
    "            success = model.save_checkpoint(ckpt_path, epoch, checkpoint_state_dict)\n",
    "            accelerator.print(f\"Saved checkpoint to: {ckpt_path}: {success}\")\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        accelerator.save(save_obj, ckpt_path)\n",
    "        accelerator.print(f\"Saved checkpoint to: {ckpt_path}\")\n",
    "    else:\n",
    "        if accelerator.is_main_process:\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            ckpt_path = str(output_dir) + f\"/step_{global_step}.ckpt\"\n",
    "            save_obj = {\n",
    "                \"model\": unwrapped_model.state_dict(),\n",
    "                \"global_step\": global_step,\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "            }\n",
    "            accelerator.save(save_obj, ckpt_path)\n",
    "            accelerator.print(f\"Saved checkpoint to: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(\n",
    "    ckpt_path,\n",
    "    accelerator,\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    global_step,\n",
    "    strict=True,\n",
    "    model_only=False,\n",
    "    resume_global_step=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    if accelerator.state.deepspeed_plugin is not None:\n",
    "        _, checkpoint_state_dict = model.load_checkpoint(ckpt_path, **kwargs)\n",
    "        epoch = checkpoint_state_dict[\"epoch\"]\n",
    "        global_step = checkpoint_state_dict[\"last_global_step\"]\n",
    "\n",
    "        del checkpoint_state_dict\n",
    "        accelerator.print(f\"Loaded checkpoint {ckpt_path}\")\n",
    "        return global_step, epoch\n",
    "    else:\n",
    "        loaded_obj = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        model.load_state_dict(loaded_obj[\"model\"], strict=strict)\n",
    "\n",
    "        if not model_only:\n",
    "            optimizer.load_state_dict(loaded_obj[\"optimizer\"])\n",
    "            scheduler.load_state_dict(loaded_obj[\"scheduler\"])\n",
    "            global_step = (\n",
    "                loaded_obj[\"global_step\"] if resume_global_step else global_step\n",
    "            )\n",
    "            epoch = loaded_obj[\"epoch\"]\n",
    "\n",
    "        accelerator.print(f\"Loaded checkpoint {ckpt_path}\")\n",
    "        return global_step, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, args):\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=args[\"train\"][\"mixed_precision\"],\n",
    "        log_with=args[\"train\"][\"log_with\"],\n",
    "        logging_dir=args[\"train\"][\"output_dir\"],\n",
    "    )\n",
    "\n",
    "    set_seed(args[\"train\"][\"seed\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.end_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    model.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        args[\"data\"][\"data_path\"],\n",
    "        tokenizer,\n",
    "        splits=[\"train\", \"val\"],\n",
    "        batch_sizes=[args[\"data\"][\"batch_size\"], args[\"data\"][\"batch_size\"]],\n",
    "        max_length=args[\"data\"][\"max_length\"],\n",
    "        all_max_length=args[\"is_tpu\"],\n",
    "    )\n",
    "\n",
    "    rouge = load(\"rouge\")\n",
    "    bertscore = load(\"bertscore\")\n",
    "    bleu = load(\"bleu\")\n",
    "\n",
    "    def compute_metrics(predictions, references):\n",
    "        labels_ids = references\n",
    "        pred_ids = predictions\n",
    "        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "        result_dict = rouge.compute(predictions=pred_str, references=label_str)\n",
    "        bertscore_dict = bertscore.compute(\n",
    "            predictions=pred_str, references=label_str, lang=\"en\"\n",
    "        )\n",
    "        bleu_metric = bleu.compute(predictions=pred_str, references=label_str)[\"bleu\"]\n",
    "\n",
    "        result_dict[\"bert_precision\"] = np.mean(bertscore_dict[\"precision\"])\n",
    "        result_dict[\"bert_recall\"] = np.mean(bertscore_dict[\"recall\"])\n",
    "        result_dict[\"bert_f1\"] = np.mean(bertscore_dict[\"f1\"])\n",
    "\n",
    "        result_dict[\"bleu\"] = bleu_metric\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    def evaluate(model, val_loader, accelerator, epoch, global_step):\n",
    "        accelerator.print(\"\\nEvaluating...\")\n",
    "        losses = []\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in val_loader:\n",
    "            with torch.no_grad():\n",
    "                output = model(**batch)\n",
    "\n",
    "            loss = output.loss\n",
    "            predictions = output.logits.argmax(dim=-1)\n",
    "            all_predictions.append(accelerator.gather(predictions))\n",
    "            all_labels.append(accelerator.gather(batch[\"labels\"]))\n",
    "\n",
    "            losses.append(\n",
    "                accelerator.gather_for_metrics(loss.repeat(len(batch[\"input_ids\"])))\n",
    "            )\n",
    "\n",
    "        losses = torch.cat(losses)\n",
    "        accelerator.print(\"Concatenating predictions and labels...\")\n",
    "        all_predictions = torch.cat(all_predictions)[\n",
    "            : int(len(val_loader) * len(batch[\"input_ids\"]))\n",
    "        ]\n",
    "        all_labels = torch.cat(all_labels)[\n",
    "            : int(len(val_loader) * len(batch[\"input_ids\"]))\n",
    "        ]\n",
    "\n",
    "        eval_loss = torch.mean(losses)\n",
    "        accelerator.log({\"val_loss\": eval_loss.item()}, step=global_step)\n",
    "        eval_metric = compute_metrics(\n",
    "            predictions=all_predictions, references=all_labels\n",
    "        )\n",
    "        accelerator.print(f\"Metrics computed\\n{eval_metric}\")\n",
    "\n",
    "        accelerator.log(\n",
    "            {\n",
    "                \"bleu\": eval_metric[\"bleu\"],\n",
    "                \"bert_f1\": eval_metric[\"bert_f1\"],\n",
    "                \"rouge1\": eval_metric[\"rouge1\"],\n",
    "                \"rougeL\": eval_metric[\"rougeL\"],\n",
    "                \"epoch\": epoch,\n",
    "            },\n",
    "            step=global_step,\n",
    "        )\n",
    "        accelerator.print(\"Metrics loged\")\n",
    "\n",
    "    global_step = 0\n",
    "    n_epoches = args[\"train\"][\"n_epoches\"]\n",
    "    gradient_accumulation_steps = args[\"train\"][\"gradient_accumulation_steps\"]\n",
    "    learning_rate = args[\"train\"][\"learning_rate\"]\n",
    "    max_grad_norm = args[\"train\"][\"max_grad_norm\"]\n",
    "    resume_from_checkpoint = args[\"train\"][\"resume_from_checkpoint\"]\n",
    "    eval_every = args[\"train\"][\"eval_every\"]\n",
    "\n",
    "    starting_epoch = 0\n",
    "    max_steps = int(n_epoches * len(train_loader) // gradient_accumulation_steps)\n",
    "\n",
    "    if (\n",
    "        accelerator.state.deepspeed_plugin is None\n",
    "        or \"optimizer\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n",
    "    ):\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = DummyOptim(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if (\n",
    "        accelerator.state.deepspeed_plugin is None\n",
    "        or \"scheduler\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n",
    "    ):\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=max_steps,\n",
    "            num_training_steps=(max_steps) // gradient_accumulation_steps,\n",
    "        )\n",
    "    else:\n",
    "        scheduler = DummyScheduler(\n",
    "            optimizer,\n",
    "            total_num_steps=(max_steps) // gradient_accumulation_steps,\n",
    "            warmup_num_steps=max_steps,\n",
    "        )\n",
    "\n",
    "    if resume_from_checkpoint:\n",
    "        global_step, epoch = load_checkpoint(\n",
    "            resume_from_checkpoint,\n",
    "            accelerator,\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            global_step,\n",
    "            **{\"load_optimizer_states\": True, \"load_lr_scheduler_states\": True},\n",
    "        )\n",
    "\n",
    "        resume_step = global_step\n",
    "        starting_epoch = global_step // len(train_loader)\n",
    "        resume_step -= starting_epoch * len(train_loader)\n",
    "\n",
    "        (model, optimizer, scheduler, train_loader, val_loader) = accelerator.prepare(\n",
    "            model, optimizer, scheduler, train_loader, val_loader\n",
    "        )\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            accelerator.init_trackers(\n",
    "                \"CQA_RLHF\",\n",
    "                config=args,\n",
    "                init_kwargs=args[\"wandb_kwargs\"] or {},\n",
    "            )\n",
    "        progress_bar = tqdm(\n",
    "            initial=global_step,\n",
    "            total=int(max_steps),\n",
    "            disable=not accelerator.is_main_process,\n",
    "        )\n",
    "\n",
    "        for epoch in range(starting_epoch, n_epoches):\n",
    "            for step, batch in enumerate(train_loader):\n",
    "                if resume_from_checkpoint and epoch == starting_epoch:\n",
    "                    if resume_step is not None and step < resume_step:\n",
    "                        global_step += 1\n",
    "                        continue\n",
    "                with accelerator.accumulate(model):\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "                    accelerator.backward(loss)\n",
    "                    if max_grad_norm and accelerator.sync_gradients:\n",
    "                        accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    if not accelerator.optimizer_step_was_skipped:\n",
    "                        scheduler.step()\n",
    "\n",
    "                accelerator.log(\n",
    "                    {\n",
    "                        \"train_loss\": loss.item(),\n",
    "                        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                        \"global_step\": global_step,\n",
    "                        \"epoch\": epoch,\n",
    "                    },\n",
    "                    step=global_step,\n",
    "                )\n",
    "\n",
    "                global_step += 1\n",
    "                if accelerator.is_main_process:\n",
    "                    progress_bar.update(1)\n",
    "                    progress_bar.set_description(f\"loss {loss.item():.4f}\")\n",
    "\n",
    "                if global_step % eval_every == 0:\n",
    "                    model.eval()\n",
    "                    evaluate(model, val_loader, accelerator, epoch, global_step)\n",
    "                    save_checkpoint(\n",
    "                        model,\n",
    "                        accelerator,\n",
    "                        optimizer,\n",
    "                        scheduler,\n",
    "                        args[\"train\"][\"output_dir\"],\n",
    "                        epoch,\n",
    "                        global_step,\n",
    "                    )\n",
    "                    model.train()\n",
    "\n",
    "        save_checkpoint(\n",
    "            model,\n",
    "            accelerator,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            args[\"train\"][\"output_dir\"],\n",
    "            epoch,\n",
    "            global_step,\n",
    "        )\n",
    "        accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = r''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file, \"r\") as f:\n",
    "    config = yaml.load(f, Loader=CLoader)\n",
    "\n",
    "wandb.login(key=config['wandb_api'])\n",
    "\n",
    "model = AutoModelForCausalLM(config['model_name'], use_cache=config['use_cache'])\n",
    "\n",
    "if config['train']['freeze']:\n",
    "    for n, p in model.named_parameters():\n",
    "        if \"transformer.h\" in n:\n",
    "            layer_num = int(n.split(\".\")[2])\n",
    "            if \"ln_\" not in n and layer_num > 0 and layer_num < 23:\n",
    "                p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(training_loop, (model, config))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

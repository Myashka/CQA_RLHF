data:
  batch_size: 1
  data_dir: /kaggle/input/python-qa/1.0-data-div-ans-sep.json
  max_length: 1024
  pad_for_tpu: true
model_name: EleutherAI/gpt-neo-1.3B
model_params:
  adam_betas:
    - 0.9
    - 0.95
  do_compute_bertscore: false
  do_compute_metrics: false
  do_freeze: false
  learning_rate: 2.0e-05
  use_cache: false
  warmup_steps: 100
  weight_decay: 0.001
seed: 42
trainer:
  checkpoint:
    dirpath: /kaggle/working/Checkpoints
    every_n_train_steps: 1000
  ckpt_path: null
  params:
    accelerator: tpu
    accumulate_grad_batches: 1
    auto_scale_batch_size: false
    gradient_clip_val: 1
    log_every_n_steps: 20
    max_epochs: 3
    num_sanity_val_steps: 2
    overfit_batches: 0
    precision: "16"
    strategy: deepspeed_stage_2
    val_check_interval: 500
    nr_frozen_epochs: 2
wandb:
  api: text
  args:
    group: sft
    job_type: train
    name: 1.2B-freezed-tpu-kaggle
  project_name: CQA_RLHF

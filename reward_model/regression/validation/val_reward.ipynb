{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from torchmetrics import SacreBLEUScore\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import gc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = ROUGEScore()\n",
    "bleu = SacreBLEUScore()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Myashka/125M_GPTneo_reward_gen\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"Myashka/125M_GPTneo_reward_gen\").to(device)\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\"Myashka/125M_GPTneo_sft_tuned\").to(device)\n",
    "\n",
    "reward_model = reward_model.eval()\n",
    "sft_model = sft_model.eval()\n",
    "\n",
    "reward_model = torch.compile(reward_model)\n",
    "sft_model = torch.compile(sft_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {'data_file_path': '/content/1.0-data-div-ans-sep-api-usage.json',\n",
    "               \"padding\": False,\n",
    "               \"max_length_promt\": 256,\n",
    "               \"truncate_promt\": True,\n",
    "               }\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 1,\n",
    "    'num_return_sequences': 10,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 256,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    tokenizer,\n",
    "    data_config,\n",
    "    splits,\n",
    "):\n",
    "\n",
    "    def promt_tokenize(examples):\n",
    "        if data_config['truncate_promt']:\n",
    "            q_toks = tokenizer.encode(examples['Question'])\n",
    "            q_toks = q_toks[:data_config['max_length_promt']-7]\n",
    "            tmp = tokenizer.decode(q_toks).strip()\n",
    "        else:\n",
    "            tmp = examples['Question']\n",
    "\n",
    "        sample = 'Question: ' + tmp + \"\\nAnswer:\"\n",
    "\n",
    "        tokenized_dict = tokenizer(\n",
    "            [sample], padding=data_config['padding'], max_length=data_config['max_length_promt'], truncation=True)\n",
    "        \n",
    "        tokenized_dict['Question_promt'] = sample\n",
    "        tokenized_dict['Original_answer'] = examples['Answer']\n",
    "\n",
    "        return tokenized_dict\n",
    "\n",
    "    datasets = []\n",
    "    for split in splits:\n",
    "        dataset = load_dataset(\n",
    "            \"json\", data_files=f\"{data_config['data_file_path']}\", field=f'{split}')['train']\n",
    "        dataset = dataset.map(promt_tokenize)\n",
    "        dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"Question_promt\", 'Original_answer'])\n",
    "        datasets.append(dataset)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = build_dataset(tokenizer, data_config, 'val')[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate samples to eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 50,\n",
    "    'num_return_sequences': 10,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = defaultdict(list)\n",
    "for i, sample in tqdm(enumerate(val_dataset)):\n",
    "    generated_samples = sft_model.generate(sample[\"input_ids\"].to(device), **generation_kwargs)\n",
    "\n",
    "    val_dict['Question'].extend([sample['Question_promt']]*len(generated_samples))\n",
    "    val_dict['Answer_orig'].extend([sample['Original_answer']]*len(generated_samples))\n",
    "    val_dict['Q_Id'].extend([i]*len(generated_samples))\n",
    "\n",
    "    val_dict[\"Answer_gen\"].extend([tokenizer.decode(r.squeeze()[len(query_idx):], skip_special_tokens=True) for r, query_idx in zip(generated_samples, sample[\"input_ids\"].repeat(10, 1))])\n",
    "\n",
    "    del sample\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rouge1, val_rouge2, val_rougeL, val_bleu = [], [], [], []\n",
    "val_rewards = []\n",
    "\n",
    "for i in range(len(val_dict[\"Question\"])):\n",
    "    generated_answer = val_dict[\"Answer_gen\"][i]\n",
    "    original_answer = val_dict[\"Answer_orig\"][i]\n",
    "    \n",
    "    # calculate Rouge and BLEU scores\n",
    "    scores = rouge.compute(predictions=generated_answer, references=original_answer)\n",
    "    val_rouge1.append(scores['rouge1'].item())\n",
    "    val_rouge2.append(scores['rouge2'].item())\n",
    "    val_rougeL.append(scores['rougeL'].item())\n",
    "    val_bleu.append(bleu.compute(predictions=[generated_answer], references=[[original_answer]]).item())\n",
    "\n",
    "    # calculate reward score\n",
    "    reward = reward_model(original_answer, generated_answer)\n",
    "    val_rewards.append(reward)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

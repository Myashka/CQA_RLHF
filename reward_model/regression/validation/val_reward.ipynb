{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from torchmetrics import SacreBLEUScore\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import gc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = ROUGEScore()\n",
    "bleu = SacreBLEUScore(1, lowercase=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Myashka/125M_GPTneo_reward_gen\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"Myashka/125M_GPTneo_reward_gen\").to(device)\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\"Myashka/125M_GPTneo_sft_tuned\").to(device)\n",
    "\n",
    "reward_model = reward_model.eval()\n",
    "sft_model = sft_model.eval()\n",
    "\n",
    "reward_model = torch.compile(reward_model)\n",
    "sft_model = torch.compile(sft_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {'data_file_path': '/content/1.0-data-div-ans-sep-api-usage.json',\n",
    "               \"padding\": False,\n",
    "               \"max_length_promt\": 256,\n",
    "               \"truncate_promt\": True,\n",
    "               }\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 1,\n",
    "    'num_return_sequences': 10,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 256,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    tokenizer,\n",
    "    data_config,\n",
    "    splits,\n",
    "):\n",
    "\n",
    "    def promt_tokenize(examples):\n",
    "        if data_config['truncate_promt']:\n",
    "            q_toks = tokenizer.encode(examples['Question'])\n",
    "            q_toks = q_toks[:data_config['max_length_promt']-7]\n",
    "            tmp = tokenizer.decode(q_toks).strip()\n",
    "        else:\n",
    "            tmp = examples['Question']\n",
    "\n",
    "        sample = 'Question: ' + tmp + \"\\nAnswer:\"\n",
    "\n",
    "        tokenized_dict = tokenizer(\n",
    "            [sample], padding=data_config['padding'], max_length=data_config['max_length_promt'], truncation=True)\n",
    "        \n",
    "        tokenized_dict['Question_promt'] = sample\n",
    "        tokenized_dict['Original_answer'] = examples['Answer']\n",
    "\n",
    "        return tokenized_dict\n",
    "\n",
    "    datasets = []\n",
    "    for split in splits:\n",
    "        dataset = load_dataset(\n",
    "            \"json\", data_files=f\"{data_config['data_file_path']}\", field=f'{split}')['train']\n",
    "        dataset = dataset.map(promt_tokenize)\n",
    "        dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"Question_promt\", 'Original_answer'])\n",
    "        datasets.append(dataset)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = build_dataset(tokenizer, data_config, 'val')[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate samples to eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 50,\n",
    "    'num_return_sequences': 10,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = defaultdict(list)\n",
    "for i, sample in tqdm(enumerate(val_dataset)):\n",
    "    generated_samples = sft_model.generate(sample[\"input_ids\"].to(device), **generation_kwargs)\n",
    "\n",
    "    val_dict['Question'].extend([sample['Question_promt']]*len(generated_samples))\n",
    "    val_dict['Answer_orig'].extend([sample['Original_answer']]*len(generated_samples))\n",
    "    val_dict['Q_Id'].extend([i]*len(generated_samples))\n",
    "\n",
    "    val_dict[\"Answer_gen\"].extend([tokenizer.decode(r.squeeze()[len(query_idx):], skip_special_tokens=True) for r, query_idx in zip(generated_samples, sample[\"input_ids\"].repeat(10, 1))])\n",
    "\n",
    "    del sample\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('validation_results.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rouge1, val_rouge2, val_bleu = [], [], []\n",
    "\n",
    "for i in tqdm(range(len(result_df))):\n",
    "    generated_answer = result_df[\"Answer_gen\"][i]\n",
    "    original_answer = result_df[\"Answer_orig\"][i]\n",
    "    \n",
    "    # calculate Rouge and BLEU scores\n",
    "    try:\n",
    "        rouge_score = rouge(generated_answer, original_answer)\n",
    "        val_rouge1.append(rouge_score['rouge1_fmeasure'].item())\n",
    "        val_rouge2.append(rouge_score['rouge2_fmeasure'].item())\n",
    "        val_bleu.append(bleu(generated_answer, original_answer).item())\n",
    "    except:\n",
    "      val_rouge1.append(0)\n",
    "      val_rouge2.append(0)\n",
    "      val_bleu.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['rouge_1'] = val_rouge1\n",
    "result_df['rouge_2'] = val_rouge2\n",
    "result_df['bleu'] = val_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('validation_results.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/validation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    q = row['Question']\n",
    "    a = row['Answer_gen']\n",
    "    try:\n",
    "        text.append(q+a)\n",
    "    except:\n",
    "        text.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(0, len(text), batch_size)):\n",
    "    batch = text[i:i+batch_size]\n",
    "    \n",
    "    inputs = tokenizer(batch, padding='longest', return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = reward_model(**inputs).logits\n",
    "    \n",
    "    batch_predictions = outputs.detach().cpu()\n",
    "    predictions.append(batch_predictions)\n",
    "\n",
    "    del batch_predictions\n",
    "    del inputs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "predictions = torch.cat(predictions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [predictions.numpy()[i][0] for i in range(len(predictions))]\n",
    "df['rewards'] = rewards\n",
    "df['metric'] = df[['rouge_1', 'rouge_2', 'bleu']].mean(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Spearman and Kendall correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, kendalltau\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\CQA_RLHF\\reward_model\\regression\\validation\\validation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = df.groupby('Q_Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank1 = grouped_data['metric'].rank(method='min', ascending=False)\n",
    "rank2 = grouped_data['rewards'].rank(method='min', ascending=False)\n",
    "\n",
    "corr_spearman, _ = spearmanr(rank1, rank2)\n",
    "\n",
    "corr_kendall, _ = kendalltau(rank1, rank2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.09\n",
      "Kendall correlation: 0.07\n"
     ]
    }
   ],
   "source": [
    "print(f'Spearman correlation: {round(corr_spearman, 2)}\\nKendall correlation: {round(corr_kendall, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

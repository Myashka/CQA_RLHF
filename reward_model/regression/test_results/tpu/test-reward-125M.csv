,qa_pair,target_score,pred_score,mse
0,"Question\nI'm using Imageio, the python library that wraps around ffmpeg to do hardware encoding via nvenc. My issue is that I can't get more than 2 sessions to launch (I am using non-quadro GPUs). Even using multiple GPUs. I looked over NVIDIA's support matrix and they state only 2 sessions per gpu, but it seems to be per system.
For example I have 2 GPUs in a system. I can either use the env variable CUDA_VISIBLE_DEVICES or set the ffmpeg flag -gpu to select the GPU. I've verified gpu usage using Nvidia-smi cli. I can get 2 encoding sessions working on a single gpu. Or 1 session working on 2 separate gpus each. But I can't get 2 encoding sessions working on 2 gpus. 
Even more strangely if I add more gpus I am still stuck at 2 sessions. I can't launch a third encoding session on a 3rd gpu. I am always stuck at 2 regardless of the # of gpus. Any ideas on how to fix this?\nAnswer: Nvidia limits it 2 per system Not 2 per GPU. The limitation is in the driver, not the hardware. There have been unofficially drivers posted to github which remove the limitation",0.81632656,0.10868138,0.5007616877555847
1,"Question\nThe Divio Django CMS offers two servers: TEST and LIVE. Are these also two separate repositories? Or how is this done in the background?
I'm wondering because I would have the feeling the LIVE server is its own repository that just pulls from the TEST whenever I press deploy. Is that correct?\nAnswer: All Divio projects (django CMS, Python, PHP, whatever) have a Live and Test environment.
By default, both build the project from its repository's master branch (in older projects, develop). 
On request, custom tracking branches can be enabled, so that the Live and Test environments will build from separate branches.
When a build successfully completes, the Docker image can be reused until changes are made to the project's repository. This means that after a successful deployment on Test, the Docker image doesn't need to be rebuilt, and the Live environment can be deployed much faster from the pre-built image. (Obviously this is only possible when they are on the same branch.)",0.40816328,0.49761623,0.00800183042883873
2,"Question\nfor my current requirement, I'm having a dataset of 10k+ faces from 100 different people from which I have trained a model for recognizing the face(s). The model was trained by getting the 128 vectors from the facenet_keras.h5 model and feeding those vector value to the Dense layer for classifying the faces.
But the issue I'm facing currently is

if want to train one person face, I have to retrain the whole model once again.

How should I get on with this challenge? I have read about a concept called transfer learning but I have no clues about how to implement it. Please give your suggestion on this issue. What can be the possible solutions to it?\nAnswer: With transfer learning you would copy an existing pre-trained model and use it for a different, but similar, dataset from the original one. In your case this would be what you need to do if you want to train the model to recognize your specific 100 people.
If you already did this and you want to add another person to the database without having to retrain the complete model, then I would freeze all layers (set layer.trainable = False for all layers) except for the final fully-connected layer (or the final few layers). Then I would replace the last layer (which had 100 nodes) to a layer with 101 nodes. You could even copy the weights to the first 100 nodes and maybe freeze those too (I'm not sure if this is possible in Keras). In this case you would re-use all the trained convolutional layers etc. and teach the model to recognise this new face.",0.20408164,0.15240848,0.002670115325599909
3,"Question\nI'm using Python and Flask, served by Waitress, to host a POST API. I'm calling the API from a C# program that posts data and gets a string response. At least 95% of the time, it works fine, but sometimes the C# program reports an error: 
(500) Internal Server Error.
There is no further description of the error or why it occurs. The only clue is that it usually happens in clusters -- when the error occurs once, it likely occurs several times in a row. Without any intervention, it then goes back to running normally.
Since the error is so rare, it is hard to troubleshoot. Any ideas as to how to debug or get more information? Is there error handling I can do from either the C# side or the Flask/Waitress side?\nAnswer: Your flask application should be logging the exception when it occurs. Aside from combing through your logs (which should be stored somewhere centrally) you could consider something like Sentry.io, which is pretty easy to setup with Flask apps.",0.0,0.24021125,0.05770144239068031
4,"Question\nI`m new to python. I have a csv file. I need to check whether the inputs are correct or not. The ode should scan through each rows. 
All columns for a particular row should contain values of same type: Eg:
All columns of second row should contain only string, 
All columns of third row should contain only numbers... etc
I tried the following approach, (it may seem blunder):
I have only 15 rows, but no idea on number of columns(Its user choice)
df.iloc[1].str.isalpha()
This checks  for string. I don`t know how to check??\nAnswer: Simple approach that can be modified:

Open df using df = pandas.from_csv(<path_to_csv>)
For each column, use df['<column_name>'] = df['<column_name>'].astype(str) (str = string, int = integer, float = float64,..etc).

You can check column types using df.dtypes",0.40816328,0.20955849,0.039443861693143845
5,"Question\nI cannot upgrade pip on my Mac from the Terminal. 
According to the documentation I have to type the command:
pip install -U pip
I get the error message in the Terminal:
pip: command not found
I have Mac OS 10.14.2, python 3.7.2 and pip 18.1.
I want to upgrade to pip 19.2.3\nAnswer: I came on here to figure out the same thing but none of this things seemed to work. so I went back and looked how they were telling me to upgrade it but I still did not get it. So I just started trying things and next thing you know I seen the downloading lines and it told me that my pip was upgraded. what I used was (pip3 install -- upgrade pip). I hope this can help anyone else in need.",0.0,0.15833426,0.025069735944271088
6,"Question\nI cannot upgrade pip on my Mac from the Terminal. 
According to the documentation I have to type the command:
pip install -U pip
I get the error message in the Terminal:
pip: command not found
I have Mac OS 10.14.2, python 3.7.2 and pip 18.1.
I want to upgrade to pip 19.2.3\nAnswer: pip3 install --upgrade pip

this works for me!",0.45351472,0.14844418,0.09306804090738297
7,"Question\nI cannot upgrade pip on my Mac from the Terminal. 
According to the documentation I have to type the command:
pip install -U pip
I get the error message in the Terminal:
pip: command not found
I have Mac OS 10.14.2, python 3.7.2 and pip 18.1.
I want to upgrade to pip 19.2.3\nAnswer: I have found an answer that worked for me:
sudo pip3 install -U pip --ignore-installed pip
This installed pip version 19.2.3 correctly.
It was very hard to find the correct command on the internet...glad I can share it now.
Thanks.",0.13605443,0.14145315,2.9146183806005865e-05
8,"Question\nI have two data from.
df1 with columns: id,x1,x2,x3,x4,....xn
df2 with columns: id,y.
df3 =pd.concat([df1,df2],axis=1)
when I use pandas concat to combine them, it became
id,y,id,x1,x2,x3...xn.
there are two id here.How can I get rid of one.
I have tried :
df3=pd.concat([df1,df2],axis=1).drop_duplicates().reset_index(drop=True).
but not work.\nAnswer: drop_duplicates() only removes rows that are completely identical.
what you're looking for is pd.merge().
pd.merge(df1, df2, on='id)",0.0,0.2865491,0.08211038261651993
9,"Question\nI'm trying to make a classifier for uncertain data (e.g ranged data) using python. in certain dataset, the list is a 2D array or array of record (contains float numbers for data and a string for labels), where in uncertain dataset the list is a 3D array (contains range of float numbers for data and a string for labels). i managed to manipulate a certain dataset to be uncertain using uniform probability distribution. A research paper says that i have to use supremum distance metric. how do i implement this metric in python? note that in uncertain dataset, both test set and training set is uncertain\nAnswer: I found out using scipy spatial distance and tweaking for-loops in standard knn helps a lot",0.0,0.18438119,0.033996421843767166
10,"Question\nI am experiencing performance issues in my pipeline in a DoFn that uses large side input of ~ 1GB. The side input is passed using the pvalue.AsList(), which forces materialization of the side input.
The execution graph of the pipeline shows that the particular step spends most of the time for reading the side input. The total amount of data read exceeds the size of the side input by far. Consequently, I conclude that the side input does not fit into memory / cache of the workers even though their RAM is sufficient (using n1-highmem4 workers with 26 GB RAM).
How do I know how big this cache actually is? Is there a way to control its size using Beam Python SDK 2.15.0 (like there was the pipeline option --workerCacheMb=200 for Java 1.x SDK)?
There is no easy way of shrinking my side input more than 10%.\nAnswer: If you are using AsList, you are correct that the whole side input should be loaded into memory. It may be that your worker has enough memory available, but it just takes very long to read 1GB of data into the list. Also, the size of the data that is read depends on the encoding of it. If you can share more details about your algorithm, we can try to figure out how to write a pipeline that may run more efficiently.

Another option may be to have an external service to keep your side input - for instance, a Redis instance that you write to on one side, and red from on the other side.",0.0,0.17447048,0.03043995052576065
11,"Question\nI recently installed Anaconda in my Windows. I did that to use some packages from some specific channels required by an application that is using Python 3.5 as its scripting language.
I adjusted my PATH variable to use Conda, pointing to the Python environment of the particular program, but now I would like to use Conda as well for a different Python installation that I have on my Windows.
When installing Anaconda then it isn't asking for a Python version to be related to. So, how can I use Conda to install into the other Python installation. Both Python installations are 'physical' installations - not virtual in any way.\nAnswer: Uninstall the other python installation and create different conda environments, that is what conda is great at. 
Using conda from your anaconda installation to manage packages from another, independent python installation is not possible and not very feasible.
Something like this could serve your needs:

Create one env for python 3.5 conda create -n py35 python=3.5
Create one env for some other python version you would like to use, e.g. 3.6: conda create -n py36 python=3.6
Use conda activate py35, conda deactivate, conda activate py36 to switch between your virtual environments.",0.40816328,0.29421878,0.012983349151909351
12,"Question\nI been learning how to use Apache-Airflow the last couple of months and wanted to see if anybody has any experience with transferring CSV files from S3 to a Mysql database in AWS(RDS). Or from my Local drive to MySQL.
I managed to send everything to an S3 bucket to store them in the cloud using airflow.hooks.S3_hook and it works great. I used boto3 to do this.
Now I want to push this file to a MySQL database I created in RDS, but I have no idea how to do it. Do I need to use the MySQL hook and add my credentials there and then write a python function?
Also, It doesn't have to be S3 to Mysql, I can also try from my local drive to Mysql if it's easier.
Any help would be amazing!\nAnswer: were you able to resolve the 'MySQLdb._exceptions.OperationalError: (2068, 'LOAD DATA LOCAL INFILE file request rejected due to restrictions on access' issue",0.0,-0.18351316,0.033677082508802414
13,"Question\nI have a dataset with the first column as date in the format: 2011-01-01 and type(data_raw['pandas_date']) gives me pandas.core.series.Series
I want to convert the whole column into date time object so I can extract and process year/month/day from each row as required.
I used pd.to_datetime(data_raw['pandas_date']) and it printed output with dtype: datetime64[ns] in the last line of the output. I assume that values were converted to datetime.
but when I run type(data_raw['pandas_date']) again, it still says pandas.core.series.Series and anytime I try to run.dt function on it, it gives me an error saying this is not a datetime object.
So, my question is - it looks like to_datetime function changed my data into datetime object, but how to I apply/save it to the pandas_date column? I tried 
data_raw['pandas_date'] = pd.to_datetime(data_raw['pandas_date'])
but this doesn't work either, I get the same result when I check the type. Sorry if this is too basic.\nAnswer: type(data_raw['pandas_date']) will always return pandas.core.series.Series, because the object data_raw['pandas_date'] is of type pandas.core.series.Series. What you want is to get the dtype, so you could just do data_raw['pandas_date'].dtype.

data_raw['pandas_date'] = pd.to_datetime(data_raw['pandas_date'])

This is correct, and if you do data_raw['pandas_date'].dtype again afterwards, you will see that it is datetime[64].",0.0,0.53565764,0.28692910075187683
14,"Question\nYou have a 2005 Honda Accord with 50 miles (weight max) left in the tank. Which McDonalds locations (graph nodes) can you visit within a 50 mile radius? This is my question.  
If you have a weighted directed acyclic graph, how can you find all the nodes that can be visited within a given weight restriction? 
I am aware of Dijkstra's algorithm but I can't seem to find any documentation of its uses outside of min-path problems. In my example, theres no node in particular that we want to end at, we just want to go as far as we can without going over the maximum weight. It seems like you should be able to use BFS/DFS in order to solve this, but I cant find documentation for implementing those in graphs with edge weights (again, outside of min-path problems).\nAnswer: Finding the longest path to a vertex V (a McDonald's in this case) can be accomplished using topological sort. We can start by sorting our nodes topologically, since sorting topologically will always return the source node U, before the endpoint, V, of a weighted path. Then, since we would now have access to an array in which each source vertex precedes all of its adjacent vertices, we can search through every path beginning with vertex U and ending with vertex V and set a value in an array with an index corresponding to U to the maximum edge weight we find connecting U to V. If the sum of the maximal distances exceeds 50 without reaching a McDonalds, we can backtrack and explore the second highest weight path going from U to V, and continue backtracking should we exhaust every path exiting from vertex U. Eventually we will arrive at a McDonalds, which will be the McDonalds with the maximal distance from our original source node while maintaining a total spanning distance under 50.",0.0,0.3144107,0.09885407984256744
15,"Question\nYou have a 2005 Honda Accord with 50 miles (weight max) left in the tank. Which McDonalds locations (graph nodes) can you visit within a 50 mile radius? This is my question.  
If you have a weighted directed acyclic graph, how can you find all the nodes that can be visited within a given weight restriction? 
I am aware of Dijkstra's algorithm but I can't seem to find any documentation of its uses outside of min-path problems. In my example, theres no node in particular that we want to end at, we just want to go as far as we can without going over the maximum weight. It seems like you should be able to use BFS/DFS in order to solve this, but I cant find documentation for implementing those in graphs with edge weights (again, outside of min-path problems).\nAnswer: For this problem, you will want to run a DFS from the starting node. Recurse down the graph from each child of the starting node until a total weight of over 50 is reached. If a McDonalds is encountered along the traversal record the node reached in a list or set. By doing so, you will achieve the most efficient algorithm possible as you will not have to create a complete topological sort as the other answer to this question proposes. Even though this algorithm still technically runs in O(ElogV) time, by recursing back on the DFS when a path distance of over 50 is reached you avoid traversing through the entire graph when not necessary.",0.0,0.30060303,0.09036218374967575
16,"Question\nI'm starting to use Qt Designer.
I am trying to create a game, and the first task that I want to do is to create a window where you have to input the name of the map that you want to load. If the map exists, I then switch to the main game window, and if the name of the map doesn't exist, I want to display a popup window that tells the user that the name of the map they wrote is not valid. 
I'm a bit confused with the part of showing the ""not valid"" pop-up window.
I realized that I have two options:

Creating 2 separated.ui files, and with the help of the.show() and.hide() commands show the correspoding window if the user input is invalid. 
The other option that I'm thinking of creating both windows in the same.ui file, which seems to be a better option, but I don't really know how to work with windows that come from the same file. Should I create a separate class for each of the windows that come from the Qt Designer file? If not, how can I access both windows from the same class?\nAnswer: Your second option seems impossible, it would be great to share the.ui since in my years that I have worked with Qt Designer I have not been able to implement what you point out.
An.ui is an XML file that describes the elements and their properties that will be used to create a class that is used to fill a particular widget. So considering the above, your second option is impossible.
This concludes that the only viable option is its first method.",0.0,0.2601726,0.06768978387117386
17,"Question\nSo, this is for my assignment and I have to create a flight booking system. One of the requirements is that it should create 3 digit passenger code that does not start with zeros (e.g. 100 is the smallest acceptable value) and I have no idea how I can do it since I am a beginner and I just started to learn Python. I have made classes for Passenger, Flight, Seating Area so far because I just started on it today. Please help. Thank you.\nAnswer: I like list comprehension for making a list of 100 to 999:
flights = [i for i in range(100, 1000)]
For the random version, there is probably a better way, but Random.randint(x, y) creates a random in, inclusive of the endpoints:
from random import Random
rand = Random()
flight = rand.randint(100,999)
Hope this helps with your homework, but do try to understand the assignment and how the code works...lest you get wrecked on the final!",0.0,0.27107045,0.07347919046878815
18,"Question\nI want to select all values bigger than 8000 within a pandas dataframe. 
new_df = df.loc[df['GM'] > 8000]
However, it is not working. I think the problem is, that the value comes from an Excel file and the number is interpreted as string e.g. ""1.111,52"". Do you know how I can convert such a string to float / int in order to compare it properly?\nAnswer: You can see value of df.dtypes to see what is the type of each column. Then, if the column type is not as you want to, you can change it by df['GM'].astype(float), and then new_df = df.loc[df['GM'].astype(float) > 8000] should work as you want to.",0.20408164,0.28986144,0.007358174305409193
19,"Question\nI have a  caltech101 dataset for object detection. Can we detect multiple objects in single image using model trained on caltech101 dataset?
This dataset contains only folders (label-wise) and in each folder, some images label wise.
I have trained model on caltech101 dataset using keras and it predicts single object in image. Results are satisfactory but is it possible to detect multiple objects in single image?
As I know some how regarding this. for detecting multiple objects in single image, we should have dataset containing images and bounding boxes with name of objects in images.
Thanks in advance\nAnswer: The dataset can be used for detecting multiple objects but with below steps to be followed:

The dataset has to be annotated with bounding boxes on the object present in the image
After the annotations are done, you can use any of the Object detectors to do transfer learning and train on the annotated caltech 101 dataset

Note: - Without annotations, with just the caltech 101 dataset, detecting multiple objects in a single image is not possible",0.0,0.0048810244,2.3824399249861017e-05
20,"Question\nI am writing a serial data logger in Python and am wondering which data type would be best suited for this. Every few milliseconds a new value is read from the serial interface and is saved into my variable along with the current time. I don't know how long the logger is going to run, so I can't preallocate for a known size.
Intuitively I would use an numpy array for this, but appending / concatenating elements creates a new array each time from what I've read.
So what would be the appropriate data type to use for this?
Also, what would be the proper vocabulary to describe this problem?\nAnswer: Python doesn't have arrays as you think of them in most languages. It has ""lists"", which use the standard array syntax myList[0] but unlike arrays, lists can change size as needed. using myList.append(newItem) you can add more data to the list without any trouble on your part.
Since you asked for proper vocabulary in a useful concept to you would be ""linked lists"" which is a way of implementing array like things with varying lengths in other languages.",0.0,0.30248046,0.09149442613124847
21,Question\nOnce you commit in pycharm it takes you to a second window to go through with the push. But if you only hit commit and not commit/push then how do you bring up the push option. You can't do another commit unless changes are made.\nAnswer: In the upper menu [VCS] -> [Git...] -> [Push],0.81632656,0.39607072,0.17661496996879578
22,"Question\nTrying to run the python-telegram-bot library through Jupyter Notebook I get this question error. I tried many ways to reinstall it, but nothing from answers at any forums helped me. What should be a mistake and how to avoid it while installing?\nAnswer: Do you have a directory with ""telegram"" name? If you do,rename your directory and try it again to prevent import conflict.
good luck:)",0.40816328,0.27916187,0.016641363501548767
23,"Question\nI am writing a slack bot, and I am using argsparse to parse the arguments sent into the slackbot, but I am trying to figure out how to get the help message string so I can send it back to the user via the slack bot. 
I know that ArgumentParser has a print_help() method, but that is printed via console and I need a way to get that string.\nAnswer: I just found out that there's a method called format_help() that generates that help string",0.40816328,0.22686064,0.03287064656615257
24,"Question\nThere will be an unordered_map in c++ dll containing some'vectors' mapped to its 'names'. For each of these 'names', the python code will keep on collecting data from a web server every 5 seconds and fill the vectors with it.
Is such a dll possible? If so, how to do it?\nAnswer: You can make the Python code into an executable. Run the executable file from the DLL as a separate process and communicate with it via TCP localhost socket - or some other Windows utility that allows to share data between different processes.
That's a slow mess. I agree, but it works.
You can also embed Python interpreter and run the script it on the dll... I suppose.",0.0,0.2067895,0.042761895805597305
25,"Question\nI'm having trouble connecting the mathematical concept of spline interpolation with the application of a spline filter in python. My very basic understanding of spline interpolation is that it's fitting the data in a piece-wise fashion, and the piece-wise polynomials fitted are called splines. But its applications in image processing involve pre-filtering the image and then performing interpolation, which I'm having trouble understanding.
To give an example, I want to interpolate an image using scipy.ndimage.map_coordinates(input, coordinates, prefilter=True), and the keyword prefilter according to the documentation:

Determines if the input array is prefiltered with spline_filter before interpolation 

And the documentation for scipy.ndimage.interpolation.spline_filter simply says the input is filtered by a spline filter. So what exactly is a spline filter and how does it alter the input data to allow spline interpolation?\nAnswer: I'm guessing a bit here. In order to calculate a 2nd order spline, you need the 1st derivative of the data. To calculate a 3rd order spline, you need the second derivative. I've not implemented an interpolation motor beyond 3rd order, but I suppose the 4th and 5th order splines will require at least the 3rd and 4th derivatives.
Rather than recalculating these derivatives every time you want to perform an interpolation, it is best to calculate them just once. My guess is that spline_filter is doing this pre-calculation of the derivatives which then get used later for the interpolation calculations.",0.40816328,0.37509882,0.001093258149921894
26,"Question\nI need to know how to make a highlighted label(or small box )appears when the mouse is on widget like when you are using browser and put the mouse on (reload/back/etc...) button a small box will appear and tell you what this button do
and i want that for any widget not only widgets on toolbar\nAnswer: As the comment of @ekhumoro says
setToolTip is the solution",-0.71428573,0.27304614,0.9748241901397705
27,"Question\nI am having hard time to install a python lib called python3-saml
To narrow down the problem I created a very simple application on ibm-cloud and I can deploy it without any problem, but when I add as a requirement the lib python3-saml 
I got an exception saying:
pkgconfig.pkgconfig.PackageNotFoundError: xmlsec1 not found
The above was a deployment on ibm-cloud, but I did try to install the same python lib locally and I got the same error message, locally I can see that I have the xmlsec1 installed.
Any help on how to successfully deploy it on ibm-cloud using python3-saml?
Thanks in advance\nAnswer: I had a similar issue and I had to install the ""xmlsec1-devel"" on my CentOS system before installing the python package.",0.40816328,0.1688205,0.057284966111183167
28,"Question\nI'm new with Python and new on Stackoverflow, so please let me know if this question should be posted somewhere else or you need any other info :). But I hope someone can help me out with what seems to be a rather simple mistake...
I'm working with Python in Jupyter Notebook and am trying to create my own module with some selfmade functions/loops that I often use. However, when I try to some of the functions from my module, I get an error related to the import of the built-in module that is used in my own module.
The way I created my own module was by:

creating different blocks of code in a notebook and downloading it
as  'Functions.py' file.
saving this Functions.py file in the folder that i'm currently working in (with another notebook file)
in my current notebook file (where i'm doing my analysis), I import my module with 'import Functions'.

So far, the import of my own module seems to work. However, some of my self-made functions use functions from built-in modules. E.g. my plot_lines() function uses math.ceil() somewhere in the code. Therefore, I imported'math' in my analysis notebook as well. But when I try to run the function plot_lines() in my notebook, I get the error ""NameError: name'math' is not defined"".
I tried to solve this error by adding the code 'import math' to the function in my module as well, but this did not resolve the issue. 
So my question is: how can I use functions from built-in Python modules in my own modules?
Thanks so much in advance for any help!\nAnswer: If anyone encounters the same issue:
add 'import math' to your own module. 
Make sure that you actually reload your adjusted module, e.g. by restarting your kernell!",0.0,0.1970278,0.03881995379924774
29,"Question\nI use rawpy module in python to post-process raw images, however, no matter how I set the Params, the output is different from the default RGB in camera ISP, so anyone know how to operate on this please?
I have tried the following ways:
Default:
output = raw.postprocess()
Use Camera White balance:
output = raw.postprocess(use_camera_wb=True)
No auto bright:
output = raw.postprocess(use_camera_wb=True, no_auto_bright=True)
None of these could recover the RGB image as the camera ISP output.\nAnswer: The dcraw/libraw/rawpy stack is based on publicly available (reverse-engineered) documentation of the various raw formats, i.e., it's not using any proprietary libraries provided by the camera vendors. As such, it can only make an educated guess at what the original camera ISP would do with any given image. Even if you have a supposedly vendor-neutral DNG file, chances are the camera is not exporting everything there in full detail.
So, in general, you won't be able to get the same output.",0.0,0.2944486,0.08669998496770859
30,"Question\nI am trying to improve mobilenet_v2's detection of boats with about 400 images I have annotated myself, but keep on getting an underfitted model when I freeze the graphs, (detections are random does not actually seem to be detecting rather just randomly placing an inference). I performed 20,000 steps and had a loss of 2.3.
I was wondering how TF knows that what I am training it on with my custom label map
ID:1
Name: 'boat'
Is the same as what it regards as a boat ( with an ID of 9) in the mscoco label map.
Or whether, by using an ID of 1, I am training the models' idea of what a person looks like to be a boat?
Thank you in advance for any advice.\nAnswer: so I managed to figure out the issue.
We created the annotation tool from scratch and the issue that was causing underfitting whenever we trained regardless of the number of steps or various fixes I tried to implement was that When creating bounding boxes there was no check to identify whether the xmin and ymin coordinates were less than the xmax and ymax I did not realize this would be such a large issue but after creating a very simple check to ensure the coordinates are correct training ran smoothly.",0.0,0.107631385,0.011584514752030373
31,"Question\nI am trying to improve mobilenet_v2's detection of boats with about 400 images I have annotated myself, but keep on getting an underfitted model when I freeze the graphs, (detections are random does not actually seem to be detecting rather just randomly placing an inference). I performed 20,000 steps and had a loss of 2.3.
I was wondering how TF knows that what I am training it on with my custom label map
ID:1
Name: 'boat'
Is the same as what it regards as a boat ( with an ID of 9) in the mscoco label map.
Or whether, by using an ID of 1, I am training the models' idea of what a person looks like to be a boat?
Thank you in advance for any advice.\nAnswer: The model works with the category labels (numbers) you give it.  The string ""boat"" is only a translation for human convenience in reading the output.
If you have a model that has learned to identify a set of 40 images as class 9, then giving it a very similar image that you insist is class 1 will confuse it.  Doing so prompts the model to elevate the importance of differences between the 9 boats and the new 1 boats.  If there are no significant differences, then the change in weights will find unintended features that you don't care about.
The result is a model that is much less effective.",0.0,0.2830283,0.0801050215959549
32,"Question\nI'm working with odoo11 community version and currently I have some problem.
This is my exmplanation of problem:
In company I have many workcenters, and for each workcenter:
1) I want to create separate warehouse for each workcenter
or
2) Just 1 warehouse but different storage areas for each workcenter
(currently I made second option) and each workcenter have their own operation type: Production
Now my problem started, There are manufacturing orders and each manufacturing order have few workorders, And I want to do something that when some workorder is started then products are moved to this workcenter's warehouse/storage area and they are there untill next workorders using different workcenter starting then product are moved to next workcenter warehouse/storage area.
I can only set that after creating new sale order production order is sent to first Workcenter storage area and he is ther untill all workorders in production order are finished, I don't know how to trigger move routes between workcenters storage areas. for products that are still in production stage
Can I do this from odoo GUI, or maybe I need to do this somewhere in code?\nAnswer: Ok, I found my answer, which is that to accomplish what I wanted I need to use Manufacturing with Multi levell Bill of material, it working in way that theoretically 3 steps manufacturing order is divided into 3 single manufacture orders with 1 step each, and for example 2 and 3 prodcution order which before were 2 and 3 step are using as components to produce product that are finished in previous step which now is individual order.",0.0,-0.041719735,0.0017405363032594323
33,"Question\nGood day folks
Recently, I made a python based web crawler machine that scrapes_ some news ariticles and django web page that collects search title and url from users.
But I do not know how to connect the python based crawler machine and django web page together, so I am looking for the any good resources that I can reference.
If anyone knows the resource that I can reference,
Could you guys share those?
Thanks\nAnswer: There are numerous ways you could do this. 
You could directly integrate them together. Both use Python, so the scraper would just be written as part of Django. 
You could have the scraper feed the data to a database and have Django read from that database. 
You could build an API from the scraper to your Django implementation. 
There are quite a few options for you depending on what you need.",0.0,0.060730815,0.003688231809064746
34,"Question\nI was wondering if it is possible for me to use Django code I have for my website and somehow use that in a mobile app, in a framework such as, for example, Flutter.
So is it possible to use the Django backend I have right now and use it in a mobile app?
So like the models, views etc...\nAnswer: Yes. There are a couple ways you could do it

Use the Django Rest Framework to serve as the backend for something like React Native. 
Build a traditional website for mobile and then run it through a tool like PhoneGap. 
Use the standard Android app tools and use Django to serve and process data through API requests.",1.0,0.20876706,0.6260495781898499
35,"Question\nCan anyone please let me know how to simulate mouse hover event using robot framework on a desktop application. I.e if I mouse hover on a specific item or an object, the sub menus are listed and i need to select one of the submenu item.\nAnswer: It depends on the automation library that you are using to interact with the Desktop application. 
The normal approach is the following: 

Find the element that you want to hover on (By ID or some other unique locator)
Get the attribute position of the element (X,Y)
Move your mouse to that position. 

In this way you don´t ""hardcode"" the x,y position what will make your test case flaky.",0.0,0.21539605,0.04639545828104019
36,"Question\nI understand how it works when you have one column output but could not understand how it is done for 4 column outputs.\nAnswer: It’s not advised to calculate accuracy for continuous values. For such values you would want to calculate a measure of how close the predicted values are to the true values. This task of prediction of continuous values is known as regression. And generally R-squared value is used to measure the performance of the model.
If the predicted output is of continuous values then mean square error is the right option 
For example:
Predicted o/p vector1-----> [2,4,8] and
Actual o/p vector1 -------> [2,3.5,6]
1.Mean square error is  sqrt((2-2)^2+(4-3.5)^2+(8-6)^2 )
2.Mean absolute error..etc.
(2)if the output is of classes then accuracy is the right metric to decide on model performance
Predicted o/p vector1-----> [0,1,1]
Actual o/p vector1 -------> [1,0,1]
Then accuracy calculation can be done with following:
1.Classification Accuracy
2.Logarithmic Loss
3.Confusion Matrix
4.Area under Curve
5.F1 Score",0.40816328,0.2514187,0.024568859487771988
37,"Question\nI tried type(+) hoping to know more about how is this operator represented in python but i got SyntaxError: invalid syntax.
My main problem is to cast as string representing an operation :""3+4"" into the real operation to be computed in Python (so to have an int as a return: 7).
I am also trying to avoid easy solutions requiring the os library if possible.\nAnswer: Operators don't really have types, as they aren't values. They are just syntax whose implementation is often defined by a magic method (e.g., + is defined by the appropriate type's __add__ method).
You have to parse your string:

First, break it down into tokens: ['3', '+', '4']
Then, parse the token string into an abstract syntax tree (i.e., something at stores the idea of + having 3 and 4 as its operands).
Finally, evaluate the AST by applying functions stored at a node to the values stored in its children.",1.0,0.25357246,0.5571540594100952
38,"Question\nI want to write a program to simulate 5-axis cnc gcode with vpython and I need to rotate trail of the object that's moving. Any idea how that can be done?\nAnswer: It's difficult to know exactly what you need, but if instead of using ""make_trail=True"" simply create a curve object to which you append points. A curve object named ""c"" can be rotated using the usual way to rotate an object: c.rotate(.....).",0.0,0.25870973,0.06693072617053986
39,"Question\nI am new to deep learning, I was wondering if there is a way to extract parts of images containing the different label and then feed those parts to different model for further processing?
For example,consider the dog vs cat classification.
Suppose the image contains both cat and dog.
We successfully classify that the image contains both, but how can we classify the breed of the dog and cat present?
The approach I thought of was,extracting/cutting out the parts of the image containing dog and cat.And then feed those parts to the respective dog breed classification model and cat breed classification model separately.
But I have no clue on how to do this.\nAnswer: Your thinking is correct, you can have multiple pipelines based on the number of classes.

Training:
Main model will be an object detection and localization model like Faster RCNN, YOLO, SSD etc trained to classify at a high level like cat and dog. This pipeline provides you bounding box details (left, bottom, right, top) along with the labels.
Sub models will be multiple models trained on a lover level. For example a model that is trained to classify breed. This can be done by using models like vgg, resnet, inception etc. You can utilize transfer learning here.
Inference: Pass the image through Main model, crop out the detection objects using bounding box details (left, bottom, right, top) and based on the label information, feed it appropriate sub model and extract the results.",0.40816328,0.19652528,0.04479064419865608
40,"Question\nI'm new to dask and trying to use it in our cluster which uses NC job scheduler (from Runtime Design Automation, similar to LSF). I'm trying to create an NCCluster class similar to LSFCluster to keep things simple. 
What are the steps involved in creating a job scheduler for custom clusters?
Is there any other way to interface dask to custom clusters without using JobQueueCluster?
I could find info on how to use the LSFCluster/PBSCluster/..., but couldn't find much information on creating one for a different HPC.
Any links to material/examples/docs will help
Thanks\nAnswer: Got it working after going through the source code.
Tips for anyone trying:

Create a customCluster & customJob class similar to LSFCluster & LSFJob.
Override the following


submit_command
cancel_command
config_name (you'll have to define it in the jobqueue.yaml)
Depending on the cluster, you may need to override the _submit_job, _job_id_from_submit_ouput and other functions.


Hope this helps.",0.0,0.28062505,0.07875041663646698
41,"Question\nI am trying to get some code working on mac and to do that I have been using an anaconda virtual environment. I have all of the dependencies loaded as well as my script, but I don't know how to execute my file in the virtual environment on mac. The python file is on my desktop so please let me know how to configure the path if I need to. Any help?\nAnswer: If you have a terminal open and are in your virtual environment then simply invoking the script should run it in your environment.",0.40816328,0.16465938,0.05929414927959442
42,"Question\nI am trying to deploy a Python webapp on AWS that takes a USERNAME and PASSWORD as input from a user, inputs them into a template Python file, and logs into their Instagram account to manage it automatically. 
In Depth Explanation:
I am relatively new to AWS and am really trying to create an elaborate project so I can learn. I was thinking of somehow receiving the user input on a simple web page with two text boxes to input their Instagram account info (username & pass). Upon receiving this info, my instinct tells me that I could somehow use Lambda to quickly inject it into specific parts of an already existing template.py file, which will then be taken and combined with the rest of the source files to run the code. These source files could be stored somewhere else on AWS (S3?). I was thinking of running this using Elastic Beanstalk. 
I know this is awfully involved, but my main issue is this whole dynamic injection thing. Any ideas would be so greatly appreciated. In the meantime, I will be working on it.\nAnswer: One way in which you could approach this would be have a hosted website on a static s3 bucket. Then, when submitting a request, goes to an API Gateway POST endpoint, This could then trigger a lambda (in any language of choice) passing in the two values.
This would then be passed into the event object of the lambda, you could store these inside secrets manager using the username as the Key name so you can reference it later on. Storing it inside a file inside a lambda is not a good approach to take. 
Using this way you'd learn some key services:

S3 + Static website Hosting
API Gateway 
Lambdas  
Secrets Manager

You could also add alias's/versions to the lambda such as dev or production and same concept to API Gateways with stages to emulate doing a deployment.
However there are hundreds of different ways to also design it. And this is only one of them!",0.0,0.38639864,0.14930391311645508
43,"Question\nI have this html code:
<button class=""_2ic5v""><span aria-label=""Like"" class=""glyphsSpriteComment_like u-__7""></span></button>
I am trying to locate all the elements that meet this class with phyton, and selenium webdriver library:
likeBtn = driver.find_elements_by_class_name('_2ic5v')
but when I print 
likeBtn
it prints 
[]
I want to locate all of the buttons that much this div/span class, or aria-label
how do I do that successfully? Thanks in advance
update - when I do copy Xpath from page the print stays the same\nAnswer: Is it button class name dynamic or static? 
How if you try choose By.CssSelector?
You can find element by copy selector in element",0.0,-0.09392345,0.00882161408662796
44,"Question\nI'm using python's findall function with a reg expression that should work but can't get the function to output results with quotation marks in them ('""). 
This is what I tried:
Description = findall('<p>([A-Za-z,\.\—'"":;0-9]+).</p>\n', text)
The quotation marks inside the reg expression are creating the hassle and I have no idea how to get around it.\nAnswer: Placing the backslash before the single quote like Sachith Rukshan suggested makes it work",0.0,0.1333943,0.017794039100408554
45,"Question\nI have created a machine learning software that detects objects(duh!), processes the objects based on some computer vision parameters and then triggers some hardware that puts the object in the respective bin. The objects are placed on a conveyer belt and a camera is mounted at a point to snap pictures of objects(one object at a time) when they pass beneath the camera. I don't have control over the speed of the belt.
Now, the challenge is that I have to configure a ton of things to make the machine work properly.
The first problem is the time model takes to create segmentation masks, it varies from one object to another.
Another issue is how do I maintain signals that are generated after computer vision processing, send them to actuators in a manner that it won't get misaligned with the computer vision-based inferencing.
My initial design includes creating processes responsible for a specific task and then make them communicate with one other as per the necessity. However, the problem of synchronization still persists.
As of now, I am thinking of treating the software stack as a group of services as we usually do in backend and make them communicate using something like celery and Redis queue.
I am a kind of noob in system design, come from a background of data science. I have explored python's multithreading module and found it unusable for my purpose(all threads run on single core). I am concerned if I used multiprocessing, there could be additional delays in individual processes due to messaging and thus, that would add another uncertainty to the program.
Additional Details:

Programming Frameworks and Library: Tensorflow, OpenCV and python
Camera Resolution: 1920P
Maximum Accutuation Speed: 3 triggers/second
Deep Learning Models: MaskRCNN/UNet

P.S: You can also comment on the technologies or the keywords I should search for because a vanilla search yields nothing good.\nAnswer: Let me summarize everything first. 

What you want to do

The ""object"" is on the conveyer belt
The camera will take pictures of the object
MaskRCNN will run to do the analyzing

Here are some problems you're facing

""The first problem is the time model takes to create segmentation masks, it varies from one object to another.""   

-> if you want to reduce the processing time for each image, then an accelerator (FPGA, Chip",0.40816328,-0.0053862333,0.1710232049226761
46,"Question\nAs the title says, that's basically it. I have tried to install matplotlib already but:

I am on Windows and ""sudo"" doesn't work
Every solution and answers on Stack Overflow regarding matplotlib (or some other package) not being able to be installed doesn't work for me...
I get ""Error Code 1""

So! Is there any other way to plot a graph in python without matplotlib? If not, can I have help with how to install matplotlib, successfully?\nAnswer: in cmd (coammand prompt) type pip install matplotlib",-0.71428573,0.19937903,0.8347833156585693
47,"Question\nI have a simple task I want to perform over ssh: return all files from a given file list that do not exist.
The way I would go about doing this would be to wrap the following in an ssh session:
for f in $(files); do stat $f > /dev/null ;done
The stdout redirect will ignore all good files and then reading the stderr will give me a list of all non found files.
I first thought of using this bash code with the ssh part inside a subprocess.run(..., shell=True) but was discouraged to do so. Instead,paramikowas suggested. 
I try to understand why and when native python is better than subprocessing bash

Computability with different OS (not an issue for me as the code is pretty tightly tied to Ubuntu)
Error and exception handling - this one I do get and think it's important, though catching an exception or exit code from subprocess is kinda easy too

The con in my eyes with native python is the need to involve somewhat complicated modules such as paramiko when bash's ssh and stat seem to me as more plain and easy to use
Are there any guidelines for when and how to choose bash over python?
This question is mainly about using a command over ssh, but is relevant for any other command that bash is doing in a short and easy way and python wraps\nAnswer: There are really three choices here: doing something in-process (like paramiko), running ssh directly (with subprocess), and running ssh with the shell (also with subprocess).  As a general rule, avoid running the shell programmatically (as opposed to, say, upon interactive user request).
The reason is that it’s a human-oriented interface (thus the easy separation of words with spaces and shortcuts for $HOME and globbing) that is vastly underpowered as an API.  Consider, for example, how your code would detect that ssh was missing: the situation doesn’t arise with paramiko (so long as it is installed), is obvious with subprocess, and is just an (ambiguous) exit code and stderr message from the shell.  Also consider how you supply the command to run: it already must be a command suitable for the shell (due to limitations in the SSH protocol), but if you invoke ssh with the shell it must be encoded (sometimes called “doubly escaped”) so",0.40816328,0.20593643,0.040895696729421616
48,"Question\nI want VS Code to turn venv on run, but I can't find how to do that.
I already tried to add to settings.json this line:

""terminal.integrated.shellArgs.windows"": [""source${workspaceFolder}\env\Scripts\activate""]

But, it throws me an 127 error code. I found what 127 code means. It means, Not found. But how it can be not found, if I see my venv folder in my eyes right now?
I think it's terminal fault. I'm using Win 10 with Git Bash terminal, that comes when you install Git to your machine.\nAnswer: There is a new flag that one can use: ""python.terminal.activateEnvironment"": true",0.6530612,0.37829834,0.07549463212490082
49,"Question\nI want VS Code to turn venv on run, but I can't find how to do that.
I already tried to add to settings.json this line:

""terminal.integrated.shellArgs.windows"": [""source${workspaceFolder}\env\Scripts\activate""]

But, it throws me an 127 error code. I found what 127 code means. It means, Not found. But how it can be not found, if I see my venv folder in my eyes right now?
I think it's terminal fault. I'm using Win 10 with Git Bash terminal, that comes when you install Git to your machine.\nAnswer: This is how I did it in 2021:

Enter Ctrl+Shift+P in your vs code.

Locate your Virtual Environment:
Python: select interpreter > Enter interpreter path > Find

Once you locate your virtual env select your python version:
your-virtual-env > bin > python3.

Now in your project you will see.vscode directory created open settings.json inside of it and add:
""python.terminal.activateEnvironment"": true
don't forget to add comma before to separate it with already present key value pair.

Now restart the terminal.


You should see your virtual environment activated automatically.",1.0,0.39291573,0.3685513138771057
50,"Question\nI've read some articles and most of them say that 3-ply improves the performance of the self-player train. 
But what is this in practice? and how is that implemented?\nAnswer: There is stochasticity in the game because of the dice rolls, so one approach would be evaluate state positions by self play RL, and then while playing do a 2-ply search over all the possible dice combinations. That would be 36 + 6 i.e. 42 possible rolls, and then you have to make different moves that are available which increases the breath of the tree to an insane degree. I tried this and it failed because my Mac could not handle such computation. Instead what we could do is just randomize a few dice rolls and perform a MiniMax tree search with Alpha Beta pruning ( using the AfterState value function).
For a 1 ply search we just use the rolled dice, or if we want to predict the value before we roll the dice then we can simply loop over all the possible combinations. Then we just argmax over the afterstates.",0.0,0.115145504,0.013258486986160278
51,"Question\nI am currently overhauling a project here at work and need some advice. We currently have a morning checklist that runs daily and executes roughly 30 SQL files with 1 select statement each. This is being done in an excel macro which is very unreliable. These statements will be executed against an oracle database.
Basically, if you were re-implementing this project, how would you do it? I have been researching concurrency in python, but have not had any luck. We will need to capture the results and display them, so please keep that in mind.If more information is needed, please feel free to ask.
Thank you.\nAnswer: There are lots of ways depending on how long the queries run, how much data is output, are there input parameters and what is done to the data output.
Consider:
1. Don't worry about concurrency up front
2. Write a small python app to read in every *.sql file in a directory and execute each one.
3. Modify the python app to summarize the data output in the format that it is needed
4. Modify the python app to save the summary back into the database into a daily check table with the date / time the SQL queries were run.  Delete all rows from the daily check table before inserting new rows
5. Have the Excel spreadsheet load it's data from that daily check table including the date / time the data was put in the table 
6. If run time is slows, optimize the PL/SQL for the longer running queries
7. If it's still slow, split the SQL files into 2 directories and run 2 copies of the python app, one against each directory.
8. Schedule the python app to run at 6 AM in the Windows task manager.",0.81632656,0.19419354,0.38704949617385864
52,"Question\nI need to write some messages in discord with my bot, but I don't know how to do it. It seems that discord.py can't send messages autonomously.
Does anyone know how to do it?\nAnswer: I solved putting a while loop inside the function on_message.
So I need to send only a message and then my bot can write as many messages as he wants",0.0,0.27106354,0.0734754428267479
53,"Question\nI'm using boto3 to download files from an s3 bucket & I need to support canceling an active file transfer in my client UI - but I can't find how to do it.
There is a progress callback that I can use for transfer status, but I can not cancel the transfer from there.
I did find that boto3's s3transfer.TransferManager object has a.shutdown() member, but it is buggy (.shutdown() passes the wrong params to._shutdown() a few lines below it) & crashes.
Is there another way to safely cancel an active file_download?\nAnswer: Can you kill the process associated with the file? 
kill $(ps -ef | grep 'process-name' | awk '{print $2}')",0.20408164,0.22746378,0.000546724593732506
54,"Question\nI'm a beginner, its been ~2 months since i started learning python. 
I've written a code about a function that takes two strings, and outputs the common characters between those 2 strings. The issue with my code is that it returns all common characters that the two inputs have. For example:
input: common, moron
the output is ""oommoon"" when ideally it should be ""omn"".
i've tried using the count() function, and then the replace function, but it ended up completely replacing the letters that were appearing more than once in the output, as it should.
how should i go about this? i mean it's probably an easy solution for most of the ppl here, but what will the simplest approach be such that i, a beginner with okay-ish knowledge of the basics, understand it?\nAnswer: You can try this:
''.join(set(s1).intersection(set(s2)))",0.0,0.040311813,0.0016250422922894359
55,"Question\nSo,I need to iterate over a dictionary in python where the keys are a tuple and the values are integers.
I only need to print out the keys and values.
I tried this:
for key,value in dict:
but didn't work because it assigned the first element of the tuple to the key and value and the second to the value.
So how should I do it?\nAnswer: Just use 
for key in dict
and then access the value with dict[key]",0.10204082,0.08227348,0.00039074759115464985
56,"Question\nI have a dataset that I have loaded as a data frame in Python. It consists of 21392 rows (the data instances, each row is one sample) and 79 columns (the features). The last column i.e. column 79 has string type labels. I would like to use a CNN to classify the data in this case and predict the target labels using the available features. This is a somewhat unconventional approach though it seems possible. However, I am very confused on how the methodology should be as I could not find any sample code/ pseudo code guiding on using CNN for Classifying non-image data, either in Tensorflow or Keras. Any help in this regard will be highly appreciated. Cheers!\nAnswer: You first have to know, if it is sensible to use CNN for your dataset. You could use sliding 1D-CNN if the features are sequential eg) ECG, DNA, AUDIO. However I doubt that this is not the case for you. Using a Fully Connected Neural Net would be a better choice.",0.40816328,0.25868654,0.022343294695019722
57,"Question\nIs it possible to train an XGboost model in python and use the saved model to predict in spark environment?  That is, I want to be able to train the XGboost model using sklearn, save the model. Load the saved model in spark and predict in spark. Is this possible?
edit:
Thanks all for the answer, but my question is really this. I see the below issues when I train and predict different bindings of XGBoost.

During training I would be using XGBoost in python, and when  predicting I would be using XGBoost in mllib.
I have to load the saved model from XGBoost python (Eg: XGBoost.model file) to be predicted in spark, would this model be compatible to be used with the predict function in the mllib
The data input formats of both XGBoost in python and XGBoost in spark mllib are different. Spark takes vector assembled format but with python, we can feed the dataframe as such. So, how do I feed the data when I am trying to predict in spark with a model trained in python. Can I feed the data without vector assembler? Would XGboost predict function in spark mllib take non-vector assembled data as input?\nAnswer: You can run your python script on spark using spark-submit command so that can compile your python code on spark and then you can predict the value in spark.",0.0,0.11539781,0.013316654600203037
58,"Question\nIs it possible to train an XGboost model in python and use the saved model to predict in spark environment?  That is, I want to be able to train the XGboost model using sklearn, save the model. Load the saved model in spark and predict in spark. Is this possible?
edit:
Thanks all for the answer, but my question is really this. I see the below issues when I train and predict different bindings of XGBoost.

During training I would be using XGBoost in python, and when  predicting I would be using XGBoost in mllib.
I have to load the saved model from XGBoost python (Eg: XGBoost.model file) to be predicted in spark, would this model be compatible to be used with the predict function in the mllib
The data input formats of both XGBoost in python and XGBoost in spark mllib are different. Spark takes vector assembled format but with python, we can feed the dataframe as such. So, how do I feed the data when I am trying to predict in spark with a model trained in python. Can I feed the data without vector assembler? Would XGboost predict function in spark mllib take non-vector assembled data as input?\nAnswer: you can 

load data/ munge data using pyspark sql,  
then bring data to local driver using collect/topandas(performance bottleneck)
then train xgboost on local driver
then prepare test data as RDD, 
broadcast the xgboost model to each RDD partition, then predict data in parallel

This all can be in one script, you spark-submit, but to make the things more concise, i will recommend split train/test in two script.
Because step2,3 are happening at driver level, not using any cluster resource, your worker are not doing anything",0.0,0.16865501,0.02844451181590557
59,"Question\nI have a binary file on my PC that contains data in big-endian. The file contains around 121 MB.
The problem is I would like to convert the data into little-endian with a python script. 
What is currently giving me headaches is the fact that I don't know how to convert an entire file. If I would have a short hex string I could simply use struct.pack to convert it into little-endian but if I see this correctly I can't give struct.pack a binary file as input.
Is there an other function/utility that I can use to do that or how should my approach look like?\nAnswer: We need a document or knowledge of the file's exact structure.
Suppose that there is a 4 byte file. If this file has just a int, we need to flip that. But if it is a combination of 4 char, we should leave it as it be.
Above all, you should find the structure. Then we can talk about the translation. I think there is no translation tools to support general data, but you need to parse that binary file following the structure.",0.0,0.1461342,0.02135520428419113
60,"Question\nSomeone worked before with streaming data into (google) BigQuery using Google Cloud Functions (insert_rows_from_dataframe())?
My problem is it seems like sometimes the table schema is not updated immediately and when you try to load some data into table immediately after creation of a new field in the schema it returns an error:

BigQueryError: [{""reason"": ""invalid"", ""location"": ""test"", ""debugInfo"": """", ""message"": ""no such field.""}]""

However, if I try to load again after few seconds it all works fine, so my question if someone knows the maximum period of time in seconds for this updating (from BigQuery side) and if is possible somehow to avoid this situation?\nAnswer: Because the API operation on BigQuery side is not atomic, you can't avoid this case. 
You can only mitigate the impact of this behavior and perform a sleep, a retries, or set a Try-catch to replay the insert_rows_from_dataframe() several times (not infinite, in case of real problem, but 5 times for example) until it pass.
Nothing is magic, if the consistency is not managed on a side, the other side has to handle it!",0.40816328,0.4811872,0.005332492291927338
61,"Question\nI was trying to install kivy_deps.glew(version).whl with 

pip install absolute/path/to/file/kivy_deps.glew

And I get this error:

kivy_deps.glew(version).whl is not a supported wheel on this version

I searched in the web and saw that some people said that the problem is because you shoud have python 2.7 and I have python 3.7. The version is of glew is cp27. So if this is the problem how to install python 2.7 and 3.7 in the same time and how to use both of them with pip.(i.e maybe you can use 

pip2.7 install 

For python 2.7 and

pip install

For python 3.7
P.S: My PC doesn't have an internet connection that's why i'm installing it with a wheel file. I have installed all dependecies except glew and sdl2. If there is any unofficial file for these two files for python 3.7 please link them.
I know this question has been asked before in stackoverflow but I didn't get any solution from it(it had only 1 anwser tho)
Update: I uninstalled python 3.7 and installed python 2.7, but pip and python weren't commands in cmd because python 2.7 hadn't pip. So I reinstalled python 3.7\nAnswer: I fixed it. Just changed in the name of the file cp27 to cp37",0.0,-0.04096344,0.0016780034638941288
62,"Question\ni made a one ML model in python now i want to  use this model in react native app means that frontend will be based on react native and model is made on python,how can i connect both thing with each other\nAnswer: create a REST Api in flask/django to deploy your model on server.create end points for separate functions.Then call those end points in your react native app.Thats how it works.",0.13605443,0.35624653,0.04848456382751465
63,"Question\ni made a one ML model in python now i want to  use this model in react native app means that frontend will be based on react native and model is made on python,how can i connect both thing with each other\nAnswer: You can look into the CoreMl library for react native application if you are developing for IOS platform else creating a restAPI is a good option. (Though some developers say that latency is an issue but it also depends on what kind of model and dataset you are using ).",0.0,0.2441034,0.05958646908402443
64,"Question\nI have a dataframe with datetime index, where the data was sampled irregularly (the datetime index has gaps, and even where there aren't gaps the spacing between samples varies).
If I do:
df['my column'].autocorr(my_lag)
will this work?  Does autocorr know how to handle irregularly sampled datetime data?\nAnswer: This is not quite a programming question.
Ideally, your measure of autocorrelation would use data measured at the same frequency/same time interval between observations.  Any autocorr function in any programming package will simply measure the correlation between the series and whatever lag you want.  It will not correct for irregular frequencies.
You would have to fix this yourself but 1) setting up a series with a regular frequency, 2) mapping the actual values you have to the date structure, 3) interpolating values where you have gaps/NaN, and then 4) running your autocorr.  
Long story short, autocorr would not do all this work for you.   
If I have misunderstood the problem you are worried about, let me know. It would be helpful to know a little more about the sampling frequencies.  I have had to deal with things like this a lot.",0.0,0.15873271,0.025196073576807976
65,"Question\nI'm writing a module which only contains functions. Is it good practice to put these inside a class, even if there are no class arguments and the __init__ function is pointless? And if so how should I write it?\nAnswer: It is good to build modules that contain a class for better organization and manipulation depending on how big the code is and how it will be used, but yes it is good to get use to building classes with methods in them. Can you post your code?",0.0,0.044308662,0.0019632575567811728
66,"Question\nI came across the following example of creating an Internet Checksum:

Take the example IP header 45 00 00 54 41 e0 40 00 40 01 00 00 0a 00 00 04 0a 00 00 05:

Adding the fields together yields the two’s complement sum 01 1b 3e.
Then, to convert it to one’s complement, the carry-over bits are added to the first 16-bits: 1b 3e + 01 = 1b 3f.
Finally, the one’s complement of the sum is taken, resulting to the checksum value e4c0.


I was wondering how the IP header is added together to get 01 1b 3e?\nAnswer: The IP header is added together with carry in hexadecimal numbers of 4 digits.
i.e. the first 3 numbers that are added are 0x4500 + 0x0054 + 0x41e0 +...",0.20408164,0.17297089,0.0009678786736913025
67,"Question\nI am a python3 beginner, and I've been stuck on how to utilize my data at my scripts.
My data is stored in an external hdd and I am seeking for the way to retrieve the data to use on a program in jupyter notebook somehow.
Does anyone know how to make an access to external hdd?\nAnswer: Hard to say what the issue is without seeing any code. In general make sure your external hard drive is connected to your machine, and when loading your data (depends on what kind of data you want to use) specify the full path to your data.",0.40816328,0.33581847,0.00523377163335681
68,"Question\nI want to schedule emails using Django. Example ---> I want to send registered users their shopping cart information everyday at 5:00 P.M.
How would I do this using Django? I have read a lot of articles on this problem but none of them have a clear and definite solution. I don't want to implement a workaround.
Whats the proper way of implementing this? Can this be done within my Django project or do I have to use some third-party service?
If possible, please share some code. Otherwise, details on how I can implement this will do.\nAnswer: There's no built-in way to do what you're asking.  What you could do, though, is write a management command that sends the emails off and then have a crontab entry that calls that command at 5PM (this assumes your users are in the same timezone as your server).
Another alternative is using celery and celery-beat to create scheduled tasks, but that would require more work to set up.",0.40816328,0.21162719,0.03862643614411354
69,"Question\nI am designing a web application that has users becoming friends with other users. I am storing the users info in a database using sqlite3. 
I am brainstorming on how I can keep track on who is friends with whom.
What I am thinking so far is; to make a column in my database called Friendships where I store the various user_ids( integers) from the user's friends.
I would have to store multiple integers in one column...how would I do that?
Is it possible to store a python list in a column?
I am also open to other ideas on how to store the friendship network information in my database....
The application runs through FLASK\nAnswer: What you are trying to do here is called a ""many-to-many"" relationship. Rather than making a ""Friendships"" column, you can make a ""Friendship"" table with two columns: user1 and user2. Entries in this table indicate that user1 has friended user2.",0.13605443,0.24449229,0.011758770793676376
70,"Question\nI am designing a web application that has users becoming friends with other users. I am storing the users info in a database using sqlite3. 
I am brainstorming on how I can keep track on who is friends with whom.
What I am thinking so far is; to make a column in my database called Friendships where I store the various user_ids( integers) from the user's friends.
I would have to store multiple integers in one column...how would I do that?
Is it possible to store a python list in a column?
I am also open to other ideas on how to store the friendship network information in my database....
The application runs through FLASK\nAnswer: It is possible to store a list as a string into an sql column.  
However, you should instead be looking at creating a Friendships table with primary keys being the user and the friend.
So that you can call the friendships table to pull up the list of friends.
Otherwise, I would suggest looking into a Graph Database, which handles this kind of things well too.",0.13605443,0.1617322,0.0006593479192815721
71,"Question\nManim noobie here. 
I am trying to run two animations at the same time, notably, I'm trying to display a dot transitioning from above ending up between two letters. Those two letters should create some space in between in the meantime. 
Any advice on how to do so? Warm thanks in advance.\nAnswer: To apply two transformations at the same time, you can do self.play(Transformation1, Transformation2). This way, since the two Transformations are in the same play statement, they will run simultaneously.",1.0,0.22629881,0.5986135601997375
72,"Question\nI need to query from the BBG API the nearest quote to 14:00 o'clock for a number of FX currency pairs. I read the developers guide and I can see that reference data request provides you with the latest quote available for a currency however if I run the request at 14.15 it will give me the nearest quote to that time not 14.00. Historical and intraday data output too many values as I need only the latest quote to a given time. 
Would you be able to advise me if there is a type of request which will give me what I am looking for.\nAnswer: Further to previous suggestions, you can start subscription to //blp/mktdata service before 14:00 for each instrument to receive stream of real-time ticks. Cache the last tick, when hitting 14:00 mark the cache as pre-14:00, then mark the first tick after as post:14, select the nearest to 14:00 from both.",0.0,0.07267833,0.005282139405608177
73,"Question\nI have a Python bot running PRAW for Reddit. It is open source and thus users could schedule this bot to run at any frequency (e.g. using cron). It could run every 10 minutes, or every 6 hours.
I have a specific function (let's call it check_logs) in this bot that should not run every execution of this bot, but rather only once a day. The bot does not have a database.
Is there a way to accomplish this in Python without external databases/files?\nAnswer: Generally speaking, it's better (and easier) to use the external database or file. But, if you absolutely need it you could also:

Modify the script itself, e.g. store the date of the last run in commented out last line of the script. 
Store the date of the last update on the web, for example, in your case it could be a Reddit post or google doc or draft email or a site like Pastebin, etc.
Change the ""modified date"" of the script itself and use it as a reference.",0.0,0.33547384,0.11254269629716873
74,"Question\nI have been able to freeze a Python/PySide2 script with fbs on macOS, and the app seems to work.
However, I got some errors from the freeze process stating: 

Can not find path./libshiboken2.abi3.5.13.dylib. 

Does anyone know how to fix that?\nAnswer: Try to use the --runtime-tmpdir because while running the generated exe file it needs this file libshiboken2.abi3.5.13.dylib and unable hook that file. 
Solution: use --add-data & --runtime-tmpdir to pyinstaller command line.
pyinstaller -F --add-data ""path/libshiboken2.abi3.5.13.dylib"":""**PATH""
--runtime-tmpdir temp_dir_name your_program.py
here PATH  = the directory name of that file looking for.-F = one file",0.0,0.2957903,0.08749190717935562
75,"Question\nI have a function that checks if a date ( int number ) that is written in this format: ""YYYYMMDD"" is valid or not.
My question is how do i get to the first 4 numbers for example ( the year )?
the month ( the 5th and 6th number ) and the days.
Thanks\nAnswer: Probably the easiest way would be to convert it to a string and use substrings or regular expressions. If you need performance, use a combination of modulo and division by powers of 10 to extract the desired parts.",0.20408164,0.12923837,0.005601515527814627
76,"Question\nDoes aubio have a way to detect sections of a piece of audio that lack tonal elements -- rhythm only? I tested a piece of music that has 16 seconds of rhythm at the start, but all the aubiopitch and aubionotes algorithms seemed to detect tonality during the rhythmic section. Could it be tuned somehow to distinguish tonal from non-tonal onsets? Or is there a related library that can do this?\nAnswer: Use a spectrum analyser to detect sections with high amplitude. If you program - you could take each section and make an average of the freqencies (and amplitudes) present to give you an idea of the instrument(s) involved in creating that amplitude peak.
Hope that helps - if you're using python I could give you some pointers how to program this!?
Regards
Tony",0.0,0.06950259,0.0048306104727089405
77,"Question\nI'm using Tensorflow 1.14 and the tf.keras API to build a number (>10) of differnet neural networks. (I'm also interested in the answers to this question using Tensorflow 2). I'm wondering how I should organize my project.
I convert the keras models into estimators using tf.keras.estimator.model_to_estimator and Tensorboard for visualization. I'm also sometimes using model.summary(). Each of my models has a number (>20) of hyperparameters and takes as input one of three types of input data. I sometimes use hyperparameter optimization, such that I often manually delete models and use tf.keras.backend.clear_session() before trying the next set of hyperparameters. 
Currently I'm using functions that take hyperparameters as arguments and return the respective compiled keras model to be turned into an estimator. I use three different ""Main_Datatype.py"" scripts to train models for the three different input data types. All data is loaded from.tfrecord files and there is an input function for each data type, which is used by all estimators taking that type of data as input. I switch between models (i.e. functions returning a model) in the Main scripts. I also have some building blocks that are part of more than one model, for which I use helper functions returning them, piecing together the final result using the Keras functional API.
The slight incompatibilities of the different models are begining to confuse me and I've decided to organise the project using classes. I'm planing to make a class for each model that keeps track of hyperparameters and correct naming of each model and its model directory. However, I'm wondering if there are established or recomended ways to do this in Tensorflow.
Question: Should I be subclassing tf.keras.Model instead of using functions to build models or python classes that encapsulate them? Would subclassing keras.Model break (or require much work to enable) any of the functionality that I use with keras estimators and tensorboard? I've seen many issues people have with using custom Model classes and am somewhat reluctant to put in the work only to find that it doesn't work for me. Do you have other suggestions how to better organize my project?
Thank you very much in advance.\nAnswer: Subclass",0.6122449,-0.04101509,0.4267486035823822
78,"Question\nI need to automate some workflows to control some Mac applications, I have got a way to do this with Pyautogui module,but I don't want to simulate keyboard or mouse actions anymore, I think if I can get the variables under any GUI elements and program with them directly it would be better, how can I do this?\nAnswer: This is not possible unless the application has some kind of api.
For Web GUIs you can use Selenium and directly select the DOM elements.",0.40816328,0.23222214,0.03095528483390808
79,"Question\nI am looking to connect to a car wirelessly using socketCAN protocol on MacOS using the module python-can on python3. I don't know how to install the socketCAN protocol on MacOS. Pls help.\nAnswer: SocketCAN is implemented only for the Linux kernel. So it is not available on other operating systems. But as long as your CAN adapter is supported by python-can, you don't need SocketCAN.",0.0,0.3944947,0.1556260734796524
80,"Question\nBrief intro of the app: 

I'm working on MLM Webapp and want to make payment on every 15th and last day of every month.
Calculation effect for every user when a new user comes into the system.

What I did [ research ]

using django crontab extension
celery

Question is:
-- Concern about the database insertion/update query:

on the 15th-day hundreds of row generating with income calculation for users. so is there any better option to do that?
how to observe missed and failed query transaction?

Please guide me, how to do this with django, Thanks to everyone!\nAnswer: For your 1st question, i don't think there will be any issue if you're using celery and celery beat for scheduling this task. Assuming your production server has 2 cores (so 4 threads hopefully), you can configure your celery worker (not the beat scheduler) to run using 1 worker with 1/2 thread. At the 15th of a month, beat will see that a task is due and will call your celery worker to accomplish this task. While doing this your worker will be using 1 thread and the other threads will be open (so your server won't go down). There are different ways to configure your celery worker depending on your use case (e.g. using gevent rather than regular thread), but the basic config should be fine.
Well I think you should keep a column in your table to track which ones were successfully handled by your code, and which failed. Celery dashboards will only show if total work succeeded or not, and won't give any further insights.
Hope this helps!",0.0,0.19992411,0.03996964916586876
81,"Question\nI have a dataframe containing the coordinates of millions of particles which I want to use to train a Neural network. These particles build individual clusters which are already identified and labeled; meaning that every particle is already assigned to its correct cluster (this assignment is done by a density estimation but for my purpose not that relevant).
the challenge is now to build a network which does this clustering after learning from the huge data. there are also a few more features in the dataframe like clustersize, amount of particles in a cluster etc. 
since this is not a classification problem but more a identification of clusters-challenge what kind of neural network should i use? I have also problems to build this network: for example a CNN which classifies wheather there is a dog or cat in the picture, the output is obviously binary. so also the last layer just consists of two outputs which represent the probability for being 1 or 0. But how can I implement the last layer when I want to identify clusters? 
during my research I heard about self organizing maps. would these networks do the job?
thank you\nAnswer: These particles build individual clusters which are already identified
  and labeled; meaning that every particle is already assigned to its
  correct cluster (this assignment is done by a density estimation but
  for my purpose not that relevant).
  the challenge is now to build a network which does this clustering
  after learning from the huge data.

Sounds pretty much like a classification problem to me. Images themselves can build clusters in their image space (e.g. a vector space of dimension width * height * RGB).

since this is not a classification problem but more a identification
  of clusters-challenge what kind of neural network should i use?

You have data of coordinates, you have labels. Start with a simple fully connected single/multi-layer-perceptron i.e. vanilla NN, with as many outputs as number of clusters and softmax-activation function.
There are tons of blogs and tutorials for Deep Learning libraries like keras out there in the internet.",0.0,0.27343118,0.07476460933685303
82,"Question\nI have a dataframe containing the coordinates of millions of particles which I want to use to train a Neural network. These particles build individual clusters which are already identified and labeled; meaning that every particle is already assigned to its correct cluster (this assignment is done by a density estimation but for my purpose not that relevant).
the challenge is now to build a network which does this clustering after learning from the huge data. there are also a few more features in the dataframe like clustersize, amount of particles in a cluster etc. 
since this is not a classification problem but more a identification of clusters-challenge what kind of neural network should i use? I have also problems to build this network: for example a CNN which classifies wheather there is a dog or cat in the picture, the output is obviously binary. so also the last layer just consists of two outputs which represent the probability for being 1 or 0. But how can I implement the last layer when I want to identify clusters? 
during my research I heard about self organizing maps. would these networks do the job?
thank you\nAnswer: If you want to treat clustering as a classification problem, then you can try to train the network to predict whether two points belong to the same clusters or to different clusters.
This does not ultimately solve your problems, though - to cluster the data, this labeling needs to be transitive (which it likely will not be) and you have to label n² pairs, which is expensive.
Furthermore, because your clustering is density-based, your network may need to know about further data points to judge which ones should be connected...",0.20408164,0.09199172,0.012564149685204029
83,"Question\nI am looking to make a python program in which I can have a sidebar GUI along with an interactive 2d pymunk workspace to the right of it, which is to be docked within the same frame.
Does anyone know how I might implement this?\nAnswer: My recommendation is to use pygame as your display. If an object is chosen, you can add it to the pymunk space at the same time as using pymunk to get each body's space and draw it onto the display. This is how I've written my games.",0.0,0.11384928,0.012961658649146557
84,"Question\nI'm using Visual Studio 1.39.2 on Windows 10. I'm very happy that you can run Jupyter Notebook natively through VS Code as of October this year (2019), but one thing I don't get right is how to set my PYTHONPATH prior to booting up a local Jupyter server.
What I want is to be able to import a certain module which is located in another folder (because the module is compiled from C++ code). When I run a normal Python debugging session, I found out that I can set environment variables of the integrated terminal, via the setting terminal.integrated.env.linux. Thus, I set my PYTHNPATH through this option when debugging as normal. But when running a Jupyter Notebook, the local Jupyter server doesn't seem to run in the integrated terminal (at least not from what I can see), so it doesn't have the PYTHONPATH set.
My question is then, how can I automatically have the PYTHONPATH set for my local Jupyter Notebook servers in VS Code?\nAnswer: I'm a developer on this extension. If you have a specific path for module resolution we provide a setting for the Jupyter features called:
Python->Data Science: Run Startup Commands
That setting will run a series of python instructions in any Jupyter session context when starting up. In that setting you could just append that path that you need to sys.path directly and then it will run and add that path every time you start up a notebook or an Interactive Window session.",0.6122449,0.17415333,0.19192422926425934
85,"Question\nI am working on a text adventure with python and the issue i am having is getting spyder to open a interactive cmd window. so far i have tried os.systems('cmd / k') to try and open this which it did but i could not get any code to run and kept getting an app could not run this file error. my current code runs off a import module that pulls the actual adventure from another source code file. how can i make it to where only one file runs and opens the cmd window to play the text adventure?\nAnswer: (Spyder maintainer here) Cmd windows are hidden by default because there are some packages that open lot of them while running code (e.g. pyomo).
To change this behavior, you need to go to
Tools > Preferences > IPython console > Advanced settings > Windows adjustments
and deactivate the option called Hide command line output windows generated by the subprocess module.",0.20408164,0.3699562,0.027514368295669556
86,"Question\nI'm developing something like an API (more like a communications server? Idk what to call it!) to receive data from a POST message from an external app. Basically this other app will encounter an error, then it sends an error ID in a post message to my API, then I send off an email to the affected account. 
My question is how do I handle this in Django without any form of UI or forms? I want this to pretty much be done quietly in the background. At most a confirmation screen that the email is sent.
I'm using a LAMP stack with Python/Django instead of PHP.\nAnswer: A Django view doesn't have to use a form. Everything that was POSTed is there in request.POST which you may access directly. (I commonly do this to see which of multiple submit buttons was clicked).  
Forms are a good framework for validating the data that was POSTed, but you don't have to use their abilities to generate content for rendering. If the data is validated in the front-end, you can use the form validation framework to check against front-end coding errors and malicious POSTs not from your web page, and simply process the cleaned_data if form.is_valid() and do ""Something went wrong"" if it didn't (which you believe to be impossible, modulo front-end bugs or malice).",0.40816328,0.2717746,0.018601873889565468
87,"Question\non pyfpdf documentation it is said that it is possible to specify a format while adding a page (fpdf.add_page(orientation = '', format = '', same = False)) but it gives me an error when specifying a format.
error:

pdf.add_page(format = (1000,100)) TypeError: add_page() got an
  unexpected keyword argument 'format'

i've installed pyfpdf via pip install and setup.py install but it doesnt work in both ways
how can i solve this?\nAnswer: Your problem is that two packages of pypdf exist, fpdf and fpdf2. They both use from fpdf import FPDF, but only fpdf2 has also a format= keyword in the add_page() method.
So you need to install the fpdf2 package.",0.20408164,0.18106717,0.0005296658491715789
88,"Question\nI have a Python program named read.py which reads data from serial communication every second, and  another python program called calculate.py which has to take the real time values from read.py.
Using subprocess.popen('read.py',shell=True) I am able to run read.py from calculate.py
May I know how to read or use the value from read.py in calculate.py?
Since the value changes every second I am confused how to proceed like, saving value in registers or producer consumer type, etc.
for example : from import datetime 
when ever strftime %s is used, the second value is given 
how to use the same technique to use variable from another script?\nAnswer: I can suggest writing values to a.txt file for later reading",0.40816328,0.11620289,0.08524087071418762
89,"Question\nHow do I redeploy an updated version of the Flask web app in Google App Engine.
For example, I have running web app and now there are new features added into it and needs redeployment. How can I do that?
Also how to remove the previous version.\nAnswer: Add --no-promote if you want to deploy without routing service to the latest version deployed.",0.0,0.47487378,0.22550511360168457
90,"Question\nI have a multitude of mature curves (days are plotted on X axis and data is >= 90 days old so the curve is well developed).
Once a week I get a new set of data that is anywhere between 0 and 14 days old.
All of the data (old and new), when plotted, follows a log curve (in shape) but with different slopes. So some weeks have a higher slope, curve goes higher, some smaller slope, curve is lower. At 90 days all curves flatten. 
From the set of ""mature curves"" I need to select the one whose slope matches the best the slope of my newly received date. Also, from the mature curve I then select the Y-value at 90 days and associate it with my ""immature""/new curve.
Any suggestions how to do this? I can seem to find any info. 
Thanks much!\nAnswer: This seems more like a mathematical problem than a coding problem, but I do have a solution.
If you want to find how similar two curves are, you can use box-differences or just differences.
You calculate or take the y-values of the two curves for each x value shared by both the curves (or, if they share no x-values because, say, one has even and the other odd values, you can interpolate those values).
Then you take the difference of the two y-values for every x-value.
Then you sum up those differences for all x-values.
The resulting number represents how different the two curves are.
Optionally, you can square all the values before summing up, but that depends on what definition of ""likeness"" you are using.",0.0,0.23955774,0.05738791078329086
91,"Question\nLet's say we have a string R of length 20000 (or another arbitrary length). I want to get 8 random non-overlapping sub strings of length k from string R.
I tried to partition string R into 8 equal length partitions and get the [:k] of each partition but that's not random enough to be used in my application, and the condition of the method to work can not easily be met.
I wonder if I could use the built-in random package to accomplish the job, but I can't think of a way to do it, how can I do it?\nAnswer: You could simply run a loop, and inside the loop use the random package to pick a starting index, and extract the substring starting at that index. Keep track of the starting indices that you have used so that you can check that each substring is non-overlapping. As long as k isn't too large, this should work quickly and easily.
The reason I mention the size of k is because if it is large enough, then it could be possible to select substrings that don't allow you to find 8 non-overlapping ones. But that only needs to be considered if k is quite large with respect to the length of the original string.",0.0,0.22373807,0.05005872622132301
92,"Question\nI am new to Django. I am using Python 3.7 with Django 2.2.6.
My Django development environment is as the below.

I am using Microsoft Visual Studio Code on a Windows 8.1 computer
To give the commands I am using 'DOS Command Prompt' & 'Terminal window' in 
VS Code.
Created a virtual environment named myDjango
Created a project in the virtual environment named firstProject
Created an app named firstApp.

At the first time I could run the project using >python manage.py runserver
Then I had to restart my computer.

I was able to go inside the previously created virtual environment using
workon myDjango command.

But my problem is I don't know how to go inside the previously created project 'firstProject' and app 'firstApp' using the 'Command prompt' or using the 'VSCode Terminal window'
Thanks and regards,
Chiranthaka\nAnswer: Simply navigate to the folder containing the app you want to manage using the command prompt.
The first cd should contain your parent folder.
The second cd should contain the folder that has your current project.
The third cd should be the specific app you want to work on.
After that, you can use the python manage.py to keep working on your app.",0.13605443,0.12702048,8.1612219219096e-05
93,"Question\ni have a question for you
I'm using the udp socket server using baserequesthandler on python
I want to protect the server against spoofing - source address changes.
Does client_address is the actual ip address of established to server?
If not, how do I get the actual address?\nAnswer: Authenticate the packets so that you know that every message in session X from source address Y is from the same client.
By establishing a shared session key which is then used along with a sequence number to produce a hash of the packet keyed by the (sequence, session_key) pair. Which is then included in every packet. This can be done in both directions protecting both the client and server.
When you receive a packet you use its source address and the session number  to look up the session, then you compute HMAC((sequence, session_key), packet) and check if the MAC field in the message matches. If it doesn't discard the message.
This might not be a correct protocol but it is close enough to demonstrate the principle.",0.0,0.27809066,0.07733441144227982
94,"Question\nDo I need SQLAlchemy if I want to use PostgreSQL with Python Pyramid, but I do not want to use the ORM? Or can I just use the psycopg2 directly? And how to do that?\nAnswer: Even if you do not want to use ORM, you can still use SQLAlchemy's query
language.
If you do not want to use SQLAlchemy, you can certainly use psycopg2 directly. Look into Pyramid cookbook - MongoDB and Pyramid or CouchDB and Pyramid for inspiration.",0.0,0.35399222,0.1253104954957962
95,"Question\nI decided yesterday to do a clean install of Mac OS (as in, erase my entire disk and reinstall the OS).
I am on a Macbook Air 2018. I did a clean install of Mac OS 10.15.1.
I did this clean install due my previous Python environment being very messy.
It was my hope that I could get everything reigned in and installed properly.
I've started reinstalling my old applications, and took care to make sure nothing was installed in a weird location.
However, when I started setting up VS Code, I noticed that my options for Python interpreters showed 4 options. They are as follows:

Python 2.7.16 64-bit, located in /usr/bin/python
Python 2.7.16 64-bit, located in /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python
Python 3.7.3 64-bit, located in /user/bin/python
Python 3.7.3 64-bit, located in /Library/Developer/CommandLineTools/usr/bin/python3

In terminal, if I enter where python python3
it returns 
/usr/bin/python /usr/bin/python3.
How in the world did python3 get there?
My only idea is that it now is included in the Xcode Developer Tools 11.2 package, as I did install that. I cannot find any documentation of this inclusion.
Any ideas how this got here? More importantly, how do I remove it? I want to use Homebrew for all of my installation needs. Also, why does VS Code show 4 options?
Thanks!\nAnswer: The command line tool to run the python 2.7 environment is at /usr/bin/python, but the framework and dependencies for it are in /System. This includes the Python.app bundle, which is just a wrapper for scripts that need to run using the Mac's UI environment.
Although these files are separate executables, it's likely that they point to the same environment.
Every MacOS has these.
Catalina does indeed also include /usr/bin/python3 by default. The first time you run it, the OS will want to download Xcode or the Command line tools to install the 'complete' python3. So these pair are also the same environment",1.0,0.29641855,0.4950268566608429
96,"Question\nI am finished with my project and now I want to put it on my website where people could download it and use it. My project is connected to my MySQL and it works on my machine. On my machine, I can read, and modify my database with python. It obviously, will not work if a person from another country tries to access it. How can I make it so a person from another town, city, or country could access my database and be able to read it?
I tried using SSH but I feel like it only works on a local network.
I have not written a single line of code on this matter because I have no clue how to get started. 
I probably know how to read my database on a local network but I have no clue how to access it from anywhere else.
Any help, tips, or solutions would be great and appreciated. 
Thank you!\nAnswer: If I'm understanding correctly, you want to run a MySQL server from your home PC and allow others to connect and access data? Well, you would need to make sure the correct port is forwarded in your router and firewall, default is TCP 3306. Then simply provide the user with your current IP address (could change).

Determine the correct MySQL Server port being listened on.
Allow port forwarding on the TCP protocol and the port you determined, default is 3306.
Allow incoming connections on this port from software firewall if any.
Provide the user with your current IP Address, Port, and Database name.
If you set login credentials, make sure the user has this as well.
That's it. The user should be able to connect with the IP Address, Port, Database Name, Username, and Password.",0.0,0.11720216,0.013736346736550331
97,"Question\nI was making a subscription payment system from scratch in python in Django. I am using celery-beat for a scheduled task with RabbitMQ as a queue broker. django_celery_beat uses DatabaseScheduler which is causing problems.

Takes a long time to dispatch simple-task to the broker. I was using it to expire users. For some expiration tasks, it took around 60 secs - 150secs. But normally it used to take 100ms to 500ms.
Another problem is that, while I re-schedule some task, while it is being written into the database it blocks the scheduler for some bizarre reason and multiple tasks are missed because of that.

I have been looking into Apache Airflow because it is marketed as an industry-standard scheduling solution.
But I don't think, it is applicable and feasible for my small project.
If you have worked and played with a subscription payment system, can you advise me how to go forward with this?\nAnswer: I have a long winding solution that I implemented for a similar project.

First I save the schedule as a model in the database
Next I have a cron job that gets the entries that need to be run for that day
Then schedule those jobs as normal Celery jobs by setting the ETA based on the time set in the schedule model.

This way Celery just runs off the messages from Redis in my case. Try it if don't get a direct answer.",0.0,0.13072228,0.01708831638097763
98,"Question\nI want to customise the functions that process the results of completing the flask-user registration and login forms. I know how to customise the html forms themselves, but I want to change how flask-user performs the registration process. For example, I want to prevent the flask-user login and registration process from creating flash messages and I want registration to process a referral code.
I understand how to add an _after_registration_hook to perform actions after the registration function has completed, but, this doesn't allow me to remove the flash messages that are created in the login and registration processes.
My custom login and registration processes would build on the existing flask-user login and registration functions with functionality added or removed.\nAnswer: You seem to be asking about the flask-user package - however you tagged this with flask-security (which is a different package but offers similar functionality). I can answer for flask-security-too (my fork of the original flask-security) - if you are/want to use flask-user - it might be useful to change your tags.
In a nutshell - for flask-security - you can turn off ALL flashes with a config variable.
For registration - you can easily override the form and add a referral code - and validate/process that as part of form validation.",0.0,0.44860137,0.20124319195747375
99,"Question\nI use Python as a high-level wrapper and a loaded C++ kernel in the form of a binary library to perform calculations. I debug high level Python code in IDE Eclipse in the usual way, but how do I debug C++ code?
Thank you in advance for your help.\nAnswer: Try using gdb's ""attach "" command (or ""gdb -p "" command-line option) to attach to the python process that has the C++ kernel library loaded.",0.40816328,0.36977446,0.001473701442591846
0,"Question\nI have added the C:\Users\Admin\Anaconda3\python.exe path to my system environment variables PATH but still when I run python command it opens Windows app store! Why bthis happens and how to fix it?\nAnswer: the PATH variable should contain 
C:\Users\Admin\Anaconda3 
not 
C:\Users\Admin\Anaconda3\python.exe",0.40816328,0.25994527,0.021968577057123184
1,"Question\nI am trying to automate downloading of certain csv files from a website.
This is how I manually do it:

I log in to the website.
Click on the button export as csv.
The file gets downloaded. 

The problem is the button does not have any link to it so I was not able to automate it using wget or requests.\nAnswer: You can use selenium in python. There is an option to click using ""link text"" or ""partial link text"". It is quite easy and efficient.
driver.findElement(By.linkText(""click here"")).click()
It kind of looks like this.",0.0,0.36219746,0.13118699193000793
2,"Question\nLast thing I did was pip install boto3 and fastai through git bash yesterday. I can't imagine if anything else could have had any influence.
I have been using python for a few months now, but today it stopped running. I opened my
Sublime Text - and after running some simple code I got:
""Python was not found but can be installed from the Microsoft Store"". 
GIT bash:

$ python --version bash:
  /c/Users/.../AppData/Local/Microsoft/WindowsApps/python: Permission
  denied

But if I open up a file of python 3 in this link:

C:\Users...\AppData\Roaming\Microsoft\Windows\Start
  Menu\Programs\Python 3.7

My Python works.
I think I have to redirect my main python file from the first link directory to the second and have no clue how to do this, that my Git and Sublime would be able to pick on this.\nAnswer: So, I gave up and just installed the recommended link from Microsoft store. So now I possibly have 4 pythons with 2 different versions in 3 locations, but hey.... it works :)
Regarding a comment below my first questions:
When I run $ ls -l which python in GITbash, it gives:
-rwxr-xr-x 1... 197121 97296 Mar 25  2019 /c/Users/.../AppData/Local/Programs/Python/Python37-32/python*

/.../ is just my user name

Yesterday I tried that as well, the start was identical, although I can't really remember the link, if it was the same.",0.0,0.032523632,0.0010577866341918707
3,"Question\nI have been working on a project built with Django. When I run profiler due to slowness of a page in project, this was a line of the result:

10    0.503    0.050    0.503    0.050 {method'recv_into' of '_socket.socket' objects}

Which says almost 99% of passed time was for the method recv_into(). After some research, I learned the reason is Nagel's algorithm which targets to send packets only when the buffer is full or there are no more packets to transmit. I know I have to disable this algorithm and use TCP_NODELAY but I don't know how, also it should only affect this Django project.
Any help would be much appreciated.\nAnswer: Are you using cache settings in the settings.py file? Please check that maybe you have tcp_nodelay enable there, if so then remove it or try to clear browser cache.",-0.17857143,0.16481292,0.11791281402111053
4,"Question\nAm a beginner on Python (self studying) and got introduced to Lambda (nameless) function but I am unable to deduce the below expression for Fibonacci series (got from Google) but no explanation available online (Google) as to how this is evaluated (step by step). Having a lot of brain power here, I thought somebody can help me with that. Can you help evaluate this step by step and explain? 
lambda n: reduce(lambda x, _: x+[x[-1]+x[-2]],range(n-2), [0, 1])
Thanks in advance.
(Thanks xnkr, for the suggestion on a reduce function explained and yes, am able to understand that and it was part of the self training I did but what I do not understand is how this works for lambda x, _ :  x+[x[-1]+x[-2]],range(n-2), [0, 1]. It is not a question just about reduce but about the whole construct - there are two lambdas, one reduce and I do not know how the expression evaluates to. What does underscore stand for, how does it work, etc)
Can somebody take the 2 minutes that can explain the whole construct here?\nAnswer: Break it down piece by piece:
lambda n: - defines a function that takes 1 argument (n); equivalent to an anonymous version of: def somefunc(n):
reduce() - we'll come back to what it does later; as per docs, this is a function that operates on another function, an iterable, and optionally some initial value, in that order. These are:

A) lambda x, _: - again, defines a function. This time, it's a function of two arguments, and the underscore as the identifier is just a convention to signal we're not gonna use it.
B) X + [ <stuff> ] - prepend some list of stuff with the value of the first arg. We already know from the fact we're using reduce that the arg is some list.
C) The <stuff> is x[-1] + x[-2] - meaning the list we're prepending our X to is, in this case, the sum of the last two items already in X, before we do anything to X in this iteration.
range(n-2) is the iterable we're",1.0,-0.14085603,1.3015525341033936
5,"Question\nI have a JSON file whose size is about 5GB. I neither know how the JSON file is structured nor the name of roots in the file. I'm not able to load the file in the local machine because of its size So, I'll be working on high computational servers. 
I need to load the file in Python and print the first 'N' lines to understand the structure and Proceed further in data extraction. Is there a way in which we can load and print the first few lines of JSON in python?\nAnswer: You can use the command head to display the N first line of the file. To get a sample of the json to know how is it structured.
And use this sample to work on your data extraction.
Best regards",-0.35714287,-0.061038017,0.08767808228731155
6,"Question\nThere is a server that contains a json dataset that I need 
I can manually use chrome to login
to the url and use chrome developer tool to read the request header for said json data
I determined that the minimum required headers that should be sent to the json endpoint are ['cookie', 'x-xsrf-token', 'user-agent']
I don't know how I can get these values so that I can automate fetching this data. I would like to use request module to get the data
I tried using selenium, to navigate to the webpage that exposes these header values, but cannot get said headers values (not sure if selenium supports this)
Is there a way for me to use request module to inch towards getting these header values...by following the request header ""bread crumbs"" so to speak?
Is there an alternative module that excels at this?
To note, I have used selenium to get the required datapoints successfully, but selenium is resource heavy and prone to crash; 
By using the request module with header values greatly simplifies the workflow and makes my script reliable\nAnswer: Based on pguardiario's comment
Sessions cookies and csrf-token are provided by the host when a request is made against the Origin url. These values are needed to make subsequent requests against the endpoint with the JSON payload. By using request.session() against the Origin url, and then updating the header when using request.get(url, header). I was able to access the json data",0.0,-0.20480174,0.0419437512755394
7,"Question\nI am trying to create a regex expression in Python for non-hyphenated words but I am unable to figure out the right syntax.
The requirements for the regex are:

It should not contain hyphens AND 
It should contain atleast 1 number

The expressions that I tried are:= 

^(?!.*-)


This matches all non-hyphenated words but I am not able to figure out how to additionally add the second condition.


^(?!.*-(?=/d{1,}))


I tried using double lookahead but I am not sure about the syntax to use for it. This matches ID101 but also matches STACKOVERFLOW

Sample Words Which Should Match:
1DRIVE, ID100, W1RELESS
Sample Words Which Should Not Match:
Basically any non-numeric string (like STACK, OVERFLOW) or any hyphenated words (Test-11, 24-hours)
Additional Info:
I am using library re and compiling the regex patterns and using re.search for matching.
Any assistance would be very helpful as I am new to regex matching and am stuck on this for quite a few hours.\nAnswer: I came up with -
^[^-]*\d[^-]*$

so we need at LEAST one digit (\d)
We need the rest of the string to contain anything BUT a - ([^-])
We can have unlimited number of those characters, so [^-]*
but putting them together like [^-]*\d would fail on aaa3- because the - comes after a valid match- lets make sure no dashes can sneak in before or after our match ^[-]*\d$
Unfortunately that means that aaa555D fails. So we actually need to add the first group again- ^[^-]*\d[^-]$  --- which says start - any number of chars that aren't dashes - a digit - any number of chars that aren't dashes - end
Depending on style, we could also do ^([^-]*\d)+$ since the order of the digits/numbers dont matter, we can have as many of those as we want.

However, finally... this is how I would ACTUALLY solve this particular problem, since regexes may",0.40816328,0.3022413,0.011219467036426067
8,"Question\nI am writing a python code which writes a hyperlink into a excel file.This hyperlink should open in a specific page in a pdf document.
I am trying something like
Worksheet.write_url('A1',""C:/Users/...../mypdf#page=3"") but this doesn't work.Please let me know how this can be done.\nAnswer: Are you able to open the pdf file directly to a specific page even without xlsxwriter? I can not.
From Adobe's official site:

To target an HTML link to a specific page in a PDF file, add
  #page=[page number] to the end of the link's URL.
For example, this HTML tag opens page 4 of a PDF file named
  myfile.pdf:

Note: If you use UNC server locations (\servername\folder) in a link,
  set the link to open to a set destination using the procedure in the
  following section. 
If you use URLs containing local hard drive addresses (c:\folder), you cannot link to page numbers or set destinations.",0.40816328,0.31966245,0.007832396775484085
9,"Question\nSo I’m a new programmer and starting to use Python 3, and I see some videos of people teaching the language and use “import”. My question is how they know what to import and where you can see all the things you can import. I used import math in one example that I followed along with, but I see other videos of people using import JSON or import random, and I’m curious how they find what they can import and how they know what it will do.\nAnswer: In all programming languages, whenever you actually need a library, you should import it. For example, if you need to generate a random number, search for this function in your chosen programming language, find the appropriate library, and import it into your code.",0.13605443,0.14689648,0.00011755016021197662
10,"Question\nDoes anyone know how to make Spyder stop automatically inserting closing brackets?
It often results in complete mess when you have multiple levels of different brackets. I had a look around and could only find posts about auto-closing quotes, but I'm not really interested in these. But those brackets are making me slightly miserable.
I had a look in Preferences but the closest I could find is 'Automatic code completion'. But I certainly don't want all of it off especially when working with classes.\nAnswer: In Spyder 4 and Spyder 5 go to:

Tools - Preferences - Editor - Source code

and deselect the following items:

Automatic insertion of parentheses, braces and brackets

Automatic insertion of closing quotes (since it's the same nuisance than with brackets)",1.0,0.35219336,0.41965344548225403
11,"Question\nLet's say I have a Hash-table, each key is defined as a tuple with 4 integers (A, B, C, D). where are integers represent a quantity of a certain attribute, and its corresponding value is a tuple of gears that satisfy (A, B, C, D).
I wanted to write a program that do the following: with any given attribute tuple (x, y, z, w), I want to find all the keys satisfying (|A - x| + |B - y| + |C - z| + |D - w|) / 4 <= i where i is a user defined threshold; return the value of these keys if exist and do some further calculation.  (|A - x| means the absolute value of A - x)
To my experience, this kind of thing can be better done with Answer set programming, Haskell, Prolog and all this kind of logical programming languages, but I'm forced to use python for this is a python project...
I can hard code for a particular ""i"" but I really have no idea how to do this for arbitrary integers. please tell me how I can do this in pure python, Thank you very much!!!!\nAnswer: Just write a function that loops over all values in the table and checks them one by one.  The function will take the table and i as arguments.",0.20408164,-0.043334246,0.0612146221101284
12,"Question\nI am trying to accomplish the following task using Airflow. I have an address and I want to run 3 different tasks taskA, taskB, taskC. Each task returns True if the address was detected. Store the times when each of the functions detected the address.
I want to accomplish the below logic.

Run all three tasks to start off with. 
If any of them return True, store the current time.
Wait for 1 minute and rerun only the tasks that did not return True.
If all have returned True end the job. 

I am not sure how I can accomplish selectively running only those tasks that returned False from the last run. 
I have so far looked at the BranchPythonOperator but I still haven't been able to accomplish the desired result.\nAnswer: You can get last run status value from airflow db.",0.0,0.11512333,0.013253381475806236
13,"Question\nI am unable to download a.py file from an email. I get the error ""file not supported."" The file was saved from a Jupyter-notebook script.
I have Python 3.6.6 and Jupyter downloaded on my Windows 10 laptop and tried to access the file through Chrome and through my computer's email app, but this didn't resolve the problem.
Any ideas on how to make the file compatible with my computer?
EDIT: I had to have the.ipynb file sent rather than the.py file.\nAnswer: Generally email providers block any thing which can possibly execute on clients machine.
Best option to share will be transfer via email will be.py.txt 
or any cloud drives.",0.0,0.14726865,0.021688055247068405
14,"Question\nfor an academic project, I am currently using the python framework jam.py 5.4.83 to develop a back office for a new company. 
I would like to use views instead of tables for reporting but I don't find how to do it, I can only import data from tables.
So if someone already used this framework, I would be very thankful.
Regards,
Yoan\nAnswer: the use of database views are not supported in Jam.py
However, you can import tables as read only if used for reporting.
Than you can build Reports as you would.
Good luck.",0.0,0.18097454,0.03275178372859955
15,"Question\nLet's say there are two sub images of a large image. I am trying to detect the overlapping area of two sub images. I know that template matching can help to find the templates. But i'm not sure how to find the intersected area and remove them in either one of the sub images. Please help me out.\nAnswer: MatchTemplate returns the most probable position of a template inside a picture. You could do the following steps:

Find the (x,y) origin, width and height of each picture inside the larger one
Save them as rectangles with that data(cv::Rect r1, cv::Rect r2)
Using the & operator, find the overlap area between both rectangles (r1&r2)",0.20408164,0.3294735,0.01572311669588089
16,"Question\nI have this numpy matrix:
x = np.random.randn(700,2)
What I wanna do is scale the values of the first column between that range: 1.5 and 11 and the values of the second column between `-0.5 and 5.0. Does anyone have an idea how I could achieve this? Thanks in advance\nAnswer: subtract each column's minimum from itself
for each column of the result divide by its maximum
for column 0 of that result multiply by 11-1.5
for column 1 of that result multiply by 5--0.5
add 1.5 to column zero of that result
add -0.5 to column one of that result

You could probably combine some of those steps.",0.40816328,0.184291,0.05011879652738571
17,"Question\nI'm using MAC OS Catalina Version 10.15.1 and I'm working on a python project. Every time I use the command ""import OS"" on the command line Version 2.10 (433), I get this message: zsh: command not found: import. I looked up and followed many of the solutions listed for this problem but none of them have worked. The command worked prior to upgrading my MAC OS. Any suggestion on how to fix it?\nAnswer: The file is being interpreted as zsh, not a python. I suggest you to add this to the first line:
#!/usr/bin/env python",0.20408164,0.30747068,0.010689293965697289
18,"Question\nI'm using MAC OS Catalina Version 10.15.1 and I'm working on a python project. Every time I use the command ""import OS"" on the command line Version 2.10 (433), I get this message: zsh: command not found: import. I looked up and followed many of the solutions listed for this problem but none of them have worked. The command worked prior to upgrading my MAC OS. Any suggestion on how to fix it?\nAnswer: Don't capitalize it.
import os",0.0,0.13544697,0.01834588125348091
19,"Question\nI want to start the conda Prompt from cmd, because I want to use the promt as a terminal in Atom.io.
There is no Conda.exe and the path to conda uses cmd to jump into the prompt. But how do I start it inside of cmd?\nAnswer: I guess what you want is to change to Anaconda shell using cmd, you can find the address for your Anaconda and run the following in your cmd:
%windir%\System32\cmd.exe ""/K"" ""Address""\anaconda3
Or, you can find your Anaconda prompt shortcut, right click on that, and open its properties window. In the properties window, find Target. Then, copy the whole thing in Target and paste it into your cmd.",0.20408164,0.33799517,0.0179328341037035
20,"Question\nI'm worried about the security of my web app, I'm using Django and sometimes I use AJAX to call a Django url that will execute code and then return an HttpResponse with a message according the result, the user never notice this as it's happening in background.
However, if the url I'm calling with AJAX is, for example, ""runcode/"", and the user somehow track this and try to send a request to my domain with that url (something like ""www.example.com/runcode/""), it will not run the code as Django expects the csrf token to be send too, so here goes the question.
It is possible that the user can obtain the csrf token and send the POST?, I feel the answer for that will be ""yes"", so anyone can help me with a hint on how to deny these calls if they are made without the intended objective?\nAnswer: Not only django but this behavior is common in all others,
You can only apply 2 solution,

Apply CORS and just allow your domain, to block other domain to access data from your API response, but this will not effective if a user direct call your API end-point.
As lain said in comment, If data is sensitive or user's personal, add authentication in API. 

Thanks",0.0,-0.04200864,0.001764725660905242
21,"Question\nI am currently trying to automate a process using Selenium with python, but I have hit a roadblock with it. The list is part of a list which is under a tree. I have identified the base of the tree with the following xpath 
item = driver.find_element_by_xpath(""//*[@id='filter']/ul/li[1]//ul//li"")
items = item.find_elements_by_tag_name(""li"")
I am trying to Loop through the ""items"" section but need and click on anything with an ""input"" tag
for k in items:
    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((k.find_element(By.TAG_NAME, ""input"")))).click()
When execute the above I get the following error:
""TypeError: find_element() argument after * must be an iterable, not WebElement""
For some reason.click() will not work if I use something like the below.
k.find_element_by_tag_name(""input"").click()
it only works if i use the webdriverwait. I have had to use the web driver wait method anytime i needed to click something on the page.
My question is:
What is the syntax to replicate items = item.find_elements_by_tag_name(""li"")
for WebDriverWait(driver, 10).until(EC.element_to_be_clickable((k.find_element(By.TAG_NAME, ""input"")))).click()
i.e how do I use a base path and append to the using the private methods find_elements(By.TAG_NAME)
Thanks in advance\nAnswer: I have managed to find a work around and get Selenium to do what i need. 
I had to call the  javascript execution, so instead of trying to get
WebDriverWait(driver, 10).until(EC.element_to_be_clickable((k.find_element(By.TAG_NAME, ""input"")))).click()  to work, i just used
driver.execute_script(""arguments[0].click();"", k.find_element_by_tag_name(""input""))
Its doing exactly what I needed it to do.",0.0,0.2080462,0.04328322038054466
22,"Question\nI wish to use the latest version of SQLite3 (3.30.1) because of its new capability to handle SQL 'ORDER BY... ASC NULLS LAST' syntax as generated by the SQLAlchemy nullslast() function.
My application folder env\Scripts contains the existing (old) version of sqlite3.dll (3.24), however when I replace it, there is no effect. In fact, if I rename that DLL, the application still works fine with DB accesses.
So, how do I update the SQLite version for an application?
My environment:
Windows 10, 64-bit (I downloaded a 64-bit SQlite3 DLL version). I am running with pyCharm, using a virtual env.\nAnswer: I have found that the applicable sqlite3.dll is determined first by a Windows OS defined lookup. It first goes through the PATH variable, finding and choosing the first version it finds in any of those paths.
In this case, probably true for all pyCharm/VirtualEnv setups, a version found in my user AppData\Local\Programs\Python\Python37\DLLs folder was selected.
When I moved that out of the way, it was able to find the version in my env\Scripts folder, so that the upgraded DLL was used, and the sQLAlchemy nullslast() function did its work.",0.0,0.14279902,0.02039155922830105
23,"Question\nI developed a program to interact between Telegram and other 3rd party Software. It's written in Python and I used the Telethon library. 
Everything works fine, but since it uses my personal configuration including API ID, API hash, phone number and username, I would like to know how to handle all of this if I wanted to distribute the software to other people. 
Of course they can't use my data, so should they login into Telegram development page and get all the info? Or, is there a more user-friendly way to do it?\nAnswer: Since the API ID and the API Hash in Telegram are supposed to be distributed with your client all you need to do is prompt the user for their Phone Number. 
You could do this using a GUI Library (like PySide2 using QInputDialog) or if it is a command line application using input(). Keep in mind that the user will also need a way to enter the code they receive from Telegram and their 2FA Password if set.",0.40816328,0.19858056,0.04392491653561592
24,"Question\nSo i am trying to Pack my python script with pyarmor pack however, when i pack the script it does not work, it throws check restrict mode failed. If i Obfuscate the script normally with pyarmor obfuscate instead of pack the script it works fine, and is obfuscated fine. This version runs no problem. Wondering how i can get pack to work as i want my python file in an exe
I have tried to compile the obfuscated script with pyinstaller however this does not work either
Wondering what else i can try?\nAnswer: I had this problem, fixed by adding --restrict=0
For example: pyarmor obfuscate --restrict=0 app.py",0.40816328,0.05146283,0.12723520398139954
25,"Question\nI have a dataframe with 5000 items(rows) and 2048 features(columns). 
Shape of my dataframe is (5000, 2048).
when I calculate cosine matrix using pairwise distance in sklearn, I get (5000,5000) matrix. 
Here I can compare each other. 
But now If I have a new vector shape of (1,2048), how can find cosine similarity of this item with early dataframe which I had, using (5000,5000) cosine matrix which I have already calculated? 
EDIT 
PS: I can append this new vector to my dataframe and calculate again cosine similarity. But for large amount of data it gets slow. Or is there any other fast and accurate distance metrics?\nAnswer: The initial (5000,5000) matrix encodes the similarity values of all your 5000 items in pairs (i.e. symmetric matrix).
To have the similarities in case of a new item, concatenate and make a (5001, 2048) matrix and then estimate similarity again to get (5001,5001)
In other words, you can not directly use the (5000,5000) precomputed matrix to get the similarity with the new (1,2048) vector.",0.0,0.22415811,0.05024685710668564
26,"Question\nI have a dataframe with 5000 items(rows) and 2048 features(columns). 
Shape of my dataframe is (5000, 2048).
when I calculate cosine matrix using pairwise distance in sklearn, I get (5000,5000) matrix. 
Here I can compare each other. 
But now If I have a new vector shape of (1,2048), how can find cosine similarity of this item with early dataframe which I had, using (5000,5000) cosine matrix which I have already calculated? 
EDIT 
PS: I can append this new vector to my dataframe and calculate again cosine similarity. But for large amount of data it gets slow. Or is there any other fast and accurate distance metrics?\nAnswer: Since cosine similarity is symmetric. You can compute the similarity meassure with the old data matrix, that is similarity between the new sample (1,2048) and old matrix (5000,2048) this will give you a vector of (5000,1) you can append this vector into the column dimension of the pre-computed cosine matrix making it (5000,5001) now since you know the cosine similarity of the new sample to itself. you can append this similarity to itself, back into the previously computed vector making it of size (5001,1), this vector you can append in the row dimension of the new cosine matrix that will make it (5001,5001)",0.0,0.5138902,0.2640831470489502
27,"Question\nMy actual problem is that python sqlite3 module throws database disk image malformed.
Now there must be a million possible reasons for that. However, I can provide a number of clues:

I am using python multiprocessing to spawn a number of workers that all read (not write) from this DB
The problem definitely has to do with multiple processes accessing the DB, which fails on the remote setup but not on the local one. If I use only one worker on the remote setup, it works
The same 6GB database works perfectly well on my local machine. I copied it with git and later again with scp to remote. There the same script with the copy of the original DB gives the error
Now if I do PRAGMA integrity_check on the remote, it returns ok after a while - even after the problem occurred
Here are the versions (OS are both Ubuntu):

local:  sqlite3.version >>> 2.6.0, sqlite3.sqlite_version >>> 3.22.0
remote: sqlite3.version >>> 2.6.0, sqlite3.sqlite_version >>> 3.28.0


Do you have some ideas how to allow for save ""parallel"" SELECT?\nAnswer: The problem was for the following reason (and it had happened to me before):
Using multiprocessing with sqlite3, make sure to create a separate connection for each worker!
Apparently this causes problems with some setups and sometimes doesn't.",0.0,0.20498121,0.04201729595661163
28,"Question\nI have GitHub Actions that build and test my Python application. I am also using pytest-cov to generate a code coverage report. This report is being uploaded to codecov.io.
I know that codecov.io can't fail your build if the coverage lowers, so how do I go about with GitHub Actions to fail the build if the coverage drops? Do I have to check the previous values and compare with the new ""manually"" (having to write a script)? Or is there an existing solution for this?\nAnswer: There is nothing built-in, instead you should use one of the many integrations like sonarqube, if I don’t want to write a custom script.",0.0,0.22675854,0.05141943693161011
29,"Question\nI know in conda I can use conda env list to get a list of all conda virtual environments, what's the corresponding command in python venv that can list all the virtual environments in a given venv? also, is there any way I can print/check the directory of current venv? somehow I have many projects that have same name.venv for their virtual environment and I'd like to find a way to verify which venv I'm in. Thanks\nAnswer: Virtual environments are simple a set of files in a directory on your system. You can find them the same way you would find images or documents with a certain name. For example, if you are using Linux or macOS, you could run find / | grep bin/activate in terminal. Not too sure about Windows but I suspect you can search for something similar in Windows Explorer.",0.0,0.28095156,0.07893377542495728
30,"Question\nI know in conda I can use conda env list to get a list of all conda virtual environments, what's the corresponding command in python venv that can list all the virtual environments in a given venv? also, is there any way I can print/check the directory of current venv? somehow I have many projects that have same name.venv for their virtual environment and I'd like to find a way to verify which venv I'm in. Thanks\nAnswer: I'm relatively new to python venv as well. I have found that if you created your virtual environment with python -m venv <yourvenvname> with in a project folder.
I'm using windows, using cmd, for example, you have a Dash folder located in C:\Dash, when you created a venv called testenv by
python -m venv testenv,
you can activate the virtual environment by just input
C:\Dash\testenv\Scripts\activate,
then you can deactivate it by just type in deactivate.
If you want to list the venv that you have, you go to the C:\Dash folder. type
dir
in cmd, it will list the list of the virtual env you have, similar to conda env list. if you want to delete that virtual env, simply do
rm -rf testenv
you can list the packages installed within that venv by doing
pip freeze.
I hope this helps. please correct me if I'm wrong.",0.20408164,0.1014961,0.010523793287575245
31,"Question\nwe are currently messing around with an Slamtec RPLidar A1. We have a robot with the lidar mounted on it. 
Our aim is to retrieve a x and y position in the room. (it's a closed room, could have more than 4 corners but the whole room should be recognized once (it does not matter where the RPlidar stands).
Anyway the floormap is not given.
We got so far a x and y position with BreezySLAM but we recognized, wherever the RPlidar stands, it always sees itself as center, so we do not really know how to retrieve correct x and y from this information.
We are new to this topic and maybe someone can give us a good hint or link to find a simple solution.
PS: We are not intending to track the movement of the robot.\nAnswer: Any sensor sees itself in the center of the environment. The idea of recording map is good one, if not. You can presume that any of the corners can be your zero points, you your room is not square, you can measure length of the wall and and you down to 2 points. Unfortunately if you don't have any additional markers in the environment or you can't create map before actual use, I'm afraid there is no chance for robot to correctly understand where is desired (0,0) point is.",0.0,0.13879949,0.019265297800302505
32,"Question\nHow can I quickly check what are the possible inputs to a specific function? For example, I want to plot a histogram for a data frame: df.hist(). I know I can change the bin size, so I know probably there is a way to give the desired bin size as an input to the hist() function. If instead of bins = 10 I use df.hist(bin = 10), Python obviously gives me an error and says hist does not have property bin. 
I wonder how I can quickly check what are the possible inputs to a function.\nAnswer: Since your question tag contains jupyter notebook I am assuming you are trying on it. So in jupyter notebook 2.0 Shift+Tab would give you function arguments.",0.30612245,0.024992585,0.0790340006351471
33,"Question\nI am trying to make a calculator (with matrix calculation also). I want to make interface in JavaScript and calculation stuff in Python. But I don't know how to send parameters from python to JavaScript and from JavaScript to python.
Edit: I want to send data via JSON (if possible).\nAnswer: You would have to essentially set both of them up as API's and access them via endpoints. 
For Javascript, you can use node to set up your API endpoint, and for Python use Flask.",0.20408164,0.33870125,0.018122438341379166
34,"Question\nI know that other languages like Javascript have 
a == and ===  operator also they have a!= and!== operator does Python also has a === and!== (ie. a single operator that checks the type and compares the value at the same time like === operator) and if not how can we implement it.\nAnswer: No, and you can't really implement it yourself either.
You can check the type of an object with type, but if you just write a function that checks type(x) is type(y) and x == y, then you get results like [1] and [1.0] showing up as equivalent. While that may fulfill the requirements you stated, I've never seen a case where this wasn't an oversight in the requirements.
You can try to implement your own deep type-checking comparison, but that requires you to know how to dig into every type you might have to deal with to perform the comparison. That can be done for the built-in container types, but there's no way to make it general.
As an aside, is looks vaguely like what you want if you don't know what is does, but it's actually something entirely different. is checks object identity, not type and value, leading to results like x = 1000; x + 1 is not 1001.",0.08163265,0.4217744,0.11569640785455704
35,"Question\nBasically i am trying to do text summarize using spacy and nltk in python. Now i want to summarize the normal 6-7 lines text and show the summarized text on the localhost:xxxx so whenever i run that python file it will show on the localhost. 
Can anyone tell is it possible or not and if it is possible how to do this. Since there would be no databse involved.\nAnswer: You have to create a RESTFUL Api using FLASK or DJAngo with some UI elements and call your model. Also you can use displacy( Spacy UI Bro) Directly on your system.",0.0,0.17156255,0.029433708637952805
36,"Question\nGiven an array of integers, I need to reduce it to a single number by repeatedly replacing any two numbers with their difference, to produce the maximum possible result.
Example1 - If I have array of [0,-1,-1,-1] then performing (0-(-1)) then (1-(-1)) and then (2-(-1)) will give 3 as maximum possible output
Example2- [3,2,1,1] we can get maximum output as 5 { first (1-1) then (0-2) then (3-(-2)}
Can someone tell me how to solve this question?\nAnswer: The other answers are fine, but here's another way to think about it:
If you expand the result into individual terms, you want all the positive numbers to end up as additive terms, and all the negative numbers to end up as subtractive terms.
If you have both signs available, then this is easy:

Subtract all but one of the positive numbers from a negative number
Subtract all of the negative numbers from the remaining positive number

If all your numbers have the same sign, then pick the one with the smallest absolute value at treat it as having the opposite sign in the above procedure.  That works out to:

If you have only negative numbers, then subtract them all from the least negative one; or
If you have only positive numbers, then subtract all but one from the smallest, and then subtract the result from the remaining one.",0.0,0.2682445,0.0719551146030426
37,"Question\nIn my dataset X I have two continuous variables a, b and two boolean variables c, d, making a total of 4 columns. 
I have a multidimensional target y consisting of two continuous variables A, B and one boolean variable C. 
I would like to train a model on the columns of X to predict the columns of y. However, having tried LinearRegression on X it didn't perform so well (my variables vary several orders of magnitude and I have to apply suitable transforms to get the logarithms, I won't go into too much detail here). 
I think I need to use LogisticRegression on the boolean columns. 
What I'd really like to do is combine both LinearRegression on the continuous variables and LogisticRegression on the boolean variables into a single pipeline. Note that all the columns of y depend on all the columns of X, so I can't simply train the continuous and boolean variables independently.
Is this even possible, and if so how do I do it?\nAnswer: If your target data Y has multiple columns you need to use multi-task learning approach. Scikit-learn contains some multi-task learning algorithms for regression like multi-task elastic-net but you cannot combine logistic regression with linear regression because these algorithms use different loss functions to optimize. Also, you may try neural networks for your problem.",0.0,0.13525772,0.018294651061296463
38,"Question\nIn my dataset X I have two continuous variables a, b and two boolean variables c, d, making a total of 4 columns. 
I have a multidimensional target y consisting of two continuous variables A, B and one boolean variable C. 
I would like to train a model on the columns of X to predict the columns of y. However, having tried LinearRegression on X it didn't perform so well (my variables vary several orders of magnitude and I have to apply suitable transforms to get the logarithms, I won't go into too much detail here). 
I think I need to use LogisticRegression on the boolean columns. 
What I'd really like to do is combine both LinearRegression on the continuous variables and LogisticRegression on the boolean variables into a single pipeline. Note that all the columns of y depend on all the columns of X, so I can't simply train the continuous and boolean variables independently.
Is this even possible, and if so how do I do it?\nAnswer: What i understand you want to do is to is to train a single model that both predicts a continuous variable and a class. You would need to combine both loses into one single loss to be able to do that which I don't think is possible in scikit-learn. However I suggest you use a deep learning framework (tensorflow, pytorch, etc) to implement your own model with the required properties you need which would be more flexible. In addition you can also tinker with solving the above problem using neural networks which would improve your results.",0.0,0.13887191,0.019285406917333603
39,"Question\nHi I have a built up Python 2.7 environment with Ubuntu 19.10. 
I would like to build a whl package for pandas.
I pip installed the pandas but do not know how to pack it into whl package.
May I ask what I should do to pack it.
Thanks\nAnswer: You cannot pack back an installed wheel. Either you download a ready-made wheel with pip download or build from sources: python setup.py bdist_wheel (need to download the sources first).",0.0,0.13636416,0.01859518513083458
40,"Question\nI'm making a game with Pygame, and now I stuck on how to process collision between player and wall. This is 2D RPG with cells, where some of them are walls. You look on world from top, like in Pacman.
So, I know that i can get list of collisions by pygame.spritecollide() and it will return me list of objects that player collides. I can get ""collide rectangle"" by player.rect.clip(wall.rect), but how I can get player back from the wall?
So, I had many ideas. The first was push player back in opposite direction, but if player goes, as example, both right and bottom directions and collide with vertical wall right of itself, player stucks, because it is needed to push only left, but not up.
The second idea was implement diagonally moving like one left and one bottom. But in this way we don't now, how move first: left or bottom, and order becomes the most important factor.
So, I don't know what algorithm I should use.\nAnswer: If you know the location of the centre of the cell and the location of the player you can calculate the x distance and the y distance from the wall at that point in time. Would it be possible at that point to take the absolute value of each distance and then take the largest value as the direction to push the player in.
e.g. The player collides with the right of the wall so the distance from the centre of the wall in the y direction should be less than the distance in x.
Therefore you know that the player collided with the left or the right of the wall and not the top, this means the push should be to the right or the left.
If the player's movement is stored as in the form [x, y] then knowing whether to push left or right isn't important since flipping the direction of movement in the x axis gives the correct result.
The push should therefore be in the x direction in this example
e.g. player.vel_x = -player.vel_x.
This would leave the movement in the y axis unchanged so hopefully wouldn't result in the problem you mentioned.
Does that help?",0.0,0.2110781,0.0445539690554142
41,"Question\nThank you all for always willing to help.
I have a Django app with countries and state choices fields. However, I have no idea whatsoever on how to load the related states for each country. What I mean here is, if I choose ""Nigeria"" in the list of countries, how can I make all Nigerian states to automatically load in the state choice field?\nAnswer: You have to create many to many field state table, then you can multiple select state as per country.
this feature available on django- country package or django- cities package.",0.0,0.23283261,0.054211024194955826
42,"Question\nI have a big dataset containing almost 0.5 billions of tweets. I'm doing some research about how firms are engaged in activism and so far, I have labelled tweets which can be clustered in an activism category according to the presence of certain hashtags within the tweets.
Now, let's suppose firms are tweeting about an activism topic without inserting any hashtag in the tweet. My code won't categorized it and my idea was to run a SVM classifier with only one class.
This lead to the following question:

Is this solution data-scientifically feasible?
Does exists any other one-class classifier?
(Most important of all) Are there any other ways to find if a tweet is similar to the ensable of tweets containing activism hashtags?

Thanks in advance for your help!\nAnswer: Sam H has a great answer about using your dataset as-is, but I would strongly recommend annotating data so you have a few hundred negative examples, which should take less than an hour. Depending on how broad your definition of ""activism"" is that should be plenty to make a good classifier using standard methods.",0.20408164,0.173558,0.0009316927753388882
43,"Question\nI made a TensorFlow model without using CUDA, but it is very slow. Fortunately, I gained access to a Linux server (Ubuntu 18.04.3 LTS), which has a Geforce 1060, also the necessary components are installed - I could test it, the CUDA acceleration is working.
The tensorflow-gpu package is installed (only 1.14.0 is working due to my code) in my virtual environment.
My code does not contain any CUDA-related snippets. I was assuming that if I run it in a pc with CUDA-enabled environment, it will automatically use it.
I tried the with tf.device('/GPU:0'): then reorganizing my code below it, didn't work. I got a strange error, which said only XLA_CPU, CPU and XLA_GPU is there. I tried it with XLA_GPU but didn't work.
Is there any guide about how to change existing code to take advantage of CUDA?\nAnswer: Not enough to give exact answer.
Have you installed tensorflow-gpu separately? Check using pip list.
Cause, initially, you were using tensorflow (default for CPU).
Once you use want to use Nvidia, make sure to install tensorflow-gpu. 
Sometimes, I had problem having both installed at the same time. It would always go for the CPU. But, once I deleted the tensorflow using ""pip uninstall tensorflow"" and I kept only the GPU version, it worked for me.",0.0,0.16729009,0.02798597514629364
44,"Question\nWhen using tensorflow to train a neural network I can set the loss function arbitrarily. Is there a way to do the same in sklearn when training a SVM? Let's say I want my classifier to only optimize sensitivity (regardless of the sense of it), how would I do that?\nAnswer: This is not possible with Support Vector Machines, as far as I know. With other models you might either change the loss that is optimized, or change the classification threshold on the predicted probability. 
SVMs however minimize the hinge loss, and they do not model the probability of classes but rather their separating hyperplane, so there is not much room for manual adjustements.
If you need to focus on Sensitivity or Specificity, use a different model that allows maximizing that function directly, or that allows predicting the class probabilities (thinking Logistic Regressions, Tree based methods, for example)",0.40816328,0.38155198,0.0007081612129695714
45,"Question\nI use ubuntu (through Windows Subsystem For Linux) and I created a new conda environment, I activated it and I installed a library in it (opencv). However, I couldn't import opencv in Jupyter lab till I created a new kernel that it uses the path of my new conda environment. So, my questions are:

Do I need to create a new kernel every time I create a new conda environment in order for it to work? I read that in general we should use kernels for using different versions of python, but if this is the case, then how can I use a specific conda environment in jupyter lab? Note that browsing from Jupyter lab to my new env folder or using os.chdir to set up the directory didn't work.
Using the new kernel that it's connected to the path of my new environment, I couldn't import matplotlib and I had to activate the new env and install there again the matplotlib. However, matplotlib could be imported when I was using the default kernel Python3. 
Is it possible to have some standard libraries to use them with all my conda environments (i.e. install some libraries out of my conda environments, like matplotlib and use them in all my enviroments) and then have specific libraries in each of my environments? I have installed some libraries through the base environment in ubuntu but I can't import these in my new conda environment.

Thanks in advance!\nAnswer: To my best understanding:
You need ipykernel in each of the environments so that jupyter can import the other library.
In my case, I have a new environment called TensorFlow, then I activate it and install the ipykernel, and then add it to jupyter kernelspec. Finally I can access it in jupyter no matter the environment is activated or not.",0.20408164,0.15613967,0.002298432169482112
46,"Question\nI am running venv based kernel and I am getting trouble in returning a proper answer from which python statement from my JupyterLab notebook. When running this command from terminal where I have my venv activated it works (it returns a proper venv path ~/venvs/my_venv/bin/python), but it does not work in the notebook.
!which python
returns the host path:
/usr/bin/python
I have already tried with os.system() and subprocess, but with no luck.
Does anyone know how to execute this command from the Jupyter notebook?\nAnswer: It sounds like you are starting the virtual environment inside the notebook, so that process's PATH doesn't reflect the modifications made by the venv. Instead, you want the path of the kernel that's actually running: that's sys.executable.",0.40816328,0.08083737,0.10714225471019745
47,"Question\nI am running venv based kernel and I am getting trouble in returning a proper answer from which python statement from my JupyterLab notebook. When running this command from terminal where I have my venv activated it works (it returns a proper venv path ~/venvs/my_venv/bin/python), but it does not work in the notebook.
!which python
returns the host path:
/usr/bin/python
I have already tried with os.system() and subprocess, but with no luck.
Does anyone know how to execute this command from the Jupyter notebook?\nAnswer: maybe it's because you are trying to run the command outside venv
try source /path/to/venv/bin/active first and then try which python",-0.35714287,0.2572828,0.377518892288208
48,"Question\nI've been trying to look for ways to call my python script from my perl script and pass the database handle from there while calling it. I don't want to establish another connection in my python script and just use the db handle which is being used by the perl script. Is it even possible and if yes then how?\nAnswer: There answer is that almost all databases (Oracle, MySQL, Postgresql) will NOT allow you to pass open DB connections between processes (even parent/child). This is a limit on the databases connection, which will usually be associated with lot of state information.
If it was possible to'share' such a connection, it will be a challenge for the system to know where to ship the results for queries sent to the database (will the result go to the parent, or to the child?).
Even if it is possible somehow to forward connection between processes, trying to pass a complex object (database connection is much more the socket) between Perl (usually DBI), and Python is close to impossible.
The 'proper' solution is to pass the database connection string, username, and password to the Python process, so that it can establish it's own connection.",0.0,0.25406873,0.06455092132091522
49,"Question\nI'm new to programming and following a course where I must install Tensorflow. The issue is that I'm using Python 3.8 which I understand isn't supported by Tensorflow.
I've downloaded Python 3.6 but I don't know how to switch this as my default version of python.
Would it be best to set up a venv using python 3.6 for my program and install Tensorflow in this venv?
Also, I using Windows and Powershell.\nAnswer: it would have been nice if you would have the share the error screenshot
though as per i got the case 
tensorflow work in both 3.8 and 3.6 just you have to check that you have 64bit version not 32 bit
you can acess both version from thier respective folder no need to install a venv",0.0,-0.17663935,0.03120145946741104
50,"Question\nI'm new to programming and following a course where I must install Tensorflow. The issue is that I'm using Python 3.8 which I understand isn't supported by Tensorflow.
I've downloaded Python 3.6 but I don't know how to switch this as my default version of python.
Would it be best to set up a venv using python 3.6 for my program and install Tensorflow in this venv?
Also, I using Windows and Powershell.\nAnswer: If you don't want to use Anaconda or virtualenv, then actually multiple Python versions can live side by side. I use Python38 as my default and Python35 for TensorFlow until they release it for Python38. If you wish to use the ""non-default"" Python, just invoke with the full path of the python.exe (or create a shortcut/batch file for it). Python then will take care of using the correct Python libs for that version.",0.0,0.2664554,0.07099848985671997
51,"Question\nI have several binary files, which are mostly bigger than 10GB.
In this files, I want to find patterns with Python, i.e. data between the pattern 0x01 0x02 0x03 and 0xF1 0xF2 0xF3.
My problem: I know how to handle binary data or how I use search algorithms, but due to the size of the files it is very inefficient to read the file completely first. That's why I thought it would be smart to read the file blockwise and search for the pattern inside a block.
My goal: I would like to have Python determine the positions (start and stop) of a found pattern. Is there a special algorithm or maybe even a Python library that I could use to solve the problem?\nAnswer: The common way when searching a pattern in a large file is to read the file by chunks into a buffer that has the size of the read buffer + the size of the pattern - 1.
On first read, you only search the pattern in the read buffer, then you repeatedly copy size_of_pattern-1 chars from the end of the buffer to the beginning, read a new chunk after that and search in the whole buffer. That way, you are sure to find any occurence of the pattern, even if it starts in one chunk and ends in next.",1.0,0.26306647,0.5430710315704346
52,"Question\nI'm using PyCharm. I try to install Selenium but I have a problem with proxy. I try to add packages manually to my project/environment but I don't know how.
I downloaded files with Selenium. Could you tell me how to add this package to Project without using pip?\nAnswer: open pycharm
click on settings (if u use mac click on preference )
click project
then click projecti nterpreter
click the + button on the bottom of the window you can see a new window search Selenium package and install",0.0,0.37853807,0.14329107105731964
53,"Question\nI just trained my model successfully and I have some checkpoints from the training process. Can you explain to me how to use this data to recognize the objects live with the help of a webcam?\nAnswer: Congratulations :) 
First of all, you use the model to recognize the objects, the model learned from the data, minor detail.
It really depends on what you are aiming for, as the comment suggest, you should probably provide a bit more information. 
The simplest setup would probably be to take an image with your webcam, read the file, pass it to the model and get the predictions. If you want to do it live, you are gonna have the stream from the webcam and then pass the images to the model.",0.0,0.043616116,0.0019023655913770199
54,"Question\nI'm developing a 8 Puzzle game solver in python lately and I need a bit of help
So far I finished coding the A* algorithm using Manhattan distance as a heuristic function.
The solver runs and find ~60% of the solutions in less than 2 seconds
However, for the other ~40%, my solver can take up to 20-30 minutes, like it was running without heuristic.
I started troubleshooting, and it seems that the openset I use is causing some problems :

My open set is an array
Each iteration, I loop through the openset to find the lowest f(n) (complexity : O(n) )

I have the feeling that O(n) is way too much to run a decent A* algorithm with such memory used so I wanted to know how should I manage to make the openset less ""time eater""
Thank you for your help! Have a good day
EDIT: FIXED
I solved my problem which was in fact a double problem.
I tried to use a dictionary instead of an array, in which I stored the nodes by their f(n) value and that allowed me to run the solver and the ~181000 possibilities of the game in a few seconds
The second problem (I didn't know about it because of the first), is that I didn't know about the solvability of a puzzle game and as I randomised the initial node, 50% of the puzzles couldn't be solved. That's why it took so long with the openset as the array.\nAnswer: The open set should be a priority queue. Typically these are implemented using a binary heap, though other implementations exist.
Neither an array-list nor a dictionary would be efficient.

The closed set should be an efficient set, so usually a hash table or binary search tree, depending on what your language's standard library defaults to.
A dictionary (aka ""map"") would technically work, but it's conceptually the wrong data-structure because you're not mapping to anything. An array-list would not be efficient.",0.0,0.318007,0.10112844407558441
55,"Question\nMy scripts don't work anymore and I can't figure it out.
It is a chrome version problem apparently... But I don't know how to switch to another version (not the latest?) Does exist another way?
My terminal indicates : 
Traceback (most recent call last):
File ""/Users/.../Documents/SCRIPTS/PYTHON/Scripts/# -- coding: utf-8 --.py"", line 21, in 
    driver = webdriver.Chrome()
File ""/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py"", line 81, in init
    desired_capabilities=desired_capabilities)
File ""/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 157, in init
    self.start_session(capabilities, browser_profile)
File ""/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
File ""/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 321, in execute
    self.error_handler.check_response(response)
File ""/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: Chrome version must be between 71 and 75
(Driver info: chromedriver=2.46.628411 (3324f4c8be9ff2f70a05a30ebc72ffb013e1a71e),platform=Mac OS X 10.14.5 x86_64)
Any idea?\nAnswer: This possibly happens, as your Chrome Browser or Chromium may be updated to newer versions automatically. But you still",0.0,-0.20880556,0.04359976202249527
56,"Question\nI made models with sklearn, something like this:
clf = SGDClassifier(loss=""log"")
clf.fit(X, Y)
And then now I would like to add data to learn for this model, but with more important weight. I tried to use partial_fit with sample_weight bigger but not working. Maybe I don't use fit and partial_fit as good, sorry I'm beginner...
If someone know how to add new data I could be happy to know it :)
Thanks for help.\nAnswer: Do you think it has other way to do a first learning and then add new data more important for the model? Keras?
Thanks guys",0.0,-0.1748927,0.0305874552577734
57,"Question\nI would like to know how back-end automation is possible through RPA.
I'd be interested in solving this scenario relative to an Incident Management Application, in which authentication is required. The app provide:

An option useful to download/export the report to a csv file
Sort the csv as per the requirement
Send an email with the updated csv to the team

Please let me know how this possible through RPA and what are those tools 
available in RPA to automate this kind of scenario?\nAnswer: RPA tools are designed to automate mainly front-end activities by mimicing human actions. It can be done easily using any RPA tool.
However, if you are interested in back-end automation the first question would be, if specific application has an option to interact in the way you want through the back-end/API?
If yes, in theory you could develop RPA robot to run pre-developed back-end script. However, if all you need would be to run this script, creating robot for this case may be redundant.",0.40816328,0.32131684,0.00754230422899127
58,"Question\nI would like to know how back-end automation is possible through RPA.
I'd be interested in solving this scenario relative to an Incident Management Application, in which authentication is required. The app provide:

An option useful to download/export the report to a csv file
Sort the csv as per the requirement
Send an email with the updated csv to the team

Please let me know how this possible through RPA and what are those tools 
available in RPA to automate this kind of scenario?\nAnswer: There are several ways to do it. It is especially useful when your backed are 3rd party applications where you do not have lot of control. Many RPA products like Softomotive WinAutomation, Automation Anywhere, UiPath etc. provide file utilities, excel utilities, db utilities, ability to call apis, OCR capabilities etc., which you can use for backed automation.",-0.35714287,0.12966812,0.23698493838310242
59,"Question\nI'm trying to run a file (an executable) in google colab  I mounted the drive and everything is ok however whenever i try to run it using :
!  'gdrive/My Drive/path/myfile' 
I get this output of the cell:
/bin/bash: 'gdrive/My Drive/path/myfile : Permission denied
any ideas how to overcome the permissions?\nAnswer: you first need to permit that file/folder as:
    chmod 755 file_name",1.0,0.090355396,0.8274533152580261
60,"Question\nWe are doing a POC to see how to access SAS data sets from Anaconda 
All documentation i find says only SASpy works with SAS 9.4 or higher
Our SAS version is 9.04.01M3P062415
Can this be done? If yes any documentation in this regard will be highly appreciated
Many thanks in Advance!\nAnswer: SAS datasets are ODBC compliant. SasPy is for running SAS code. If the goal is to read SAS datasets, only, use ODBC or OleDb. I do not have Python code but SAS has a lot of documentation on doing this using C#. Install the free SAS ODBC drivers and read the sas7bdat. The drivers are on the SAS website.
Writing it is different but reading should be fine. You will lose some aspects of the dataset but data will come through.",0.0,0.12546378,0.01574116013944149
61,"Question\nI was hoping that just using something like
with open(file_name, ""w"") as f:
would not change ctime if the file already existed. Unfortunately it does.
Is there a version which will leave the ctime intact?
Motivation:
I have a file that contains a list of events. I would like to know how old the oldest event is. It seems this should be the files ctime.\nAnswer: Because fopen works that way when using 'w' as an option. From the manual:

""w""   write: Create an empty file for output operations.
  If a file with the same name already exists, its contents are discarded and the file is treated as a new empty file.

If you don't want to create a new file use a+ to append to the file. This leaves the create date intact.",0.10204082,0.39942104,0.08843498677015305
62,"Question\nI was hoping that just using something like
with open(file_name, ""w"") as f:
would not change ctime if the file already existed. Unfortunately it does.
Is there a version which will leave the ctime intact?
Motivation:
I have a file that contains a list of events. I would like to know how old the oldest event is. It seems this should be the files ctime.\nAnswer: Beware, ctime is not the creation time but the inode change time. It is updated each time you write to the file, or change its meta-data, for example rename it. So we have:

atime : access time - each time the file is read
mtime : modification time - each time file data is change (file is written to)
ctime : change time - each time something is changed in the file, either data or meta-data like name or (hard) links

I know no way to reset the ctime field because even utimes and its variant can only set the atime and mtime (and birthtime for file systems that support it like BSD UFS2) - except of course changing the system time with all the involved caveats...",0.30612245,0.05943823,0.060853105038404465
63,"Question\nI am solving a system of non-linear equations using the Newton Raphson Method in Python. This involves using the solve(Ax,b) function (spsolve in my case, which is for sparse matrices) iteratively until the error or update reduces below a certain threshold. My specific problem involves calculating functions such as x/(e^x - 1), which are badly calculated for small x by Python, even using np.expm1().
Despite these difficulties, it seems like my solution converges, because the error becomes of the order of 10^-16. However, the dependent quantities, do not behave physically, and I suspect this is due to the precision of these calculations. For example, I am trying to calculate the current due to a small potential difference. When this potential difference becomes really small, this current begins to oscillate, which is wrong, because currents must be conserved.
I would like to globally increase the precision of my code, but I'm not sure if that's a useful thing to do since I am not sure whether this increased precision would be reflected in functions such as spsolve. I feel the same about using the Decimal library, which would also be quite cumbersome. Can someone give me some general advice on how to go about this or point me towards a relevant post?
Thank you!\nAnswer: You can try using mpmath, but YMMV. generally scipy uses double precision. For a vast majority of cases, analyzing the sources of numerical errors is more productive than just trying to reimplement everything with higher widths floats.",0.0,0.19628763,0.03852883353829384
64,"Question\nPresently, we send entire files to the Cloud (Google Cloud Storage) to be imported into BigQuery and do a simple drop/replace. However, as the file sizes have grown, our network team doesn't particularly like the bandwidth we are taking while other ETLs are also trying to run. As a result, we are looking into sending up changed/deleted rows only. 
Trying to find the path/help docs on how to do this. Scope - I will start with a simple example. We have a large table with 300 million records. Rather than sending 300 million records every night, send over X million that have changed/deleted. I then need to incorporate the change/deleted records into the BigQuery tables. 
We presently use Node JS to move from Storage to BigQuery and Python via Composer to schedule native table updates in BigQuery.
Hope to get pointed in the right direction for how to start down this path.\nAnswer: Stream the full row on every update to BigQuery.
Let the table accommodate multiple rows for the same primary entity.
Write a view eg table_last that picks the most recent row. 
This way you have all your queries near-realtime on real data. 
You can deduplicate occasionally the table by running a query that rewrites self table with latest row only. 
Another approach is if you have 1 final table, and 1 table which you stream into, and have a MERGE statement that runs scheduled every X minutes to write the updates from streamed table to final table.",0.40816328,0.27632153,0.017382247373461723
65,"Question\nNote: I did research on this over web but all of them are pointing to the solution which works on prem/desktops. This case is on databricks notebook, I referred databricks help guide but could not find the solution.
Dear all, 
In my local desktop i used to import the objects from other python files by referring their absolute path such as 
""from dir.dira.dir0.file1 import *""
But in Databricks python notebook i'm finding it difficult to crack this step since 2 hours. Any help is appreciated.
Below is how my command shows,
from dbfs.Shared.ABC.models.NJ_WrkDir.test_schdl import *
also tried below ways, none of them worked
from dbfs/Shared/ABC/models/NJ_WrkDir/test_schdl import *
from \Shared\ABC\models\NJ_WrkDir\test_schdl import *
from Shared/ABC/models/NJ_WrkDir/test_schdl import *
from Shared.ABC.models.NJ_WrkDir.test_schdl import *
The error messages shows:
ModuleNotFoundError: No module named 'Shared
ModuleNotFoundError: No module named 'dbfs
SyntaxError: unexpected character after line continuation character
  File """", line 2
    from \Shared\ABC\models\NJ_WrkDir\test_schdl import *
                                                         ^
Thank you!\nAnswer: The solution is, include the command in child databricks python notebook as 
""%run /path/parentfile""
(from where we want to import the objects from)",0.0,0.33958668,0.11531911045312881
66,"Question\nI have a PyTorch video feature tensor of shape [66,7,7,1024] and I need to convert it to [1024,66,7,7]. How to rearrange a tensor shape? Also, how to perform mean across dimension=1? i.e., after performing mean of the dimension with size 66, I need the tensor to be [1024,1,7,7].
I have tried to calculate the mean of dimension=1 but I failed to replace it with the mean value. And I could not imagine a 4D tensor in which one dimension is replaced by its mean. 
Edit:
 I tried torch.mean(my_tensor, dim=1). But this returns me a tensor of shape [1024,7,7]. The 4D tensor is being converted to 3D. But I want it to remain 4D with shape [1024,1,7,7].
Thank you very much.\nAnswer: The first part of the question has been answered in the comments section. So we can use tensor.transpose([3,0,1,2]) to convert the tensor to the shape [1024,66,7,7].
Now mean over the temporal dimension can be taken by 
torch.mean(my_tensor, dim=1)
This will give a 3D tensor of shape [1024,7,7].
To obtain a tensor of shape [1024,1,7,7], I had to unsqueeze in dimension=1:
tensor = tensor.unsqueeze(1)",0.81632656,0.308811,0.2575720250606537
67,"Question\nI was trying to manually calculate TPR and FPR for the given data. But unfortunately I dont have any false positive cases in my dataset and even no true positive cases. 
So I am getting divided by zero error in pandas. So I have an intuition that fpr=1-tpr. Please let me know my intuition is correct if not let know how to fix this issue.
Thank you\nAnswer: It is possible to have FPR = 1 with TPR = 1 if your prediction is always positive no matter what your inputs are.
TPR = 1 means we predict correctly all the positives. FPR = 1 is equivalent to predicting always positively when the condition is negative.
As a reminder:

FPR = 1 - TNR = [False Positives] / [Negatives]
TPR = 1 - FNR = [True Positives] / [Positives]",0.0,0.42505205,0.18066924810409546
68,"Question\nI am writing a script that automates the use of other scripts. I've set it up to automatically import other modules from.py files stored in a directory called dependencies using importlib.import_modules()
Originally, I had dependencies as a subdirectory of the root of my application, and this worked fine. However, it's my goal to have the dependencies folder stored potentially anywhere a user would like. In my personal example, it's located in my dropbox folder while my script is run from a different directory entirely.
I cannot for the life of me seem to get the modules to be detected and imported anymore and I'm out of ideas.
Would someone have a better idea of how to achieve this?
This is an example of the path structure:

E:
|_ Scripts:
| |_ Mokha.py
|
|_ Dropbox:
| |_ Dependencies:
|   |_ utils.py

Here's my code for importing: (I'm reading in a JSON file for the dependency names and looping over every item in the list)

def importPythonModules(pythonDependencies):
    chdir(baseConfig[""dependencies-path])
    for dependency in pythonDependencies:
        try:
            moduleImport = dependency
            module = importlib.import_module(moduleImport)
            modules[dependency] = module
            print(""Loaded module: %s"" % (dependency))
        except ModuleNotFoundError as e:
            print(e)
            raise Exception(""Error importing python dependecies."")
    chdir(application_path)

The error I get is No module named 'utils'
I've tried putting an init.py in both the dependencies folder, the root of my dropbox, and both at the same time to no avail.
This has got to be possible, right?\nAnswer: UPDATE: I solved it.
sys.path.append(baseConfig['dependencies-path",0.0,0.054501668,0.002970431698486209
69,"Question\nI have created a class Node having two data members: data and next.
I have created another class LinkedList to having a data member: head
Now I want to store an image in the node but I have no idea how to do it. The syntax for performing this operation would be very much helpful.\nAnswer: PIL is the Python Imaging Library which provides the python interpreter with image editing capabilities.
USE from PIL import Image after installing. 
Windows: Download the appropriate Pillow package according to your python version. Make sure to download according to the python version you have.
pip install Pillow for Linux users.
Then u can easily add image to your linked list by assigning it to a variable",0.0,0.059579253,0.0035496873315423727
70,"Question\nI had Python 3.7.4 in D:\python3.7.4 before but for some reason, I uninstalled it today, then I changed the folder name to D:\python3.7.5 and installed python 3.7.5 in it, then, when I try to use pip in cmd I got a fatal error saying 

Unable to create processing using '""Unable to create process using '""d:\python3.7.4\python.exe""  ""D:\Python3.7.5\Scripts\pip.exe""' 

I tried to change all things contain python3.7.4 in environment variable to python3.7.5 but the same error still exists, does anyone know how to fix this? 
Thanks\nAnswer: Try to create a new folder and run the installation there.
This should work, as I did the same myself to go install 2 different versions before",0.0,-0.10719052,0.01148980762809515
71,"Question\nI have the logic for email verification, but I am not sure how to make it such that only after clicking the link on the verification email, the user is taken to the second page of the form, and only after filling the second part the user is saved.\nAnswer: I would say that much better idea is to save user to database anyway, but mark him as inactive (simple boolean field in model will be enough). Upon registration, before confirming email mark him as inactive and as soon as he confirms email and fills second part of your registration form that you mentioned change that boolean value to true. If you don't want to keep inactive users data in your database, you can set up for example cron, that will clean users that haven't confirmed their email for few days.",0.20408164,0.1399818,0.004108788445591927
72,"Question\nit confused me long time.
my program has two process, both read data from disk, disk max read speed 10M/s
1. if two process both read 10M data, is two process spend time same with one process read twice?
2. if two process both read 5M data, two process read data spend 1s, one process read twice spend 1s, i know multi process can save time from IO, but the spend same time in IO, multi process how to save time?\nAnswer: It's not possible to increase disk read speed by adding more threads. With 2 threads reading you will get at best 1/2 the speed per thread (in practice even less), with 3 threads - 1/3 the speed, etc.
With disk I/O it is the difference between sequential and random access speed that is really important. For example, sequential read speed can be 10 MB/s, and random read just 10 KB/s. This is the case even with the latest SSD drives (although the ratio may be less pronounced).
For that reason you should prefer to read from disk sequentially from only one thread at a time. Reading the file in 2 threads in parallel will not only reduce the speed of each read by half, but will further reduce because of non-sequential (interleaved) disk access.

Note however, that 10 MB is really not much; modern OSes will prefetch the entire file into the cache, and any subsequent reads will appear instantaneous.",0.0,0.30545658,0.09330372512340546
73,"Question\nI am making a python 3 application (flask based) and for that I created a virtualenv in my development system, installed all packages via pip and my app worked fine.
But when I moved that virtualenv to a different system (python3 installed) and ran my application with the absolute path of my virtualenv python (c:/......./myenv/Scripts/python.exe main.py) then it threw the errors that packages are not installed,
I activated the virtualenv and used pip freeze and there were no packages were installed.
But under virtualenv there is 'Site-Packages' (myenv -> lib -> site-packages), all my installed packages were persent there.
My Question is how to use the packages that are inside'site-packages' even after moving the virtualenv to different system in Python 3.\nAnswer: Maybe you can consider using pipenv to control the virtualenvs on different computer or environment.",0.0,0.18291414,0.03345758095383644
74,"Question\nI am making a python 3 application (flask based) and for that I created a virtualenv in my development system, installed all packages via pip and my app worked fine.
But when I moved that virtualenv to a different system (python3 installed) and ran my application with the absolute path of my virtualenv python (c:/......./myenv/Scripts/python.exe main.py) then it threw the errors that packages are not installed,
I activated the virtualenv and used pip freeze and there were no packages were installed.
But under virtualenv there is 'Site-Packages' (myenv -> lib -> site-packages), all my installed packages were persent there.
My Question is how to use the packages that are inside'site-packages' even after moving the virtualenv to different system in Python 3.\nAnswer: You Must not copy & paste venv, even in the same system. 
If you install new package in venv-copied, then it would installed in venv-original.  Becaus settings are bound to specific directory.",0.0,0.2749259,0.07558424025774002
75,"Question\nI have a python program which is an interpreter, for a language that I have made. It is called cbc.py, and it is in a certain directory. Now, I want to know how I can call it, along with sys.argv arguments (like python3 cbc.py _FILENAME_TO_RUN_) in any directory. I have done research on the.bashrc file and on the PATH variable, but I can't find anything that really helps me with my problem. Could someone please show me how to resolve my problem?\nAnswer: You need to make your script executable first and then add it to your PATH.
If you have your python script at ~/path/to/your/script/YOUR_SCRIPT_NAME:

add #!/usr/bin/python3 at the top of you script,
give executable permision to your script using sudo chmod a+x YOUR_SCRIPT_NAME,
edit ~/.bashrc to add your script path, e.g. echo PATH=""$HOME/path/to/your/script:$PATH"" >> ~/.bashrc,
restart or re-login or run source ~/.bashrc,
now you can access your script via YOUR_SCRIPT_NAME anywhere.",0.0,0.34147215,0.11660322546958923
76,"Question\nBlueprism gives the possibility to spy elements (like buttons and textboxes) in both web-browsers and windows applications. How can I spy (windows-based only) applications using Python, R, Java, C++, C# or other, anything but not Blueprism, preferrably opensource.

For web-browsers, I know how to do this, without being an expert. Using Python or R, for example, I can use Selenium or RSelenium, to spy elements of a website using different ways such as CSS selector, xpath, ID, Class Name, Tag, Text etc.
But for Applications, I have no clue. BluePrism has mainly two different App spying modes which are WIN32 and Active Accessibility. How can I do this type of spying and interacting with an application outside of Blueprism, preferrably using an opensource language?

(only interested in windows-based apps for now)
The aim is of course to create robots able to navigate the apps as a human would do.\nAnswer: There is a free version of Blue Prism now :) Also Blue Prism uses win32, active accessibility and UI Automation which is a newer for of the older active accessibility.
To do this yourself without looking into Blue Prism you would need to know how to use UIA with C#/VB.new or C++. There are libraries however given that Blue Prism now has a free version I would recommend using that.  Anything specific can be developed withing a code stage within Blue Prism.",0.0,0.18243861,0.03328384831547737
77,"Question\nI have 120 samples and the shape of features for each sample is matrix of 15*17. how to use SVM to classify? Is it simply to reshape the matrix to long vector?\nAnswer: Yes, that would be the approach I would recommend. It is essentially the same procedure that is used when utilizing images in image classification tasks, since each image can be seen as a matrix. 
So what people do is to write the matrix as a long vector, consisting of every column concatenated to one another.
So you can do the same here.",0.0,0.14303404,0.020458737388253212
78,"Question\nI am rather new to Django and I need to fetch some data from a website. For example I want the top ten posts of the day from Reddit. I know of a ""request"" module for the same.But I am not sure where and how should I implement it and will it be important to store the data in a model or not.\nAnswer: You can create a helper class named like network.py and implement functions to fetch the data.
If you want to store them in the database you can create appropriate models otherwise you can directly import and call the function and use the data returned from network.py in your view.",0.0,0.18053639,0.032593388110399246
79,"Question\nI am trying to build an app through react-native wherein I need to upload a JSON file to my account folder hosted on pythonanywhere.
Can you please tell me how can I upload a JSON file to the pythonanywhere folder through react-native?\nAnswer: The web framework that you're using will have documentation about how to create a view that can accept filee uploads. Then you can use the fetch API in your javascript to send the file to it.",0.81632656,0.2296527,0.3441862165927887
80,"Question\nNo Python at 'C:\Users\Mr_Le\AppData\Local\Programs\Python\Python38-32\python.exe'
Any time I try to run my code it keeps prompting me this ^^^ but I had recently deleted Python 3.8 to downgrade to Python 3.6 and just installed Python 3.6 to run pytorch.
Does anyone know how to fix this?\nAnswer: For other users: just check the ""C:\Users<>\AppData\Local\Programs\Python""  folder on your PC and remove any folders belonging to previous installations of Python. Also check if environmental variables are correct.",0.0,0.12693799,0.016113251447677612
81,"Question\nNo Python at 'C:\Users\Mr_Le\AppData\Local\Programs\Python\Python38-32\python.exe'
Any time I try to run my code it keeps prompting me this ^^^ but I had recently deleted Python 3.8 to downgrade to Python 3.6 and just installed Python 3.6 to run pytorch.
Does anyone know how to fix this?\nAnswer: 1.In your windows search bar find python 3.9.8.
[Searching for Windows][1]
[1]: https://i.stack.imgur.com/vNMxT.png

Right click on your the app

Click on App Settings
[Your settings will populate][2]
[2]: https://i.stack.imgur.com/E4yM3.png

Scroll down on this page
[][3]
[3]: https://i.stack.imgur.com/HFc1J.png

Hit the Repair box

Try to run your python script again after restarting all your programs",0.0,0.15633413,0.024440361186861992
82,"Question\nI have CNN that I have built using on Tensor-flow 2.0. I need to access outputs of the intermediate layers. I was going over other stackoverflow questions that were similar but all had solutions involving Keras sequential model. 
I have tried using model.layers[index].output but I get 

Layer conv2d has no inbound nodes.

I can post my code here (which is super long) but I am sure even without that someone can point to me how it can be done using just Tensorflow 2.0 in eager mode.\nAnswer: The most straightforward solution would go like this:
mid_layer = model.get_layer(""layer_name"")
you can now treat the ""mid_layer"" as a model, and for instance:
mid_layer.predict(X)
Oh, also, to get the name of a hidden layer, you can use this:
model.summary() 
this will give you some insights about the layer input/output as well.",0.0,0.21340013,0.04553961381316185
83,"Question\nI am trying to plot the endpoints of the line segment which is a tangent to a circle in Python.
I know the circle has center of (A, B), and a radius of r. The point at which I want to find the tangent at is (a, b). I want the tangent to be a segment of length c. How do I write a code which allows me to restrict the length of the line?
I have the equation of the tangent to be y = (-(B - b)/(A - a))(x - a) + b. So I know how to plot the two endpoints if the length of the segment did not matter. But how would I determine the x-coordinates of the point? Is there some sort of command which allows me to limit the length of a line?
Thank you!!!\nAnswer: I don't know thonny, and it sounds like your implementation will depend a bit on the context of this computation. 
That said, it sounds like what you're looking for is the two points of intersection of your tangent line and a (new, conceptual) cicle with a given radius centered on (a,b). You should be able to put together the algebraic expression for those points, and simplify it into something tidy. Watch out for special cases though, where the slope of the tangent is undefined (or where it's zero).",0.0,0.25098455,0.06299324333667755
84,"Question\nI have the list of APIs,
Input  = [WriteConsoleA, WSAStartup, RegCloseKey, RegCloseKey, RegCloseKey, NtTerminateProces, RegCloseKey]
expected output = [WriteConsoleA, WSAStartup, RegCloseKey, NtTerminateProces, RegCloseKey]\nAnswer: you can simply convert set(list) i.e. set(Input) to remove all the duplicates.",0.0,0.2610321,0.0681377574801445
85,"Question\nI am doing a small project on sentiment analysis using TextBlob. I understand there are are 2 ways to check the sentiment of tweet:

Tweet polarity: Using it I can tell whether the tweet is positive, negative or neutral
Training a classifier: I am using this method where I am training a TextBlob Naive Bayes classifier on positive and negative tweets and using the classifier to classify tweet either as 'positive' or 'negative'.

My question is, using the Naive bayes classifier, can I also classify the tweet as 'neutral'? In other words, can the'sentiment polarity' defined in option 1 can somehow be used in option 2?\nAnswer: If you have only two classes, Positive and Negative, and you want to predict if a tweet is Neutral, you can do so by predicting class probabilities.
For example, a tweet predicted as 80% Positive remains Postive. However, a tweet predicting as 50% Postive could be Neutral instead.",0.0,0.28069055,0.0787871852517128
86,"Question\nI am confused as to how I can use Doc2Vec(using Gensim) for IMDB sentiment classification dataset. I have got the Doc2Vec embeddings after training on my corpus and built my Logistic Regression model using it. How do I use it to make predictions for new reviews? sklearn TF-IDF has a transform method that can be used on test data after training on training data, what is its equivalent in Gensim Doc2Vec?\nAnswer: To get a vector for an unseen document, use vector = model.infer_vector([""new"", ""document""])
Then feed vectorinto your classifier: preds = clf.predict([vector]).",0.20408164,0.28635657,0.006769163999706507
87,"Question\nI am working on an assignment and am stuck with the following problem:
I have to connect to an oracle database in Python to get information about a table, and display this information for each row in an.html-file. Hence, I have created a python file with doctype HTML and many many ""print"" statements, but am unable to embed this to my main html file. In the next step, I have created a jinja2 template, however this passes the html template data (incl. ""{{ to be printed }}"") to python and not the other way round. I want to have the code, which is executed in python, to be implemented on my main.html file.
I can't display my code here since it is an active assignment. I am just interested in general opinions on how to pass my statements from python (or the python file) into an html file. I can't find any information about this, only how to escape html with jinja. 
Any ideas how to achieve this?
Many thanks.\nAnswer: You can't find information because that won't work. Browser cannot run python, meaning that they won't be able to run your code if you embed it into an html file. The setup that you need is a backend server that is running python (flask is a good framework for that) that will do some processing depending on the request that is being sent to it. It will then send some data to a template processor (jinja in this case work well with flask). This will in turn put the data right into the html page you want to generate. Then this html page will be returned to the client making the request, which is something the browser will understand and will show to the user. If you want to do some computation dynamically on the browser you will need to use javascript instead which is something a browser can run (since its in a sandbox mode).
Hope it helps!",0.0,0.14133918,0.019976764917373657
88,"Question\nI am working on an assignment and am stuck with the following problem:
I have to connect to an oracle database in Python to get information about a table, and display this information for each row in an.html-file. Hence, I have created a python file with doctype HTML and many many ""print"" statements, but am unable to embed this to my main html file. In the next step, I have created a jinja2 template, however this passes the html template data (incl. ""{{ to be printed }}"") to python and not the other way round. I want to have the code, which is executed in python, to be implemented on my main.html file.
I can't display my code here since it is an active assignment. I am just interested in general opinions on how to pass my statements from python (or the python file) into an html file. I can't find any information about this, only how to escape html with jinja. 
Any ideas how to achieve this?
Many thanks.\nAnswer: Thanks for the suggestions. What I have right now is a perfectly working python file containing jinja2 and the html output I want, but as a python file. When executing the corresponding html template, the curly expressions {{name}} are displayed like this, and not as the functions executed within the python file. Hence, I still have to somehow tell my main html file to execute this python script on my webpage, which I cannot manage so far.
Unfortunately, it seems that we are not allowed to use flask, only jinja and django.",0.0,0.079844,0.006375064142048359
89,"Question\nSo I made Pong using PyGame and I want to use genetic algorithms to have an AI learn to play the game. I want it to only know the location of its paddle and the ball and controls. I just don't know how to have the AI move the paddle on its own. I don't want to do like: ""If the ball is above you, go up."" I want it to just try random stuff until it learns what to do.
So my question is, how do I get the AI to try controls and see what works?\nAnswer: So you'd want as the AI input the position of the paddle, and the position of the ball. The AI output is two boolean output whether the AI should press up or down button on the next simulation step.
I'd also suggest adding another input value, the ball's velocity. Otherwise, you would've likely needed to add another input which is the location of the ball in the previous simulation step, and a much more complicated middle layer for the AI to learn the concept of velocity.",0.0,0.025873065,0.0006694155163131654
90,"Question\nI need to get 50 latest data (based on timestamp) from BigTable.
I get the data using read_row and filter using CellsRowLimitFilter(50). But it didn't return the latest data. It seems the data didn't sorted based on timestamp? how to get the latest data?
Thank you for your help.\nAnswer: Turns out the problem was on the schema. It wasn't designed for timeseries data. I should have create the rowkey with id#reverse_timestamp and the data will be sorted from the latest. Now I can use CellsRowLimitFilter(50) and get 50 latest data.",0.20408164,0.28249043,0.006147938780486584
91,"Question\nI am building a GUI software using PyQt5 and want to connect it with MySQL to store the data. 
In my computer, it will work fine, but what if I transfer this software to other computer who doesn't have MySQL, and if it has, then it will not have the same password as I will add in my code (using MySQL-connector)a password which I know to be used to connect my software to MySQL on my PC. 
My question is, how to handle this problem???\nAnswer: If you want your database to be installed with your application and NOT shared by different users using your application, then using SQLite is a better choice than MySQL. SQLite by default uses a file that you can bundle with your app. That file contains all the database tables including the connection username/password.",0.81632656,0.17592418,0.41011521220207214
92,"Question\nI want to do some ML on my computer with Python, I'm facing problem with the installation of tensorflow and I found that tensorflow could work with GPU, which is CUDA enabled. I've got a GPU Geforce gtx 1650, will tensorflow work on that.
If yes, then, how could I do so?\nAnswer: Here are the steps for installation of tensorflow:

Download and install the Visual Studio.
Install CUDA 10.1
Add lib, include and extras/lib64 directory to the PATH variable.
Install cuDNN
Install tensorflow by pip install tensorflow",0.0,0.09656626,0.009325042366981506
93,"Question\nI Have created a python based tool for my teammates, Where we group all the similar JIRA tickets and hence it becomes easier to pick the priority one first. But the problem is every time I make some changes I have to ask people to get the latest one from the Perforce server. So I am looking for a mechanism where whenever anyone uses the tool a pop up should come up as ""New version available"" please install.
Can anyone help how to achieve that?\nAnswer: I have an idea,you can use requests module to crawl your website(put the number of version in the page) and get the newest version.
And then,get the version in the user's computer and compare to the official version.If different or lower than official version,Pop a window to remind user to update",0.27210885,-0.24152967,0.2638244926929474
94,"Question\nI Have created a python based tool for my teammates, Where we group all the similar JIRA tickets and hence it becomes easier to pick the priority one first. But the problem is every time I make some changes I have to ask people to get the latest one from the Perforce server. So I am looking for a mechanism where whenever anyone uses the tool a pop up should come up as ""New version available"" please install.
Can anyone help how to achieve that?\nAnswer: You could maintain the latest version code/tool on your server and have your tool check it periodically against its own version code. If the version code is higher on the server, then your tool needs to be updated and you can tell the user accordingly or raise appropriate pop-up recommending for an update.",0.13605443,0.051284373,0.007185962051153183
95,"Question\nI Have created a python based tool for my teammates, Where we group all the similar JIRA tickets and hence it becomes easier to pick the priority one first. But the problem is every time I make some changes I have to ask people to get the latest one from the Perforce server. So I am looking for a mechanism where whenever anyone uses the tool a pop up should come up as ""New version available"" please install.
Can anyone help how to achieve that?\nAnswer: On startup, or periodically while running, you could have the tool query your Perforce server and check the latest version. If it doesn't match the version currently running, then you would show the popup, and maybe provide a download link.
I'm not personally familiar with Perforce, but in Git for example you could check the hash of the most recent commit. You could even just include a file with a version number that you manually increment every time you push changes.",0.27210885,0.19901294,0.005343012977391481
96,"Question\nI'm using Selenium with Python API and Chrome to do the followings:

Collect the Performance Log;
Click some <a, target='_blank'> tags to get into other pages;

For example, I click a href in Page 'A', which commands the browser opens a new window to load another URL 'B'.
But when I use driver.get_log('performance') to get the performance log, I can only get the log of Page 'A'. Even though I switch to the window of 'B' as soon as I click the href, some log entries of the page 'B' will be lost.
So how can I get the whole performance log of another page without setting the target of <a> to '_top'?\nAnswer: I had the same problem and I think it is because the driver does not immediately switch to a new window.
I switched to page ""B"" and reloaded this page, then uses get_log and it worked.",0.0,0.24444401,0.059752874076366425
97,"Question\nIn LXML python library, how to iterate? and what is the difference between iterdescendants() and iterchildren() in lxml python?\nAnswer: when you use iterchildren() you iterate over first level childs. When you use iterdescendants() you iterate over childs and childs of childs.",0.0,0.27508807,0.07567344605922699
98,"Question\nI have included Python.h in my module header file and it was built successfully. 
Somehow when I enabled-examples configuration to compile the example.cc file, which includes the module header file. It reported the Python.h file can not be found - fatal error.
I have no clue at the moment what is being wrong.
Could anyone give a hint? It is for the NS3(Network Simulator 3) framework.\nAnswer: thanks for writing back to me:).
I solved the issue by adding the pyembed feature in the wscript within the same folder as my.cc file.
Thanks again:).
J.",0.0,0.18110216,0.03279799222946167
99,"Question\nTraditionally I've used Notepad ++ along with the Anaconda prompt to write and run scripts locally on my Windows PC.
I had my PC upgraded and thought I'd give Virtual Studio Code a chance to see if I liked it.  
Now, every time I try to execute a.py file in the Anaconda prompt Visual Studio 2017 launches.  I hate this and can't figure out how to stop it.
I've tried the following:

Uninstalling Virtual Studio Code.
Changing environments in Anaconda.
Reinstalling Anaconda.  I did not check the box for the %PATH option.
Reboots at every step.

On my Windows 10 laptop Visual Studio 2017 doesn't appear in my Apps and Features to uninstall.  I've tried Googling and am stuck.
The programs involved are:
Windows 10 Professional
Visual Studio 2017
Anaconda version 2019.10 Build Channel py37_0
Can someone help me figure out how to stop this?\nAnswer: How were you running the scripts before? python script.py or only script.py?
If it is the latter, what happened probably is that Windows has associated.py files to Visual Studio. Right click on the file, go to Open With, then select Python if you want to run them, or Notepad++ if you want to edit them.",0.40816328,0.12638354,0.07939981669187546
0,"Question\nI have an AWS Lambda that uses 2 environment variables. I want to run this lambda up to several hundred times a day, however i need to change the environment variables between runs.
Ideally, I would like something where I could a list a set of variables pairs and run the lambdas on a schedule
The only way I see of doing this, is have separate lambdas and setting the environment variables for each manually
Any Ideas about how to achieve this\nAnswer: You could use an SQS queue for this. Instead of your scheduler initiating the Lambda function directly, it could simply send a message with the two data values to an SQS queue, and the SQS queue could be configured to trigger the Lambda. When triggered, the Lambda will receive the data from the message. So, the Lambda function does not need to change.
Of course, if you have complete control over the client that generates the two data values then that client could also simply invoke the Lambda function directly, passing the two data values in the payload.",0.81632656,0.13355684,0.46617448329925537
1,"Question\nI have imported excel file into python pandas. but when I display customer numbers I get in float64 format i.e 

7.500505e+09, 7.503004e+09
  how do convert the column containing these numbers\nAnswer: int(yourVariable) will cast your float64 to a integer number.
Is this what you are looking for?",0.0,0.0588485,0.0034631460439413786
2,"Question\nI am starting out with computer vision and opencv. I would like to try camera calibration for the images that I have to see how it works. I have a very basic doubt.
Should I use the same camera from which the distorted images were captured or I can use any camera to perform my camera calibration?\nAnswer: Camera calibration is supposed to do for the same camera. Purpose of calibrating a camera is to understand how much distortion the image has and to correct it before we use it to take actual pics. Even if you do not have the original camera, If you have the checkerboard images taken from that camera it is sufficient. Otherwise, look for a similar camera with features as similar as possible (focal length etc.) to take checker board images for calibration and this will somewhat serve your purpose.",0.40816328,0.123553514,0.08100271970033646
3,"Question\nI have a dataset of dialogues with various parameters (like if it is a question, an action, what emotion it conveys etc ). I have 4 different ""informations"" per sentence.
let s say A replys to B
A has an additive parameter in a different list for its possible emotions (1.0.0.0) (angry.happy.sad.bored) - an another list for it s possible actions (1.0.0.0) (question.answer.inpulse.ending)
I know how to build a regular RNN model (from the tutorials and papers I have seen here and there), but I can t seem to find a ""parameters"" architecture.
Should I train multiple models? (like sentence A --> emotions, then sentence B -->actions) then train the main RNN separately and predicting the result through all models?
or is there a way to build one single model with all the information stored right at the beginning?
I apologize for my approximate English, witch makes my search for answers even more difficult.\nAnswer: From the way I understand your question, you want to find emotions/actions based on a particular sentence. Sentence A has emotions as labels and Sentence B has actions as labels. Each of the labels has 4 different values with a total of 8 values. And you are confused about how to implement labels as input.
Now, you can give all these labels their separate classes. Like emotions will have labels (1.2.3.4) and actions will have labels (5.6.7.8). Then concat both the datasets and run Classification through RNN.
If you need to pass emotions/actions as input, then add them to vectorized matrix. Suppose you have Sentence A stating ""Today's environment is very good"" with happy emotion. Add the emotion with it's matrix row, like this:
Today | Environment | very | good | health
1     | 1           | 1    | 1    | 0  
Now add emotion such that:
Today | Environment | very | good | health | emotion
1     | 1           | 1    | 1    | 0      | 2(for happy)
I hope this answers your question.",0.0,0.0987041,0.009742499329149723
4,"Question\nIn pika, I have called channel.confirm_delivery(on_confirm_delivery) in order to be informed when messages are delivered successfully (or fail to be delivered). Then, I call channel.basic_publish to publish the messages. Everything is performed asynchronously.
How, when the on_confirm_delivery callback is called, do I find what the concerned message? In the parameters, The only information that changes in the object passed as a parameter to the callback is delivery_tag, which seems to be an auto-incremented number. However, basic_publish doesn't return any delivery tag.
In other words, if I call basic_publish twice, how do I know, when I receive an acknowledgement, whether it's the first or the second message which is acknowledged?\nAnswer: From RabbitMQ document, I find: 

Delivery tags are monotonically growing positive integers and are presented as such by client libraries.

So you can keep a growing integer in your code per channel, set it to 0 when channel is open, increase it when you publish a message. Then this integer will be same as the delivery_tag.",0.40816328,0.397577,0.0001120695669669658
5,"Question\n11am – 4pm, 7:30pm – 11:30pm (Mon-Sun)------(this is opening and closing time of restaurant)
  [i have this kind of format in my TIME column and this is not converting into datetime format...so how to prepare the data so that i can apply linear regression???]

ValueError: ('Unknown string format:', '11am – 4pm, 7:30pm – 11:30pm (Mon-Sun)')\nAnswer: From my understanding, datetime format requires the 24h format, or - 00:00:00
So instead of 7:30pm, it would be 19:30:00.",0.0,0.11938268,0.014252224005758762
6,"Question\nI'm sorry, i know that this is a very basic question but since i'm still a beginner in machine learning, determining what model suits best for my problem is still confusing to me, lately i used linear regression model (causing the r2_score is so low) and a user mentioned i could use certain model according to the curve of the plot of my data and when i see another coder use random forest regressor (causing the r2_score 30% better than the linear regression model) and i do not know how the heck he/she knows better model since he/she doesn't mention about it. I mean in most sites that i read, they shoved the data to some models that they think would suit best for the problem (example: for regression problem, the models could be using linear regression or random forest regressor) but in some sites and some people said firstly we need to plot the data so we can predict what exact one of the models that suit the best. I really don't know which part of the data should i plot? I thought using seaborn pairplot would give me insight of the shape of the curve but i doubt that it is the right way, what should i actually plot? only the label itself or the features itself or both? and how can i get the insight of the curve to know the possible best model after that?\nAnswer: This question is too general, but I will try to give an overview of how to choose the model. First of all you should that there is no general rule to choose the family of models to use, it is more a choosen by experiminting different model and looking to which one gives better results. You should also now that in general you have multi-dimensional features, thus plotting the data will not give you a full insight of the dependance of your features with the target, however to check if you want to fit a linear model or not, you can start plotting the target vs each dimension of the input, and look if there is some kind of linear relation. However I would recommand that you to fit a linear model, and check if if this is relvant from a statistical point of view (student test, smirnov test, check the residuals...). Note that in real life applications, it is not likeley that linear regression will be the best model, unless you do a lot of featue engineering. So I would recommand you to use more advanced methods (RandomFore",0.20408164,-0.14822572,0.12412048876285553
7,"Question\nI'm sorry, i know that this is a very basic question but since i'm still a beginner in machine learning, determining what model suits best for my problem is still confusing to me, lately i used linear regression model (causing the r2_score is so low) and a user mentioned i could use certain model according to the curve of the plot of my data and when i see another coder use random forest regressor (causing the r2_score 30% better than the linear regression model) and i do not know how the heck he/she knows better model since he/she doesn't mention about it. I mean in most sites that i read, they shoved the data to some models that they think would suit best for the problem (example: for regression problem, the models could be using linear regression or random forest regressor) but in some sites and some people said firstly we need to plot the data so we can predict what exact one of the models that suit the best. I really don't know which part of the data should i plot? I thought using seaborn pairplot would give me insight of the shape of the curve but i doubt that it is the right way, what should i actually plot? only the label itself or the features itself or both? and how can i get the insight of the curve to know the possible best model after that?\nAnswer: If you are using off-the-shelf packages like sklearn, then many simple models like SVM, RF, etc, are just one-liners, so in practice, we usually try several such models at the same time.",0.0,0.20322138,0.04129892960190773
8,"Question\nI've got 2 Python 3.6 pods currently running. They both used to run collectstatic upon redeployment, but then one wasn't working properly, so I deleted it and made a new 3.6 pod. Everything is working perfectly with it, except it no longer is running collectstatic on redeployment (so I'm doing it manually). Any thoughts on how I can get it running again? 
I checked the documentation, and for the 3.11 version of openshift still looks like it has a variable to disable collectstatic (which i haven't done), but the 4.* versions don't seem to have it. Don't know if that has anything to do with it. 
Edit: 
So it turns out that I had also updated the django version to 2.2.7. 
As it happens, the openshift infrastructure on openshift online is happy to collectstatic w/ version 2.1.15 of Django, but not 2.2.7 (or 2.2.9). I'm not quite sure why that is yet. Still looking in to it.\nAnswer: Currently Openshift Online's python 3.6 module doesn't support Django 2.2.7 or 2.2.9.",0.0,0.37653512,0.14177869260311127
9,"Question\nI want to take an elf file and then based on the content add a section with data and add symbols.  Using objcopy --add-section I can add a section with the content that I would like. I cannot figure out how to add a symbol. 
Regardless, I would prefer not run a series of programs in order to do what I want but rather do it natively in c or python. In pyelftools I can view an elf, but I cannot figure out how to edit and elf. 
How can I add custom sections and symbols in Python or C?\nAnswer: ELF has nothing to do with the symbols stored in it by programs.  It is just a format to encode everything.  Symbols are generated normally by compilers, like the C compiler, fortran compiler or an assembler, while sections are fixed by the programming language (e.g. the C compiler only uses a limited number of sections, depending on the kind of data you are using in your programs).  Some compilers have extensions to associate a variable to a section, so the linke will consider it special in some way.  The compiler/assembler generates a symbol table in order for the linker to be able to use it to resolve dependencies.
If you want to add symbols to your program, the easiest way it to create an assembler module with the sections and symbols you want to add to the executable, then assemble it and link to the final executable.
Read about ld(1) program (the linker), and how it uses the link scripts (special hidden files that direct the linker on how to organize the sections in the different modules at link time) to handle the sections in an object file.  ELF is just a format.  If you use a link script and the help of the assembler, you'll be able to add any section you want or modify the normal memory map that programs use to have.",0.0,0.5355556,0.28681981563568115
10,"Question\nHow should a beginner start learning Google Earth Engine coding with python using colab? I know python, but how do I come to know about the objects of images and image classification.\nAnswer: i use geemap package for convert shape file to earth engine variable without uploading file on assets",0.0,0.2673117,0.07145553827285767
11,"Question\nI have to predict the winner of the Australian Open 2020. My dataset has these features: Location / Tournament / Date / Series / Court / Surface / Round / Winner / Loser etc. 
I trained my model using just these features 'Victory','Series','Court','Surface','WinRank','LoseRank','WPts','LPts','Wsets','Lsets','Weather' and I have a 0.93 accuracy but now I have to predict the name of the winner and I don't have any idea how to do it based on the model that I trained. 
Example:  If I have Dimitrov G. vs Simion G using random forest the model has to give me one of them as the winner of the match. 
I transformed the names of the players in dummy variables but after that, I don't know what to do? 
Can anyone give me just an idea of how could I predict the winner? so I can create a Tournament, please?\nAnswer: To address such a problem, I would suggest creation of a custom target variable. 
Firstly, the transformation of names of players into dummy variables seems reasonable (Just make sure, the unique player is identified with the same first and last name combinations thereby, avoiding duplications and thus, having the correct dummy code for the player name).
Now, to create the target variable ""wins"" - 

Use the two player names - P1, P2 of the match as input features for your model.
Define the ""wins"" as 1 if P1 wins and 0 if P2 wins.
Run your model with this set up. 
When you want to create a tournament and predict the winner, the inputs will be your 2 players and other match features. If, ""wins"" is close to 1, it means your P1 wins and output that player name.",0.0,0.21874744,0.04785044118762016
12,"Question\nI would like to retrieve the following (historical) information while using the 
ek.get_data()
function: ISIN, MSNR,MSNP, MSPI, NR, PI, NT
for some equity indices, take "".STOXX"" as an example. How do I do that? I want to specify I am using the get data function instead of the timeseries function because I need daily data and I would not respect the 3k rows limit in get.timeseries. 
In general: how do I get to know the right names for the fields that I have to use inside the 
ek.get_data()
function? I tried with both the codes that the Excel Eikon program uses and also the names used in the Eikon browser but they differ quite a lot from the example I saw in some sample code on the web (eg. TR.TotalReturnYTD vs TR.PCTCHG_YTD. How do I get to understand what would be the right name for the data types  I need?\nAnswer: Considering the codes in your function (ISIN, MSNR,MSNP, MSPI, NR, PI, NT), I'd guess you are interested in the Datastream dataset. You are probably beter off using the DataStream WebServices (DSWS) API instead of the Eikon API. This will also relieve you off your 3k row limit.",0.0,0.29502547,0.08704002946615219
13,"Question\nI'm trying to install a package name cutdapt in a windows server. I'm trying to do it this way: 
pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org cutadapt
But every time I try to install it I get this error: Building wheel for cutadapt (PEP 517): finished with status 'error'
Any ideas on how to pass this issue?\nAnswer: Turns out, that I had some problems with python 3.5, so I switched to python 3.8 and managed to install the package.",0.0,0.16597313,0.0275470782071352
14,"Question\nI am trying to create a website in Node.js. Though, as I am taking a course on how to use Artificial Intelligence and would like to implement such into my program. Therefore, I was wondering if it was feasible to connect Python Spyder to a Node.js based web application with somewhat ease.\nAnswer: Yes. That is possible. There are a few ways you can do this. You can use the child_process library, as mentioned above. Or, you can have a Python API that takes care of the AI stuff, which your Node app communicates with. 
The latter example is what I prefer as most my projects run on containers as micro services on Kubernates.",0.20408164,0.25351274,0.002443433739244938
15,"Question\nMy application is database heavy (full of very complex queries and stored procedures), it would be too hard and inefficient to write these queries in a lambda way, for this reason I'll have to stick with raw SQL.
So far I found these 2'micro' ORMs but none are compatible with MSSQL:
PonyORM
Supports: SQLite, PostgreSQL, MySQL and Oracle
Peewee
Supports: SQLite, PostgreSQL, MySQL and CockroachDB
I know SQLAlchemy supports MSSQL, however it would bee too big for what I need.\nAnswer: As of today - Jan 2020 - it seems that using pyodbc is still the way to go for SQL Server + Python if you are not using Django or any other big frameworks.",0.40816328,0.15342611,0.0648910254240036
16,"Question\nI am using catboost classifier for my binary classification model where I have a highly imbalance dataset of 0 -> 115000 & 1 -> 10000.
Can someone please guide me in how to use the following parameters in catboostclassifier:
1. class_weights 
2. scale_pos_weight? 
From the documentation, I am under the impression that I can use
Ratio of sum of negative class by sum of positive class i.e. 115000/10000=11.5 as the input for scale_pos_weight but I am not sure.
Please let me know what exact values to use for these two parameters and method to derive that value?
Thanks\nAnswer: For scale_pos_weight you would use negative class // positive class. in your case it would be 11 (I prefer to use whole numbers).
For class weight you would provide a tuple of the class imbalance. in your case it would be: class_weights = (1, 11)
class_weights is more flexible so you could define it for multi-class targets. for example if you have 4 classes you can set it: class_weights = (0.5,1,5,25)
and you need to use only one of the parameters. for a binary classification problem I would stick with scale_pos_weight.",0.81632656,0.09620249,0.5185786485671997
17,"Question\nSo I'm trying to write this exact string but I don't \n to make a new line I want to actually print \n on the screen. Any thoughts on how to go about this? (using python
Languages:\npython\nc\njava\nAnswer: adding a backslash will interpret the succeeding backslash character literally. print(""\\n"").",0.0,0.1611237,0.02596084401011467
18,"Question\nMy dataset has a column where upon printing the dataframe each entry in the column is like so:
{""Wireless Internet"",""Air conditioning"",Kitchen}
There are multiple things wrong with this that I would like to correct

Upon printing this in the console, python is printing this:'{""Wireless Internet"",""Air conditioning"",Kitchen}' Notice the quotations around the curly brackets, since python is printing a string. 
Ideally, I would like to find a way to convert this to a list like: [""Wireless Internet"",""Air conditioning"",""Kitchen""] but I do not know how. Further, notice how some words so not have quotations, such as Kitchen. I do not know how to go about correcting this. 

Thanks\nAnswer: what you have is a set of words, curly brackets are for Dictionary use such as {'Alex,'19',Marry','20'} its linking it as a key and value which in my case it name and age, rather than that you can use to_list command in python maybe it suits your needs.",0.0,0.18908489,0.03575309365987778
19,"Question\nI am using databases package in my fastapi app.  databases has execute and fetch functions, when I tried to return column values after inserting or updating using execute, it returns only the first value, how to get all the values without using fetch..
This is  my query

INSERT INTO table (col1, col2, col3, col4)
  VALUES ( val1, val2, val3, val4 ) RETURNING col1, col2;\nAnswer: INSERT INTO table (col1, col2, col3, col4) VALUES ( val1, val2, val3, val4 ) RETURNING (col1, col2);

you can use this query to get all columns",0.13605443,0.0014556646,0.018116826191544533
20,"Question\nI am using databases package in my fastapi app.  databases has execute and fetch functions, when I tried to return column values after inserting or updating using execute, it returns only the first value, how to get all the values without using fetch..
This is  my query

INSERT INTO table (col1, col2, col3, col4)
  VALUES ( val1, val2, val3, val4 ) RETURNING col1, col2;\nAnswer: I had trouble with this also, this was my query:

INSERT INTO notes (text, completed) VALUES (:text, :completed) RETURNING notes.id, notes.text, notes.completed

Using database.execute(...) will only return the first column.
But.. using database.fetch_one(...) inserts the data and returns all the columns.
Hopes this helps",0.0,0.18704501,0.03498583659529686
21,"Question\nI hope someone can help me as I would like to use PyCharm to develop in Python.
I have looked around but do not seem to be able to find any solutions to my issue.
I have Python 3 installed using the Windows msi.
I am using Windows 10. have downloaded PyCharm version 2019.3.1 (Community Edition).
I create a new project using the Pure Python option.
On trying to pip install any package, I get the error 
pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available
If I try this in VSCode using the terminal it works fine.
Can anyone tell me how to resolve this issue. It would appear to be a problem with the virtual environment but I do not know enough to resolve the issue.
Thanks for your time.\nAnswer: Sorry guys, it appears the basic interpreter option was on Anaconda, that I had installed sometime ago, forgotten about and it defaulted to it. Changing my basic interpreter option to my Python install (Python.exe) solved the issue.  
Keep on learning",0.81632656,0.017109752,0.63874751329422
22,"Question\nI was using tesseract-ocr (pytesseract) for spanish and it achieves very high accuracy when you set the language to spanish and of course, the text is in spanish. If you do not set language to spanish this does not perform that good. So, I'm assuming that tesseract is using many postprocessing models for spellchecking and improving the performance, I was wondering if anybody knows some of those models (ie edit distance, noisy channel modeling) that tesseract is applying. 
Thanks in advance!\nAnswer: Your assumption is wrong: If you do not specify language, tesseract uses English model as default for OCR. That is why you got wrong result for Spanish input text. There is no spellchecking post processing.",0.0,0.21692634,0.04705703631043434
23,"Question\nI am trying to get clear concept on how to get the Erwin generated DDL objects with python? I am aware Erwin API needs to be used. What i am looking if what Python Module and what API needs to used and how to use them ? I would be thankful for some example!\nAnswer: Here is a start:
import win32com.client
ERwin = win32com.client.Dispatch(""erwin9.SCAPI"")
I haven't been able to browse the scapi dll so what I know is from trial and error. Erwin publishes VB code that works, but it is not straightforward to convert.",0.20408164,0.24903077,0.0020204242318868637
24,"Question\nIm trying to install a venv in python3 (on CentOS). However i get the following error:

Error: Command '['/home/cleared/Develop/test/venv/bin/python3', '-Im',
  'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit
  status 1.

I guess there is some problem with my ensurepip...
Running python3 -m ensurepip results in 

FileNotFoundError: [Errno 2] No such file or directory:
  '/usr/lib64/python3.6/ensurepip/_bundled/pip-9.0.3-py2.py3-none-any.whl'

Looking in the /usr/lib64/python3.6/ensurepip/_bundled/ I find pip-18.1-py2.py3-none-any.whl and setuptools-40.6.2-py2.py3-none-any.whl, however no pip-9.0.3-py2.py3-none-any.whl
Running pip3 --version gives

pip 20.0.1 from /usr/local/lib/python3.6/site-packages/pip (python
  3.6)

Why is it looking for pip-9.0.3-py2.py3-none-any.whl when I'm running pip 20.0.1, and why to i have pip-18.1-py2.py3-none-any.whl? And how to I fix this?\nAnswer: These versions are harcoded at the beginning of./lib/python3.8/ensurepip/__init__.py. You can edit this file with the correct ones.
Regarding the reason of this corruption, I can only guess. I would bet on a problem during the installtion of this interpreter.",0.40816328,0.18644154,0.04916052892804146
25,"Question\nIm trying to install a venv in python3 (on CentOS). However i get the following error:

Error: Command '['/home/cleared/Develop/test/venv/bin/python3', '-Im',
  'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit
  status 1.

I guess there is some problem with my ensurepip...
Running python3 -m ensurepip results in 

FileNotFoundError: [Errno 2] No such file or directory:
  '/usr/lib64/python3.6/ensurepip/_bundled/pip-9.0.3-py2.py3-none-any.whl'

Looking in the /usr/lib64/python3.6/ensurepip/_bundled/ I find pip-18.1-py2.py3-none-any.whl and setuptools-40.6.2-py2.py3-none-any.whl, however no pip-9.0.3-py2.py3-none-any.whl
Running pip3 --version gives

pip 20.0.1 from /usr/local/lib/python3.6/site-packages/pip (python
  3.6)

Why is it looking for pip-9.0.3-py2.py3-none-any.whl when I'm running pip 20.0.1, and why to i have pip-18.1-py2.py3-none-any.whl? And how to I fix this?\nAnswer: I would make a clean reinstall of Python (and maybe some of its dependencies as well) with your operating system's package manager (yum?).",0.0,0.2163021,0.0467865988612175
26,"Question\nI’m way new to ML so I have a really rudimentary question. I would appreciate it if one clarifies it for me.
Suppose I have a set of tweets which labeled as negative and positive. I want to perform some sentiment analysis. 
I extracted 3 basic features: 

Emotion icons 
Exclamation marks 
Intensity words(very, really etc.). 

How should I use these features with SVM or other ML algorithms? 
In other words, how should I deploy the extracted features in SVM algorithm?
I'm working with python and already know how should I run SVM or other algorithms, but I don't have any idea about the relation between extracted features and role of them in each algorithm!
Based on the responses of some experts I update my question:
At first, I wanna appreciate your time and worthy explanations. I think my problem is solving… So in line with what you said, each ML algorithm may need some vectorized features and I should find a way to represent my features as vectors. I want to explain what I got from your explanation via a rudimentary example.
Say I have emoticon icons (for example 3 icons) as one feature:
1-Hence, I should represent this feature by a vector with 3 values. 
2-The vectorized feature can initial in this way : [0,0,0] (each value represents an icon = :) and :( and :P ). 
3-Next I should go through each tweet and check whether the tweet has an icon or not. For example [2,1,0] shows that the tweet has: :) 2 times, and :( 1 time, and :p no time.
4-After I check all the tweets I will have a big vector with the size of n*3 (n is the total number of my tweets). 
5-Stages 1-4 should be done for other features.
6-Then I should merge all those features by using m models of SVM (m is the number of my features) and then classify by majority vote or some other method.
Or should create a long vector by concatenating all of the vectors, and feed it to the SVM.
Could you please correct me if there is any misunderstanding? If it is not correct I will delete it otherwise I should let it stay cause It can be practical for any beginners such as me...
Thanks a bunch",1.0,-0.065218,1.1346893310546875
27,"Question\nI am trying to write a Python script to search a (very large) SVN repository for specific files (ending with.mat). Usually I would use os.walk() to walk through a directory and then search for the files with a RegEx. Unfortunately I can't use os.walk() for a repository, since it is not a local directory. 
Does anyone know how to do that? The repository is too large to download, so I need to search for it ""online"". 
Thanks in advance.\nAnswer: Something like
svn ls -R REPO-ROOT | grep PATTERN
will help",0.40816328,0.19653893,0.04478486627340317
28,"Question\n1) My goal is to create a sequence that is a list that contains ordered dictionaries. The only problem for me will be described below.

I want the list to represent a bunch of ""points"" which are for all intents and purposes just an ordered dictionary. However, I notice that when I use OrderedDict class, when I print the dictionary it comes up as OrderedDict([key value pair 1, key value pair 2,... etc)] For me, I would rather it behave like an ordered dictionary, BUT not having those DOUBLE ""messy/ugly"" ""end marks"" which are the ""[( )]"". I don't mind if the points have ONE, and only one, type of ""end marks"". Also I would also like it if when I print this data type that stuff like OrderedDict() doesn't show up. However, I do not mind if it shows up in return values. Like you know how when you print a list it doesn't show up as list(index0, index1,... etc) but instead it shows up as [index0, index1,... etc]. That is what I mean. Inside the point, it would look like this

point = {'height': 1, 'weight': 3, 'age': 5, etc} <- It could be brackets or braces or parentheses. Just some type of ""end mark"", but I preferably would like it to be in {} and having key value pairs indicated by key: value and have them separated by commas.
what_i_am_looking_for = [point0, point1, point2, point3,... etc]\nAnswer: In Python 3.6, the ordinary dict implementation was re-written and maintains key insertion order like OrderedDict, but was considered an implementation detail. Python 3.7 made this feature an official part of the language spec, so if you use Python 3.6+ just use dict instead of OrderedDict if you don't care about backward-compatibility with Python 3.5 or earlier.",0.0,0.47515315,0.2257705181837082
29,"Question\nI have a web page with data in different tables. I want to extract a particular table and compare with an excel sheet and see whether there are any differences. Note the web page is in a internal domain. I tried with requests and beautifulsoup but I got 401 error. Could anyone help how I can achieve this?\nAnswer: 401 is an Unauthorized Error - which suggests your username and password may be getting rejected, or their format not accepted. Review your credentials and the exact format / data names expected by the page to ensure you're correctly trying to connect.",0.0,0.14959705,0.022379277274012566
30,"Question\nI have a device with USB interface which I can connect to both my Ubuntu 18.04 machine and my Windows 10 machine. On Windows 10 I have to install the CP210x driver and manually attach it to the device (otherwise Windows tries to find the device manufacturer's driver - it's a CP210x serial chip), and in Linux write the vendorID and productID to the cp210x driver to allow it to attach to ttyUSB0. This works fine.  
The Windows driver is from SiliconLabs - the manufacturer of the UART-USB chip in the device.
So on Windows it is attached to COM5 and Linux to ttyUSB0 (Ubuntu, Raspbian)
Using Wireshark I can snoop the usb bus successfully on both operating systems.
The USB device sends data regularly over the USB bus and on Windows using Wireshark I can see this communication as ""URB_INTERRUPT in"" messages with the final few bytes actually containing the data I require.
On Linux it seems that the device connects but using Wireshark this time I can only see URB_BULK packets. Examining the endpoints using pyusb I see that there is no URB_Interrupt endpoint only the URB_Bulk.
Using the pyusb libraries on Linux it appears that the only endpoints available are URB_BULK.
Question mainly is how do I tell Linux to get the device to send via the Interrupt transfer mechanism as Windows seems to do.  I don't see a method in pyusb's set_configuration to do this (as no Interrupt transfer endpoints appear) and haven't found anything in the manufacturer's specification.
Failing that, of course, I could snoop the configuration messages on Windows, but there has to be something I'm missing here?\nAnswer: Disregard this, the answer was simple in the end: Windows was reassigning the device address on the bus to a different device.",0.0,0.19956326,0.03982549533247948
31,"Question\nI'm writing a software in python for windows which should be connected to a database. Using py2exe i want to make an executable file so that I don't have to install python in the machines the software is running. The problem is that I want the user to define where the database is located the very first time the software starts, but I don't know how to store this information so that the user doesn't have to tell everytime where is the database. I have no idea how to deal with it. (the code cannot be changed because it's just a.exe file). How would you do that?\nAnswer: I can think of some solutions:

You can assume the DB is in a fixed location - bad idea, might move or change name and then your program stop working
You can assume the DB is in the same folder as the.exe file and guide the user to run it in the same folder - better but still not perfect
Ask the user for the DB location and save the path in a configuration file. If the file doesn't exist or path doesn't lead to the file, the user should tell the program where is the DB, otherwise, read it from the config file - I think this is the best option.",0.0,0.039767444,0.0015814496437087655
32,"Question\nI've been going around but was not able to find a definitive answer...
So here's my question.. 
I come from javascript background. I'm trying to pickup python now.
In javascript, the basic practice would be to npm install (or use yarn)
This would install some required module in a specific project.
Now, for python, I've figured out that pip install is the module manager. 
I can't seem to figure out how to install this specific to a project (like how javascript does it)
Instead, it's all global.. I've found --user flag, but that's not really I'm looking for.
I've come to conclusion that this is just a complete different schema and I shouldn't try to approach as I have when using javascript.
However, I can't really find a good document why this method was favored.
It may be just my problem but I just can't not think about how I'm consistently bloating my pip global folder with modules that I'm only ever gonna use once for some single project.
Thanks.\nAnswer: A.) Anaconda (the simplest) Just download “Anaconda” that contains a lots of python modules pre installed just use them and it also has code editors. You can creat multiple module collections with the GUI.
B.) Venv = virtual environments (if you need something light and specific that contains specific packages for every project
macOS terminal commands:

Install venv
pip install virtualenv
Setup Venve (INSIDE BASE Project folder)
python3 -m venv thenameofyourvirtualenvironment
Start Venve
source thenameofyourvirtualenvironment/bin/activate
Stop Venve
deactivate
while it is activated you can install specific packages ex.: 
pip -q install bcrypt

C.) Use “Docker” it is great if you want to go in depth and have a solide experience, but it can get complicated.",0.0,0.24980026,0.062400173395872116
33,"Question\nFor reference, I'm trying to re-learn programming and python basics after years away.
I recently downloaded Anaconda as part of an online Python Course. However, every time I open Spyder or the Navigator they instantly create folders for what I assume are all the relevant libraries in  C:Users/Myself. These include.conda,.anaconda,.ipython,.matplotlib,.config and.spyder-py3.
My goal is to figure out how change where these files are placed so I can clean things up and have more control. However, I am not entirely sure why this occurs. My assumption is it's due to that being the default location for the Working Directory, thought the solutions I've seen to that are currently above me. I'm hoping this is a separate issue with a simpler solution, and any light that can be shed on this would be appreciated.\nAnswer: They are automatically created to store configuration changes for those related tools.  They are created in %USERPROFILE% under Windows.  
The following is NOT recommended:
You can change this either via the setx command or by opening the Start Menu search for variables.
- This opens the System Properties menu on the Advanced tab
- Click on Environmental Variables
- Under the user section, add a new variable called USERPROFILE and set the value to a location of your choice.",0.0,0.34416258,0.11844788491725922
34,"Question\nFor reference, I'm trying to re-learn programming and python basics after years away.
I recently downloaded Anaconda as part of an online Python Course. However, every time I open Spyder or the Navigator they instantly create folders for what I assume are all the relevant libraries in  C:Users/Myself. These include.conda,.anaconda,.ipython,.matplotlib,.config and.spyder-py3.
My goal is to figure out how change where these files are placed so I can clean things up and have more control. However, I am not entirely sure why this occurs. My assumption is it's due to that being the default location for the Working Directory, thought the solutions I've seen to that are currently above me. I'm hoping this is a separate issue with a simpler solution, and any light that can be shed on this would be appreciated.\nAnswer: Go to:
~\anaconda3\Lib\site-packages\jupyter_core\paths.py
in def get_home_dir():
You can specify your preferred path directly.
Other anaconda applications can be mortified by this way but you have to find out in which scripts you can change the homedir, and sometimes it has different names.",0.0,0.2066307,0.04269624873995781
35,"Question\nThe computer on which I want to install pip and modules is a secure offline environment.
Only Python 2.7 is installed on this computers(centos and ubuntu).
To run the source code I coded, I need another module.
But neither pip nor module is installed.
It looks like i need pip to install all of dependency files.
But I don't know how to install pip offline.
and i have no idea how to install the module offline without pip.
The only network connected is pypi from the my nexus3 repository.
Is there a good way?
Would it be better to install pip and install modules?
Would it be better to just install the module without installing pip?\nAnswer: using pip it is easier to install the packages as it manages certian things on its own. You can install modules manually by downloading its source code and then compiling it yourself. The choice is upto you, how you want to do things.",0.0,0.14672863,0.021529292687773705
36,"Question\nI've a python application in which I'm using print() method tho show text to a user.  When I interact with this application manually using kubectl exec... command I can see the output of prints. 
However, when script is executed automatically on container startup with CMD python3 /src/my_app.py (last entry in Dockerfile) then, the prints are gone (not shown in kubectl logs). Ayn suggestin on how to fix it?\nAnswer: It turned out to be a problem of python environment. Setting, these two environment variables PYTHONUNBUFFERED=1 and PYTHONIOENCODING=UTF-8 fixed the issue.",0.6122449,0.28313982,0.10831015557050705
37,"Question\nIn Spotfire I have a dashboard that uses both filtering (only one filtering scheme) and multiple markings to show the resulting data in a table.
I have created a data function which takes a column and outputs the data in the column after the active filtering scheme and markings are applied. 
However, this output column is only calculated if I have something marked in every marking. 
I want the output column to be calculated no matter how many of the markings are being used. Is there a way to do this? 
I was thinking I could use an IronPython script to edit the data function parameters for my input column to only check the boxes for markings that are actively being used. However, I can't find how to access those parameters with IronPython. 
Thanks!\nAnswer: I think it would be a combination of visuals being set to OR instead of AND for markings (if you have a set of markings that are being set from others). 
Also are all the input parameters set to required parameter perhaps unchecking that option would still run the script. In the r script you may want to replace null values as well. 
Not too sure without some example.",0.0,0.21430045,0.04592468589544296
38,"Question\nI should do it wiht only import os
I have problem, that i don't know how to make program after checking the specific folder for folders to do the same for folders in these folders and so on.\nAnswer: You can use os.walk(directory)",0.0,0.09366161,0.00877249613404274
39,"Question\nI installed the keyboard module for python with pip3 and after I runned my code the terminal shows me this message: ""ImportError: You must be root to use this library on linux."" Can anybody help me how to run it well? I tried to run it by switching to ""su -"" and tried it on this place as well.\nAnswer: Can you please post your script?
If you are just starting the program without a shebang it probably should not run and probably throw an ImportError
Try adding a shebang (#!) at the first line of you script.
A shebang is used in unix to select the interpreter you want to run your script.
Write this in the first line: #!/usr/bin/env python3
If this doesn't help try running it from the terminal using a precending dot like this:
python3./{file's_name}.py",0.81632656,0.22477508,0.3499331474304199
40,"Question\nI am trying to help someone put a python text game to be displayed with the inputs and output on his html website. What's the easiest way to do this, regarding the many outputs and inputs? Would it be to make it a flask app? I don't really know how else to describe the situation. Answers would be much appreciated.\nAnswer: I am developing a website with python3.8 and Sanic. It was pretty to use async, await and := ~",0.0,-0.00039237738,1.5396000208056648e-07
41,"Question\nI'm a complete beginner and a relative of mine asked me to build a simple 'contact us' website for them. It should include some information about his company and a form in which people that visit the website are able to send mails to my relative. I have been playing around with vue.js in order to build the frontend. I now want to know how to put the form to send mails and I read it has to be done with backend, so I thought I could use django as I have played with it in the past and I am confident using python. Is it too much for the work that I have to do? Should I use something simpler? I accept any suggestions please, Thanks.\nAnswer: You will probably should use something ready like Wix or Wordpress if want to do it fast if you prefer to learn in the process you can do it with Django and Vue, but this is indeed little bit overkill",0.0,-0.33982855,0.11548344045877457
42,"Question\nI'm a complete beginner and a relative of mine asked me to build a simple 'contact us' website for them. It should include some information about his company and a form in which people that visit the website are able to send mails to my relative. I have been playing around with vue.js in order to build the frontend. I now want to know how to put the form to send mails and I read it has to be done with backend, so I thought I could use django as I have played with it in the past and I am confident using python. Is it too much for the work that I have to do? Should I use something simpler? I accept any suggestions please, Thanks.\nAnswer: Flask
You can use Flask. it is simpler than Django and easy to learn. you can build a simple website like the one you want in less than 50 line.

Wordpress
If you want you can use Wordpress. it's easy to install and many hosting services support it already. Wordpress has so many plugins and templates to build contact us website in 10 minutes.

Wix
wix is easy, drag-n-drop website builder with many pre-build templates that you can use, check them out and you will find what you need.",0.0,0.15969875,0.025503691285848618
43,"Question\nnew user of mitmproxy here. I've figured out how to edit a request and replay it, and I'm wondering how to undo my edit.
More specifically, I go to a request's flow, hit 'e', then '8' to edit the request headers. Then I press 'd' to delete one of the headers. What do I press to undo this change? 'u' doesn't work.\nAnswer: It's possible to revoke changes to a flow, but not while editing. In your case, 'e' -> '8' -> 'd' headers, now press 'q' to go back to the flow -> press 'V' to revoke changes to the flow.",0.0,0.36189783,0.1309700310230255
44,"Question\nThis problem has been reported earlier but I couldn't find the exact solution for it. I installed ActiveTCL and downloaded tktable.py by  ""Guilherme Polo <ggpolo@gmail.com>"" to my site-packages, also added Tktable.dll, pkgindex.tcl, and tktable.tcl from ActiveTCL\lib\Tktable2.11 to my python38-32\tcl and dlls. I also tried setting the env variable for TCL_LIBRARY and TK_LIBRARY to tcl8.6 and tk8.6 respectively. But I am still getting invalid command name ""table"".
What is that I am missing? Those who made tktable work on windows 10 and python 3, how did you do it? I am out of ideas and would be grateful for some tips on it.\nAnswer: Seems like there was problem running the Tktable dlls in python38-32 bit version. It worked in 64 bit version. 
Thanks @Donal Fellows for your input.",0.0,0.09443277,0.008917548693716526
45,"Question\nI am currently, successfully, importing stock information from Yahoo using pandas-datareader. However, before the extracted data, I always get the following message: 

FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.

Would anyone have an idea of what it means and how to fix it?\nAnswer: You may find the 'util.testing' code in pandas_datareader, which is separate from pandas.",-0.11904762,0.36801827,0.23723317682743073
46,"Question\nI am currently, successfully, importing stock information from Yahoo using pandas-datareader. However, before the extracted data, I always get the following message: 

FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.

Would anyone have an idea of what it means and how to fix it?\nAnswer: For mac OS open /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas_datareader/compat/__init__.py
change: from pandas.util.testing import assert_frame_equal
to: from pandas.testing import assert_frame_equal",-0.23809524,0.18871439,0.1821664422750473
47,"Question\nI am currently, successfully, importing stock information from Yahoo using pandas-datareader. However, before the extracted data, I always get the following message: 

FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.

Would anyone have an idea of what it means and how to fix it?\nAnswer: Cause: The cause of this warning is that, basically, the pandas_datareader is importing a module from the pandas library that will be deprecated. Specifically, it is importing pandas.util.testing whereas the new preferred module will be pandas.testing.
Solution: First off this is a warning, and not an outright error, so it won't necessarily break your program. So depending on your exact use case, you may be able to ignore it for now.
That being said, there are a few options you can consider:

Option 1: Change the code yourself -- Go into the pandas_datareader module and modify the line of code in compat_init.py that currently says from pandas.util.testing import assert_frame_equal simply to from pandas.testing import assert_frame_equal. This will import the same function from the correct module.
Option 2: Wait for pandas-datareader to update --You can also wait for the library to be upgraded to import correctly and then run pip3 install --upgrade pandas-datareader. You can go to the Github repo for pandas-datareader and raise an issue.
Option 3: Ignore it -- Just ignore the warning for now since it doesn't break your program.",0.0,0.63797176,0.40700796246528625
48,"Question\nI've installed Python 3.7, and since installed python 3.8.
I've added both their folders and script folders to PATH, and made sure 3.8 is first as I'd like that to be default.
I see that the Python scripts folder has pip, pip3 and pip3.8 and the python 3.7 folder has the same (but with pip3.7 of course), so in cmd typing pip or pip3 will default to version 3.8 as I have that first in PATH.
This is great, as I can explicitly decide which pip version to run. However I don't know how to do to the same for Python. ie. run Python3.7 from cmd.
And things like Jupyter Notebooks only see a ""Python 3"" kernel and don't have an option for both.
How can I configure the PATH variables so I can specify which version of python3 to run?\nAnswer: What OS are you running? If you are running linux and used the system package panager to install python 3.8 you should be able to invoke python 3.8 by typing python3.8. Having multiple binaries named python3 in your PATH is problematic, and having python3 in your PATH point to python 3.8 instead of the system version (which is likely a lower version for your OS) will break your system's package manager. It is advisable to keep python3 in your PATH pointing to whatever the system defaults to, and use python3.8 to invoke python 3.8.
The python version that Jupyter sees will be the version from which you installed it. If you want to be able to use Jupyter with multiple python versions, create a virtual environment with your desired python version and install Jupyter in that environment. Once you activate that specific virtual env you will be sure that the jupyter command that you invoke will activate the currect python runtime.",0.20408164,0.36913455,0.02724246121942997
49,"Question\nI'm trying to remove pymssql and migrate to pyodbc on a python 3.6 project that I'm currently on. The network topology involves two machines that are both on the same LAN and same subnet. The client is an ARM debian based machine and the server is a windows box. Port 1433 is closed on the MSSQL box but port 32001 is open and pymssql is still able to remotely connect to the server as it somehow falls back to using the named pipe port (32001). 
My question is how is pymssql able to fall back onto this other port and communicate with the server? pyodbc is unable to do this as if I try using port 1433 it fails and doesn't try to locate the named pipe port. I've tried digging through the pymssql source code to see how it works but all I see is a call to dbopen which ends up in freetds library land. Also just to clarify, tsql -LH  returns the named pip information and open port which falls in line with what I've seen using netstat and nmap.  I'm 100% sure pymssql falls back to using the named pipe port as the connection to the named pipe port is established after connecting with pymssql.
Any insight or guidance as to how pymssql can do this but pyodbc can't would be greatly appreciated.\nAnswer: Removing the PORT= parameter and using the SERVER=ip\instance in the connection string uses the named pipes to do the connection instead of port 1433.  I'm still not sure how the driver itself knows to do this but it works and resolved my problem.",0.40816328,0.11661625,0.08499967306852341
50,"Question\nI can't find any example on how to make a convertcurrency request using the paypal API in python, can you give me some examples for this simple request?\nAnswer: Is this an existing integration for which you have an Adaptive APP ID? If not, the Adaptive Payments APIs are very old and deprecated, so you would not have permissions to use this, regardless of whether you can find ready-made code samples for Python.",0.0,0.22721833,0.05162816867232323
51,"Question\nI have just installed python3.8 and sublime text editor. I am attempting to run the python build on sublime text but I am met with ""Python was not found but can be installed"" error.
Both python and sublime are installed on E:\
When opening cmd prompt I can change dir and am able to run py from there without an issue.
I'm assuming that my sublime is not pointing to the correct dir but don't know how to resolve this issue.\nAnswer: i had the same problem, so i went to the microsoft store (windos 10) and simply installed ""python 3.9"" and problem was gone!
sorry for bad english btw",-0.71428573,-0.25059476,0.21500931680202484
52,"Question\nI want to improve my understanding of how to use logging correctly in Python. I want to use.ini file to configure it and what I want to do:

define basic logger config through.fileConfig(...) in some.py file
import logger, call logger = logging.getLogger(__name__) across the app and be sure that it uses my config file that I was loaded recently in different.py file

I read few resources over Internet ofc but they are describing tricks of how to configure it etc, but want I to understand is that.fileConfig works across all app or works only for file/module where it was declared.
Looks like I missed some small tip or smth like that.\nAnswer: It works across the whole app. Be sure to configure the correct loggers in the config. logger = logging.getLogger(__name__) works well if you know how to handle having a different logger in every module, otherwise you might be happier just calling logger = logging.getLogger(""mylogger"") which always gives you the same logger. If you only configure the root logger you might even skip that and simply use logging.info(""message"") directly.",0.40816328,0.37386,0.0011767148971557617
53,"Question\nAs I get, tf.layers.conv1d uses pipeline like this: BatchToSpaceND -> conv1d -> SpaceToBatchND. So the question is how to remove (or disable) BatchToSpaceND and SpaceToBatchND from the pipeline?\nAnswer: As I've investigated it's impossible to remove BatchToSpaceND and SpaceToBatchND from tf.layers.conv1d without changing and rebuilding tensorflow source code. One of the solutions is to replace layers to tf.nn.conv1d, which is low-level representation of convolutional layers (in fact tf.layers.conv1d is a wrapper around tf.nn.conv1d). These low-level implementations doesn't include BatchToSpaceND and SpaceToBatchND.",0.0,0.3597738,0.12943719327449799
54,"Question\nWhat exactly does this error mean and how can i fix it, am running server on port 8000 of local host.
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\nAnswer: Is firewall running on the server? If so, that may be blocking connections. You could disable firewall or add an exception on the server side to allow connections on port 8000.",0.0,0.26226854,0.06878478825092316
55,"Question\nFor example if I have 10 indices with similar names and they all alias to test-index, how would I get test-index-1 test-index-2 test-index-3 test-index-4, test-index-5, test-index-6, test-index-7, test-index-8, test-index-9, and test-index-10 to all point to the mapping in use currently when you to a GET /test-index/_mapping?\nAnswer: Not sure what you define as  'unified' mapping - but you can always use wildcards in mapping request. For example : /test-inde*/_mapping would give mapping of all indices in that pattern.",0.0,0.35538387,0.1262976974248886
56,"Question\nI have a webmap which is made in python using Folium.  I am adding various geojson layers from an underlying database.  I would like to do spatial analysis based on the user's location and their position relative to the various map overlays.  As part of this I want to display a marker on the map which indicates the user's current position, and which updates regularly as they move around.
I know how to add markers to the map from within python, using Folium.
I know how to get a constantly updating latitude / longitude of the user using JS 
navigator.geolocation.watchPosition(showPosition)
which then passes a position variable to the function showPosition.
I am currently just displaying this as text on the website for now.
What I have not been able to do is to add a marker to the Folium map from inside the webpage, using JS/Leaflet (as Folium is just a wrapper for Leaflet, i think I should be able to do this).  
The Folium map object seems to be assigned a new variable name every time the webpage is loaded, and I don't know how to ""get"" the map element and add a marker using the Leaflet syntax 
L.marker([lat, lon]).addTo(name_of_map_variable_which_keeps_changing)
Alternatively there might be a way to ""send"" the constantly changing lat/lon variables from the webpage back to the python script so that I can just use folium to add the marker.  
But I have been unable to figure this out or find the right assistance online and would appreciate any help.\nAnswer: OK, I have figured out a main part of the question - how to add a user location marker to the Folium map.  It is actually very simple:
https://python-visualization.github.io/folium/plugins.html#folium.plugins.LocateControl
I am still unable to pass the user's lat/lon through to my python script so that I can perform spatial queries using that location.  So am looking forward to anyone being able to answer that part.  Though I may have to post that as a separate question perhaps...",0.0,0.10326916,0.010664518922567368
57,"Question\nI created a Client with Java and a Server with Python. The Java client receive data using readUTF() of the class DataInputStream. My problem is that the function readUTF() expects a modified version of 'utf-8' that I don't know how to generate in the (Python) server side.\nAnswer: I got it!. Using the function read() of the class DataInputStream do work. The problem was that I initialized the destination buffer like this: byte[] ans = {}, instead of allocating some bytes. Thanks for everyone!",0.0,0.15398288,0.02371072582900524
58,"Question\nI just installed ubuntu 18.04 and I really don't know how does everything work yet. I use the last version of python in my windows system (3.8.1) and would like to use that version as well in ubuntu, but the ""pre-installed"" version of python is 2.7. Is there a way to uninstall that old version of python instead of changin the alias of the python command to match the version I want to use? Can you do that or does ubuntu need to have that version? If you could help me or explain this to me I would appreciate it.\nAnswer: Some services and application in Ubuntu use Python 2.x to run. It is not advisable to remove it. Rather, virtual environments maybe a good practice. There, you can work on Python 3.x, as per your needs, without messing up with the system's dependencies.",0.0,0.053444028,0.0028562641236931086
59,"Question\nI'm building my own 3d engine, I need to import 3d models into it, but I don't know how to do it.
I wonder if it is possible to convert a 3d model into an array of points; if it is possible, how do you do it?\nAnswer: This isn't something I've done before; but the premise is interesting so I thought I'd share my idea as I have worked with grids (pretty much an array) in 3D space during my time at university.
If you consider 3D space, you could represent that space as a three dimensional array quite simply with each dimension representing an axis. You could then treat each element in that array as a point in space and populate that with a value (say a Boolean of true/false, 1/0) to identify the points of your model within that three dimensional space.
All you'd need is the Height, Width and Depth of your model, with each one of these being the dimensions in your array. Populate the values with 0/false if the model does not have a point in that space, or 1/true if it does. This would then give you a representation of your model as a 3D array.",0.0,0.22448832,0.05039500445127487
60,"Question\nI'm trying to add someone to a specific server and then DM said person with just the discord ID.
The way it works is that someone is logging himself in using discord OAuth2 on a website and after he is logged in he should be added to a specific server and then the bot should DM saying something like Welcome to the server! 
Has anyone an idea how to do that?
Thanks for any help\nAnswer: It is not possible to leave or join servers with OAuth2. Nor is it possible to DM a user on Discord with a bot unless they share a mutual server.",0.0,0.14619201,0.021372105926275253
61,"Question\nI am an absolute beginner in AWS: I have created a key and an instance, the python script I want to run in the EC2 environment needs to loop through around 80,000 filings, tokenize the sentences in them, and use these sentences for some unsupervised learning. 
This might be a duplicate; but I can't find a way to copy these filings to the EC2 environment and run the python script in EC2, I am also not very sure as to how I can use boto3. I am using Mac OS.  I am just looking for any way to speed things up. Thank you so so much! I am forever grateful!!!\nAnswer: Here's one way that might help:

create a simple IAM role that allows S3 access to the bucket holding your files
apply that IAM role to the running EC2 instance (or launch a new instance with the IAM role)
install the awscli on the EC2 instance
SSH to the instance and sync the S3 files to the EC2 instance using aws s3 sync
run your app

I'm assuming you've launched EC2 with enough diskspace to hold the files.",0.0,0.24704015,0.06102883815765381
62,"Question\nI am an absolute beginner in AWS: I have created a key and an instance, the python script I want to run in the EC2 environment needs to loop through around 80,000 filings, tokenize the sentences in them, and use these sentences for some unsupervised learning. 
This might be a duplicate; but I can't find a way to copy these filings to the EC2 environment and run the python script in EC2, I am also not very sure as to how I can use boto3. I am using Mac OS.  I am just looking for any way to speed things up. Thank you so so much! I am forever grateful!!!\nAnswer: Here's what I tried recently:

Create the bucket and keep the bucket accessible for public.
Create the role and add HTTP option.
Upload all the files and make sure the files are public accessible.
Get the HTTP link of the S3 file.
Connect the instance through putty.
wget  copies the file into EC2
instance.

If your files are in zip format, one time copy enough to move all the files into instance.",0.0,0.19882601,0.03953178599476814
63,"Question\n: 1495 : 2020-02-11 11:55:00 (1, 0)
Here is my sample result but then when I'm trying to split it gives me error
Process terminate : 'Attendance' object has no attribute'split'
In the documentation it says 
print (attendance) # Attendance object
How to access it?\nAnswer: found the solution
i check in the github repository of pyzk and look for the attendance class and found all the object being return by the live_capture thank you :)",0.0,0.0634473,0.004025559406727552
64,"Question\nIf I type in which python I get: /home/USER/anaconda3/bin/python
If I type in echo $PYTHONPATH I get: /home/USER/terrain_planning/devel/lib/python2.7/dist-packages:/opt/ros/melodic/lib/python2.7/dist-packages
Should that not be the same? And is it not better to set it: usr/lib/python/
How would I do that? Add it to the PYTHONPATH or set the PYTHONPATH to that? But how to set which python?\nAnswer: You're mixing 2 environment variables:

PATH where which looks up for executables when they're accessed by name only. This variable is a list (colon/semi-colon separated depending on the platform) of directories containing executables. Not python specific. which python just looks in this variable and prints the full path
PYTHONPATH is python-specific list of directories (colon/semi-colon separated like PATH) where python looks for packages that aren't installed directly in the python distribution. The name & format is very close to system/shell PATH variable on purpose, but it's not used by the operating system at all, just by python.",0.81632656,0.38610977,0.18508648872375488
65,"Question\nI am new in python and I have a folder with 15 excel files  and I am trying to rename a specific column in each file to a standard name, for instance I have a columns named ""name, and server"" on different files but they entail of the same information so I need to rename them to a standard name like "" server name"" and I don't know how to start\nAnswer: If the position of the columns are the same across all excel file, you can iterate all the 15 excel files, locate the position of the column and replace the text directly. 
Alternatively, you can iterate all the files via read_xls (or read_csv depending on your context), reading them as dataframe and replace the necessary column name, and overwrite the file. Below is a reference syntax for your reference. 
df.rename(columns={ df.columns[1]: ""your value"" }, inplace = True)",0.0,0.4298529,0.1847735196352005
66,"Question\nBasically, I'm fighting with the age-old problem that Python's default json encoder does not support datetime. However all the solutions I can find call to json.dumps and manually pass the ""proper"" encoder on each invocation. And honestly, that can't be the best way to do it. Especially if you want to use a wrapper like jsonify to set up your response object properly, where you can't even specify these parameters. 
So: long story short: how to override the global default encoder in Python's JSON implementation to a custom one, that actually does support the features I want?
EDIT: ok so I figured out how to do this for my specific use case (inside Flask). You can do app.json_encoder = MyCustomJSONEncoder there. However how to do this outside of flask would still be an interesting question.\nAnswer: Unfortunately, I could not find a way to set default encoders or decoders for the json module.
So the best way is to do what flask do, that is wrapping the calls to dump or dumps, and provide a default in that wrapper.",0.0,0.24783963,0.061424482613801956
67,"Question\nNote : radish is a ""Gherkin-plus"" framework—it adds Scenario Loops and Preconditions to the standard Gherkin language, which makes it more friendly to programmers.
So how i can use it or use an other method to use Gherkin step autocomplete with Pycharm.
Thank's\nAnswer: I have solve this problem by buying a professional version of PyCharm, autocomplete is not available for Community version :(",0.0,0.12582508,0.015831949189305305
68,"Question\nIf I have a list of files in a directory is it possible to estimate a memory use number that would be taken up by reading or concatenating the files using pd.read_csv(file) or pd.concat([df1, df2])?
I would like to break these files up into concatenation 'batches' where each batch will not exceed a certain memory usage so I do not run into local memory errors.
Using os.path.getsize() will allow me to obtain the file sizes  and df.memory_usage() will tell me how much memory the dataframe will use once it's already read in but is there a way to estimate this with just the files themselves?\nAnswer: You could open each CSV, read first 1000 lines only into DataFrame, and then check memory usage. Then scale estimated memory usage by number of lines in the file.
Note that memory_usage() isn't accurate with default arguments, because it won't count strings' memory usage. You need memory_usage(deep=True), although that might overestimate memory usage in some cases. But better to overestimate than underestimate.",0.0,0.3332399,0.11104883998632431
69,"Question\nI have two different excel files. One of them is including time series data (268943 accident time rows) as below
The other file is value of 14 workers measured daily from 8 to 17 and during 4 months(all data merged in one file) 
I am trying to understand correlation between accident times and  values (hourly from 8 to 17 per one hour and daily from Monday to Friday and monthly)
Which statistical method is fit(Normalized Auto or cross correlation) and how can I do that?
Generally, in the questions, the correlation analysis are performed between two time series based values, but I think this is a little bit different. Also, here times are different.
Thank your advance..\nAnswer: I think the accident times and the bloodsugar levels are not coming from the same source, and so I think it is not possible to draw a correlation between these two separate datasets. If you would like to assume that the blood sugar levels of all 14 workers reflect that of the workers accident dataset, that is a different story. But what if those who had accidents had a significantly different blood sugar level profile than the rest, and what if your tiny dataset of 14 workers does not comprise such examples? I think the best you may do is to graph the blood sugar level of your 14 worker dataset and also similarly analyze the accident dataset separately, and try to see visually whether there is any correlation here.",0.81632656,0.02849412,0.6206799745559692
70,"Question\nf""{f.__module__}.{f.__name__}"" doesn't work because function f can be local, eg inside another function. We need to add some kind of marked (.<local>.) in the path to specify that this function is local. But how to determine when we need to add this marker?\nAnswer: Use f.__qualname__ instead of __name__.",0.40816328,0.44482905,0.001344378455542028
71,"Question\nI have been working on an algorithm trading project where I used R to fit a random forest using historical data while the real-time trading system is in Python. 
I have fitted a model I'd like to use in R and am now wondering how can I use this model for prediction purposes in the Python system.
Thanks.\nAnswer: There are several options:
(1) Random Forest is a well researched algorithm and is available in Python through sci-kit learn. Consider implementing it natively in Python if that is the end goal.
(2) If that is not an option, you can call R from within Python using the Rpy2 library. There is plenty of online help available for this library, so just do a google search for it.
Hope this helps.",0.40816328,0.25882196,0.02230282872915268
72,"Question\nI have a Django application, which I hosted on pythonanywhere. For the database, I have used SQLite(default).
So I want to know how many users my applications can handle?
And what if two user register form or make post at same time, will my application will crash?\nAnswer: SQLite supports multiple users, however it locks the database when write operations is being executed.
In other words,concurrent writes cannot be treated with this database, so is not recommended.
You can use PostgreSQL or MySQL as an alternative.",0.0,0.25252002,0.06376636028289795
73,"Question\nHow can I cycle through items in a DynamoDB table?
That is, if I have a table containing [A,B,C], how can I efficiently get item A with my first call, item B with my second call, item C with my third call and item A again with my fourth call, repeat?
This table could in the future expand to include D, E, F etc and I would like to incorporate the new elements into the cycle. 
The current way I am doing it is giving each item an attribute ""seen"". We scan the whole table, find an element that's not ""seen"" and put it back as ""seen"". When everything has been ""seen"", make all elements not ""seen"" again. This is very expensive.\nAnswer: I think the simplest option is probably:

use scan with Limit=1 and do not supply ExclusiveStartKey, this will get the first item
if an item was returned and LastEvaluatedKey is present in the response, then re-run scan with ExclusiveStartKey set to the LastEvaluatedKey of the prior response and again Limit=1, repeat step 2 until no item returned or LastEvaluatedKey is absent
when you get zero items returned, you've hit the end of the table, goto step 1

This is an unusual pattern and probably not super-efficient, so if you can share any more about what you're actually trying to do here, then we might be able to propose better options.",0.40816328,0.29997778,0.011704102158546448
74,"Question\nHow can I cycle through items in a DynamoDB table?
That is, if I have a table containing [A,B,C], how can I efficiently get item A with my first call, item B with my second call, item C with my third call and item A again with my fourth call, repeat?
This table could in the future expand to include D, E, F etc and I would like to incorporate the new elements into the cycle. 
The current way I am doing it is giving each item an attribute ""seen"". We scan the whole table, find an element that's not ""seen"" and put it back as ""seen"". When everything has been ""seen"", make all elements not ""seen"" again. This is very expensive.\nAnswer: The efficient way to return items that haven't been seen would be to have an attribute of seen=no included when inserted.  Then you could have a global secondary index over that attribute which you could then Query().
There isn't an efficient way to reset all the seen=yes attributes back to no.  Scan() and Query() would both end up returning the entire table and you'd end up updating records one by one.  That will not be fast nor cheap with a large table. 
EDIT
Once all the records have seen=""yes"" and you want to reset them back to seen=""no""  A query on the GSI suggested above will work exactly like a scan...every record will have to be read and then updated.  
If you have 1M records, each about 1K, and you want to reset them...you're going to need
250K reads (since you can read 4 records with a single 4KB RCU)
1M writes",0.0,0.29199284,0.0852598175406456
75,"Question\nI'm on Debian using python3.7. I have a network drive that I typically mount to /media/N_drive with dir_mode=0777 and file_mode=0777. I generally have no issues with reading/writing files in this network drive.
Occasionally, especially soon after mounting the drive, if I try to run any Python script with os.getcwd() (including any imported libraries like pandas) I get the error FileNotFoundError: [Errno 2] No such file or directory. If I cd up to the local drive (cd /media/) the script runs fine.
Doing some reading, it sounds like this error indicates that the working directory has been deleted. Yet I can still navigate to the directory, create files, etc. when I'm in the shell. It only seems to be Python's os.getcwd() that has problems.
What is more strange is that this behavior is not predictable. Typically if I wait ~1 hour after mounting the drive the same script will run just fine. 
I suspect this has something to do with the way the drive is mounted maybe? Any ideas how to troubleshoot it?\nAnswer: To me, it seems a problem with the mount, e.g. the network disk will be disconnected, and reconnected. So your cwd is not more valid. Note: cwd is pointing to a disk+inode, it is not a name (which you will see). So /media/a is different to /media/a after a reconnection.
If you are looking on how to solve the mounting, you are in the wrong place. Try Unix&Linux sister site, or Serverfault (also a sister site).
If you are looking how to solve programmatically: save cwd at beginning of the script and use os.path.join() at every path access, so that you forcing absolute paths, and not relative paths, and so you should be on the correct location.  This is not save, if you happen to read a file during disconnection.",0.40816328,0.22410566,0.033877208828926086
76,"Question\nI have been told it is 'bad practice' to return data from a Django view and use those returned items in Javascript that is loaded on the page.
For example: if I was writing an app that needed some extra data to load/display a javascript based graph, I was told it's wrong to pass that data directly into the javascript on the page from a template variable passed from the Django view.
My first thought: 

Just get the data the graph needs in the django view and return it in a context variable to be used in the template. Then just reference that context variable directly in the javascript in the template. 

It should load the data fine - but I was told that is the wrong way.
So how is it best achieved?
My second thought: 

Spin up Django Rest Framework and create an endpoint where you pass any required data to and make an AJAX request when the page loads - then load the data and do the JS stuff needed. 

This works, except for one thing, how do I get the variables required for the AJAX request into the AJAX request itself? 
I'd have to get them either from the context (which is the 'wrong way') or get the parameters from the URL. Is there any easy way to parse the data out of the URL in JS? It seems like a pain in the neck just to get around not utilizing the view for the data needed and accessing those variables directly in the JS.
So, is it really 'bad practice' to pass data from the Django view and use it directly in the Javascript? 
Are both methods acceptable?
What is the Django appropriate way to get data like that into the Javascript on a given page/template?\nAnswer: Passing data directly is not always the wrong way to go. JS is there so you can execute code when everything else is ready. So when they tell you it's the wrong way to pass data directly, it's because there is no point in making the page and data heavier than it should be before JS kicks in. 
BUT it's okay to pass the essential data so your JS codes knows what it has to do. To make it more clear, let's look into your case:
You want to render a graph. And graphs are sometimes heavy to render and it can make the first render slow. And most of the time, graphs are not so useful without the extra context that your page provides.",0.81632656,0.2826388,0.28482264280319214
77,"Question\nI recently downloaded python for the first time and when I load into pycharm to create a new project and it asks to select an interpreter python doesn't show up even when I click the plus sign and search through all my files it doesn't show even though I have the latest python version installed and I have windows 10 I tried deleting both programs and redownloading them but that doesn't seem to work either please if possible and the answer may be obvious but sorry I'm a beginner and also looking at videos didn't help either.\nAnswer: You have no navigate to the folder where python is downloaded and just select there.
Try the following path C:\Users\YourName\AppData\Local\Programs\Python\Python38-32\python.exe",0.81632656,0.30709308,0.2593187391757965
78,"Question\nI have a program written in python 2.7.5 scipy 0.18.1 that is able to run scipy.interpolate.UnivariateSpline with arrays that are non-sequential. When I try to run the same program in python 2.7.14 / scipy 1.0.0 I get the following error:
File ""/usr/local/lib/python2.7/site-packages/scipy/interpolate/fitpack2.py"", line 176, in init
    raise ValueError('x must be strictly increasing')
Usually I would just fix the arrays to remove the non-sequential values. But in this case I need to reproduce the exact same solution produced by the earlier version of python/scipy. Can anyone tell me how the earlier code dealt with the situation where the values were not sequential?\nAnswer: IIRC this was whatever the FITPACK (the fortran library the univariatespline class wraps) was doing. So the first stop would be to remove the check from your local scipy install and see if this does the trick",0.40816328,-0.08820093,0.24637742340564728
79,"Question\nI've a conceptual doubt, I don't know if it's even possible.
Assume I log on a Windows equipment with an account (let's call it AccountA from UserA). However, this account has access to the mail account (Outlook) of the UserA and another fictional user (UserX, without any password, you logg in thanks to Windows authentication), shared by UserA, UserB and UserC.
Can I send a mail from User A using the account of User X via Python? If so, how shall I do the log in?
Thanks in advance\nAnswer: A interesting feature with Windows Authentication is that is uses the well known Kerberos protocol under the hood. In a private environment, that means if a server trusts the Active Directory domain, you can pass the authentication of a client machine to that server provided the service is Kerberized, even if the server is a Linux or Unix box and is not a domain member.
It is mainly used for Web servers in corporate environment, but could be used for any kerberized service. Postfix for example is know to accept this kind of authentication.

If you want to access an external mail server, you will have to store the credential in plain text on the client machine, which is bad. An acceptable way would be to use a file only readable by the current user (live protection) in an encrypted folder (at rest protection).",0.40816328,0.23882431,0.028675686568021774
80,"Question\nI was wondering how to switch two words around in a file document in python. Example: I want to switch the words motorcycle to car, and car to motorcycle.
The way I'm doing it is making it have all the words motorcycle change to car, and because car is being switched to motorcycle, it get's switched back to car. Hopefully that makes sense.\nAnswer: First, replace all the motocycle to carholder
Second, replace all car to motocycle
Third, replace all carholder to car
That's it",0.81632656,-0.02573955,0.7090753316879272
81,"Question\nI am trying to install micropython-umqtt.robust on my Wemos D1 mini.
The way i tried this is as follow.
I use the Thonny editor

I have connected the wemos to the internet.
in wrepl type:
import upip
upip.install('micropython-umqtt.simple')
I get the folowing error: Installing to: /lib/
Error installing'micropython-umqtt.simple': Package not found, packages may be partially        installed
upip.install('micropython-umqtt.robust')
I get the folowing error: Error installing'micropython-umqtt.robust': Package not found,   packages may be partially installed

Can umqtt be installed on Wemos D1 mini? if yes how do I do this?\nAnswer: I think the MicroPython build available from micropython.org already bundles MQTT so no need to install it with upip. Try this directly from the REPL:
from umqtt.robust import MQTTClient
or
from umqtt.simple import MQTTClient
and start using it from there
mqtt = MQTTClient(id, server, user, password)",0.0,0.15211302,0.023138370364904404
82,"Question\nI am trying to install micropython-umqtt.robust on my Wemos D1 mini.
The way i tried this is as follow.
I use the Thonny editor

I have connected the wemos to the internet.
in wrepl type:
import upip
upip.install('micropython-umqtt.simple')
I get the folowing error: Installing to: /lib/
Error installing'micropython-umqtt.simple': Package not found, packages may be partially        installed
upip.install('micropython-umqtt.robust')
I get the folowing error: Error installing'micropython-umqtt.robust': Package not found,   packages may be partially installed

Can umqtt be installed on Wemos D1 mini? if yes how do I do this?\nAnswer: Thanks for your help Reilly, 
The way I solved it is as follow. With a bit more understanding of mqtt and micropython I found that the only thing that happens when you try to install umqtt simple and umqtt robust,is that it makes in de lib directory of your wemos a new directory umqtt. Inside this directory it installs two files robust.py and simple.py. While trying to install them I kept having error messages. But I found a GitHub page for these two files, so I copied these files. Made the umqtt directory within the lib directory and in this umqtt directory I pasted the two copied files. Now I can use mqtt on my wemos.",0.40816328,0.2335409,0.030492978170514107
83,"Question\nI have implemented a Zynq ZCU102 board in vivado and I want to use final "".XSA"" file into VITIS, but after creating a new platform, its languages are C and C++, While in the documentation was told that vitis supports python.
My question is how can I add python to my vitis platform?
Thank you\nAnswer: Running Python in FPGA needs an Operating System. I had to run Linux OS on my FPGA using petaLinux and then run python code on it.",0.0,0.11163241,0.012461794540286064
84,"Question\nI just can’t find any information about the implementation of the system of adding to favorites for registered users.
The model has a Post model. It has a couple of fields of format String. The author field, which indicates which user made the POST request, etc.
But how to make it so that the user can add this Post to his “favorites”, so that later you can get a JSON response with all the posts that he added to himself. Well, respectively, so that you can remove from favorites.
Are there any ideas?\nAnswer: You can add a favorite_posts field (many-to-many) in your Author model.",-0.71428573,0.25495172,0.9394212365150452
85,"Question\nHow can i customize hrs,days,months of date time module in python?
day of 5 hrs only, a month of 20 days only, and a year of 10 months only.
using date time module.\nAnswer: I agree with @TimPeters. This just doesn't fit in what datetime does.  
For your needs, I would be inclined to start my own class from scratch, as that is pretty far from datetime. 
That said...you could look into monkeypatching datetime...but I would recommend against it. It's a pretty complex beast, and changing something as fundamental as the number of hours in a day will blow away unknown assumptions within the code, and would certainly turn its unit tests upside down. 
Build your own from scratch is my advice.",0.0,0.09324837,0.00869525782763958
86,"Question\nWhenever I run a flask command in my project, I get an error of the form zsh: (correct file path)/venv/bin/flask: bad interpreter: (incorrect, old file path)/venv/bin/python3. I believe the error is due to the file paths not matching, and the second file path no longer existing. I changed the name of the directory for my project when I changed the name of the project, but I don't know how to change the path that flask searches for the interpreter in.
Thanks in advance.
Edit: I just tried going into the flask file at (correct file path)/venv/bin. I saw that it still had #!(incorrect, old file path)/venv/bin/python3 at the top. I tried changing this to #!(correct file path)/venv/bin/python3, but the same error as before persisted, as well as the flask app not being able to find the flask_login module, which it was not having issues with before.\nAnswer: Ok, I figured out how to fix it. I had to go into my (correct file path)/venv/bin/flask file and change the file path after the #! to the correct file path. I had to do the same for pip, pip3, and pip3.7 which were all in the same location as the flask file. Then I had to reinstall the flask_login package. This fixed everything.",0.0,0.33864355,0.11467945575714111
87,"Question\nI have a python regression model that predicts one's level of happiness based on user-input data, i have trained and tested it using Python.
But I'm using React Native to create my mobile application. 
My mobile application will take in the user-input data needed and will output a prediction on their level of happiness. Anyone has an idea on how to implement this? Any advice would be appreciated! I lack the experience, but have an interest in this area, Im still learning so please help me out :)\nAnswer: You need to create python API and call it from the mobile application by passing the input features. Python API will return you the forecasted value. This API will load the regression model and make a forecast on given input features. I hope It will help.",0.40816328,0.16203517,0.06057904660701752
88,"Question\nI have set up a small flask webpage but in only runs on localhost  while I would like to make it run on my local network, how do I do that?\nAnswer: Just my 2 cents on this, I just did some research, there are many suggestions online...
Adding a parameter to your app.run(), by default it runs on localhost, so change it to app.run(host= '0.0.0.0') to run on your machines IP address.
Few other things you could do is to use the flask executable to start up your local server, and then you can use flask run --host=0.0.0.0 to change the default IP which is 127.0.0.1 and open it up to non local connections.
The thing is you should use the app.run() method which is much better than any other methods.
Hope it helps a little, if not good luck :)",0.40816328,0.37976235,0.0008066127193160355
89,"Question\n.doc files,.pdf files, and some image formats all contain metadata about the file, such as the author.
Is a.py file just a plain text file whose contents are all visible once opened with a code editor like Sublime, or does it also contain metadata? If so, how does one access this metadata?\nAnswer: On Linux and most Unixes,.py's are just text (sometimes unicode text).
On Windows and Mac, there are cubbyholes where you can stash data, but I doubt Python uses them.
.pyc's, on the other hand, have at least a little metadata stuff in them - or so I've heard.  Specifically: there's supposed to be a timestamp in them, so that if you copy a filesystem hierarchy, python won't automatically recreate all the.pyc's on import.  There may or may not be more.",0.81632656,0.40905905,0.16586682200431824
90,"Question\nI am working with spyder - python. I want to test my codes. I have followed the pip install spyder-unittest and pip install pytest. I have restarted the kernel and restarted my MAC as well. Yet, Unit Testing tab does not appear. Even when I drop down Run cannot find the Run Unit test. Does someone know how to do this?\nAnswer: So, I solved the issue by running the command:
conda config --set channel_priority false. 
And then proceeded with the unittest download with the command run:
conda install -c spyder-ide spyder-unittest. 
The first command run conda config --set channel_priority false may solve other issues such as:
Solving environment: failed with initial frozen solve. Retrying with flexible solve",1.0,0.089854,0.8283657431602478
91,"Question\nI am trying to open an existing file in a subfolder of the current working directory. This is my command:
fyle = open('/SPAdes/default/{}'.format(file), 'r')
The filevariable contains the correct filename, the folder structure is correct (working on macOS), and the file exists.
This command, however, results if this error message:
FileNotFoundError: [Errno 2] No such file or directory: [filename]
Does it have anything to do with the way JupyterLab works? How am I supposed to specify the folder srtucture on Jupyter? I am able to create a new file in the current folder, but I am not able to create one in a subfolder of the current one (results in the same error message).
The folder structure is recognized on the same Jupyter notebook by bash commands, but I am somehow not able to access subfolders using python code.
Any idea as to what is wrong with the way I specified the folder structure?
Thanks a lot in advance.\nAnswer: There shouldn’t be a forward slash in front of SPAdes. 
Paths starting with a slash exist high up in file hierarchy. You said this is a sub-directory of your current working directory.",0.81632656,0.25255316,0.31784042716026306
92,"Question\nI know how to do a few things already:

Summarise a model with model.summary(). But this actually doesn't print everything about the model, just the coarse details.
Save model with model.save() and load model with keras.models.load_model()
Get weights with model.get_weights()
Get the training history from model.fit()

But none of these seem to give me a catch all solution for saving everything from end to end so that I can 100% reproduce a model architecture, training setup, and results.
Any help filling in the gaps would be appreciated.\nAnswer: model.to_json() can be used to convert model config into json format and save it as a json. 
You can recreate the model from json using model_from_json found in keras.models
Weights can be saved separately using model.save_weights.
Useful in checkpointing your model. Note that model.save saves both of these together. Saving only the weights and loading them back useful when you need to work with the variables used in defining the model. In that case create the model using the code and do model.load_weights.",0.0,0.35529715,0.12623606622219086
93,"Question\nI have to analyze a log file which will generate continuously 24*7. So, the data will be huge. I will have credentials to where log file is generating. But how can I get that streaming data ( I mean like any free tools or processes) so that I can use it in my python code to extract some required information from that log stream and will have to prepare a real time dashboard with that data. please tell some possibilities to achieve above task.\nAnswer: Just a suggestion
You could try with ELK:
ELK, short for Elasticsearch (ES), Logstash, and Kibana, is the most popular open source log aggregation tool. Es is a NoSQL. Logstash is a log pipeline system that can ingest data, transform it, and load it into a store like Elasticsearch. Kibana is a visualization layer on top of Elasticsearch.
or 
you could use Mongo DB to handle such huge amount of data:
MongoDB is an open-source document database and leading NoSQL. Mongo DB stores data in a json format. Process the logs and store it in a json format and retrieve it for any further use. 
Basically its not a simple question to explain, it depends on the scenarios.",0.0,0.20182443,0.04073309898376465
94,"Question\nI'm trying to create a function on Azure Function Apps that is given back a PDF and uses the python tika library to parse it.
This setup works fine locally, and I have the python function set up in Azure as well, however I cannot figure out how to include Java in the environment? 
At the moment, when I try to run the code on the server I get the error message 

Unable to run java; is it installed?
  Failed to receive startup confirmation from startServer.\nAnswer: So this isnt possible at this time. To solve it, I abstracted out the tika code into a Java Function app and used that instead.",0.0,0.23824155,0.05675903707742691
95,"Question\nI've been doing Node programming for a while and one thing I'm just very tired of is having to worry about blocking the event loop with anything that requires lots of cpu time. I'd also like to expand my language skills to something more focused on machine learning, so python seemed like a good choice based on what I've read.
However, I keep seeing that python is also single threaded, but I get the feeling this wording is being used in a different way than how it's usually used in node. Python is the go to language for a lot of heavy data manipulation so I can't imagine it blocks the same way node does. Can someone with more familiarity with python (and some with node) explain how their processing of concurrent requests differs when 1 request is cpu intensive?\nAnswer: First of all Python is not single-threaded, but its standard library contains everything required to manage threads. It works fine for IO bound tasks, but does not for CPU bound tasks because of the Global Interpretor Lock which prevents more than one thread to execute Python code at the same time.
For data processing tasks, several modules exist that add low level (C code level) processing and internally manage the GIL to be able to use multi-core processing. The most used modules here are scipy and numpy (scientific and numeric processing) and pandas which is an efficient data frame processing tools using numpy arrays for its underlying containers.
Long story short: For io bound tasks, Python is great. If your problem is vectorizable through numpy or pandas, Python is great. If your problem is CPU  intensive and neither numpy nor pandas will be used, Python is not at its best.",0.40816328,0.51549226,0.011519510298967361
96,"Question\nI'm running the following command using subprocess.check_call 
['/home/user/anaconda3/envs/hum2/bin/bowtie2-build', '-f', '/media/user/extra/tmp/subhm/sub_humann2_temp/sub_custom_chocophlan_database.ffn', '/media/user/extra/tmp/subhm/sub_humann2_temp/sub_bowtie2_index','--threads 8']
But for some reason, it ignores the --threads argument and runs on one thread only. I've checked outside of python with the same command that the threads are launched. This only happens when calling from subprocess, any idea on how to fix this? 
thanks\nAnswer: You are passing '--threads 8' and not '--threads',  '8'. Although it could be '--threads=8' but I don't know the command.",0.40816328,0.29041618,0.01386437937617302
97,"Question\nI just want to know how can I change the name of mp4 video using python. I tried looking on the internet but could not find it. I am a beginner in python\nAnswer: you can use os module to rename as follows...

import os
os.rename('full_file_path_old','new_file_name_path)",0.0,-0.09184921,0.008436276577413082
98,"Question\nI am developing a robot based on StereoPI. I have successfully calibrated the cameras and obtained a fairly accurate depth map. However, I am unable to convert my depth map to point cloud so that I can obtain the actual distance of an object. I have been trying to use cv2.reprojectImageTo3D, but see no success. May I ask if there is a tutorial or guide which teaches how to convert disparity map to point cloud? 
I am trying very hard to learn and find reliable sources but see on avail. So, Thank you very much in advance.\nAnswer: By calibrating your cameras you compute their interior orientation parameters (IOP - or intrinsic parameters). To compute the XYZ coordinates from the disparity you need also the exterior orientation parameters (EOP).
If you want your point cloud relative to the robot position, the EOP can be simplified, otherwise, you need to take into account the robot's position and rotation, which can be retrieved with a GNSS receiver and intertial measurement unit (IMU). Note that is very likely that such data need to be processed with a Kalman filter.
Then, assuming you got both (i) the IOP and EOP of your cameras, and (ii) the disparity map, you can generate the point cloud by intersection. There are several ways to accomplish this, I suggest using the collinearity equations.",0.0,0.12749302,0.016254471614956856
99,"Question\nI appreciate it if somebody gives the main idea of how to handle submission/retrieval form implementation in Bootstrap modals. I saw many examples on google but it is still ambiguous for me. Why it is required to have a separate Html file for modal-forms template? Where SQL commands will be written? What is the flow in submission/retrieval forms (I mean steps)? What is the best practice to implement these kind of forms? I'm fairly new to Django, please be nice and helpful.\nAnswer: No need for separate file for modal-form. Here MVT structure following, whenever forms are used. Easy interaction to template. Moreover if you go through Django documentation, you will get to know easily. 
Submission - mention the form action url. It will call that and check the django forms",0.0,0.23107612,0.05339617282152176
0,"Question\nWhen the outputs (prediction) are the probabilities coming from a Softmax function, and the training target is one-hot type, how do we compare those two different kinds of data to calculate the accuracy? 
(the number of training data classified correctly) / (the number of the total training data) *100%\nAnswer: Usually, we assign the class label with highest probability in the output of the soft max function as the label.",0.0,0.14060509,0.019769791513681412
1,"Question\nMac has recently updated its terminal shell to Zsh from bash. As a python programmer, I'd like to have a consistency in python versions across all the systems that includes terminals, & IDE.
On a bash shell, to update the python version in the terminal to 3.8.1, I had followed the below process
nano ~/.bash_profile
alias python=python3
ctrl + x
y
enter
This enabled me to update the python version from 2.7.6 to 3.8.1. However, repeating the same steps for zsh shell didn't work out. Tried a tweak of the above process, and somehow stuck with 3.7.3
steps followed
which python3 #Location of the python3.8.1 terminal command file is found. Installed it.
python --version #returned python 3.7.3
PS: I am an absolute beginner in python, so please consider that in your response. I hope i am not wasting your time.\nAnswer: it is actually not recommendet to update the default Python executable system-wide because some applications are depending on it.
Although, you can use venv (virtual environment) or for using another version of Python within your ZSH you can also put an alias like python='python3' in  your ~/.zsh_profile and source it.
Hope that helps.
Greetings",0.40816328,-0.15487245,0.31700924038887024
2,"Question\nSo I'm trying to make a color gradient, from a color to completely black, as well as from a color to completely white.
So say I have (175, 250, 255) and I want to darken that color exactly 10 times to end at (0, 0, 0), how could I do this?
I'd also like to brighten the color, so I'd like to brighten it exactly 10 times and end at (255, 255, 255).\nAnswer: Many ways to solve this one. One idea would be to find the difference between your current value to the target value and divide that by 10. 
So (175, 250, 255) to (0, 0, 0) difference is (175, 250, 255), then divide that by ten to have what you would subtract each of the ten steps. So subtract (-17.5, -25, -25.5) every step, rounding when needed.",0.0,0.19476521,0.03793348744511604
3,"Question\nI have a table field with entries such as e.g. 02-65-04-12-88-55.
Each position (separated by -) represents something. (There is no '-' in the database, that's how it's displayed to the user).
Users would like to search by the entry's specific position. I am trying to create a queryset to do this but cannot figure it out. I could handle startswith, endswith but the rest - I have no idea.
Other thoughs would be to split the string at '-' and then query at each specific part of the field (if this is possible).
How can a user search the field's entry at say positions 0-1, 6-7, 10-11 and have the rest wildcarded and returned?
Is this possible? I may be approaching this wrong? Thoughts?\nAnswer: You could use a something__like='__-__-__-__-88-__' query, but it's likely to not be very efficient (since the database will have to scan through all rows to find a match).
If you need to lots of these queries, it'd be better to split these out to actual fields (something_1, something_2, etc.)",0.0,0.22236067,0.04944426938891411
4,"Question\nAs part of a larger project, I'm currently writing a python script that runs Linux commands in a vApp.
I'm currently facing an issue where after working with a mounted iso, it may or may not unmount as expected.
To check the mount status, I want to run the df -hk /directory and du -sch /directory commands respectively, and compare the outputs.
If the iso is not unmounted, the result for the df command should return a larger value than the du command as the df command includes the mount size in the result, while du does not.
I'm just wondering how can i compare these values or if there is a better way for me to run this check in the first place.\nAnswer: why don't you use /proc/mounts?
First column is you blockdevice, second is the mountpoint.
If you mountpoint is not in /proc/mounts you have nothing mounted here.",0.40816328,0.37392336,0.001172372023575008
5,"Question\nI am learning how to use VS code and in the process, I learnt about linting and formatting with ""pylint"" and ""black"" respectively.
Importantly, I have Anaconda installed as I often use conda environments for my different projects. I have therefore installed ""pylint"" and ""black"" into my conda environment.
My questions are as follows:

If ""pylint"" and ""black"" are Python packages, why do they not need to be imported into your script when you use them? (i.e. ""import pylint"" and ""import black"" at the top of a Python script you want to run). I am very new to VS code, linting and formatting so maybe I'm missing something obvious but how does VS code know what to do when I select ""Run Linting"" or ""Format document"" in the command palette? Or is this nothing to do with VS code?

I guess I am just suprised at the fact we don't need to import these packages to use them. In contrast you would always be using import for other packages (sys, os, or any other).

I'm assuming if I used a different conda environment, I then need to install pylint and black again in it right?\nAnswer: Yes, black and pylint are only available in the conda environment you installed them in. You can find them in the ""Scripts""-folder of your environment.
VS Code knows where to look for those scripts, I guess you can set which package is used for ""Run Linting"" or ""Format document"".
You only need to import python modules or functions that you want to use inside your python module. But that's not what you do.",0.0,0.17878222,0.031963083893060684
6,"Question\nWith dask dataframe using
df = dask.dataframe.from_pandas(df, npartitions=5)
series = df.apply(func) 
future = client.compute(series)
progress(future)
In a jupyter notebook I can see progress bar for how many apply() calls completed per partition (e.g  2/5).
Is there a way for dask to report progress inside each partition?
Something like tqdm progress_apply() for pandas.\nAnswer: If you mean, how complete each call of func() is, then no, there is no way for Dask to know that. Dask calls python functions which run in their own python thread (python threads cannot be interrupted by another thread), and Dask only knows whether the call is done or not.
You could perhaps conceive of calling a function which has some internal callbacks or other reporting system, but I don't think I've seen anything like that.",0.0,0.2177887,0.04743191599845886
7,"Question\nFirst of all, my apologies if I am not following some of the best practices of this site, as you will see, my home is mostly MSE (math stack exchange).
I am currently working on a project where I build a vacation recommendation system. The initial idea was somewhat akin to 20 questions: We ask the user certain questions, such as ""Do you like museums?"", ""Do you like architecture"", ""Do you like nightlife"" etc., and then based on these answers decide for the user their best vacation destination. We answer these questions based on keywords scraped from websites, and the decision tree we would implement would allow us to effectively determine the next question to ask a user. However, we are having some difficulties with the implementation. Some examples of our difficulties are as follows:
There are issues with granularity of questions. For example, to say that a city is good for ""nature-lovers"" is great, but this does not mean much. Nature could involve say, hot, sunny and wet vacations for some, whereas for others, nature could involve a brisk hike in cool woods. Fortunately, the API we are currently using provides us with a list of attractions in a city, down to a fairly granular level (for example, it distinguishes between different watersport activities such as jet skiing, or white water rafting). My question is: do we need to create some sort of hiearchy like:

nature-> (Ocean,Mountain,Plains) (Mountain->Hiking,Skiing,...)

or would it be best to simply include the bottom level results (the activities themselves) and just ask questions regarding those? I only ask because I am unfamiliar with exactly how the classification is done and the final output produced. Is there a better sort of structure that should be used?
Thank you very much for your help.\nAnswer: I think using a decision tree is a great idea for this problem.  It might be an idea to group your granular activities, and for the ""nature lovers"" category list a number of different climate types: Dry and sunny, coastal, forests, etc and have subcategories within them.
For the activities, you could make a category called watersports, sightseeing, etc.  It sounds like your dataset is more granular than you want your decision tree to be, but you can just keep dividing that granularity down into more categories on the tree until you reach a level you're happy with",0.0,0.234974,0.055212780833244324
8,"Question\nFirst of all, my apologies if I am not following some of the best practices of this site, as you will see, my home is mostly MSE (math stack exchange).
I am currently working on a project where I build a vacation recommendation system. The initial idea was somewhat akin to 20 questions: We ask the user certain questions, such as ""Do you like museums?"", ""Do you like architecture"", ""Do you like nightlife"" etc., and then based on these answers decide for the user their best vacation destination. We answer these questions based on keywords scraped from websites, and the decision tree we would implement would allow us to effectively determine the next question to ask a user. However, we are having some difficulties with the implementation. Some examples of our difficulties are as follows:
There are issues with granularity of questions. For example, to say that a city is good for ""nature-lovers"" is great, but this does not mean much. Nature could involve say, hot, sunny and wet vacations for some, whereas for others, nature could involve a brisk hike in cool woods. Fortunately, the API we are currently using provides us with a list of attractions in a city, down to a fairly granular level (for example, it distinguishes between different watersport activities such as jet skiing, or white water rafting). My question is: do we need to create some sort of hiearchy like:

nature-> (Ocean,Mountain,Plains) (Mountain->Hiking,Skiing,...)

or would it be best to simply include the bottom level results (the activities themselves) and just ask questions regarding those? I only ask because I am unfamiliar with exactly how the classification is done and the final output produced. Is there a better sort of structure that should be used?
Thank you very much for your help.\nAnswer: Bins and sub bins are a good idea, as is the nature, ocean_nature thing.
I was thinking more about your problem last night, TripAdvisor would be a good idea.  What I would do is, take the top 10 items in trip advisor and categorize them by type.  
Or, maybe your tree narrows it down to 10 cities.  You would rank those cities according to popularity or distance from the user.
I’m not sure how to decide which city would be best for watersports, etc.  You could even have cities pay to be",0.0,0.87101346,0.7586644291877747
9,"Question\nI have a LSTM Keras Tensorflow model trained and exported in.h5  (HDF5) format. 
My local machine does not support keras tensorflow. I have tried installing. But does not work.
Therefore, i used google colabs and exported the model. 
I would like to know, how i can use the exported model in pycharm
Edit : I just now installed tensorflow on my machine
Thanks in Advance\nAnswer: You still need keras and tensorflow to use the model.",0.0,0.09791893,0.009588116779923439
10,"Question\nAs the title says i want to know how to make PyQt5 program starts like pycharm/spyder/photoshop/etc so when i open the program an image shows with progress bar(or without) like spyder,etc\nAnswer: Sounds like you want a splash screen. QSplashScreen will probably be your friend.",0.81632656,0.38109136,0.18942968547344208
11,"Question\nThis question is not about how to use sys.exit (or raising SystemExit directly), but rather about why you would want to use it.

If a program terminates successfully, I see no point in explicitly exiting at the end.
If a program terminates with an error, just raise that error. Why would you need to explicitly exit the program or why would you need an exit code?\nAnswer: Letting the program exit with an Exception is not user friendly. More exactly, it is perfectly fine when the user is a Python programmer, but if you provide a program to end users, they will expect nice error messages instead of a Python stacktrace which they will not understand.
In addition, if you use a GUI application (through tkinter or pyQt for example), the backtrace is likely to be lost, specially on Windows system. In that case, you will setup error processing which will provide the user with the relevant information and then terminate the application from inside the error processing routine. sys.exit is appropriate in that use case.",1.0,0.45515007,0.2968614399433136
12,"Question\nTrying to deploy Azure Functions written in Python and looks like the only option to do that is through VS Code.
I have Python and Azure Functions extensions, and normally use PyCharm with Anaconda interpreter.
I also have azure-functions-core-tools installed and calling ""func"" in PS works.
In the VS Code I create a virtual environment as it suggests. But when tyring to debug  any Azure Function (using one of their templates for now) I get the error above.
As far as I understand it tries to install ""azure-functions"" module as specified in the ""requirements.txt"" file and tries to do that with pip. pip works normally if I use it through Anaconda prompt or with my global env python, but I have to use the virtual environment created by VS Code for this one.
Any suggestions on how to get through this? Thanks in advance.\nAnswer: Just solved the problem after wasting my valuable whole afternoon. The problem lies on the side of Anaconda.
As you described in your question, pip works normally (only) in your Anaconda prompt. Which means, it doesn't work anywhere outside, no matter in a CMD or a PowerShell (although pip and conda seem work outside of the prompt, SSL requests get somehow always refused). However, VS Code, when you simply press F5 instead of using func start command, uses an external PowerShell to call pip. No wonder it'll fail.
The problem can be solved, when you install Anaconda on Windows 10, by choosing to add Anaconda's root folder to PATH. This being said, Anaconda's installer strongly doesn't recommend choosing this option (conflicts with other apps blabla)... And if you try to install Anaconda through some package manager such as scoop, it'll install it without asking you for this detail, which is logical.
The ""fun"" part is philosophically Anaconda itself doesn't suggest using conda or pip command outside Anaconda Prompt, while other apps want and may have to do it the other way. Very very confusing and annoying.",0.40816328,0.3036021,0.010933040641248226
13,"Question\nI work with wxpython and threads in my project. I think that I didn't understand well how to use wx.CallAfter and when to us it. I read few thing but I still didn't got the point. Someone can explain it to me?\nAnswer: In a nutshell, wx.CallAfter simply takes a callable and the parameters that should be passed to it, bundles that up into a custom event, and then posts that event to the application's pending event queue. When that event is dispatched the handler calls the given callable, passing the given parameters to it.
Originally wx.CallAfter was added in order to have an easy way to invoke code after the current and any other pending events have been processed. Since the event is always processed in the main UI thread, then it turns out that wx.CallAfter is also a convenient and safe way for a worker thread to cause some code to be run in the UI thread.",0.81632656,0.15551525,0.43667158484458923
14,"Question\nI see there exits two configs of the T5model - T5Model and TFT5WithLMHeadModel. I want to test this for translation tasks (eg. en-de) as they have shown in the google's original repo. Is there a way I can use this model from hugging face to test out translation tasks. I did not see any examples related to this on the documentation side and was wondering how to provide the input and get the results. 
Any help appreciated\nAnswer: T5 is a pre-trained model, which can be fine-tuned on downstream tasks such as Machine Translation. So it is expected that we get gibberish when asking it to translate -- it hasn't learned how to do that yet.",0.20408164,0.28453445,0.006472655571997166
15,"Question\nI want to know how to create a list called ""my_list"" in Python starting with a value in a variable ""begin"" and containing 10 successive integers starting with ""begin"". 
For example, if begin = 2, I want my_list = [2,3,4,5,6,7,8,9,10,11]\nAnswer: Simply you can use extend method of list and range function.
start = 5
my_list = []
my_list.extend(range(start,start+11))
print(my_list)",0.0,0.12467909,0.015544875524938107
16,"Question\nI am learning to use django and my question is if it is possible to change the system to reset the users' password, the default system of sending a link by mail I do not want to use it, my idea is to send a code to reset the password, but I don't know how it should be done and if possible, I would also need to know if it's safe.
What I want is for the user who wants to recover his password to go to the recovery section, fill in his email and choose to send and enable a field to put the code that was sent to the mail.
I don't know how I should do it or is there a package for this?
Thank you very much people greetings.\nAnswer: You can do this, when user clicks on reset password ask for users email id, verify that email id provided is same as what you have in DB. If the email id matches you can generate a OTP and save it in DB(for specific time duration like 3 mins) and send it to user's Email id. Now User enters the OTP. If the OTP provided by user matches the one you have in DB, open the page where user can enter new password.",0.0,-0.00499171,2.4917169866967015e-05
17,"Question\nI was wondering how the threads are executed on hardware level, like a process would run on a single processing core and make a context switch on the processor and the MMU in order to switch between processes. How do threads switch? Secondly when we create/spawn a new thread will it be seen as a new process would for the processor and be scheduled as a process would?
Also when should one use threads and when a new process?
I know I probably am sounding dumb right now, that's because I have massive gaps in my knowledge that I would like fill. Thanks in advance for taking the time and explaining things to me. :)\nAnswer: Think of it this way:  ""a thread is part of a process.""
A ""process"" owns resources such as memory, open file-handles and network ports, and so on.  All of these resources are then available to every ""thread"" which the process owns.  (By definition, every ""process"" always contains at least one (""main"") ""thread."")
CPUs and cores, then, execute these ""threads,"" in the context of the ""process"" which they belong to.
On a multi-CPU/multi-core system, it is therefore possible that more than one thread belonging to a particular process really is executing in parallel.  Although you can never be sure.
Also:  in the context of an interpreter-based programming language system like Python, the actual situation is a little bit more complicated ""behind the scenes,"" because the Python interpreter context does exist and will be seen by all of the Python threads.  This does add a slight amount of additional overhead so that it all ""just works.""",0.13605443,0.34190506,0.04237448051571846
18,"Question\nI was wondering how the threads are executed on hardware level, like a process would run on a single processing core and make a context switch on the processor and the MMU in order to switch between processes. How do threads switch? Secondly when we create/spawn a new thread will it be seen as a new process would for the processor and be scheduled as a process would?
Also when should one use threads and when a new process?
I know I probably am sounding dumb right now, that's because I have massive gaps in my knowledge that I would like fill. Thanks in advance for taking the time and explaining things to me. :)\nAnswer: There are a few different methods for concurrency. The threading module creates threads within the same Python process and switches between them, this means they're not really running at the same time. The same happens with the Asyncio module, however this has the additional feature of setting when a thread can be switched. 
Then there is the multiprocessing module which creates a separate Python process per thread. This means that the threads will not have access to shared memory but can mean that the processes run on different CPU cores and therefore can provide a performance improvement for CPU bound tasks. 
Regarding when to use new threads a good rule of thumb would be:

For I/O bound problems, use threading or async I/O. This is because you're waiting on responses from something external, like a database or browser, and this waiting time can instead be filled by another thread running it's task. 
For CPU bound problems use multiprocessing. This can run multiple Python processes on separate cores at the same time. 

Disclaimer: Threading is not always a solution and you should first determine whether it is necessary and then look to implement the solution.",0.27210885,0.3151213,0.0018500699661672115
19,"Question\nI converted a python script to an exe using pyinstaller. I want to know how I can change the icon it gave me to the default icon. In case you don't know what I mean, look at C:\Windows\System32\alg.exe. There are many more files with that icon, but that is one of them. Sorry if this is the wrong place to ask this, and let me know if you have any questions\nAnswer: I would suggest to use auto-py-to-exe module for conversion of python script to exe. At first install using command pip install auto-py-to-exe after that run it through python command line just by typing auto-py-to-exe, you'll get an window where you'll get the icon option.
Please vote if you find your solution.",-0.23809524,-0.007911384,0.052984606474637985
20,"Question\nI converted a python script to an exe using pyinstaller. I want to know how I can change the icon it gave me to the default icon. In case you don't know what I mean, look at C:\Windows\System32\alg.exe. There are many more files with that icon, but that is one of them. Sorry if this is the wrong place to ask this, and let me know if you have any questions\nAnswer: You'll need to extract the icon from the exe, and set that as the icon file with pyinstaller -i extracted.ico myscript.py. You can extract the icon with tools available online or you can use pywin32 to extract the icons.",0.0,0.07694918,0.005921176169067621
21,"Question\nI'm trying to develop a windows gui app with python and i will distribute that later. I don't know how to set the app for some future releasing updates or bug fix from a server/remotely. How can I handle this problem? Can I add some auto-update future to app? What should write for that in my code and what framework or library should I use? 
Do pyinstaller/ inno setup have some futures for this?
Thanks for your help.\nAnswer: How about this approach:

You can use a version control service like github to version control your code. 
Then checkout the repository on your windows machine. 
Write a batch/bash script to checkout the latest version of your code and restart the app. 
Then use the Windows task scheduler to periodically run this script.",0.40816328,0.2111327,0.03882104530930519
22,"Question\nI am using Word2vec model to extract similar words, but I want to know if it is possible to get words while using unseen words for input. 
For example, I have a model trained with a corpus [melon, vehicle, giraffe, apple, frog, banana]. ""orange"" is unseen word in this corpus, but when I put it as input, I want [melon, apple, banana] for result. 
Is this a possible situation?\nAnswer: The original word2vec algorithm can offer nothing for words that weren't in its training data.
Facebook's 'FastText' descendent of the word2vec algorithm can offer better-than-random vectors for unseen words – but it builds such vectors from word fragments (character n-gram vectors), so it does best where shared word roots exist, or where the out-of-vocabulary word is just a typo of a trained word.
That is, it won't help in your example, if no other words morphologically similar to 'orange' (like 'orangey', 'orangade', 'orangish', etc) were present. 
The only way to learn or guess a vector for 'orange' is to have some training examples with it or related words. (If all else failed, you could scrape some examples from other large corpora or the web to mix with your other training data.)",0.81632656,0.4048369,0.16932374238967896
23,"Question\nI am having a little tough time importing the xmltodict module into my visual studio code. 
I setup the module in my windows using pip. it should be working on my visual studio as per the guidelines and relevant posts I found here. 
but for some reasons it isn't working in the visual studio. 
Please advise on how can I get the xmltodict module installed or imported on visual studio code 
Thanks in Advance\nAnswer: I had the same issue and it turned out that it wasn't installed in that virtual environment even though that was what I had done. Try:
venv/Scripts/python.exe -m pip install xmltodict",0.0,0.099401146,0.009880587458610535
24,"Question\nI want to open a python file in cmder terminal quickly. Currently, the fastest way i know how is to navigate to the directory of the python file in cmder terminal and then run it by calling ""python file.py"". This is slow and cumbersome. Is there a way for me to have a file or exe, that, when i run it (or drag the program onto it), automatically makes the program run in cmder straight away.
Windows 10
Clarification: I'm using cmder terminal specifically because it supports text coloring. Windows terminal and powershell do not support this.\nAnswer: On windows you can go to the directory with the file in the explorer and then simply hold shift as you right click at the same time. This will open the menu and there you will have the option to use the command shell/powershell and then you don't have to navigate to the directory inside the shell anymore and can just execute the python file.
I hope that helps.",0.0,0.09885323,0.009771960787475109
25,"Question\nI want to open a python file in cmder terminal quickly. Currently, the fastest way i know how is to navigate to the directory of the python file in cmder terminal and then run it by calling ""python file.py"". This is slow and cumbersome. Is there a way for me to have a file or exe, that, when i run it (or drag the program onto it), automatically makes the program run in cmder straight away.
Windows 10
Clarification: I'm using cmder terminal specifically because it supports text coloring. Windows terminal and powershell do not support this.\nAnswer: Answer: The escape codes just weren't properly configured for the windows terminals. You can get around this by using colorama's colorama.init(). It should work after that.",0.0,0.13431376,0.018040185794234276
26,"Question\nMy question is this: If I were to make a command with a loop (for example ""start"") where it would say something like:""It has been 3 hours since..."" and it loops for 10800 seconds (3 hours) and then says:""It has been 6 hours since..."", so the part where I'm stuck is: If I were to make a command called ""stop"" how would I implement it in the command ""start"" where it would check if the command ""stop"" has been used. If yes the loop is cancelled, if it hasn't been used the loop continues.\nAnswer: but if you run the command several times or on different servers, one stop command stops them all. Is there not a way to stop just one loop with one command",0.0,0.39443254,0.15557703375816345
27,"Question\nI am writing an application which extracts some data from HTML using BeautifoulSoup4. These are search results of some kind, to be more specific. I thought it would be a good a idea to have a Parser class, storing default values like URL prefixes, request headers etc. After configuring those parameters, the public method would return a list of objects, each of them containing a single result or maybe even an object with a list composed into it alongside with some other parameters. I'm struggling to decouple small pieces of logic that build that parser implementation from the parser class itself. I want to write dozens of parser private utility methods like: _is_next_page_available, _are_there_any_results, _is_did_you_mean_available etc. However, these are the perfect candidates for writing unit tests! And since I want to make them private, I have a feeling that I'm missing something...
My other idea was to write that parser as a function, calling bunch of other utility functions, but that would be just equal to making all of those methods public, which doesn't make sense, since they're implementation details.
Could you please advice me how to design this properly?\nAnswer: I think you're interpreting the Single-Responsibility Principle (SRP) a little differently. It's actual meaning is a little off from 'a class should do only one thing'. It actually states that a class should have one and only one reason to change.
To employ the SRP you have to ask yourself to what/who would your parser module methods be responsible, what/who might make them change. If the answer for each method is the same, then your Parser class employs the SRP correctly. If there are methods that are responsible to different things (business-rule givers, groups of users etc.) then those methods should be taken out and be placed elsewhere.
Your overall objective with the SRP is to protect your class from changes coming from different directions.",0.81632656,0.27529562,0.2927144765853882
28,"Question\nI'm helping out a newly formed startup build a social media following, and I have a csv file of thousands of email addresses of people I need to follow. From looking at the twitter API, I see its possible to follow the accounts if I knew their usernames, but its unclear how to look them up by email. Any ideas?\nAnswer: This does not appear to be an option with their API, you can use either user_id or screen name with their GET users/show or GET users/lookup options.",0.20408164,0.13770711,0.004405577667057514
29,"Question\nI'm helping out a newly formed startup build a social media following, and I have a csv file of thousands of email addresses of people I need to follow. From looking at the twitter API, I see its possible to follow the accounts if I knew their usernames, but its unclear how to look them up by email. Any ideas?\nAnswer: There is no way to do a lookup based on email address in the Twitter API.",0.0,0.111656964,0.012467277236282825
30,"Question\nI applied batch normalization technique to increase the accuracy of my cnn model.The accuracy of model without batch Normalization was only 46 % but after applying batch normalization it crossed 83% but a here arisen a bif overfitting problem that the model was giving validation Accuracy only 15%. Also please tell me how to decide no of filters strides in convolution layer and no of units in dence layer\nAnswer: Batch normalization has been shown to help in many cases but is not always optimal. I found that it depends where it resides in your model architecture and what you are trying to achieve. I have done a lot with different GAN CNNs and found that often BN is not needed and can even degrade performance. It's purpose is to help the model generalize faster but sometimes it increases training times. If I am trying to replicate images, I skip BN entirely. I don't understand what you mean with regards to the accuracy. Do you mean it achieved 83% accuracy with the training data but dropped to 15% accuracy on the validation data? What was the validation accuracy without the BN? In general, the validation accuracy is the more important metric. If you have a high training accuracy and a low validation accuracy, you are indeed overfitting. If you have several convolution layers, you may want to apply BN after each. If you still over-fit, try increasing your strides and kernel size. If that doesn't work you might need to look at the data again and make sure you have enough and that it is somewhat diverse. Assuming you are working with image data, are you creating samples where you rotate your images, crop them, etc. Consider synthetic data to augment your real data to help combat overfiiting.",0.0,0.19724232,0.03890453279018402
31,"Question\nIn my company, I have got task to create dash board using python whose complete look and feel should be like qlicksense. I am fresher in data science field I don't know how to do this. I did lots of R & D and plotly and dash is the best option as much according to R & D on internet dash table is also a good option but I am not able to create the things what it should look like. If any one know how to start plz help me..\nAnswer: you can use django or other web framework to develop the solution, 
keep in mind that you probably will need to handle lots of front end stuff like builiding the UI of the system, 
Flask also is very lightweight option, but it needs lots of customization.
Django comes with pretty much everything you might need out of the box.",0.0,0.17324322,0.03001321479678154
32,"Question\nI have a text file. I need to identify specific paragraph headings and if true i need to extract relevant tables and paragraph wrt that heading using python.  can we do this by nlp or machine learning?. if so please help me out in gathering basics as i am new to this field.I was thinking of using a rule like:
if (capitalized) and heading_length <50:
    return heading_text
how do i parse through the entire document and pick  only the header names? this is like automating human intervention of clicking document,scrolling to relevant subject and picking it up.
please help me out in this\nAnswer: You probably don't need NLP or machine learning to detect these headings. Figure out the rule you actually want and if indeed it is such a simple rule as the one you wrote, a regexp will be sufficient. If your text is formatted (e.g. using HTML) it might be even simpler.
If however, you can't find a rule, and your text isn't really formatted consistently, your problem will be hard to solve.",0.20408164,0.29846144,0.008907546289265156
33,"Question\nI have a text file. I need to identify specific paragraph headings and if true i need to extract relevant tables and paragraph wrt that heading using python.  can we do this by nlp or machine learning?. if so please help me out in gathering basics as i am new to this field.I was thinking of using a rule like:
if (capitalized) and heading_length <50:
    return heading_text
how do i parse through the entire document and pick  only the header names? this is like automating human intervention of clicking document,scrolling to relevant subject and picking it up.
please help me out in this\nAnswer: I agree with lorg. Although you could use NLP, but that might just complicate the problem. This problem could be an optimization problem if performance is a concern.",0.0,0.25501412,0.06503219902515411
34,"Question\nI want to know how much time has been taken by the whole test suite to complete the execution. How can I get it in Pytest framework. I can get the each test case execution result using pytest <filename> --durations=0 cmd. But, How to get whole suite execution time>\nAnswer: Use pytest-sugar
pip install pytest-sugar
Run your tests after it,
You could something like Results (10.00s) after finishing the tests",0.20408164,0.2740276,0.004892435390502214
35,"Question\nFirst time programming in python and I guess you will notice it after reading my question:
  + How can I remove the message ""cryptography is not installed, use of crypto disabled"" when running the application?
I have created a basic console application using the pyinstaller tool and the code is written in python.
When I run the executable, I am getting the message ""cryptography is not installed, use of crypto disabled"". The program still runs, but I would prefer to get rid off the message.
Can someone help me?
Thanks in advance.\nAnswer: cryptography and crypto are 2 different modules.
try:
pip install cryptography
pip install crypto",1.0,0.32850748,0.45090219378471375
36,"Question\nI need select all objects in Maya with name ""shd"" and after that I need assigned to them specific material.
I don't know how to do that because when I wrote: select -r ""shd""; it send me the message: More than one object matches name: shd // 
So maybe I should select them one by one in some for loop or something. I am 3D artist so sorry for the lame question.\nAnswer: You can use select -r ""shd*"" to select all objects with a name stating with ""shd"".",0.0,0.10752267,0.01156112365424633
37,"Question\nThe whole project is as follows:
I'm trying to build a Django based web-app for my college library. This app when idle will be showing a slideshow of pictures on the screen. However when an input is received from the barcode scanner, it is supposed to redirect it to a different age containing information related to that barcode. I'm not able to figure out how to get an input from the scanner and only then redirect it to the page for 3 seconds containing the relevant information, after the interval, it should redirect back to the page containing the slideshow.\nAnswer: you should communicate with the bar-code scanner to receive scanning-done event which has nothing to do with django but only javascript or even an interface software which the user must install, like a driver, so you can detect the bar-code scanner from javascript(web browser) then you can get your event in javascript and redirect the page on the event or do whatever you want",0.0,0.50383854,0.2538532614707947
38,"Question\nI am trying to pull data from an external API and dump it on S3. I was thinking on writing and Airflow Operator rest-to-s3.py which would pull in data from external Rest API.
My concerns are :

This would be a long running task, how do i keep track of failures?
Is there a better alternative than writing an operator?
Is it advisable to do a task that would probably run for a couple of hours and wait on it?

I am fairly new to Airflow so it would be helpful.\nAnswer: Errors - one of the benefits of using a tool like airflow is error tracking. Any failed task is subject to rerun (based on configuration) will persist its state in task history etc..
Also, you can branch based on the task status to decide if you want to report error e.g. to email
An operator sounds like a valid option, another option is the built-in PythonOperator and writing a python function.
Long-running tasks are problematic with any design and tool. You better break it down to small tasks (and maybe parallelize their execution to reduce the run time?) Does the API take long time to respond? Or do you send many calls? maybe split based on the resulting s3 files? i.e. each file is a different DAG/branch?",1.0,0.115596116,0.7821702361106873
39,"Question\nI am using VScode with python code and I have a folder with sub-directories (2-levels deep) containing python tests. 
When I try ""Python: Discover Tests"" it asks for a test framework (selected pytest) and the directory in which tests exist. At this option, it shows only the top-level directories and does not allow to select a sub-directory. 
I tried to type the directory path but it does not accept it. 
Can someone please help on how to achieve this?\nAnswer: There are two options. One is to leave the selection as-is and make sure your directories are packages by adding __init__.py files as appropriate. The other is you can go into your workspace settings and adjust the ""python.testing.pytestArgs"" setting as appropriate to point to your tests.",0.0,0.27836978,0.07748973369598389
40,"Question\nI am using VScode with python code and I have a folder with sub-directories (2-levels deep) containing python tests. 
When I try ""Python: Discover Tests"" it asks for a test framework (selected pytest) and the directory in which tests exist. At this option, it shows only the top-level directories and does not allow to select a sub-directory. 
I tried to type the directory path but it does not accept it. 
Can someone please help on how to achieve this?\nAnswer: Try opening the ""Output"" log (Ctrl+Shift+U) and run ""Python: Discover Tests"". Alternatively, you may type pytest --collect-only into the console. Maybe you are experiencing some errors with the tests themselves (such as importing errors).
Also, make sure to keep __init__.py file in your ""tests"" folder.
I am keeping the pytest ""tests"" folder within a subdirectory, and there are no issues with VS Code discovering the tests.",0.0,0.23392785,0.05472223833203316
41,"Question\nI am facing issue with SQLite vulnerability which fixed in SQLite version 3.31.1.
I am using the python3.7.4-alpine3.10 image, but this image uses a previous version of SQLite that isn't patched.
The patch is available in python3.8.2-r1 with alpine edge branch but this image is not available in docker hub.
Please help how can i fix this issue?\nAnswer: Your choices are limited to two options:

Wait for the official patched release
Patch it yourself

Option 1 is easy, just wait and the patch will eventually propagate through to docker hub. Option 2 is also easy, just get the code for the image from github, update the versions, and run the build yourself to produce the image.",0.0,0.30631888,0.09383125603199005
42,"Question\nI am doing performance/memory analysis on a certain method that is wrapped with the functools.lru_cache decorator. I want to see how to inspect the current size of my cache without doing some crazy inspect magic to get to the underlying cache.
Does anyone know how to see the current cache size of method decorated with functools.lru_cache?\nAnswer: Digging around in the docs showed the answer is calling.cache_info() on the method.

To help measure the effectiveness of the cache and tune the maxsize parameter, the wrapped function is instrumented with a cache_info() function that returns a named tuple showing hits, misses, maxsize and currsize. In a multi-threaded environment, the hits and misses are approximate.",1.0,0.35922688,0.41059020161628723
43,"Question\nI have read lots of documentation and articles about using signals in Django, but I cannot understand the concept.  

What is the purpose of using signals in Django?
How does it work?

Please explain the concept of signals and how to use it in Django code.\nAnswer: The Django Signals is a strategy to allow decoupled applications to get notified when certain events occur. Let’s say you want to invalidate a cached page everytime a given model instance is updated, but there are several places in your code base that this model can be updated. You can do that using signals, hooking some pieces of code to be executed everytime this specific model’s save method is trigged.
Another common use case is when you have extended the Custom Django User by using the Profile strategy through a one-to-one relationship. What we usually do is use a “signal dispatcher” to listen for the User’s post_save event to also update the Profile instance as well.",0.81632656,0.4447291,0.13808467984199524
44,"Question\nI am coding a PyQt5 based GUI application needs to be able to create and run arbitrary Python scripts at runtime. If I convert this application to  a.exe, the main GUI Window will run properly. However, I do not know how I can run the short.py scripts that my application creates. Is it possible to runs these without a system wide Python installation?

I don't want ways to compile my python application to exe. This problem relates to generated.py scripts\nAnswer: No, to run a Python file you need an interpreter.
It is possible that your main application can contain a Python interpreter so that you don't need to depend on a system-wide Python installation.",0.40816328,0.3025664,0.011150699108839035
45,"Question\nI have been programming with python for about half a year, and I would like to try manim ( the animation programme of 3blue1brown from youtube), but I am not sure where to start. I have not installed it, but I have tried to read up on it. And to be honest I do not understand much of the requirements of the program, and how to run it.
Google has left me without much help, so I decided to check here to see if anyone here is able to help.
From what I understand, you run manim directly in python and the animations are based on a textfile with code i assume is LaTex. I have almost no experience with python itself, but I have learned to use it through Thonny, and later Pycharm.
My main questions are: (Good sources to how to do this without being a wizard would be really helpful if they exist☺️)

Is it possible to install manim in pycharm, and how? Do i need some extra stuff installed to pycharm in order to run it? (I run a windows 64-bit computer)
If i manage to do this in pycharm, Will I then be able to code the animations directly in pycharm (in.py or.txt files), or is it harder to use in pycharm?

All help or insights is very appreciated As I said I am not extremely knowledgeable in computers, but I am enjoying learning how to code and applications of coding\nAnswer: Yes, you can
1.Write your code in pycharm
2.save it
3.copy that.py file to where you installed manim. In my case, it is

This pc>> C drive >> manim-master >> manim-master

4.select on the path and type ""cmd"" to open terminal from there

Type this on the terminal

python -m manim -pql projectname.py
This will do.
To play back the animation or image, open the media folder.",0.0,-0.08728111,0.007617991883307695
46,"Question\nFFmpeg is installed in C:\FFmpeg, and I put C:\FFmpeg\bin in the path. Does anyone know how to fix?
Thanks!\nAnswer: You added C:\FFmpeg\bin\ffmpeg.exe to your path, instead, you need to add only the directory:
C:\FFmpeg\bin\",-0.71428573,-0.23482805,0.22987966239452362
47,"Question\nI made an android app using python-kivy (Buildozer make it to apk file)
Now I want to put an image for the icon of the application. I mean the picture for the app-icon on your phone.
how can I do this? I cannot find any code in kv\nAnswer: Just uncomment icon.filename: in the buildozer spec file and write a path to your icon image.",0.40816328,0.19904888,0.04372883215546608
48,"Question\nI'm new to google colab.
I'm trying to do deep learning there.
I have written a class to create and train a LSTM net using just python - not any specific deep learning library as tensorflow, pytorch, etc.
I thought I was using a gpu because I had chosen the runtime type properly in colab.
During the code execution, however, I was sometimes getting the message to quit gpu mode because I was not making use of it.
So, my question: how can one use google colab gpu, using just plain python, without special ai libraries? Is there something like ""decorator code"" to put in my original code so that the gpu get activated?\nAnswer: It's just easier to use frameworks like PyTorch or Tensorflow.
If not, you can try pycuda or numba, which are closer to ""pure"" GPU programming. That's even harder than just using PyTorch.",0.20408164,0.27468824,0.004985292442142963
49,"Question\nCurrently I am making a very simple interface which asks user to input parameters for a test and then run the test. The test is running brushless dc motor for several minutes. So when the run button is pressed the button is engaged for the time period till the function is finished executing. I have another stop button which should kill the test but currently cant use it since the run button is kept pressed till the function is finished executing and stop button cant be used during the test. I want to stop the test with pressing the stop button even if the run button function is currently being executed. The run button should release and the function should continuously check the stop function for stopping the test. Let me know how this can be executed.\nAnswer: Your problem is that all your code it taking place sequentially in a single thread. Once your first button is pressed, all of the results of that pressing are followed through before anything else can happen.
You can avoid this by running the motor stuff in a separate thread. Your stop button will then need to interrupt that thread.",0.0,0.070102096,0.0049143037758767605
50,"Question\nI am trying to write a function in python that takes as input two matrices X and Y and computes for every pair of rows x in X and y in  Y, the norm ||x - y||. I would like to do it without using for loops.
Do you have an idea about how to do it?\nAnswer: I just solve it :D
instead of len(np.trnspose(y)) i had to do len(y) and it perfectly worked with a for loop.",0.0,-0.09505868,0.009036152623593807
51,"Question\nI have used to create a Texture with gradient color and set to the background of Label, Button and etc. But I  am wondering how to set this to color of Label?\nAnswer: You can't set the color property to a gradient, that just isn't what it does. Gradients should be achieved using images or textures directly applied to canvas vertex instructions.",0.0,0.23578233,0.055593304336071014
52,"Question\nI've tried to run this code on Jupyter notebook python 3:
class CSRNet(nn.Module):
    def __init__(self, load_weights=False):
        super(CSRNet, self).__init__()
        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]
        self.backend_feat  = [512, 512, 512,256,128,64]
        self.frontend = make_layers(self.frontend_feat)
        self.backend = make_layers(self.backend_feat,in_channels = 512,dilation = True)
        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)
        if not load_weights:
            mod = models.vgg16(pretrained = True)
            self._initialize_weights()
            for i in range(len(self.frontend.state_dict().items())):
                self.frontend.state_dict().items()[i][1].data[:] = mod.state_dict().items()[i][1].data[:]
it displays 'odict_items' object is not subscriptable as an error in the last line of code!!how to deal with this?\nAnswer: In python3, items() returns a dict_keys object, you should try to convert it to a list:

list(self.frontend.state_dict().items())[i][1].data[:] =
list(mod.state_dict().items())[i][1].data[:]",0.40816328,0.40467358,1.21780267363647e-05
53,"Question\nI am currently trying to create a website that displays a python file (that is in the same folder as the html file) on the website, but I'm not sure how to do so.
So I just wanted to ask if anyone could describe the process of doing so (or if its even possible at all).\nAnswer: Displaying ""a python file"" and displaying ""the output"" (implied ""of a python script's execution) are totally different things. For the second one, you need to configure your server to run Python code. There are many ways to do so, but the two main options are 
1/ plain old cgi (slow, outdated and ugly as f..k but rather easy to setup - if your hosting provides support for it at least - and possibly ok for one single script in an otherwise static site)
and
2/ a modern web framework (flask comes to mind) - much cleaner, but possibly a bit overkill for one simple script.
In both cases you'll have to learn about the HTTP protocol.",0.0,0.22005707,0.04842511564493179
54,"Question\nI have 2 dataframes: 
FinalFrame:
Time | Monday | Tuesday | Wednesday |...
and df (Where weekday is the current day, whether it be monday tuesday etc):
WEEKDAY
I want to append the weekday's data to the correct column. I will need to constantly keep appending weekdays data as weeks go by. Any ideas on how to tackle this?\nAnswer: You can add index of week days instead of their name. For example, 
weekdays = ['Mon', Tue', 'Wed', 'Thu', 'Fri','Sat', 'Sun']
Time | 0 | 1 | 2....",0.0,-0.03678137,0.0013528692070394754
55,"Question\nI have 2 dataframes: 
FinalFrame:
Time | Monday | Tuesday | Wednesday |...
and df (Where weekday is the current day, whether it be monday tuesday etc):
WEEKDAY
I want to append the weekday's data to the correct column. I will need to constantly keep appending weekdays data as weeks go by. Any ideas on how to tackle this?\nAnswer: So the way you could do it isolates the series by saying weekday[whatever day you are looking at.append.",0.0,-0.13182133,0.017376864328980446
56,"Question\nMy case: 
I want to display the meal plan from my University on my own online ""Dashboard"". I've written my python script to scrape that data and I get the data I need (plain Text). Now I need to put it on my website but I don't know how to start. On my first searching sessions, I have found something with CGI but I have no clue how to use it:( Is there maybe an even easier way to solve my problem? 
Thanks\nAnswer: I suggest you to use the Django, if you don't want to use django, you can edit your output in HTML formate and publish html page, directly.",0.0,0.02341944,0.00054847018327564
57,"Question\nI've just finished my course of Python, so now I can write my own script. So to do that I started to write a script with the module Scapy, but the problem is, the documentation of Scapy is used for the interpreter Scapy, so I don't know how to use it, find the functions, etc. 
I've found a few tutorials in Internet with a few examples but it's pretty hard. For example, I've found in a script the function ""set_payload"" to inject some code in the layer but I really don't know where he found this function. 
What's your suggestion for finding how a module works and how to write correctly with it? Because I don't really like to check and pick through other scripts on Internet.\nAnswer: If I have understood the question correctly, roughly what you are asking is how to find the best source to understand a module. 
If you are using an inbuilt python module, the best source is the python documentation.
Scapy is not a built-in python module. So you may have some issues with some of the external modules (by external I mean the ones you need to explicitly install).
For those, if the docs aren't enough, I prefer to look at some of the github projects that may use that module one way or the other and most of the times it works out. If it doesn't, then I go to some blogs or some third party tutorials. There is no right way to do it, You will have to put in the effort where its needed.",0.0,0.22752327,0.05176683887839317
58,"Question\nI have a Discord bot I use on a server with friends.
The problem is some commands use web scraping to retrieve the bot response, so until the bot is finished retrieving the answer, the bot is out of commission/can't handle new commands.
I want to run multiple instances of the same bot on my host server to handle this, but don't know how to tell my code ""if bot 1 is busy with a command, use bot 2 to answer the command""
Any help would be appreciated!\nAnswer: async function myFunction () {}
this should fix your problem
having multiple instances could be possible with threads,
but this is just a much more easy way",-0.35714287,0.2973934,0.42841771245002747
59,"Question\nIf got the following problem:
Given a series of rectangles defined by {x_min, height and x_max}, I want to efficiently compute their intersection and union, creating a new series.
For instance, if I got S1 = [{1,3,3}] and S2 = [{2,3,5}], the union would result in S3 = [{1,3,5}] and intersection in S3 = [{2,3,3}]. This would be a fairly simple case, but when S1 and S2 are a list of rectangles (unordered) It get's a little bit tricky.
My idea is trying some divide and conquer strategy, like using a modificated mergesort, and in the merge phase try to also merge those buildings. But I'm a little bit unsure about how to express this.
Basically I can't write down how to compare two rectangles with those coordinates and decide if they have to be in S3, or if I have to create a new one (for the intersection).
For the union I think the idea has to be fairly similar, but the negation (i.e. if they don't interesct).
This has to be O(nlogn) for sure, given this is in a 2D plane I surely have to sort it. Currently my first approach is O(n^2).
Any help how to reduce the complexity?
PD: The implementation I'm doing is in Python\nAnswer: I tried to write the whole thing out in psudo-code, and found that it was too psudo-y to be useful and too code-y to be readable. Here's the basic idea:
You can sort each of your input lists in O(n*log(n)). 
Because we assume there's no overlap within each series, we can now replace each of those lists with lists of the form {start, height}. We can drop the ""end"" attribute by having a height-0 element start where the last element should have ended. (or not, if two elements were already abutting.) 
Now you can walk/recurse/pop your way through both lists in a single pass, building a new list of {start, height} outputs as you go. I see no reason you couldn't be building both your union and intersection lists at the same time.
Cleanup (con",0.0,-1.0057286,1.011489987373352
60,"Question\nI am learning Python by reading books, and I have a question about methods. Basically, all of the books that I am reading touch on methods and act like they just come out of thin air. For example, where can I find a list of all methods that can be applied? I can't find any documentation that lists all methods. 
The books are using things like.uppercase and.lowercase but is not saying where to find other methods to use, or how to see which ones are available and where. I would just like to know what I am missing. Thanks. Do I need to dig into Python documentation to find all of the methods?\nAnswer: There is a lot of function in Python's modules. If you want to learn were you can find them, you should ask what you want. For example there is a random module and you can find some functions like random.randint.",0.0,0.09098047,0.008277446031570435
61,"Question\nI am trying to create a web server which has Django framework and I am struggling with outer world access to server. While saying outer world I am trying to say a python program that created out of Django framework and only connects it from local PC which has only Internet connection. I can't figured it out how can I do this.
I am building this project in my local host, so I create the ""outer world python program"" outside of the project file. I think this simulation is proper.
I am so new in this web server/Django field. So maybe I am missing an essential part. If this happened here  I'm sorry but I need an answer and I think it is possible to do.
Thanks in advance...\nAnswer: Django generated fields in database are just standard fields. The tables are named like 'applicationname'_'modelname', you are free to do requests to the database directly, without django.
If you want to do it through django, your outer program can request a web page from your web server, and deal with it. (You may want to take a look at RESTs frameworks)",0.0,0.27405596,0.0751066654920578
62,"Question\nAfter using the pyintaller to transfer the py file to exe file, the exe file throws the error: ""Failed to load dynlib/dll"". Here is the error line: 

main.PyInstallerImportError: Failed to load dynlib/dll 'C:\Users\YANGYI~1\AppData\Local\Temp\_MEI215362\sklearn\.libs\vcomp140.dll'.
  Most probably this dynlib/dll was not found when the application was
  frozen. [1772] Failed to execute script 2

after get this, I did check the path and I did not find a folder called ""_MEI215362"" in my Temp folder, I have already made all files visible. Also, I have re-download the VC but and retransferring the file to exe, but it didn't work. Any ideas how to fix the issue? Thank you in advance!\nAnswer: I also encountered a similar problem like Martin.
In my case, however, it was the ANSI64.dll missing...
So, I simply put the particular dll file into the dist directory.
Lastly, I keep the exe and related raw data files (e.g. xlsx, csv) inside the ""dist"" folder and to run the compiled program. It works well for me.",0.0,0.16917378,0.028619766235351562
63,"Question\nHow to delete drawn objects with OpenCV in Python?

I draw objects on click (cv2.rectangle, cv2.circle)...
Then I would like to delete only drawn objects.
I know that i need to make a layer in behind of the real image and to draw on another one.
But I do not know how to implement this in code.\nAnswer: Have a method or something that when it's executed, will replace the image with stuff drawn on it with an original unaltered image. It's best to create a clone of your original image to draw on.",0.0,0.15310335,0.023440636694431305
64,"Question\nI have written a python script that connects to a remote Oracle database and inserts some data into its tables.
In the process I had to first import cx_Oracle package and install Oracle InstantClient on my local computer for the script to execute properly.
What I don't understand is why did I have to install InstantClient?
I tried to read through the docs but I believe I am missing some fundamental understanding of how databases work and communicate.
Why do I need all the external drivers, dlls, libraries for a python script to be able to communicate with a remote db? I believe this makes packaging and distribution of a python executable much harder.
Also what is InstantClient anyway?
Is it a driver? What is a driver? Is it simply a collection of ""programs"" that know how to communicate with Oracle databases? If so, why couldn't that be accomplished with a simple import of a python package?
This may sound like I did not do my own research beforehand, but I'm sorry, I tried, and like I said, I believe I am missing some underlying fundamental knowledge.\nAnswer: We have a collection of drivers that allow you to communicate with an Oracle Database. Most of these are 'wrappers' of a sort that piggyback on the Oracle Client. Compiled C binaries that use something we call 'Oracle Net' (not to be confused with.NET) to work with Oracle.
So our python, php, perl, odbc, etc drivers are small programs written such that they can be used to take advantage of the Oracle Client on your system. 
The Oracle Client is much more than a driver. It can include user interfaces such as SQL*Plus, SQL*Loader, etc. Or it can be JUST a set of drivers - it depends on which exact package you choose to download and install. And speaking of 'install' - if you grab the Instant Client, there's nothing to install. You just unzip it and update your environment path bits appropriately so the drivers can be loaded.",0.40816328,0.3065138,0.010332619771361351
65,"Question\nI'm having issues with importing my modules into jupyter. I did the following:

Create virtual env
Activate it (everything below is in the context of my venv)
install yahoo finance module: pip install yfinance
open python console and import it to test if working > OK!
open jupyter notebook
import yfinance throws ModuleNotFoundError: No module named 'yfinance'

Any suggestions on how to fix this?\nAnswer: try this one in your jupyter and the run it
!pip install yfinance",0.0,-0.11565137,0.013375239446759224
66,"Question\nI am trying to do a tutorial through FreeCodeCamp using Python's Flask Framework to create a web app in PyCharm and I am stuck on a section where it says 'Flask looks for HTML files in a folder called template. You need to create a template folder and put all your HTML files in there.' I am confused on how to make this template folder; is it just a regular folder or are there steps to create it and drag/drop the HTML files to it? Any tips or info would be of great help!!!\nAnswer: As the tutorial ask you, you have to create a folder call ""templates"" (not ""template""). In PyCharm you can do this by right-clicking on the left panel and select New I Directory. In this folder you can then create your template files (right click on the newly created folder and select New I File, then enter the name of your file with the.html extension).
By default, flask looks in the ""templates"" folder to find your template when you call render_template(""index.html"").  Notice that you don’t put the full path of your file at the first parameter but just the relative path to the ""templates"" folder.",0.0,0.40659618,0.16532045602798462
67,"Question\nI learned that if one needs to implement dct on a image of size (H, W), one needs a matrix A that is of size (8, 8), and one needs to use this A to compute with a (8, 8) region F on the image. That means if the image array is m, one needs to compute m[:8, :8] first, and then m[8:16, 8:16], and so on. 
How could I implement this dct when input image size is not a scale of 8. For example, when image size is (12, 12) that cannot hold two (8, 8) windows, how could I implement dct? I tried opencv and found that opencv can cope with this scenario, but I do not know how it implemented it.\nAnswer: The 8x8 is called a ""Minimum Coded Unit"" (MCU) in the specification, though video enthusiasts call them ""macroblocks"".
Poorer implementations will pad to fill with zeroes - which can cause nasty effects.
Better implementations pad to fill by repeating the previous pixel from the left if padding to the right, or from above if padding downwards.
Note that only the right side and bottom of an image can be padded.",0.40816328,0.12623996,0.07948075979948044
68,"Question\nIs it possible in Python to re-assign the backslash character to something else, like to the three dots?
I hate the backslash character. It looks ugly.
There’s a long line in my code I really need to use the \ character. But I’d rather use the... character.
I just need a simple yes/no answer. Is it possible? And in the case of yes, tell me how to re-assign that ugly thing.\nAnswer: Python syntactically uses the backslash to represent the escape character, as do other languages such as Java and C. As far as I am aware this cannot be overwritten unless you want to change the language itself.",0.0,0.18485534,0.03417149558663368
69,"Question\nI've seen how to make a post request from JavaScript to get data from the server, but how would I do this flipped. I want to trigger a function in the flask server that will then dynamically update the variable on the JavaScript side to display. Is there a way of doing this in a efficient manner that does not involve a periodic iteration. I'm using an api and I only want to the api to be called once to update.\nAnswer: There are three basic options for you:

Polling - With this method, you would periodically send a request to the server (maybe every 5 seconds for example) and ask for an update. The upside is that it is easy to implement. The downside is that many requests will be unnecessary. It sounds like this isn't a great option for you. 
Long Polling - This method means you would open a request up with the server and leave the request open for a long period of time. When the server gets new information it will send a response and close the request - after which the client will immediately open up a new ""long poll"" request. This eliminates some of the unnecessary requests with regular polling, but it is a bit of a hack as HTTP was meant for a reasonably short request response cycle. Some PaaS providers only allow a 30 second window for this to occur for example. 
Web Sockets - This is somewhat harder to setup, but ultimately is the best solution for real time server to client (and vice versa) communication. A socket connection is opened between the server and client and data is passed back and forth whenever either party would like to do so. Javascript has full web socket support now and Flask has some extensions that can help you get this working. There are even great third party managed solutions like Pusher.com that can give you a working concept very quickly.",0.40816328,0.35681462,0.0026366845704615116
70,"Question\nI have a python script that generates a heightmap depending on parameters, that will be given in HTML forms. How do I display the resulting image on a website? I suppose that the form submit button will hit an endpoint with the given parameters and the script that computes the heightmap runs then, but how do I get the resulting image and display it in the website? Also, the computation takes a few seconds, so I suppose I need some type of task queue to not make the server hang in the meanwhile. Tell me if I'm wrong.
It's a bit of a general question because I myself don't know the specifics of what I need to use to accomplish this. I'm using Flask in the backend but it's a framework-agnostic question.\nAnswer: Save the image to a file. Return a webpage that contains an <IMG SRC=...> element. The SRC should be a URL pointing at the file.
For example, suppose you save the image to a file called ""temp2.png"" in a subdirectory called ""scratch"" under your document root. Then the IMG element would be <IMG SRC=""/scratch/temp2.png"">.
If you create and save the image in the same program that generates the webpage that refers to it, your server won't return the page until the image has been saved. If that only takes a few seconds, the server is unlikely to hang. Many applications would take that long to calculate a result, so the people who coded the server would make sure it can handle such delays. I've done this under Apache, Tomcat, and GoServe (an OS/2 server), and never had a problem.
This method does have the disadvantage that you'll need to arrange for each temporary file to be deleted after an expiry period such as 12 hours or whenever you think the user won't need it any more. On the webpage you return, if the image is something serious that the user might want to keep, you could warn them that this will happen. They can always download it.
To delete the old files, write a script that checks when they were last updated, compares that with the current date and time, and deletes those files that are older than your expiry period. 
You'll need a way to automatically run it repeatedly. On Unix systems, if you have shell access, the ""cron"" command is one way to do this. Goog",0.40816328,0.6060789,0.03917060047388077
71,"Question\nI would like to create a website where I show some text but mainly dynamic data in tables and plots. Let us assume that the user can choose whether he wants to see the DAX or the DOW JONES prices for a specific timeframe. I guess these data I have to store in a database. As I am not experienced with creating websites, I have no idea what the most reasonable setup for this website would be. 

Would it be reasonable for this example to choose a database where every row corresponds of 9 fields, where the first column is the timestamp (lets say data for every minute), the next four columns correspond  to the high, low, open, close price of DAX for this timestamp and columns 5 to 9 correspond to high, low, open, close price for DOW JONES?
Could this be scaled to hundreds of columns with a reasonable speed
of the database?
Is this an efficient implementation?
When this website is online, you can choose whether you want to see DAX or DOW JONES prices for a specific timeframe. The corresponding data would be chosen via python from the database and plotted in the graph. Is this the general idea how this will be implemented?
To get the data, I can run another python script on the webserver to dynamically collect the desired data and write them in the database?

As a total beginner with webhosting (is this even the right term?) it is very hard for me to ask precise questions. I would be happy if I could find out whats the general structure I need to create the website, the database and the connection between both. I was thinking about amazon web services.\nAnswer: You could use a database, but that doesn't seem necessary for what you described.
It would be reasonable to build the database as you described.  Look into SQL for doing so.  You can download a package XAMPP that will give you pretty much everything you need for that.  This is easily scalable to hundreds of thousands of entries - that's what databases are for.
If your example of stock prices is actually what you are trying to show, however, this is completely unnecessary as there are already plenty of databases that have this data and will allow you to query them.  What you would really want in this scenario is an API.  Alpha Vantage is a free service that will serve you data on stock prices, and has plenty of documentation to help you get it set up with python.",0.0,0.26701486,0.07129693776369095
72,"Question\nThe following I had with Python 3.8.1 (on macOS Mojave, 10.14.6, as
well as Python 3.7 (or some older) on some other platforms).  I'm new
to computing and don't know how to request an improvement of a
language, but I think I've found a strange behaviour of the built-in
function map.
As the code next(iter(())) raises StopIteration, I expected to
get StopIteration from the following code:
tuple(map(next, [iter(())]))
To my surprise, this silently returned the tuple ()!
So it appears the unpacking of the map object stopped when
StopIteration came from next hitting the ""empty"" iterator
returned by iter(()).  However, I don't think the exception was
handled right, as StopIteration was not raised before the ""empty""
iterator was picked from the list (to be hit by next).

Did I understand the behaviour correctly?
Is this behaviour somehow intended?
Will this be changed in a near future?  Or how can I get it?

Edit: The behaviour is similar if I unpack the map object in different ways, such as by list, for for-loop, unpacking within a list, unpacking for function arguments, by set, dict.  So I believe it's not tuple but map that's wrong.
Edit: Actually, in Python 2 (2.7.10), the ""same"" code raises
StopIteration.  I think this is the desirable result (except that map in this case does not return an iterator).\nAnswer: Did I understand the behavior correctly?


Not quite. map takes its first argument, a function, and applies it to every item in some iterable, its second argument, until it catches the StopIteration exception. This is an internal exception raised to tell the function that it has reached the end of the object. If you're manually raising StopIteration, it sees that and stops before it has the chance to process any of the (nonexistent) objects inside the list.",0.13605443,0.3513131,0.046336304396390915
73,"Question\nI had a different question here, but realized it simplifies to this:
How do you detect when a client disconnects (closes their page or clicks a link) from a page (in other words, the socket connection closes)? I want to make a chat app with an updating user list, and I’m using Flask on Python. When the user connects, the browser sends a socket.emit() with an event and username passed in order to tell the server a new user exists, after which the server will message all clients with socket.emit(), so that all clients will append this new user to their user list. However, I want the clients to also send a message containing their username to the server on Disconnect. I couldn’t figure out how to get the triggers right. Note: I’m just using a simple html file with script tags for the page, I’m not sure how to add a JS file to go along with the page, though I can figure it out if it’s necessary for this.\nAnswer: Figured it out. socket.on('disconnect') did turn out to be right, however by default it pings each user only once a minute or so, meaning it took a long time to see the event.",1.0,0.22329146,0.6032761335372925
74,"Question\nI recently convert my model to tensorflow lite but I only got the.tflite file and not a labels.txt for my Android project. So is it possible to create my own labels.txt using the classes that I used to classify? If not, then how to generate labels.txt?\nAnswer: You should be able to generate and use your own labels.txt. The file needs to include the label names in the order you provided them in training, with one name per line.",0.40816328,0.2734868,0.01813775673508644
75,"Question\nI am using Python 3.7 (Activestate) on a windows 10 laptop. All works well until I try to use pip to install a package (any package). From command prompt, when entering ""pip install anyPackage"" I get an error - ""The system cannot find the path specified."" no other explanation or detail. 
Python is installed in ""C:\Python37"" and this location is listed in the Control Panel > System > Environment Variables > User Variables. 
In the Environment Variables > System Variables I have:
C:\Python37\
C:\Python37\DLLs\
C:\Python37\Script\
C:\Python37\Tools\
C:\Python37\Tools\ninja\
Any suggestions on how to get rid of that error, and make pip work?
Many thanks to all\nAnswer: Short : make sure that pip.exe and python.exe are running from the same location. If they don't (perhaps due to PATH environment variable), just delete the one that you don't need.
Longer:
when running pip install, check out where it tries to get python
For instance, in my own computer, it was:

pip install
Fatal error in launcher: Unable to create process using '""c:\program files\python39\python.exe""  ""C:\Program Files\Python39\Scripts\pip.exe"" ': The system cannot find the file specified.

Then I ran:
'where python.exe' // got several paths.
'where pip.exe' // got different paths.
removed the one that I don't use. Voila.",1.0,0.20198524,0.6368275284767151
76,"Question\nFirst of all, im really new at Machine Learning and Anaconda
Recently I´ve Installed Anaconda for MachineLearning but now when i try to run my old scripts from my terminal, all my packages are not there, even pip or numpy or pygame y don´t know how to change to my old python directory, I really don´t know how this works, please help me. I´m on MacOs Catalina\nAnswer: First of all, Python 3 is integrated in macOS X Catalina, just type python3. For pip, you can use pip3. Personally, I would prefer native over conda when using mac.
Next, you need to get all the modules up from your previous machine by pip freeze > requirements.txt or pip3 freeze > requirements.txt
If you have the list already, either it's from your previous machine or from a GitHub project repo, just install it via pip3 in your terminal: pip3 install -r requirements.txt
If not, you have to manually install via pip3, for example: pip3 install pygame etc.
After all dependencies are done installed, just run your.py file as usual.
Last, but not least, welcome to the macOS X family!",0.6122449,0.28494865,0.10712283849716187
77,"Question\nI am a beginner in python 3. I want to locate where the time module is in PyCharm to study it's aspects/functions further. I can't seem to find it in the library. Can someone show me an example on how to find it?
I know there are commands to find files, but I am not advanced enough to use them.\nAnswer: I think you may have a misconception - the time module is part of your Python installation, which PyCharm makes use of when you run files. Depending on your setup, you may be able to view the Python files under ""external libraries"" in your project viewer, but you could also view them from your file system, wherever Python is installed.",0.0,0.121507466,0.014764063991606236
78,"Question\nI am building a chat application using flask socketio and I want to send to a specific singular client and I'm wondering how to go about this. 
I get that emit has broadcast and include_self arguments to send to all and avoid sending oneself, but how exactly would I go about maybe emitting to a single sid?
I've built this application using standard TCP/UDP socket connection where upon client connecting, there socket info was stored in a dictionary mapped to their user object with attributes that determined what would be sent and when I wanted to emit something to the clients I would iterate through this and be able to control what was being sent.  
I'm hoping some mastermind could help me figure out how to do this in flask socket io\nAnswer: I ended up figuring it out.  Using the flask request module, you can obtain the users sid using request.sid, which can be stored and emitted to within the room parameter emit(..... room=usersid",0.20408164,0.170098,0.001154887257143855
79,"Question\nlet us say I have a python 3.7+ module/script A which does extensive computations. Furthermore, module A is being used inside another module B, where module A is fully encapsulated and module B doesn't know a lot of module's A internal implementation, just that it somehow outputs the progress of its computation.
Consider me as the responsible person (person A) for module A, and anyone else (person B) that doesn't know me, is writing module B. So person A is writing basically an open library.
What would be the best way of module A to output its progress? I guess it's a design decision.

Would a getter in module A make sense so that module B has to always call this getter (maybe in a loop) in order to retrieve the progress of A?
Would it possible to somehow use a callback function which is implemented in module A in such a way that this function is called every time the progress updates? So that this callback returns the current progress of A.
Is there maybe any other approach to this that could be used?

Pointing me towards already existing solutions would be very helpful!\nAnswer: Essentially module B want to observe module A as it goes though extensive computation steps. And it is up to module A to decide how to compute progress and share this with module B. Module B can't compute progress as it doesn't know details of computation. So its is good use of observer pattern. Module A keeps notifying B about its progress. Form of progress update is also important. It can in terms of percentage, or ""step 5 of 10"" or time. It will actually define the notification payload structure with which module A will notify module B.",0.0,0.32591993,0.10622379928827286
80,"Question\nI have a kivy app, where I use JsonStorage. Where does kivy save the json files, so how can I find it?\nAnswer: I just found out the json file is on the same level as the kivy_venv folder",0.40816328,-0.050587,0.21045181155204773
81,"Question\nI have the following setup:

One thread which runs a directory crawler and parses documents 
Another thread which processes database requests it gets in a queue - there are two basic database requests that come through - mark document processed (write operation) and is document already
processed (select operation)

I understand that an sqlite connection object cannot be shared across threads, so the connection is maintained in the database thread. I am new to threading though and in my parser thread I want to check first if a document has been processed which means a database call, but obviously cannot do this call directly and have to send the request to the database thread which is fine.
However, where I am stuck is I am not sure how to make the parser thread wait for the result of the ""has document been processed"" request in the database thread. Is this where a threading event would come in?
Thanks in advance for your help!\nAnswer: Thanks to stovfl, used a threading event to realise this. Thanks again!",0.0,0.27130687,0.0736074224114418
82,"Question\nI opened a virtual enviroment and accidentally closed the command prompt window in Windows. 
I wanted to delete the virtual enviroment folder, but when I tried, it says program is running which still uses the files. 
So how do I get back to the virtual enviroment, without opening a new one?\nAnswer: Just kill the daemon-process by command in Ctrl+Alt+Del interface. Then you can delete a folder",0.0,-0.10736668,0.011527604423463345
83,"Question\nIt's past time for me to move from my custom scientific workflow management (python) to some group effort.  In brief, my workflow involves long running (days) processes with a large number of shared parameters.  As a dependency graph, nodes are tasks that produce output or do some other work. That seems fairly universal in workflow tools.
However, key to my needs is that each task is defined by the parameters it requires.   Tasks are instantiated with respect to the state of those parameters and all parameters of its dependencies.  Thus if a task has completed its job according to a given parameter state, it is complete and not rerun.  This parameter state is NOT the global parameter state but only what is relevant to that part of the DAG. This reliance on parameter state rather than time completed appears to be the essential difference between my needs and existing tools (at least what I have gathered from a quick look at Luigi and Airflow).  Time completed might be one such parameter, but in general it is not the time that determines a (re)run of the DAG, but whether the parameter state is congruent with the parameter state of the calling task. There are non-trivial issues (to me) with 'parameter explosion' and the relationship to parameter state and the DAG, but those are not my question here.
My question -- which existing python tool would more readily allow defining 'complete' with respect to this parameter state?  It's been suggested that Luigi is compatible with my needs by writing a custom complete method that would compare the metadata of built data ('targets') with the needed parameter state.
How about Airflow? I don't see any mention of this issue but have only briefly perused the docs.  Since adding this functionality is a significant effort that takes away from my'scientific' work, I would like to start out with the better tool. Airflow definitely has momentum but my needs may be too far from its purpose.
Defining the complete parameter state is needed for two reasons -- 1) with complex, long running tasks, I can't just re-run the DAG every time I change some parameter in the very large global parameter state, and 2) I need to know how the intermediate and final results have been produced for scientific and data integrity reasons.\nAnswer: I looked further into Luigi and Airflow and as far as I could discern neither of these is suitable for modification for my needs. ",0.40816328,0.09137559,0.10035444051027298
84,"Question\nCan I change the core functionality of Python, for example, rewrite it to use say(""Hello world"") instead of print(""Hello world"")?
If this is possible, how can this be done?\nAnswer: yes you can just write 
say = print
say(""hello"")",0.0,0.112066925,0.012558995746076107
85,"Question\nI am working on wind power and speed
u and vare zonal and meridional wind. (I have the values of these 2 vectors)
The wind speed is calculated by V = np.sqrt(u2*v2)
Wind direction is given by α between 0 and 360 degree 
I know this relation holds - u / V = sin( abs(α)) and - v / V = cos( abs(α)) 
In python I am using np.arccos and np.arcsin to try to find α between 0 and 360 with the 2 equation above. For the first one, it returns the radian so I convert with np.rad2deg(...) but it gives me a value between 0 and 180 degree for the second one, I also try to convert but it returns me a valus between 0 and 90. 
Anyone knows how to code it? I am lost :(\nAnswer: The underlying problem is mathematics: cos(-x) == cos(x), so the function acos has only values in the [0,pi] interval. And for equivalent reasons asin has values in [-pi/2,pi/2] one.
But trigonometric library designers know about that, and provide a special function (atan2) which uses both coordinates (and not a ratio) to give a value in the [-pi, pi] interval.
That being said, be careful when processing wind values. A 360 wind is a wind coming from the North, and 90 is a wind coming from the East. Which is not the way mathematicians count angles...",0.40816328,0.07000446,0.11435138434171677
86,"Question\nI looked everywhere, I don't find a way to properly play Ogg files, they all play wav!
My question is: Does somebody knows how to play Ogg files in python?
If somebody knows how I'll be very thankful :)
(I am on windows)\nAnswer: The easiest way is probably to start a media player application to play the file using subprocess.Popen.
If you already have a media player associated with Ogg files installed, using the start command should work.",0.0,0.15846056,0.025109747424721718
87,Question\nI was modifying the layout in Spyder 4.1.1 and somehow lost the filename tabs (names of opened.py files) that used to appear above the central editor window.  These were the tabs that had the 'X' button in them that allowed you to quickly close them.  I've been toggling options in the View and Tools menus but can't seem to get it back.  Anyone know how to restore this?\nAnswer: Try it. From menu View --> Panes --> Editor. Clicking on Editor and putting a tick there should bring that back if I understand your question properly,0.20408164,0.068011224,0.01851515844464302
88,"Question\nI was modifying the layout in Spyder 4.1.1 and somehow lost the filename tabs (names of opened.py files) that used to appear above the central editor window.  These were the tabs that had the 'X' button in them that allowed you to quickly close them.  I've been toggling options in the View and Tools menus but can't seem to get it back.  Anyone know how to restore this?\nAnswer: (Spyder maintainer here) You can restore the tab bar in our editor by going to the menu
Tools > Preferences > Editor > Display
and selecting the option called Show tab bar.",0.20408164,0.24575537,0.0017367001855745912
89,Question\nHow do I get the instance of a parent widget from within the child widget in kivy? This is so that I can remove the child widget from within the child widget class from the parent widget.\nAnswer: use parent.<attribute> or root.ids.<id-of-the-widget-you-need>,0.0,0.51409745,0.26429620385169983
90,"Question\nI have set the interpreter to 3.8.2 but when I type in the console python --version it gives me the python 2.7.2. Why is that and how to change the console version so I can run my files with Python 3? In windows console I have of course python 3 when I type the --version.\nAnswer: (Assuming you use Visual Studio Code with the Python Extension)
The interpreter set in visual studio has nothing to do with the terminal python version when you run python --version.  
python --version is bound to what python version is bound to 'python' in your environment variables. 
Try python3 --version in the visual studio console to see what version is bound to python3.
If this is the right version, use python3 in the visual studio console from now on.",0.0,0.33943683,0.11521735787391663
91,"Question\nI have set the interpreter to 3.8.2 but when I type in the console python --version it gives me the python 2.7.2. Why is that and how to change the console version so I can run my files with Python 3? In windows console I have of course python 3 when I type the --version.\nAnswer: The console displayed by VSCode is basically an ordinary terminal. When you run the python file from VSCode using the green arrow at the top, it will call the appropriate python version displayed at the bottom of the VSCode window. You can also see what VSCode executes in the terminal seeing to which python its pointing to.",0.20408164,0.2763754,0.00522638950496912
92,"Question\nI am writing an ReactionRoles-Discord-Bot in Python (discord.py).
This Bot saves the ReactionRoles-Smileys as UFT8-Encoded. 
The type of the encoded is bytes but it's converted to str to save it.
The string looks something like ""b'\\xf0\\x9f\\x98\\x82'"".
I am using EMOJI_ENCODED = str(EMOJI.encode('utf8')) to encode it, but bytes(EMOJI_ENCODED).decode('utf8') isn't working.
Do you know how to decode it or how to save it in a better way?\nAnswer: The output of str() is a Unicode string.  EMOJI is a Unicode string.  str(EMOJI.encode('utf8')) just makes a mangled Unicode string.
The purpose of encoding is to make a byte string that can be saved to a file/database/socket.  Simply do b = EMOJI.encode() (default is UTF-8) to get a byte string and s = b.decode() to get the Unicode string back.",0.0,0.31474555,0.09906475991010666
93,"Question\nI currently find myself using the bs4/BeautifulSoup library a lot in python, and have recently been wondering how it works. I would love to see the source code for the library and don't know how. Does anyone know how to do this? Thanks.\nAnswer: If you are using any IDE, you can right click on imported line and goto Implementation.
Otherwise you can find the source code in <python_installtion_path>\Lib\site-packages directory.",0.13605443,0.22726202,0.0083188246935606
94,"Question\nI currently find myself using the bs4/BeautifulSoup library a lot in python, and have recently been wondering how it works. I would love to see the source code for the library and don't know how. Does anyone know how to do this? Thanks.\nAnswer: Go to the location where python is installed and inside the python folder, you will have a folder called Lib you can find all the packages there open the required python file you will get the code.
example location: C:\Python38\Lib",0.0,-0.077198684,0.005959636997431517
95,"Question\nI am analyzing histology tissue images stained with a specific protein marker which I would like to identify the positive pixels for that marker. My problem is that thresholding on the image gives too much false positives which I'd like to exclude.
I am using color deconvolution (separate_stains from skimage.color) to get the AEC channel (corresponding to the red marker), separating it from the background (Hematoxylin blue color) and applying cv2 Otsu thresholding to identify the positive pixels using cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU), but it is also picking up the tissue boundaries (see white lines in the example picture, sometimes it even has random colors other than white) and sometimes even non positive cells (blue regions in the example picture). It's also missing some faint positive pixels which I'd like to capture.
Overall: (1) how do I filter the false positive tissue boundaries and blue pixels? and (2) how do I adjust the Otsu thresholding to capture the faint red positives?
Adding a revised example image -

top left the original image after using HistoQC to identify tissue regions and apply the mask it identified on the tissue such that all of the non-tissue regions are black. I should tru to adjust its parameters to exclude the folded tissue regions which appear more dark (towards the bottom left of this image). Suggestions for other tools to identify tissue regions are welcome.
top right hematoxylin after the deconvolution
bottom left AEC after the deconvolution
bottom right Otsu thresholding applied not the original RGB image trying to capture only the AEC positives pixels but showing also false positives and false negatives

Thanks\nAnswer: I ended up incorporating some of the feedback given above by Chris into the following possible unconventional solution for which I would appreciate getting feedback (to the specific questions below but also general suggestions for improvement or more effective/accurate tools or strategy):

Define (but not apply yet) tissue mask (HistoQC) after optimizing HistoQC script to remove as much of the tissue folds as possible without removing normal tissue area
Apply deconvolution on the original RGB image using hax_from_rgb
Using the second channel which should correspond to the red stain",0.10204082,0.20529085,0.010660569183528423
96,"Question\nI want a function to be run as if it was written in the main program, i.e. all the variables defined therein can be accessed from the main program. I don't know if there's a way to do that, but I thought a wrapper that gives this behaviour would be cool. It's just hacky and I don't know how to start writing it.\nAnswer: I have pieces of code written inside functions, and I really want to run them and have all the variables defined therein after run without having to write the lengthy return statements. How can I do that? 

That's what classes are for. Write a class with all your functions as methods, and use instance attributes to store the shared state. Problem solved, no global required.",0.20408164,0.15815681,0.0021090898662805557
97,"Question\nI have prepared a time series model using FB Prophet for making forecasts. The model forecasts for the coming 30 days and my data ranges from Jan 2019 until Mar 2020 both months inclusive with all the dates filled in. The model has been built specifically for the UK market
I have already taken care of the following:

Seasonality
Holidaying Effect

My question is, that how do I take care of the current COVID-19 situation into the same model? The cases that I am trying to forecast are also dependent on the previous data at least from Jan 2020. So in order to forecast I need to take into account the current coronavirus situation as well that would impact my forecasts apart from seasonality and holidaying effect.
How should I achieve this?\nAnswer: I have had the same issue with COVID at my work with sales forecasting. The easy solution for me was to make an additional regressor which indicates the COVID period, and use that in my model. Then my future is not affected by COVID, unless I tell it that it should be.",0.0,0.12897837,0.01663541980087757
98,"Question\nI am new to Python language programming. I found that we can have auto completion on Jupyter notebook. I found this suggestion:
""The auto-completion with Jupyter Notebook is so weak, even with hinterland extension. Thanks for the idea of deep-learning-based code auto-completion. I developed a Jupyter Notebook Extension based on TabNine which provides code auto-completion based on Deep Learning. Here's the Github link of my work: jupyter-tabnine.
It's available on pypi index now. Simply issue following commands, then enjoy it:)
pip3 install jupyter-tabnine,
jupyter nbextension install --py jupyter_tabnine,
jupyter nbextension enable --py jupyter_tabnine,
jupyter serverextension enable --py jupyter_tabnine""
I did 4 steps installation and it looked installed well. However, when I tried using Jupyter notebook its auto completion didn't work. Basically my question is please help how to get auto completion on Jupiter notebook? Thank you very much.\nAnswer: After installing Nbextensions, go to Nbextensions in jupyter notebook, tick on Hinterland. Then reopen your jupyter notebook.",0.20408164,0.12877017,0.005671816878020763
99,"Question\nI am new to Python language programming. I found that we can have auto completion on Jupyter notebook. I found this suggestion:
""The auto-completion with Jupyter Notebook is so weak, even with hinterland extension. Thanks for the idea of deep-learning-based code auto-completion. I developed a Jupyter Notebook Extension based on TabNine which provides code auto-completion based on Deep Learning. Here's the Github link of my work: jupyter-tabnine.
It's available on pypi index now. Simply issue following commands, then enjoy it:)
pip3 install jupyter-tabnine,
jupyter nbextension install --py jupyter_tabnine,
jupyter nbextension enable --py jupyter_tabnine,
jupyter serverextension enable --py jupyter_tabnine""
I did 4 steps installation and it looked installed well. However, when I tried using Jupyter notebook its auto completion didn't work. Basically my question is please help how to get auto completion on Jupiter notebook? Thank you very much.\nAnswer: Press tab twice while you are writing your code and the autocomplete tab will show for you. Just select one and press enter",0.6122449,0.20279753,0.16764715313911438
0,"Question\nI have a binary image containing a single contiguous blob, with no holes. I would like create a polygon object based on the exterior edges of the edge pixels. I know how to get the edge pixels themselves, but I want the actual coordinates of the pixel boundaries, sorted clockwise or counter-clockwise. All of the pixels have integer coordinates. 
For example, say I have a single pixel at (2,2). The vertices of the polygon would be:
(2.5, 2.5)
(2.5, 1.5)
(1.5, 1.5)
(1.5, 2.5)
(2.5, 2.5)
Is there an exact, non-approximate way to do this? Preferably in Python?\nAnswer: Based on the comments, here is the approach that I implemented:

multiply all pixel coordinates by 10, so that we'll only deal with integers.

For each pixel, generate the 4 corners by adding +/- 5. For example, for (20,20), the corners are (25, 25) (25, 15) (15, 15) (15, 25) (25, 25). And store all the corners in a list.

Count the occurrences of each corner. If the count is odd, it is a corner to the blob. Making the coordinates integers makes this step easy. Counting floats has issues.

Divide the blob corner coordinates by 10, getting back the original resolution.

Sort the corners clockwise using a standard algorithm.",0.0,0.32872587,0.10806070268154144
1,"Question\nI finished writing the code for a simple game using Kivy. I am having a problem converting it to Android APK, since I am using a windows computer. From some earlier research I got to know that using a Virtual machine is recommended, but I have no idea on how to download and use one :(, and if my slow PC can handle it... please help me. If possible, kindly recommend another  way to  convert to APK.
I am a beginner at coding as a whole, please excuse me if my question is stupid.\nAnswer: you could just try downloading a virtual box and installing linux operating system or you could directly install it and keep it a drive called F or E and you could just install python on that and all the required pakages and start the build using buildozer as it is not available for windows. So try doing it. But I need to do it just now. Tell me after you have tried that cuz there are a lot of people online on youtube who would heloo you doing that work",0.0,-0.0752466,0.0056620510295033455
2,"Question\nwhen clicking ""Run Python file in terminal"" how do I stop the script? the only way that I found is by clicking the trashcan which deletes the log in the terminal.\nAnswer: When a Python file is running in the terminal you can hit Ctrl-C to try and interrupt it.",0.0,0.17676467,0.031245747581124306
3,"Question\nI was just wondering how to go from mvnrnd([4 3], [.4 1.2], 300); in MATLAB code to np.random.multivariate_normal([4,3], [[x_1 x_2],[x_3 x_4]], 300) in Python. 
My doubt namely lays on the sigma parameter, since, in MATLAB, a 2D vector is used to specify the covariance; whereas, in Python, a matrix must be used. 
What is the theoretical meaning on that and what is the practical approach to go from one to another, for instance, in this case? Also, is there a rapid, mechanical way?
Thanks for reading.\nAnswer: Although python expects a matrix, it is essentially a symmetric covariance matrix. So it has to be a square matrix. 
In 2x2 case, a symmetric matrix will have mirrored non diagonal elements.
I believe in python, it should look like [[.4 1.2],[1.2.4]]",0.0,0.45409513,0.20620238780975342
4,"Question\nI have written my code in python3 and solved it using Gekko solver.
After 10000 iterations, I am getting the error maximum iteration reached and solution not found.
So can I get the value of decision variables after the 10000th iteration?
I mean even when the maximum iteration is reached the solver must have a value of decision variable in the last iteration. so I want to access that values. how can I do that?\nAnswer: Question:
1) I am solving an MINLP problem with APOPT Solver. And my decision variables are defined as integer. I have retrieved the result of 10,000th iteration as you suggested. but the Decision variables values are non-integer. So why APOPT Solver is calculating a non-integer solution? 
Answer:
There is an option on what is classified as an integer. The default tolerance is any number within 0.05 of an integer value. 
you can change this by:
m.solver_options = ['minlp_integer_tol 1']
2) I am running the code for ""m.options.MAX_ITER=100"" and using m = GEKKO() i.e. using remote server. But my code is still running for 10000th iterations.
Answer: Can do it alternatively by:
m.solver_options = ['minlp_maximum_iterations 100']
Thanks a lot to Prof. John Hedengren for the prompt replies.
Gekko",0.0,0.43345475,0.18788301944732666
5,"Question\nThe main question is to find which leaf node each sample is classified. There are thousands of posts on using tree.apply. I am well aware of this function, which returns the index of the leaf node.
Now, I would like to add the leaf index in the nodes of the graph (which I generate by using Graphviz).
Drawing the enumeration technique used for the indexes won't work. The decision tree that I am developing is quite big. Therefore, I need to be able to print the leaf index in the graph.
Another option that I am open to is to generate an array with all the leaf indexes (in the same order) of the leaf nodes of the decision tree. Any hint on how to do this?\nAnswer: There is a parameter node_ids of the command export_graphviz. When this parameter is set to True, then the indexes are added on the label of the decision tree.",0.0,0.26741743,0.07151208072900772
6,"Question\nI'm using classes and such to make a calculator in Tkinter, however I want to be able to be able to reuse widgets for multiple windows. How can I do this if this is possible?\nAnswer: A widget may only exist in one window at a time, and cannot be moved between windows (the root window and instances of Toplevel).",0.20408164,0.26785678,0.004067268222570419
7,"Question\nI'm using classes and such to make a calculator in Tkinter, however I want to be able to be able to reuse widgets for multiple windows. How can I do this if this is possible?\nAnswer: As you commented:

I'm making a calculator, as mentioned and I want to have a drop down menu on the window, that when selected it closes the root window and opens another, and I want to have the drop down menu on all the different pages, 5 or 6 in all

In this case, just write a function that creates the menu.
Then call that function when creating each of the windows.",0.0,0.22643143,0.051271192729473114
8,"Question\nCan you show me how I can multiply two integers which are M bits long using at most O(N^1.63) processors in O(N) parallel time in python.
I think that karatsuba algorithm would work but I don't understand how can I implement it parallely.\nAnswer: Yes, It is parallel karatsuba algorithm.",-0.71428573,0.15300858,0.7521994113922119
9,"Question\nI entered '''idle''' on terminal
and it only shows me python2 that has already been here.
How can i see python3 idle on my mac 
while i installed python3 with pip?\nAnswer: you can specify the version
idle3",0.0,0.06488526,0.004210096783936024
10,"Question\nI am trying to get input from the user and send this input to all bot subscribers.
so I need to save his input in variable and use it after this in send_message method but I don't know how to make my bot wait for user input and what method I should use to receive user input 
thanks :]\nAnswer: If you want to get an user input, the logic is a bit different. I suppose you are using longpolling.
When the bot asks the user for input, you can just save a boolean/string in a global variable, let's suppose the variable is user_input:
You receive the update, and ask the user for input, then you set user_input[user's id]['input'] = true
Then when you receive another update you just check that variable with an if (if user_input[userid]['input']: # do something).

If your problem is 403 Forbidden for ""user has blocked the bot"", you can't do anything about it.",0.0,0.24991006,0.062455035746097565
11,"Question\nUsing Python 3.7 with IDLE, on Mac.  I want to be able to monitor key presses and immediately return either the character or its ASCII or Unicode value.  I can't see how to use pynput with Idle.  Any ideas please?\nAnswer: You can't.  IDLE uses the tkinter interface to the tcl/tk GUI framework.  The IDLE doc has a section currently title 'Running user code' with this paragraph.

When Shell has the focus, it controls the keyboard and screen.  This is normally transparent, but functions that directly access the keyboard and screen will not work.  These include system-specific functions that determine whether a key has been pressed and if so, which.",0.0,0.5463977,0.29855042695999146
12,"Question\nI'm trying to install OpenCV into my python environment (Windows), and I'm almost all of the way there, but still having some issues with autocomplete and Pycharm itself importing the library. I've been through countless other related threads, but it seems like most of them are either outdated, for prebuilt versions, or unanswered.
I'm using Anaconda and have several environments, and unfortunately installing it through pip install opencv-contrib-python doesn't include everything I need. So, I've built it from source, and the library itself seem to be working fine. The build process installed some things into./Anaconda3/envs/cv/Lib/site-packages/cv2/: __init__.py, some config py files, and.../cv2/python-3.8/cv2.cp38-win_amd64.pyd. I'm not sure if it did anything else.
But here's where I'm at:

In a separate environment, a pip install opencv-contrib-python both runs and has autocomplete working
In this environment, OpenCV actually runs just fine, but the autocomplete doesn't work and Pycharm complains about everything, eg: Cannot find reference 'imread' in '__init__.py'
Invalidate Caches / Restart doesn't help
Removing and re-adding the environment doesn't help
Deleting the user preferences folder for Pycharm doesn't help
Rebuilding/Installing OpenCV doesn't help
File->Settings->Project->Project Interpreter is set correctly
Run->Edit Configuration->Python Interpreter is set correctly

So my question is: how does Pycharm get or generate that autocomplete information? It looks like the pyd file is just a dll in disguise, and looking through the other environment's site-packages/cv2 folder, I don't see anything interesting. I've read that __init__.py has something to do with it, but again the pip version doesn't contain anything (except there's a from.cv2 import *, but I'm not sure how that factors in). The.whl file you can download is a zip that only contains the same as what 'pip install' gets.
Where does the autocomplete information get stored? Maybe there's some way to copy it from one environment to another? It would",0.81632656,0.5347171,0.07930389791727066
13,"Question\nI'm trying to install OpenCV into my python environment (Windows), and I'm almost all of the way there, but still having some issues with autocomplete and Pycharm itself importing the library. I've been through countless other related threads, but it seems like most of them are either outdated, for prebuilt versions, or unanswered.
I'm using Anaconda and have several environments, and unfortunately installing it through pip install opencv-contrib-python doesn't include everything I need. So, I've built it from source, and the library itself seem to be working fine. The build process installed some things into./Anaconda3/envs/cv/Lib/site-packages/cv2/: __init__.py, some config py files, and.../cv2/python-3.8/cv2.cp38-win_amd64.pyd. I'm not sure if it did anything else.
But here's where I'm at:

In a separate environment, a pip install opencv-contrib-python both runs and has autocomplete working
In this environment, OpenCV actually runs just fine, but the autocomplete doesn't work and Pycharm complains about everything, eg: Cannot find reference 'imread' in '__init__.py'
Invalidate Caches / Restart doesn't help
Removing and re-adding the environment doesn't help
Deleting the user preferences folder for Pycharm doesn't help
Rebuilding/Installing OpenCV doesn't help
File->Settings->Project->Project Interpreter is set correctly
Run->Edit Configuration->Python Interpreter is set correctly

So my question is: how does Pycharm get or generate that autocomplete information? It looks like the pyd file is just a dll in disguise, and looking through the other environment's site-packages/cv2 folder, I don't see anything interesting. I've read that __init__.py has something to do with it, but again the pip version doesn't contain anything (except there's a from.cv2 import *, but I'm not sure how that factors in). The.whl file you can download is a zip that only contains the same as what 'pip install' gets.
Where does the autocomplete information get stored? Maybe there's some way to copy it from one environment to another? It would",0.81632656,0.5347171,0.07930389791727066
14,"Question\nHow to classify English words according to topics with python? Such as  THE COUNTRY AND GOVERNMENT:  regime, politically, politician, official, democracy......besides, there are other topics: education/family/economy/subjects and so on.
I want to sort out The Economist magazine vocabularies and classify these according to frequency and topic. 
At present, I have completed the words frequency statistics, the next step is how to classify these words automatically with python?\nAnswer: It sounds quite tough to handle it. Also it is not a simple task. If I were you, I consider 2 ways to do what you ask.

Make your own rule for it

If you complete counting the words, then you should match those word to topic. There is no free lunch. Make own your rule for classifying category. e.g. Entertainment has many ""TV"" and ""drama"" so If some text has it, then we can guess it belongs to Entertainment.

Machine learning.

If you can't afford to make rules, let machine do it. But even in this case, you should label the article with your desired class(topics).
Unsupervised pre-training(e.g. clustering) can also be used here. but at last, we need supervised data set with topics.
You should decide taxonomy of topics.


Welcome to ML world.
Hope it helps to get the right starting point.",0.0,0.14873958,0.022123461589217186
15,"Question\nIn my login First place I wanted to send OTP and second place I wanted to verify the OTP and then return the token.
I am using rest_framework_simplejwt JWTAuthentication. First place I am verifying the user and sending the OTP, not returning the token and second place I am verifying the OTP and returning the token.
Let me know If this is the correct way to use? If not how can I implement this using JWTAuthentication.
OR If this is not correct way to use, can I implement like first place use Basic authentication to verify the user and second place jwt authentication to verify the OTP and send the tokens. Let me know your solution.\nAnswer: What I understood?
You need to send an OTP to the current user who is hitting your send_otp route after checking if the user exists or not in your system and then verify_otp route which will verify the OTP that the user has sent in the API alongwith it's corresponding mobile_number/email_id.
How to do it?

send_otp - Keep this route open, you don't need an authentication for this, not even Basic Auth (that's how it works in industry), just get the mobile_number from the user in the request, check whether it exists in the DB, and send the OTP to this number, and set the OTP to the corresponding user in your cache maybe for rechecking (redis/memcache). Use throttling for this route so that nobody will be able to exploit this API of yours.
verify_otp - This route will also be open (no authentication_class/permission_classes), get the mobile_number/email id + OTP from the user, verify it in cache, if verified, generate the token using TokenObtainPairSerializer and send the refresh + access token in the response, if the OTP is incorrect, send 401.",0.0,0.32862282,0.10799295455217361
16,"Question\nI have python script that use selenium to login website, you should insert the username and password and captcha for submit button to login.after this login webpage have send button  for send form information with post request, how can i bypass this clicking in button and  send the post request without  clicking on the button?\nAnswer: If you mean that you want to try and bypass the captcha and go straight to the send button, I doubt that's possible. If you need to solve recaptchas, check out 2captcha.com and use their API to solve it - which will unlock the send button, theoretically.",0.0,0.20336401,0.04135692119598389
17,"Question\nI am confused in celery.Example i want to load a data file and it takes 10 seconds to load without celery.With celery how will the user be benefited? Will it take same time to load data?\nAnswer: Normally, the user has to wait to load the data file to be done on the server. But with the help of celery, the operation will be performed on the server and the user will not be involved. Even if the app crashes, that task will be queued.

Celery will keep track of the work you send to it in a database
  back-end such as Redis or RabbitMQ. This keeps the state out of your
  app server's process which means even if your app server crashes your
  job queue will still remain. Celery also allows you to track tasks
  that fail.",0.0,0.46543062,0.2166256606578827
18,"Question\nI am confused in celery.Example i want to load a data file and it takes 10 seconds to load without celery.With celery how will the user be benefited? Will it take same time to load data?\nAnswer: Celery, and similar systems like Huey are made to help us distribute (offload) the amount of processes that normally can't execute concurrently on a single machine, or it would lead to significant performance degradation if you do so. The key word here is DISTRIBUTED.
You mentioned downloading of a file. If it is a single file you need to download, and that is all, then you do not need Celery. How about more complex scenario - you need to download 100000 files? How about even more complex - these 100000 files need to be parsed and the parsing process is CPU intensive?
Moreover, Celery will help you with retrying of failed tasks, logging, monitoring, etc.",0.0,0.2577055,0.06641212850809097
19,"Question\nI am working on a application, where it would be cool to change the Status of your Discord User you are currently logged in to. For example when i start the appplication then the Status should change to something like ""Playing Program"" and when you click on the User's Status then it should display the Image of the Program. 
Now i wanted to ask if this is somehow possible to make and in which programming Languages is it makeable?
EDIT: Solved the Problem with pypresence\nAnswer: In your startup, where DiscordSocketClient is available, you can use SetGameAsync(). This is for C# using Discord.NET.
To answer your question, I think any wrapper for Discord's API allows you to set the current playing game.",0.0,0.21377647,0.045700378715991974
20,"Question\nI periodically receive data (every 15 minutes) and have them in an array (numpy array to be precise) in python, that is roughly 50 columns, the number of rows varies, usually is somewhere around 100-200.
Before, I only analyzed this data and tossed it, but now I'd like to start saving it, so that I can create statistics later.
I have considered saving it in a csv file, but it did not seem right to me to save high amounts of such big 2D arrays to a csv file.
I've looked at serialization options, particularly pickle and numpy's.tobytes(), but in both cases I run into an issue - I have to track the amount of arrays stored. I've seen people write the number as the first thing in the file, but I don't know how I would be able to keep incrementing the number while having the file still opened (the program that gathers the data runs practically non-stop). Constantly opening the file, reading the number, rewriting it, seeking to the end to write new data and closing the file again doesn't seem very efficient.
I feel like I'm missing some vital information and have not been able to find it. I'd love it if someone could show me something I can not see and help me solve the problem.\nAnswer: Saving on a csv file might not be a good idea in this case, think about the accessibility and availability of your data. Using a database will be better, you can easily update your data and control the size amount of data you store.",0.40816328,0.068318486,0.11549448221921921
21,"Question\nI have a question related to an annoying behavior I have observed recently in tkinter. When there's no fixed window size defined, the main window is expanded when adding new frames, which is great. However, if prior to adding a new widget the user only so much as touches the resizing handles, resizing the main window manually, then the window does not expand to fit the new widget. Why is that so and is there a way to prevent this behavior?
Thanks in advance!\nAnswer: The why is because tkinter was designed to let the user ultimately control the size of the window. If the user sets the window size, tkinter assumes it was for a reason and thus honors the requested size. 
To get the resize behavior back, pass an empty string to the geometry method of the window.",0.40816328,0.26619935,0.02015375718474388
22,"Question\nI use python boto3
when I upload file to s3,aws lambda will move the file to other bucket,I can get object url by lambda event,like
https://xxx.s3.amazonaws.com/xxx/xxx/xxxx/xxxx/diamond+white.side.jpg
The object key is xxx/xxx/xxxx/xxxx/diamond+white.side.jpg 
This is a simple example,I can replace ""+"" get object key, there are other complicated situations,I need to get object key by object url,How can I do it?
thanks!!\nAnswer: You should use urllib.parse.unquote and then replace + with space.
From my knowledge, + is the only exception from URL parsing, so you should be safe if you do that by hand.",0.20408164,0.22178787,0.0003135105944238603
23,"Question\nI have a Django project and API view implemented with the Rest framework. I'm caching it using the @cache_page decorator but I need to implement a cache invalidation and I'm not seeing how to do that - do I need a custom decorator? 
The problem: 
The view checks the access of the API KEY and it caches it from the previous access check but, if the user changes the API KEY before the cache expires, the view will return an OK status of the key that no longer exists.\nAnswer: Yes, you'll need a cache decorator that takes the authentication/user context into account. cache_page() only works for GET requests, and keys based on the URL alone.
Better yet, though, 

Don't use a cache until you're sure you need one
If you do need it (think about why; cache invalidation is one of the two hard things), use a more granular cache within your view, not cache_page().",0.0,0.52349806,0.27405020594596863
24,"Question\nSo I am new to Kali Linux and I have installed the infamous Sherlock, nonetheless when I used the command to search for usernames it didn't work (Python3: can't open file'sherlock.py' [Errno 2] No such file or directory). Naturally I tried to look up at similiar problems and have found that maybe the problem is located on my python path. 
Which is currently located in /usr/bin/python/ and my pip is in /usr/local/bin/pip. Is my python and pip installed correctly in the path? If not, how do I set a correct path?
However if it is right and has no correlation with the issue, then what is the problem?\nAnswer: You have to change directory to sherlock twice. (it works for me)",0.20408164,0.28916204,0.007238674443215132
25,"Question\nI am new to Python scripting in Abaqus. I am aware how to use the GUI but not really familiar with the scripting interface. However, I would like to know one specific thing. I would like to know how to assign a set to each individual node on a geometry's edges. I have thought about referencing the node numbers assigned to the geometry edges but don't know how I will do it. 
The reason for creating a set for each node is that I would like to apply Periodic Boundary Conditions (PBC). Currently my model is a 2D Repeating Unit Cell (RUC) and I would like to apply a constraint equation between the opposite nodes on the opposite edges. To do that, I need to create a set for each node and then apply an equation on the corresponding set of nodes.
Just to add that the reason why I would like to use the Python scripting interface is because through the GUI, I can only make sets of nodes and create constraint equations for a simple mesh. But for a refined mesh, there will be a lot more constraint equations and a whole lot more sets. 
Any suggestion of any kind would be really helpful.\nAnswer: One of the way would be with the help of getByBoundingBox(...) method available for selecting nodes inside of a particular bounding box.

allNodes = mdb.models[name].parts[name].nodes
  allNodes.getByBoundingBox(xMin, yMin, zMin, xMax, yMax, zMax)
  mdb.models[name].parts[name].Set(name=<name_i>, region=<regionObject_corresponding_to_node_i>)

One could always look for pointers in the replay file *.rpy of the current current session, which is mostly machine generated python code of the manual steps done in CAE.
Abaqus > Scripting Reference > Python commands > Mesh commands > MeshNodeArray object and Abaqus > Scripting Reference > Python commands > Region commands > Set object contains the relevant information.",0.0,0.33202493,0.11024055629968643
26,"Question\nI want to have a list which my main process will add data to and this seperate thread will see the data added, wait a set amount of eg 1 minute then remove it from the list. Im not very experience with multi-threading in python so i dont know how to do this.\nAnswer: The way you could achieve this is by using a global variable as your list, as your thread will be able to access data from it. You can use a deque from the collections library, and each time you add something in the queue, you spawn a new thread that will just pop from the front after waiting that set amount of time. 
Although, you have to be careful with the race conditions. It may happen that you try to write something at one end in your main thread and at the same time erase something from the beginning in one of your new threads, and this will cause unexpected behavior.
Best way to avoid this is by using a lock.",0.0,0.22273934,0.04961281269788742
27,"Question\nI have a function change_weight() that modifies weights in any given model. This function resides in a different python file.
So if I have a simple neural network that classifies MNIST images, I test the accuracy before and after calling this function and I see that it works. This was easy to do in TensorFlow v1, as I just had to pass the Session sess object in the function call, and I could get the weights of this session in the other file.
With eager execution in TensorFlow v2, how do I do this? I don't have a Session object anymore. What do I pass?\nAnswer: I was able to do this by passing the Model object instead and getting the weights by model.trainable_variables in the other function.",0.0,0.28462493,0.08101135492324829
28,"Question\ni have a problem with PyQt5 Designer. I install PyQt with -pip install PyQt5 and then -pip install PyQt5-tools
everything OK. But when i try to run Designer it open messagebox with error: This application failed to start because no Qt platform plugin could be initialized!
how to deal with it?\nAnswer: I found a way of solving this:
Go to you python instalation folder Python38\Lib\site-packages\PyQt5\Qt\bin
Then copy all of that files to your clipboard and paste them at Python38\Lib\site-packages\pyqt5_tools\Qt\bin
Then open the designer.exe and it should work.",0.5442177,0.16772741,0.1417449414730072
29,"Question\ni have a problem with PyQt5 Designer. I install PyQt with -pip install PyQt5 and then -pip install PyQt5-tools
everything OK. But when i try to run Designer it open messagebox with error: This application failed to start because no Qt platform plugin could be initialized!
how to deal with it?\nAnswer: Go to => 
Python38>lib>site-packages>PyQt5>Qt>plugins
In plugins copy platform folder
After that go to
Python38>lib>site-packages>PyQt5_tools>Qt>bin
paste folder here.  Do copy and replace.

This will surely work..
Now you can use designer tool go and do some fun with python...",1.0,0.043396413,0.9150904417037964
30,"Question\ni have a problem with PyQt5 Designer. I install PyQt with -pip install PyQt5 and then -pip install PyQt5-tools
everything OK. But when i try to run Designer it open messagebox with error: This application failed to start because no Qt platform plugin could be initialized!
how to deal with it?\nAnswer: Try running it using command: pyqt5designer
It should set all the paths for libraries.
Works on Python 3.8, pyqt5-tool 5.15",1.0,0.18684876,0.6612149477005005
31,"Question\nI'm working on Django and I know to create a account to log in into admin page I have to create a superuser.And for that we have to pass the command python manage.py createsuperuser.
But my question is when we pass this command what happens first and because of what in the background and after that what happens?? Which all methods and classes are called to create a superuser??
I know its a weird question but I wanted to know how this mechanism works..
Thanks in advance!!\nAnswer: Other people will answer this in detail but let me tell you in short what happens.
First when you pass the command python manage.py createsuperuser you will be prompted to fill the fields mentioned in USERNAME_FIELD and REQUIERD_FIELDS, when you will fill those fields then django will call your user model manager to access your create_superuser function and then code in that will execute to return a superuser.
I hope this will help you.",0.0,0.25436985,0.06470402330160141
32,"Question\nI'm using the stripe subscription API to provide multi tier accounts for my users. but about 50% of the transactions that i get in stripe are declined and flagged as fraudulent. how can i diagnose this issue knowing that i'm using the default base code provided in the stripe documentation (front end) and using the stripe python module (backend).
I know that i haven't provided much information, but that is only because there isn't much to provide. the code is known to anyone who has used stripe before, and there isn't any issue with it as there are transaction that work normally.
Thank you!\nAnswer: After contacting stripe support, i found that many payments were done by people from an IP address that belongs to a certain location with a card that is registered to a different location.
for example if someone uses a French debit card from England. i did ask stripe to look into this issue.",0.0,-0.0062350035,3.8875266909599304e-05
33,"Question\nI've made a python program using Tkinter(GUI) and I would like to enter it by creating a dedicated icon on my desktop (I want to send the file to my friend, without him having to install python or any interpreter).
The file is a some-what game that I want to share with friends and family, which are not familiar with coding.
I am using Ubuntu OS.\nAnswer: you can use pip3 install pyinstaller then use pyinstaller to convert your file to a.exe file than can run on windows using this command pyninstaller --onefile -w yourfile.
it can now run without installing anything on windows. and you can use wine to run it on ubuntu",-0.71428573,0.07929987,0.6297780871391296
34,"Question\nHow to set properties (attribute) in nifi processor using pure Python in ExecuteStreamCommand processor
I don't want to use Jython, I know it can be done using pinyapi. But don't know how to do it. I just want to create an attribute using Python script.\nAnswer: How to set properties(attribute) in nifi processor using pure python in ExecuteStreamCommand processor I don't want to use Jython

You can't do it from ExecuteStreamCommand. The Python script doesn't have the ability to interact with the ProcessSession, which is what it would need to set an attribute. You'd need to set up some operations after it to add the attributes like an UpdateAttribute instance.",0.0,0.38880146,0.15116657316684723
35,"Question\nI am currently trying to find end-to-end speech recognition solutions to implement in python (I am a data science student btw). I have searched for projects on github and find it very hard to comprehend how these repositories work and how I can use them for my own project.
I am mainly confused with the following:

how do repositories usually get used by other developers and how can I use them best for my specific issue?
How do I know if the proposed solution is working in python?
What is the usual process in installing the project from the repo?

Sorry for the newbie question but I am fairly new to this. 
Thank you\nAnswer: You can read the documentation(README.md) there you can have all the information you need.
You can install the project from a repo by cloning or by downloading zip.",0.0,0.08679426,0.007533242926001549
36,"Question\nHow can you prevent the agent from non-stop repeating the same action circle?
Of course, somehow with changes in the reward system. But are there general rules you could follow or try to include in your code to prevent such a problem?

To be more precise, my actual problem is this one:
I'm trying to teach an ANN to learn Doodle Jump using Q-Learning. After only a few generations the agent keeps jumping on one and the same platform/stone over and over again, non-stop. It doesn't help to increase the length of the random-exploration-time. 
My reward system is the following:

+1 when the agent is living
+2 when the agent jumps on a platform
-1000 when it dies

An idea would be to reward it negative or at least with 0 when the agent hits the same platform as it did before. But to do so, I'd have to pass a lot of new input-parameters to the ANN: x,y coordinates of the agent and x,y coordinates of the last visited platform.
Furthermore, the ANN then would also have to learn that a platform is 4 blocks thick, and so on. 
Therefore, I'm sure that this idea I just mentioned wouldn't solve the problem, contrarily I believe that the ANN would in general simply not learn well anymore, because there are too many unuseful and complex-to-understand inputs.\nAnswer: This is not a direct answer to the very generally asked question.

I found a workaround for my particular DoodleJump example, probably someone does something similar and needs help:

While training: Let every platform the agent jumped on disappear after that, and spawn a new one somewhere else.
While testing/presenting: You can disable the new ""disappear-feature"" (so that it's like it was before again) and the player will play well and won't hop on one and the same platform all the time.",0.0,0.23712075,0.056226249784231186
37,"Question\nI am trying to wrap my head around vectorization (for numerical computing), and I'm coming across seemingly contradictory explanations:

My understanding is that it is a feature built into low-level libraries that takes advantage of parallel processing capabilities of a given processor to perform operations against multiple data points simultaneously.
But several tutorials seem to be describing it as a coding practice that one incorporates into their code for more efficiency. How is it a coding practice, if it is also a feature you have or you don't have in the framework you are using. 

A more concrete explanation of my dilemma: 

Let's say I have a loop to calculate an operation on a list of numbers in Python. To vectorize it, I just import Numpy and then use an array function to do the calculation in one step instead of having to write a time consuming loop. The low level C routines used by Numpy will do all the heavy lifting on my behalf. 

Knowing about Numpy and how to import it and use it is not a coding practice, as far as I can tell. It's just good knowledge of tools and frameworks, that's all. 
So why do people keep referring to vectorization as a coding practice that good coders leverage in their code?\nAnswer: Vectorization leverage the SIMD (Single Instruction Multiple Data) instruction set of modern processors. For example, assume your data is 32 bits, back in the old days one addition would cost one instruction (say 4 clock cycles depending on the architecture). Intel's latest SIMD instructions now process 512 bits of data all at once with one instruction, enabling you to make 16 additions in parallel.
Unless you are writing assembly code, you better make sure that your code is efficiently compiled to leverage the SIMD instruction set. This is being taking care of with the standard packages. 
Your next speed up opportunities are in writing code to leverage multicore processors and to move your loops out of the interpreted python. Again, this is being taking care of with libraries and frameworks.
If you are a data scientist, you should only care about calling the right packages/frameworks, avoid reimplementing logic already offered by the libraries (with loops being a major example) and just focus on your application. If you are a framework/low-level code developer, you better learn the good coding practices or your package will never fly.",0.27210885,0.49603796,0.050144243985414505
38,"Question\nI am trying to wrap my head around vectorization (for numerical computing), and I'm coming across seemingly contradictory explanations:

My understanding is that it is a feature built into low-level libraries that takes advantage of parallel processing capabilities of a given processor to perform operations against multiple data points simultaneously.
But several tutorials seem to be describing it as a coding practice that one incorporates into their code for more efficiency. How is it a coding practice, if it is also a feature you have or you don't have in the framework you are using. 

A more concrete explanation of my dilemma: 

Let's say I have a loop to calculate an operation on a list of numbers in Python. To vectorize it, I just import Numpy and then use an array function to do the calculation in one step instead of having to write a time consuming loop. The low level C routines used by Numpy will do all the heavy lifting on my behalf. 

Knowing about Numpy and how to import it and use it is not a coding practice, as far as I can tell. It's just good knowledge of tools and frameworks, that's all. 
So why do people keep referring to vectorization as a coding practice that good coders leverage in their code?\nAnswer: Vectorization can mean different things in different contexts.  In numpy we usually mean using the compiled numpy methods to work on whole arrays.  In effect it means moving any loops out of interpreted Python and into compiled code.  It's very specific to numpy.
I came to numpy from MATLAB years ago, and APL before that (and physics/math as a student).  Thus I've been used to thinking in terms of whole arrays/vectors/matrices for a long time.
MATLAB now has a lot just-in-time compiling, so programmers can write iterative code without a performance penalty.  numba (and cython) lets numpy users do some of the same, though there are still a lot of rough edges - as can be seen in numpa tagged questions.
Parallelization and other means of taking advantage of modern multi-core computers is a different topic.  That usually requires using additional packages.
I took issue with a comment that loops are not Pythonic.  I should qualify that a bit.  Python does have tools for avoiding large, hard to read loops, things like list comprehensions, generators and",0.6802721,0.51703244,0.026647185906767845
39,"Question\nSo when you make changes to your CSS or JS static file and run the server, sometimes what happens is that the browser skips the static file you updated and loads the page using its cache memory, how to avoid this problem?\nAnswer: Well there are multiple ways to avoid this problem

the simplest way is by:

if you are using Mac: Command+Option+R
if you are using Windows: Ctrl+F5


What it does is that it re-downloads the cache files enabling the update of the static files in the browser.

Another way is by:

making a new static file and pasting the existing code of the previously used static file and then 
running the server


What happens, in this case, is that the browser doesn't use the cache memory for rendering the page as it assumes it is a different file.",0.0,0.44302136,0.19626791775226593
40,"Question\nSo when you make changes to your CSS or JS static file and run the server, sometimes what happens is that the browser skips the static file you updated and loads the page using its cache memory, how to avoid this problem?\nAnswer: U have DEBUG = False in your settings.py. Switch on DEBUG = True and have fun",0.0,-0.12037933,0.014491182751953602
41,"Question\nI would like to know the max memory usage of a celery task, but from the documentations none of the celery monitoring tools provide the memory usage feature. How can one know how much memory a task is taking up? I've tried to get the pid with billiard.current_process and use that with memory_profiler.memory_usage but it looks like the current_process is the worker, not the task.
Thanks in advance.\nAnswer: Celery does not give this information unfortunately. With little bit of work it should not be difficult to implement own inspect command that actually samples each worker-process. Then you have all necessary data for what you need. If you do this, please share the code around as other people may need it...",0.0,-0.0011633635,1.3534145182347856e-06
42,"Question\nI have a simple flask app that talks to Google Cloud Storage.
When I run it normally with python -m api.py it inherits Google Cloud access from my cli tools.
However, when I run it with the PyCharm debugger it can no longer access any Google Services.
I've been trying to find a way to have the PyCharm debugger inherit the permissions of my usual shell but I'm not seeing any way to do that.
Any tips on how I can use the PyCharm debugger with apps that require access to Google Cloud?\nAnswer: I usually download the credentials file and set GOOGLE_APPLICATION_CREDENTIALS=""/home/user/Downloads/[FILE_NAME].json environment variable in PyCharm. 
I usually create a directory called auth and place the credential file there and add that directory to.gitignore
I don't know if it is a best practice or not but it gives me an opportunity to limit what my program can do. So if I write something that may have disrupting effect, I don't have to worry about it. Works great for me. I later use the same service account and attach it to the Cloud Function and it works out just fine for me.",0.40816328,0.2123717,0.03833433985710144
43,"Question\nWorking on a python project and using pycharm. Have installed all the packages using requirements.txt. Is it a good practice to  run it in the beginning of every sprint or how often should i run the requirements.txt file?\nAnswer: The answer is NO.
Let's say you're working on your project, already installed all the packages in the requirements.txt into your virtual environment, etc etc, at this point your environment is already setup.
Keep working on the project and installed a new package with pip or whatever, now your environment is ok but you're requirements.txt is not up to date, you need to update it adding the new package, but you don't need to reinstall all the packages in it every time this happens.
You only runs pip install -r requirements.txt when you want to run your project on a different virtual environment",0.0,0.05895877,0.003476136364042759
44,"Question\nI know I can use relativedelta to calculate difference between two dates in the calendar. However, it doesn't feet my needs.
I must consider 1 year = 365 days and/or 12 months; and 1 month = 30 days.
To transform 3 years, 2 months and 20 days into days, all I need is this formula: (365x3)+(30x2)+20, which is equal to 1175.
However, how can I transform days into years, months and days, considering that the amount of days may or may not be higher than 1 month or 1 year? Is there a method on Python that I can use?
Mathemacally, I could divide 1175 by 365, multiply the decimals of the result by 365, divide the result by 30 and multiply the decimals of the result by 30. But how could I do that in Python?\nAnswer: You can use days%365 to get number of years from days.",-0.71428573,0.14995831,0.7469177842140198
45,"Question\nI was trying to pip install netfilterqueue module with my Windows 7 system, in python 3.8
It returned an error ""Microsoft Visual C++ 14.0 is required""
My system already has got a Microsoft Visual C++ 14.25. Do I still need to install the 14.0, or is there a way that I can get out of this error?
If no, how do I install a lower version without uninstalling or replacing the higher version?\nAnswer: Alright, try uninstalling the higher version and go for the lower version making sure you download it with the same computer not with another, remembering that windows 7 no longer support some operations, and i will advice you upgrade to windows 10",0.40816328,-0.34040076,0.5603480935096741
46,"Question\ni installed pyenv and switched to python 3.6.9 (using pyenv global 3.6.9). How do i go back to my system python? Running pyenv global system didnt work\nAnswer: pyenv sets the python used according to ~/.pyenv/version. For a temporary fix, you can write system in it. Afterwards, you'll need to fiddle through your ~/.*rc files and make sure eval ""$(pyenv init -)"" is called after any changes to PATH made by other programs (such as zsh).",0.13605443,0.41368568,0.07707910984754562
47,"Question\nI've been working on a game for a month and it's quite awesome. I'm not very new to game developing.
There are no sprites and no images, only primitive drawn circles and rectangles.
Everything works well except that the FPS gets slow the more I work on it, and every now and then the computer starts accelerating and heating up.
My steps every frame (besides input handling):

updating every object state (physics, collision, etc), around 50 objects some more complex than the other
drawing the world, every pixel on (1024,512) map.
drawing every object, only pygame.draw.circle or similar functions

There is some text drawing but font.render is used once and all the text surfaces are cached.
Is there any information on how to increase the speed of the game?
Is it mainly complexity or is there something wrong with the way I'm doing it? There are far more complex games (not in pygame) that I play with ease and high FPS on my computer. 
Should I move to different module like pyglet or openGL?
EDIT: thank you all for the quick response. and sorry for the low information. I have tried so many things but in my clumsiness I heavent tried to solve the ""draw every pixel every single frame proccess"" I changed that to be drawn for changes only and now it runs so fast I have to change parameters in order to make it reasonably slow again. thank you :)\nAnswer: Without looking at the code its hard to say something helpful.
Its possible that you got unnecessary loops/checks when updating objects.
Have you tried increasing/decreasing the amount of objects? 
How does the performance change when you do that?
Have you tried playing other games made with pygame? 
Is your pc just bad?
I dont think that pygame should have a problem with 50 simple shapes. I got some badly optimized games with 300+ objects and 60+ fps (with physics(collision, gravity, etc.)) so i think pygame can easily handle 50 simple shapes. You should probably post a code example of how you iterate your objects and what your objects look like.",0.81632656,0.19140172,0.390531063079834
48,"Question\nSo I just created a simple script with selenium that automates the login for my University's portal. The first reaction I got from a friend was: ah nice, you can put that on my pc as well. That would be rather hard as he'd have to install python and run it through an IDE or through his terminal or something like that and the user friendliness wouldn't be optimal.
Is there a way that I could like wrap it in a nicer user interface, maybe create an app or something so that I could just share that program? All they'd have to do is then fill in their login details once and the program then logs them in every time they want. I have no clue what the possibilities for that are, therefore I'm asking this question. 
And more in general, how do I get to use my python code outside of my IDE? Thusfar, I've created some small projects and ran them in PyCharm and that's it. Once again, I have no clue what the possibilities are so I also don't really know what I'm asking. If anyone gets what I mean by using my code further than only in my IDE, I'd love to hear your suggestions!\nAnswer: The IDE running you program is the same as you running your program in the console. But if you dont want them to have python installed (and they have windows) you can maybe convert them to exe with py2exe. But if they have linux, they probably have python installed and can run you program with ""python script.py"". But tell your friends to install python, if they program or not, it will always come in handy",0.40816328,-0.07656181,0.23495841026306152
49,"Question\nI'm building a personal website that I need to apply modularity to it for purpose of learning. What it means is that there is a model that contains x number of classes with variations, as an example a button is a module that you can modify as much depending on provided attributes. I also have a pages model that need to select any of created modules and render it. I can't find any documentation of how to access multiple classes from one field to reference to. 
Model structure is as below:

Modules, contains module A and module B
Pages should be able to select any of module A and order its structure.

Please let me know if not clear, this is the simplest form I could describe. Am I confusing this with meta classes? How one to achieve what I'm trying to achieve?\nAnswer: I ended up using Proxy models but will also try polymorphic approach. This is exactly what is designed to do, inherit models from a parent model in both one to many and many to many relationships.",0.0,0.11188996,0.01251936238259077
50,"Question\nI am new to git and Pythonanywhere. So, I have a live Django website which is hosted with the help of Pythonanywhere. I have made some improvements to it. I have committed and pushed that changes to my Github repository. But, now I don't know that how to further push that changes to my Pythonanywhere website. I am so confused. Please help!!! Forgive me, I am new to it.\nAnswer: You need to go to the repo on PythonAnywhere in a bash console, run git pull (You may need to run./mange.py migrate if you made changes to your models) and then reload the app on ""Web"" configuration page on PythonAnywhere..",0.40816328,0.18657237,0.049102529883384705
51,"Question\nI  know that variables in Python are really just references/pointers to some underlying object(s). And since they're pointers, I guess they somehow ""store"" or are otherwise associated with the address of the objects they refer to.
Such an ""address storage"" probably happens at a low level in the CPython implementation. But 
my knowledge of C isn't good enough to infer this from the source code, nor do I know where in the source to begin looking.
So, my question is:
In the implementation of CPython, how are object addresses stored in, or otherwise associated with, the variables which point to them?\nAnswer: In module scope or class scope, variables are implemented as entries in a Python dict. The pointer to the object is stored in the dict. In older CPython versions, the pointer was stored directly in the dict's underlying hash table, but since CPython 3.6, the hash table now stores an index into a dense array of dict entries, and the pointer is in that array. (There are also split-key dicts that work a bit differently. They're used for optimizing object attributes, which you might or might not consider to be variables.)
In function scope, Python creates a stack frame object to store data for a given execution of a function, and the stack frame object includes an array of pointers to variable values. Variables are implemented as entries in this array, and the pointer to the value is stored in the array, at a fixed index for each variable. (The bytecode compiler is responsible for determining these indices.)",1.0,0.42181283,0.3343003988265991
52,"Question\nI am using Blueprints to create two separate modules, one for api and one for website. My APIs have a route prefix of api. Now, I am having a route in my website called easy and it will be fetching a JSON from a route in api called easy and it's route is /api/easy.
So, how can i call /api/easy from /easy.
I have tried using requests to call http:localhost:5000/api/easy and it works fine in development server but when I am deploying it on Nginx server, it fails probably because I am exposing port 80 there.
When I deploy my webapp on nginx, it show up perfectly just that route /easy throws Internal Server Error.\nAnswer: Okay so what worked for me is I simply ended up calling the api function from the frontend rather than doing the POST requests. Obviously, it makes no sense creating backend routes for flask seperately when you are using Flask too in frontend. Simply, a seperate utility function would be fine.",0.0,0.1811549,0.03281709924340248
53,"Question\nI have DJI M600 Drone and I'm using ROS DJI SDK on Raspberry PI to communicate with it. 
I can successfully send waypoint commands and execute them. However, I don't know how to acknowledge that the waypoints are finished. What comes to my mind is that I can check where the drone is in order to compare it with the coordinates I sent. The second solution might be to check how many waypoints are left (haven't tried it yet). 
I wonder if there is a topic that I can subscribe to so that I can ask if the waypoints are completed. What is the proper way to do that?
Thanks in advance,
Cheers\nAnswer: I use a different SDK so can't help with code example but I think you need to look into:
the wayPointEventCallback and wayPointCallback.",0.0,0.06764424,0.004575742874294519
54,"Question\nI recently installed pywin32 at a client site and after this occurred, they started experiencing MAPI errors. I cannot see how the install would have had any effect on their emails. pywin32 was simply installed with no errors. I am a novice with Python so I apologise if there is not enough detail or for the lack of understanding on my part.
Pywin32 was installed on remote desktop and the error they were receiving around this time was ""241938E error - can't open default message store (MAPI)"". The actual python script using win32.com makes no use of MAPI (simply used for word application tasks) and worked without any issues.
The IT firm for the client wants to know if pywin32 causes any changes to registry settings that could have impacted them and caused this error? Incidentally, they had an office365 change around the same time. I think the 'finger pointing' is more in that direction but I do need to rule out any related registry setting changes that pywin32 may make on install that could have caused or contributed to the problem they were experiencing.\nAnswer: SOLVED: Problem found to be a Microsoft error - reported April 27",0.40816328,-0.6274706,1.0725376605987549
55,"Question\nI spend each month a lot of time extracting numbers from an application into an Excel-spreadsheet where our company saves numbers, prices, etc. This application is not open-source or so, so unfortunately, sharing the link might not help.
Now, I was wondering whether I could write a Python program that would do this for me instead? But I'm not sure how to do this, particularly the part with extracting the numbers. Once this is done, transfering this to an Excel spreadsheet is particularly trivial.\nAnswer: 1)For this you can create a general function like getApplicationError(),
2)in this method you can get the text of the Application Error(create xpath of the application error, and check that if that element is visible than get text) and throw an exception to terminate the Script and you can send that got message into Exception constructor and print that message with Exception.
As you are creating this method for general use so you need to call this method every posible place from where the chances are to get Application Error. like just after click(Create,Save,Submit,Delet,Edit, also just entering value in mendatory Fields)",0.0,0.1635375,0.02674451470375061
56,"Question\nI'm trying to do the following, but I'm getting errors on my ExecuteStreamCommand:
Cannot run program ""C:\Python36\pythonscript.py"" error=193 not a valid Win32 application""
This is being run on my home Windows work station.

GetFile (Get my PDF)
ExecuteStreamCommand (Call Python script to parse PDF with Tika, and create JSON file)
PutFile (Output file contains JSON that I will use later)

Does NiFi have a built in PDF parser? Is there something more NiFi compatible that Tika?
If not, how do I call one from ExecuteStreamCommand?
Regards and thanks in advance!\nAnswer: Cannot run program ""C:\Python36\pythonscript.py"" error=193 not a valid Win32 application""

You need to add a reference to your Python executable to the command to run with ExecuteStreamCommand as you cannot run Python scripts on Windows with the shebang (#!/usr/bin/python for example on Linux).",0.0,0.18905187,0.03574060648679733
57,"Question\nHi everyone i want to create a new telegram bot similar to @usinifobot.
this is how it works:
you just send someone username to this bot and it will give you back his/her chat id.
i want to do same thing but i dont khow how to do it with python-telegram-bot.\nAnswer: python-telegram-bot can't help with this, because telegram bot api can't provide ids of unrelated entities to the bot - only ones with which bot iteracts
you need telegram client api for this and to hold a real account (not a bot) online to make checks by another library
and with bot library like python-telegram-bot you could only get ids of users who write to bot, channels and groups bot is in and reposted messages from other users
I created similar bot @username_to_id_bot for getting ids of any entity and faced all these moments",0.0,0.07952595,0.006324376445263624
58,"Question\nI have an app that includes some images, however when I package for my android phone the images are blank. Right now in my kv file, the images are being loaded from my D drive, so how would I get them to load on my phone?\nAnswer: Include the images during the packaging and then load them using a file path relative to your main.py.",0.0,0.23286223,0.05422481894493103
59,"Question\nI'm currently using elastic beanstalk and apscheduler to run Pandas reports everyday automatically on it. The data set is getting larger and i've already increased the memory size 3x. 
Elastic Beanstalk is running Dash - dashboard application and runs the automated Pandas reports once every night. 
I've tried setting up AWS Lambda to run Pandas reports on there but I couldn't figure out how to use it. 
I'm looking for the most cost-effective way to run my reports without having to increase memory usage on Beanstalk. When I run it locally it takes 1gb but running it on beanstalk, it's using more than 16gb. 
Curious if someone else has a better option or process how they automatically run their Pandas reports.\nAnswer: Create an.exe using Pyinstaller
Schedule.exe on Task Scheduler on computer
Cheaper than scaling AWS Beanstalk resources which use more resources calculating pandas than your computer locally at least for my case.",0.0,0.33233738,0.11044813692569733
60,"Question\nSuppose I run the GC-test:
grangercausalitytests(np.vstack((df['target'], df['feature'])).T, maxlag=5)
I can pick the lag of the ""feature"" variable, which most likely Granger-causes the ""target"" variable. 

But what number of lags does the ""target"" variable have in this model? 
Further, how do I estimate this ADL model (some autoregressive lags + some lags of the independent variable)? I've seen somewhere, that ADL should be substituted with OLS/FGLS in Python, since there is no package for ADL. Yet I do not understand how to do that\nAnswer: I found out that the model, corresponding to each particular number
of lags in the GC-test has already been fit and is contained in the
test return. The output looks messy, but it's there.  
Unfortunately,
there seems to be no capacity to estimate ADL models in Python yet
:(",0.0,-0.19180763,0.036790166050195694
61,"Question\nI’m creating a bot app in python using selenium, for people, but I would need to change the xpath code every week, how do I do this once I distribute the app to people?
Thanks In advance\nAnswer: I think the best approach is to locate selectors using Id rather than XPath since there won't be any change to Id selector once a new feature(adding a table/div to the HTML)is added. Also, this reduces the rework effort to a large extend.",0.0,0.186966,0.03495628759264946
62,"Question\nIn examples, I saw of socket programming projects (most of which were chat projects), they often saved all the clients in one array, and when a message was received from a client, in addition to saving it in the database, to all clients also was sent.
The question that comes to my mind is: How can this message received from the client  and saved in the database and send to clients when number of clients is very large? (I mean, the number of customers is so large that a single server can't meet their demand alone, and several servers are needed to connect sockets).
In this case, not all clients can be managed through the array. So how do you transfer a message that is now stored on another server (by another customer) to a customer on this server? (Speed ​​is important).
Is there a way to quickly become aware of database changes and provide them to the customer? (For example, Telegram.)
I'm looking for a perspective, not a code.\nAnswer: You should use your database as your messaging center.  Have other servers watch for changes in the database either by subscription or by pulling on a time interval.  Obviously subscription would be fastest possible.  
When a message is inserted, each server picks this up and sends to their list of clients.  This should be quite fast for broadcasting messages.",0.0,0.11769217,0.013851447030901909
63,"Question\nI am confused about how to realize the data validation in event sourced micro-service architecture.  
Let sum up some aspects that related to the micro-services.
1. Micro-services must be low coupled.
2. Micro-services better to be domain oriented  
Then due to tons of materials in the internet and the books in DDD (Domain Driven Design)
I create the next event sourced micro-service architecture.
Components
1. API getaway to receive the REST calls from the clients and transform them into the commands.
2 Command handler as a service. Receive the commands from API getaway make the validations. Save the events to the event store and publish events to the event bus.
3. Event store is the storage for all events in the system. Allows us to recreate the state of the app. The main state of truth.
4. Micro-services is small services responsible to handle the related to its domain event. Make some projections to the local private databases. Make some events too.
And I have questions that I could not answer both by myself and the internet.
1. What is actually aggregates. They are the class objects/records in databases as I think or what?
2. Who carry about aggregates. I found example that is some cases command handler use them. But in that way if aggregates stored in the private micro-services databases then we will have very high coupling between the command handler and the each of micro-services and it is wrong due to micro-service concept.  
To sum up.
I am confused about how to implement aggregation in event source micro service architecture.
For example let focus on the user registration implementation in event source micro-service architecture.  
We have the user domain so the architecture will be next.
API getaway
Command handler
Auth micro-service
User micro-service 
Please explain me the realization of command validation due to example above.\nAnswer: Command handler as a service

I think this is the main source of your confusion.
The command handler isn't normally a service in itself.  It is a pattern.  It will normally be running in the same process as the ""microservice"" itself.
IE: the command handler reads a message from so storage, and itself invokes the microservice logic that computes how to integrate the information in this message into its own view of the world.",0.40816328,0.20309299,0.04205382242798805
64,"Question\nI'm using Python 3. I am trying to remove certain lists from a list of lists. I found an excellent article that explained how to do that using list comprehension. It appears to work as expected, but it got me thinking... In my original efforts I was appending any list object that was to be deleted to a new list. I could then actually look at these objects and assure myself the right ones were being removed. With the comprehension method I can only ""see"" the ones that remain. Is there a way of ""seeing"" what's ""failed"" the list comprehension condition? It would be reassuring to know that only the correct objects gave been removed.\nAnswer: I actually managed to answer my own question by making a mistake. To see what will be removed from a list by the list comprehension, simply temporarily invert the condition logic. This will allow you to look at all the elements that will be removed. If you're happy that the removals are as you expect, then simply re-invert the logic again, back to original and execute.",0.0,0.14596099,0.02130460925400257
65,"Question\nI am new to this, so apologize if the step is easy.
I have a Device which I am programming, which uses a raspberry pi (Debian). I have connected via SSH using PuTTY.
I wish to create a virtual environment, and test a program on the device to search the WiFi network SSIDs and pick them up. I found that a great package to use is wpa_supplicant.
However, here is the problem:
The device currently has Python 2.7.9 on it. When ever I create a virtual environment using python3, it creates a venv with python 3.4. Unfortunately, wpa_supplicantm requires python 3.5 or higher to work.
When I run sudo apt-get install python3-venv, I can see in the prompt that it automatically starts installing packages for python3.4. 
Does anyone know how I can specify that I wish to install python 3.5 or 3.7?
Any help would be greatly appreciated.
Regards
Scott\nAnswer: Does it not have the python3.7 command?
I just checked a venv I have running on a 3b+ and it seems to have it.",0.0,0.18256235,0.03332901373505592
66,"Question\nI new to python but how do I import the sqlite3.dll file from a custom file location as I can't find anything about it. I can accept any option including building a new pyd,dll,etc file.
Edit:
I need it to be in a separate location.\nAnswer: Note: The following answers the above question with more thorough steps.
I had the same issue as administrative rights to the default python library is blocked in a corporate environment and its extremely troublesome to perform installations.
What works for me:

Duplicate the sqlite3 library in a new location
Put in the latest sqlite3.dll (version you want from sqlite3 web) and the old _sqlite3.pyd into the new location or the new sqlite3 library. The old _sqlite3.pyd can be found in the default python library lib/DLLs folder.
Go to the new sqlite3 library and amend the dbapi2.py as follows: Change ""from _sqlite3 import *"" to ""from sqlite3._sqlite3 import *""
Make sure python loads this new sqlite3 library first. Add the path to this library if you must.",0.0,0.2772805,0.07688447833061218
67,"Question\nI´m new to Python and I´m having a problem. I have 2 lists containing the names of the columns of a dataset: one has all the columns names (columnas = total.columns.values.tolist()); and the other one has a subset of them ( in the form of ""c = [a,b,c,d,c,e,...]"".
I would like to know how could I check if each element in ""c"" is contained in the longer list ""columnas"". The result i have been trying to get is as it follows ( this is just an example):
a: True 
b: True
c: False
...
Looking forward to your answers, Santiago\nAnswer: a=[ ]
for i in c:
  if i in columns:
     a.append(true)
  else:
     a.append(false)
a=[ ]
for i in c:
  if i in columns:
     a.append(true)
  else:
     a.append(false)",0.20408164,0.1701026,0.0011545753804966807
68,"Question\nIf I do li = [1,2,3], and then do a = li, a is assigned to li, right? However, when I do del li and then print a, it still shows [1,2,3]. When I do li.append(4) and print a then, why does it show [1,2,3,4]? 
I understand that a didn't make a copy of li (as the.copy() method is used for that), but why would a still show the value li used to have?\nAnswer: del does not delete the variable. del only deletes the name, and the garbage collector will (on its own time) search for variables that aren't referenced by anything, and properly deallocate their memory.
In this case, you're assigning the name a to reference the same variable that the name li is referencing. When you use.append(), it modifies the variable, and all names referencing the variable will be able to see the change. And when you do del li to remove the name li, it doesn't remove the name a, which is still referencing the variable. Thus, the variable doesn't get deallocated and removed.",0.40816328,0.46750605,0.0035215646494179964
69,"Question\nI have an 8 GB file with text lines (each line has a carriage return) in S3. This file is custom formatted and does NOT follow any common format like CSV, pipe, JSON... 
I need to split that file into smaller files based on the number of lines, such that each file will contains 100,000 lines or less 
(assuming the last file can have the remainder of the lines and thus may have less than 100,000 lines).

I need a method that is not based on the file size (i.e. bytes), but the number of lines. Files can't have a single line split across the two.
I need to use Python.
I need to use server-less AWS service like Lambda, Glue... I can't spin up instances like EC2 or EMR.

So far I found a lot of posts showing how to split by byte size but not by number of lines. 
Also, I do not want to read that file line by line as it will be just too slow an not efficient. 
Could someone show me a starter code or method that could accomplish splitting this 6 GB file that would 
run fast and not require more than 10 GB of available memory (RAM), at any point?
I am looking for all possible options, as long as the basic requirements above are met...
BIG thank you!
Michael\nAnswer: boto3.S3.Client.get_object() method provides object of type StreamingBody as a response. 
StreamingBody.iter_lines() method documentation states:

Return an iterator to yield lines from the raw stream.
This is achieved by reading chunk of bytes (of size chunk_size) at a
  time from the raw stream, and then yielding lines from there.

This might suit your use case. General idea is to get that huge file streaming and process its contents as they come. I cannot think of a way to do this without reading the file in some way.",0.40816328,0.18922246,0.047935083508491516
70,"Question\nI was programming python in Visual Studio Code and every time that I ran something it would use the integrated terminal (logically, because I have not changed any settings) and I was wondering, how could I get it to use the Python IDLE's shell instead of the integrated terminal (which for me is useless)?
I have also got Python IDLE installed in my mac but due to Visual Studio Code having ""intellisense"", it is way easier.\nAnswer: In VS Code you should be able to select the file which is supposed to be used in the terminal.
Under :
Preferences -> Settings -> Terminal",0.0,0.19976115,0.03990451619029045
71,"Question\nI have created a virtual environment named knowhere and I activate it in cmd using code.\knowhere\Scripts\activate. I have installed some libraries into this environment.
I have some python scripts stored on my pc. When I try to run them they are not working since they are not running in this virtual environment. Now how to make these scripts run.
Also is there any way to make ""knowhere"" as my default environment.\nAnswer: Virtual environments are only necessary when you want to work on two projects that use different versions of the same external dependency, e.g. Django 1.9 and Django 1.10 and so on. In such situations virtual environment can be really useful to maintain dependencies of both projects.
If you simply want your scripts to use Python libraries just install them on your system and you won't have that problem.",-1.0,0.2004655,1.441117286682129
72,"Question\nI have a few files. The big one is ~87 million rows. I have others that are ~500K rows. Part of what I am doing is joining them, and when I try to do it with Pandas, I get memory issues. So I have been using Dask. It is super fast to do all the joins/applies, but then it takes 5 hours to write out to a csv, even if I know the resulting dataframe is only 26 rows. 
I've read that some joins/applies are not the best for Dask, but does that mean it is slower using Dask? Because mine have been very quick. It takes seconds to do all of my computations/manipulations on the millions of rows. But it takes forever to write out. Any ideas how to speed this up/why this is happening?\nAnswer: You can use Dask Parallel Processing or try writing into Parquet file instead of CSV as Parquet operation is very fast with Dask",0.0,-0.24331039,0.059199947863817215
73,"Question\nI'd like to mix C code with Python GUI libraries. I thought about creating C library and using it with ctypes. How to create library for both Linux and Windows at the same time? On Linux, I simply use gcc -fPIC -shared -o lib.so main.c, but how to do that for Windows?\nAnswer: Many IDE for C/C++ already prepared DLL program template,such as Visual Studio,Code::Blocks,VC++6.0 etc.. Using DLL files is similar to using SO files",-0.71428573,0.058844924,0.5977309942245483
74,"Question\nI am working on project which deals with calculation of halsted matrix and mccabe cyclomatic complexity for the codes in various languages. I found this library multimeric but its documentation is not intuitive. Please explain to me how to use this library for finding code metrics.
if you know any other library which does this work then please suggest.\nAnswer: install multimetric follow the instruction from PyPI. go to the code(example.py) location. e.g. cd /user/Desktop/code
Then type these in terminal:
multimetric example.py
then you can see the result.",0.40816328,0.04684329,0.13055212795734406
75,"Question\nAs I am trying to connect to Google's BigQuery environment from a Python notebook using the google.cloud library, the response from the server is to visit a link that generates a code and to ""Enter the authorization code:"". However, as this response is just text, I do not know how to pass the code back to the server response. I am running this notebook in a Databricks environment.
Does anyone know how I can push this code back to the server and complete the authorization?\nAnswer: It doesnn't look like you are using the correct. The flow which you mentioned will work on a UI not not with any automation. I suggest you to share more dtails here and also check for more documenataion on the same.",0.0,0.13229412,0.01750173419713974
76,"Question\nI have string with html tags and I would like to pass formatting to power point.
The only idea I have now is to split it using some xml library and add bunch of ifs adding formatting to run depending on a tag.
Did you encounter similar problem or have better idea how to approach it?\nAnswer: I don't think there is a method of doing this. For a start, some HTML elements and attributes aren't likely to translate.
I've done a very limited amount of this - and actually it was mostly Markdown I was translating. (The HTML relevance is that I did work with entity references and also <br/>.)
I'm sorry to say my code is useless to you.
My advice would be to support a small subset of HTML. Perhaps <span> with some limited styling and things like <br/>.",0.40816328,0.28452092,0.015287431888282299
77,"Question\nCan you help me help my son with python homework?
His homework this week is on iteration. We've worked through most of it, but we can't make much headway with the following:
""•  Write a program that will ask a user to enter a number between 1 and 100.  The program should keep dividing the number by 2 until it reaches a number less than 2.  The program should tell the user how many times it had to divide by 2. ""
Can you help us with this, and preferably include some # lines in the code so we can better understand what's happening?\nAnswer: Great that you're helping your son with his homework! Very exciting!
If I summarize the question, it is:

take a number n
divide it by 2
repeat step 2 until your number is less than 2
output how often it had to be divided

Let's do this by hand:

I take a number, 15:
I divide once, I get 7.5
it's not less than 2, so I continue
I divide by 2 again (2 times total), I get 3.75
it's not less than 2, so I continue
I divide by 2 again (3 times total), I get 1.875
it's less than 2, so I stop

I had to divide by three times total.
If you were to take these steps and write it in code, how would you do this? (Hint: use a while loop!)",1.0,0.23385316,0.5869809985160828
78,"Question\nSome people were using my bot on a server I am a part of, and for some reason, the bot suddenly started duplicating responses to commands. Basically, instead of doing an action once, it would do it twice. I tried restarting it multiple times which didn't work, and I know it isn't a problem with my code because it was working perfectly well a few seconds ago. It probably wasn't lag either, because only a couple of people were using it. Any ideas on why this may be and how to fix it? I am also hosting it on repl.it, just so you know what ide Im using\nAnswer: Its probably because you run the script and run the host at them same time so it sends the command thorught host and code. If you dont run the code but just the host and it still dupliactes it might be an error with the host or it runs somewhere else in the backround.",0.0,0.18396276,0.0338422991335392
79,"Question\nI have a Python Django web application.
In Get method how to prevent the user from entering url and access the Data.
How do i know weather the url accessed by Code or Browser.
I tried with sessionid in Cookie, But if session exist's it allow to access the data.
Thanks.\nAnswer: I achieve it by 
if 'HTTP_REFERER' not in request.META:
it not exist's when hit directly from browser url.",0.0,0.22028321,0.04852469265460968
80,"Question\nI have a Python Django web application.
In Get method how to prevent the user from entering url and access the Data.
How do i know weather the url accessed by Code or Browser.
I tried with sessionid in Cookie, But if session exist's it allow to access the data.
Thanks.\nAnswer: To detect if request is from browser, you can check HTTP_USER_AGENT header
request.META.get(""HTTP_USER_AGENT"")",0.0,0.12806702,0.01640116050839424
81,"Question\nI have an app that uses the Spacy model ""en_core_web_sm"". I have tested the app on my local machine and it works fine.
However when I deploy it to Heroku, it gives me this error:
""Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.""
My requirements file contains spacy==2.2.4.
I have been doing some research on this error and found that the model needs to be downloaded separately using this command:
python -m spacy download en_core_web_sm
I have been looking for ways to add the same to my requirements.txt file but haven't been able to find one that works!
I tried this as well - added the below to the requirements file:
-e git://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz#egg=en_core_web_sm==2.2.0
but it gave this error:
""Cloning git://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz to /app/.heroku/src/en-core-web-sm
Running command git clone -q git://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz /app/.heroku/src/en-core-web-sm
fatal: remote error:
           explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz is not a valid repository name""
Is there a way to get this Spacy model to load from the requirements file? Or any other fix that is possible?
Thank you.\nAnswer: Ok, so after some more Goog",0.5442177,0.65829194,0.013012929819524288
82,"Question\nI've already seen multiple posts on Stackoverflow regarding this. However, some of the answers are outdated (such as using PhantomJS) and others didn't work for me.
I'm using selenium to scrape a few sports websites for their data.  However, every time I try to scrape these sites, a few of them block me because they know I'm using chromedriver.  I'm not sending very many requests at all, and I'm also using a VPN.  I know the issue is with chromedriver because anytime I stop running my code but try opening these sites on chromedriver, I'm still blocked.  However, when I open them in my default web browser, I can access them perfectly fine.  
So, I wanted to know if anyone has any suggestions of how to avoid getting blocked from these sites when scraping them in selenium.  I've already tried changing the '$cdc...' variable within the chromedriver, but that didn't work.  I would greatly appreciate any ideas, thanks!\nAnswer: Obviously they can tell you're not using a common browser. Could it have something to do with the User Agent?
Try it out with something like Postman. See what the responses are. Try messing with the user agent and other request fields. Look at the request headers when you access the site with a regular browser (like chrome) and try to spoof those.
Edit: just remembered this and realized the page might be performing some checks in JS and whatnot. It's worth looking into what happens when you block JS on the site with a regular browser.",0.40816328,0.28170168,0.015992535278201103
83,"Question\nIs there anything that someone could point me towards (a package, an example, a strategy, etc) of how I could implement the ability for an end user of my app to create a new field in a model, then add that model field to a model form and template? I’m thinking of the way that Salesforce allows users to add Custom fields.
I don’t really have any start point here I am only looking to learn if/how this might be possible in Django.
Thanks!\nAnswer: I'm also looking for same type of solution. But with some research, I came to know that we have this using ContentTypes framework.
How to do it? We can utilize ContentType's GenericForeignKeys and GenericRelations.",0.0,0.21576393,0.0465540736913681
84,"Question\nSo I am trying to create a binary file and save it into my database. I am using REDIS and SQLALCHEMY as a framework for my database. I can use send_file to send the actual file whenever the user accesses a URL but how do I make sure that the file is saved in the route and it could stay there every time a user accesses the URL.

I am sending the file from a client-python it's not in my
  directory

what I need in a nutshell is to save the file from the client-python to a database to ""downloadable"" it to the browser-client so it would actually be available for the browser-client is there any way of doing this? Maybe a different way that I didn't think about\nAnswer: I had to encode the data with base64, send it to the database and then decode it and send the file as binary data.",0.0,0.084563255,0.007150944322347641
85,"Question\nThe pretty straight forward way to do this is to upload the google-provided.html file to the root folder of your app on the server. But how to do it for a flask application?
For example, I have a flask app running on heroku and I want to do the site verification for my app using html file upload method(though alternative methods are available). I tried uploading the google-provided.html in the templates folder. Verification failed!
I have searched the internet but found no relevant answers.\nAnswer: Everything mentioned in the above answer is correct. But, just make sure that you don't rename the file given from google search console. Use the same name as it is.",-0.35714287,0.40141284,0.5754067897796631
86,"Question\nI am a new programmer that started learning Python, but there's something bothering me which I'd like to change.
As I've seen that it is possible to remove the unwanted path from the terminal when executing code, I cannot figure out how to access the Visual Studio Code launch.json file and all of the explanations on Google are quite confusing.\nAnswer: Note that if Visual Studio Code hasn't created a launch.json file for your project yet, do the following:

Click the Run | Add Configuration menu option, and one will automatically be generated for you, and opened in the editor.",0.40816328,0.2823438,0.015830541029572487
87,"Question\nI need to install Beautiful Soup 4, but every tutorial or list of instructions seems to assume I know more than I do. I am here after a number of unsuccessful attempts and at this point I am afraid of damaging something internally.
Apparently I need something called pip. I have Python 3.8, so everyone says I should have pip. Great. I have found no less than 14 different ways to check if I actually have pip and am using it. They all say to type something. One of them said to type pip --version. We are already assuming too much. Where do I type it? IDLE? The Cmd prompt? The Python shell? What folder do I need to be in? Etc Etc. I need someone to assume I am a complete beginner.
Then, how do I use it to install bs4? Again, I am supposed to type things, but no one says where. One person said to go to the folder where python is installed in the command line. So, I did, and surprise surprise, pip is not ""valid syntax"". How can I proceed with this?\nAnswer: With a little help from the kind user Tenacious B, I have solved my problem, I think. In the command prompt, I needed to type 
cd C:\Users\%userprofilenamegoeshere%\AppData\Local\Programs\Python\Python38-32\scripts 
No source that I found in my initial search included that last bit: \scripts. From here, the common suggestion of pip install beautifulsoup4 seems to have worked.",0.0,0.09569782,0.009158072993159294
88,"Question\nIs it possible to run multiple Python versions on SQL Sever 2017?
It is possible to do on Windows (2 Python folders, 2 shortcuts, 2 environment paths). But how to launch another Python version if I run Python via sp_execute_external_script in SQL Management Studio 18?
In SQL server\Launchpad\properties\Binary path there is the parameter -launcher Pythonlauncher. Probably, by changing this, it is possible to run another Python version.
Other guess: to create multiple Python folders C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\PYTHON_SERVICES. But how to switch them?
Other guess: in C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\MSSQL\Binn\pythonlauncher.config - in PYTHONHOME and ENV_ExaMpiCommDllPath parameters substitute the folder C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\ with the folder with new Python version.\nAnswer: The answer is:

Copy in 


C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER\

folder as many Python versions as you want (Python version = folder with Python like PYTHON_SERVICES)

Stop Launchpad
Change in 


C:\Program Files\Microsoft SQL
  Server\MSSQL14.MSSQLSERVER\MSSQL\Binn\pythonlauncher.config

file: in PYTHONHOME and ENV_ExaMpiCommDllPath parameters substitute the folder 

C:\Program Files\Microsoft SQL
  Server\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\

with the folder with new Python version.

Start Launchpad",0.40816328,0.37977242,0.000806040596216917
89,"Question\nWhen I type pip install pygame or pip3 install pygame on terminal, it says ""check the logs for full command output"". I already upgraded pip and it still says that. Can you tell me what this means and how to fix it?\nAnswer: You can try running the pip command by adding the --verbose flag. This will print out the logs in the Terminal, which you then can inspect. These logs often help you indicate the cause of the error.
For example:
pip install --verbose pygame
or
pip3 install --verbose pygame",0.40816328,0.3424949,0.004312335513532162
90,"Question\nWhen I type pip install pygame or pip3 install pygame on terminal, it says ""check the logs for full command output"". I already upgraded pip and it still says that. Can you tell me what this means and how to fix it?\nAnswer: When you get this message, there will be a few lines above it which tell you where the log file is and some more details about what went wrong.",0.0,0.13551354,0.018363920971751213
91,"Question\nHow could I force vscode to always find and prefer pipenv's virtual environment for python instead of the python's global settings?
When I create a pipenv environment on my workspace, it keeps using the global python version at /usr/bin/python (as defined in settings as ""python.pythonPath"": ""/usr/bin/python"") but I wonder how could I switch to something like ~/.local/share/virtualenvs/Selenium-10eAXqZ4/bin/python automatically when there is Pipenv environment detected.
Is this even possible? If this is how can I configure it?
(I'm not talking about simply overriding the python.pythonPath with local.vscode/settings.json I need this to detect the path from pipenv automatically when it exists for the current project)\nAnswer: Add PIPENV_VENV_IN_PROJECT=1 to your environment and the.venv folder will be added to your project root. VSCode has zero problems picking up Python from there. (I find it also very convenient to have everything in one place and not spread around on the entire disk.)",1.0,0.122309506,0.770340621471405
92,"Question\nI have 2 dataframes in Spark 2.4, they are close to the same size. Each has about 40 million records. One is generated simply by loading the dataframe from S3, the other loads a bunch of dataframes and uses sparkSQL to generate a big dataframe. Then I join these 2 dataframes together multiple times into multiple dataframes and try to write them as CSV to S3... However I am seeing my write times upwards of 30 minutes, I am not sure if it is re-evaluating the dataframe or if perhaps I need more CPUs for this task. Nonetheless, I was hoping someone may have some advice on how to optimize these write times.\nAnswer: So when a dataframe is created from other dataframes it seems an execution plan is what is first created. Then when executing a write operation that plan gets evaluated. 
The best way to take care of this particular situation is to take advantage of the spark lazy-loading caching (I have not seen an eager-loading solution for spark but if that exists it could be even better). 
By doing:
dataframe1.cache()
And
dataframe2.cache()
when you join these 2 dataframes the first time both dataframes are evaluated and loaded into cache. Then when joining and writing again the 2 dataframe execution plans are already evaluated and the join and write becomes much faster.
This means the first write still takes over 30 minutes but the other 2 writes are much quicker.
Additionally, you can increase performance with additional CPUs and proper paritioning and coalesce of the dataframes. That could help with the evaluation of the first join and write operation.
Hope this helps.",0.0,0.16727495,0.027980908751487732
93,"Question\nhope you are all doing well.
Im working on api project using python and flask.
The question I have to ask is, how can I get the values of multiple query string parameter?
The api client is built in PHP, and when a form is submitted, if some of the parameters are  multiple the query string is built like filter[]=1&filter[]=2&filter[]=3... and so on.
When I dump flask request, it shows something like (filter[], 1), (filter[], 2), (filter[], 3), it seems ok, but then when I do request.args.get('filter[]') it returns only the first item in the args ImmutableDict, filter[]=1, and I can't access the other values provided.
Any help regarding this issue would be aprreciated.
Happy programming!\nAnswer: try this request.args.to_dict(flat=False) to convert",0.0,0.042589724,0.0018138845916837454
94,"Question\nI have a pyramid API which has basically three layers. 

View -> validates the request and response
Controller -> Does business logic and retrieves things from the DB.
Services -> Makes calls to external third party services. 

The services are a class for each external API which will have things like authentication data. This should be a class attribute as it does not change per instance. However, I cannot work out how to make it a class attribute. 
Instead I extract the settings in the view request.registry.settings pass it to the controller which then passes it down in the init() for the service. This seems unnecessary. 
Obviously I could hard code them in code but that's an awful idea. 
Is there a better way?\nAnswer: Pyramid itself does not use global variables, which is what you are asking for when you ask for settings to be available in class-level or module-level attributes. For instance-level stuff, you can just pass the settings from Pyramid into the instance either from the view or from the config.
To get around this, you can always pass data into your models at config-time for your Pyramid app. For example, in your main just pull settings = config.get_settings() and pass some of them to where they need to be. As a general rule, you want to try to pass things around at config-time once, instead of from the view layer all the time.
Finally, a good way to do that without using class-level or module-level attributes is to register instances of your services with your app. pyramid_services library provides one approach to this, but the idea is basically to instantiate an instance of a service for your app, add it to your pyramid registry config.registry.foo =... and when you do that you can pass in the settings. Later in your view code you can grab the service from there using request.registry.foo and it's already setup for you!",0.81632656,0.37114653,0.1981852650642395
95,"Question\nIn my django application, I used to authenticate users exploiting base django rest framework authentication token. Now I've switched to Json Web Token, but browsing my psql database, I've noticed the table authtoken_token, which was used to store the DRF authentication token, is still there. I'm wondering how to get rid of it. I've thought about 2 options:

deleting it through migration: I think this is the correct and safer way to proceed, but in my migrations directory inside my project folder, I didn't find anything related to the tokens. Only stuff related to my models;
deleting it directly from the database could be another option, but I'm afraid of messing with django migrations (although it shoudn't have links with other tables anymore)

I must clarify I've already removed rest_framework.authtoken from my INSTALLED_APPS\nAnswer: You can choose the first option. There are 3 steps should you do to complete uninstall authtoken from your Django app

Remove rest_framework.authtoken from INSTALLED_APPS, this action will tell your Django app to do not take any migrations file from that module
Remove authtoken_token table, if you will
Find the record with authtoken app name in table django_migrations, you can remove it.

Note: There are several error occurs in your code, because authtoken module is removed from your INSTALLED_APPS. My advice, backup your existing database first before you do above step",0.20408164,0.05215344,0.023082178086042404
96,"Question\nCan't find 'Scripts' folder or 'pip' file in Python 3.8.2 folder for Windows 10. Trying to install pip for python. Any ideas how I can get past this problem?\nAnswer: Pip should have already been installed when you install your python, if you wanna check if you pip is install try typing pip in your command prompt or terminal and if you wanna see the file directory of the pip you have installed say pip show (here you put name of file like pygame)",0.0,0.20102203,0.04040985554456711
97,"Question\nCan't find 'Scripts' folder or 'pip' file in Python 3.8.2 folder for Windows 10. Trying to install pip for python. Any ideas how I can get past this problem?\nAnswer: As others answered already you should have got pip already as you installed python. Well pip isnt the apllication. You use pip in the application: Command prompt. You have to search for command prompt on your computer (if you have python you already have it installed) and then in command prompt you install packages that you still son't have.
For example you wan't pygame you write: pip install pygame. If you have PyCharm then there is something else you need to do. If you have PyCharm tell me as a comment to this answer and I'll tell you what to do then because command prompt would almost be useless",0.0,-0.20589256,0.04239174723625183
98,"Question\nHi i have a list with 15205 variables inside, im trying to find the relative frequency of each variable but python don't react with such a big size.
if i try len(list) it works, but max(list) gives me '>' not supported between instances of 'list' and 'int', and set(list) gives me 'type' object is not utterable. If i try to work with it as a data frame it gives me TypeError: unhashable type: 'list'
Plus, if i use a small sample of the list everything works fine.
Can anyone explain me why does this happen and how can i work it out?
thanks\nAnswer: Firstly, you shouldn't name your list 'list', since this is a reserved word in Python referring to the type. This is the origin of your'set(list)' error. 
As for the other error, at least one of the items in your list appears to be itself a list, and you can't compare the magnitude of a list and an integer.",0.81632656,0.30703878,0.2593740224838257
99,"Question\nI have a discord.py bot using the datetime and random libraries ( and discord.py of course ). My question  is how can i run it even when my computer is off. I think the answer is a rented server but i think there are cheeper options\nAnswer: You'll either have to run it on a machine you don't turn off. Or deploy it to a server. You can get cheap servers through Linode, Digital Ocean and others.",0.0,0.17684335,0.03127356991171837
0,"Question\nI am using Python to_sql function to insert data in a database table from Pandas dataframe. 
I am able to insert data in database table but I want to know in my code how many records are inserted. 
How to know record count of inserts ( i do not want to write one more query to access database table to get record count)?
Also, is there a way to see logs for this function execution. like what were the queries executed etc.\nAnswer: There is no way to do this, since python cannot know how many of the records being inserted were already in the table.",0.0,0.27355385,0.07483170926570892
1,"Question\nThe python documentation says this about the sync method: 

Write back all entries in the cache if the shelf was opened with
  writeback set to True. Also empty the cache and synchronize the
  persistent dictionary on disk, if feasible. This is called
  automatically when the shelf is closed with close().

I am really having a hard time understanding this.
How does accessing data from cache differ from accessing data from disk?
And does emptying the cache affect how we can access the data stored 
in a shelve?\nAnswer: For whoever is using the data in the Shelve object, it is transparent whether the data is cached or is on disk. If it is not on the cache, the file is read, the cache filled, and the value returned. Otherwise, the value as it is on the cache is used.
If the cache is emptied on calling sync, that means only that on the next value fetched from the same Shelve instance, the file will be read again. Since it is all automatic, there is no difference. The documentation is mostly describing how it is implemented. 
If you are trying to open the same ""shelve"" file with two concurrent apps, or even two instances of shelve on the same program, chances are you are bound to big problems. Other than that, it just behaves as a ""persistent dictionary"" and that is it.
This pattern of writing to disk and re-reading from a single file makes no difference for a workload of a single user in an interactive program. For a Python program running as a server with tens to thousands of clients, or even a single big-data processing script, where this could impact actual performance, Shelve is hardly a usable thing anyway.",0.0,0.3617413,0.13085676729679108
2,"Question\npreviously we implemented one django application call it as ""x"" and it have own database and it have django default authentication system, now we need to create another related django application call it as ""y"", but y application didn't have database settings for y application authentication we should use x applications database and existing users in x application, so is it possible to implement like this?, if possible give the way how can we use same database for two separated django applications for authentication system.
Sorry for my english
Thanks for spending time for my query\nAnswer: So, to achieve this. In your second application, add User model in the models.py and remember to keep managed=False in the User model's Meta class.
Inside your settings.py have the same DATABASES configuration as of your first application.
By doing this, you can achieve the User model related functionality with ease in your new application.",0.0,0.20814067,0.04332254081964493
3,"Question\nThe real difference between MEDIA_ROOT and STATIC_ROOT in python django and how to use them correctly? 
I just was looking for the answer and i'm still confused about it, in the end of the day i got two different answers:
- First is that the MEDIA_ROOT is for storing images and mp3 files maybe and the STATIC_ROOT for the css, js... and so on.
-Second answer is that they were only using MEDIA_ROOT in the past for static files, and it caused some errors so eventually we are only using STATIC_ROOT.
is one of them right if not be direct and simple please so everybody can understand and by how to use them correctly i mean what kind of files to put in them exactly\nAnswer: Understanding the real difference between MEDIA_ROOT and STATIC_ROOT can be confusing sometimes as both of them are related to serving files.
To be clear about their differences, I could point out their uses and types of files they serve.

STATIC_ROOT, STATIC_URL and STATICFILES_DIRS are all used to serve the static files required for the website or application. Whereas, MEDIA_URL and MEDIA_ROOT are used to serve the media files uploaded by a user.

As you can see that the main difference lies between media and static files. So, let's differentiate them.

Static files are files like CSS, JS, JQuery, scss, and other images(PNG, JPG, SVG, etc. )etc. which are used in development, creation and rendering of your website or application. Whereas, media files are those files that are uploaded by the user while using the website.

So, if there is a JavaScript file named main.js which is used to give some functionalities like show popup on button click then it is a STATIC file. Similarly, images like website logo, or some static images displayed in the website that the user can't change by any action are also STATIC files. 
Hence, files(as mentioned above) that are used during the development and rendering of the website are known as STATIC files and are served by STATIC_ROOT, STATIC_URL or STATICFILES_DIRS(during deployment) in Django.
Now for the MEDIA files: any file that the user upload",0.0,0.25365907,0.0643429234623909
4,"Question\nWhen I try to open Python it gives me an error saying:
IDLE's subprocess didn't make connection. See the'startup failure' section of the IDLE doc online
I am not sure how to get it to start. I am on the most recent version of windows, and on the most recent version of python.\nAnswer: I figured it out, thanks. All I needed to do was uninstall random.py.",0.0,0.22573543,0.05095648393034935
5,"Question\nWhen I try to open Python it gives me an error saying:
IDLE's subprocess didn't make connection. See the'startup failure' section of the IDLE doc online
I am not sure how to get it to start. I am on the most recent version of windows, and on the most recent version of python.\nAnswer: Open cmd and type python to see if python was installed. If so fix you IDE. If not download and reinstall python.",0.0,0.3155169,0.09955091029405594
6,"Question\nWe have a small website with API connected using AJAX.
We do not ask for usernames and passwords or any authentication like firebase auth.
So it's like open service and we want to avoid the service to be misused.
OAuth 2 is really effective when we ask for credentials to the user.
Can you suggest the security best practice and how it can be implemented in this context using python?
Thanks\nAnswer: Use a firewall
Allow for third-party identity providers if possible
 Separate the concept of user identity and user account",0.40816328,0.0038000345,0.16350963711738586
7,"Question\nI have created a Django project in vscode. Generally, vscode automatically prompts me to install pylint but this time it did not (or i missed it). Even though everything is running smoothly, I am still shown import errors. How do I manually install pytlint for this project?
Also,in vscode i never really create a 'workspace'. I just create and open folders and that works just fine.
ps. Im using pipenv. dont know how much necessary that info was.\nAnswer: Hi you must active your venv at the first then install pylint (pip install pylint)
In vscode: ctrl+shift+P then type linter (choose ""python:select linter"") now you can choose your linter (pylint)
I hope it helps you",0.40816328,-0.1832175,0.3497312068939209
8,"Question\nI have situation in centos where 3 different/Independent caller will try to execute same python script with respective command line args. eg: python main.py arg1, python main.py arg2, python main.py arg3 at same time.
My question is - Is it possible in the first place or I need to copy that python script, 3 times with 3 different names to be called by each process. 
If it is possible then how it should be done so that these 3 processes will not interfare and python script execution will be independent from each other.\nAnswer: All the python processes will run entirely isolated from each other, even if executing the same source file.
If they interact with any external resource other than process memory (such as files on disk), then you may need to take measures to make sure the processes don't interfere (by making sure each instance uses a different filename, for example).",0.40816328,0.29003406,0.01395451370626688
9,"Question\nIf I develop a REST service hosted in Apache and a Python plugin which services GET, PUT, DELETE, PATCH; and this service is consumed by an Angular client (or other REST interacting browser technology).  Then how do I make it scale-able with RabbitMQ (AMQP)?  
Potential Solution #1

Multiple Apache's still faces off against the browser's HTTP calls.
Each Apache instance uses an AMQP plugin and then posts message to a queue
Python microservices monitor a queue and pull a message, service it and return response
Response passed back to Apache plugin, in turn Apache generates the HTTP response 

Does this mean the Python microservice no longer has any HTTP server code at all.  This will change that component a lot.  Perhaps best to decide upfront if you want to use this pattern as it seems it would be a task to rip out any HTTP server code.
Other potential solutions?  I am genuinely puzzled as to how we're supposed to take a classic REST server component and upgrade it to be scale-able with RabbitMQ/AMQP with minimal disruption.\nAnswer: I would recommend switching wsgi to asgi(nginx can help here), Im not sure why you think rabbitmq is the solution to your problem, as nothing you described seems like that would be solved by using this method.
asgi is not supported by apache as far as I know, but it allows the server to go do work, and while its working it can continue to service new requests that come in. (gross over simplification)
If for whatever reason you really want to use job workers (rabbitmq, etc) then I would suggest returning to the user a ""token"" (really just the job_id) and then they can call with that token, and it will report back either the current job status or the result",0.81632656,0.15758538,0.43393993377685547
10,"Question\nDo you know how to create package from my python application to be installable on Windows without internet connection? I want, for example, to create tar.gz file with my python script and all dependencies. Then install such package on windows machine with python3.7 already installed. I tried setuptools but i don't see possibility to include dependencies. Can you help me?\nAnswer: Their are several Java tutorials on how to make installers that are offline. You have your python project and just use a preprogrammed Java installer to then put all of the 'goodies' inside of. Then you have an installer for windows. And its an executable.",-0.71428573,0.12603611,0.7061408162117004
11,"Question\nI am new at learning Python and i am trying to trying to set up the environment on VS code. However, the Debug icon and function is not on the menu bar. Please how do I rectify this?\nAnswer: right click on the menu bar. you can select which menus are active. it's also called run i believe.",0.0,0.17112762,0.029284661635756493
12,"Question\ni am trying implement from scipy.spatial import distance as dist library however it gives me   File ""/home/afeyzadogan/anaconda3/envs/untitled/lib/python3.7/inspect.py"", line 56, in 
    for k, v in dis.COMPILER_FLAG_NAMES.items():
AttributeError: module 'dis' has no attribute 'COMPILER_FLAG_NAMES'
error how can i solve it?
'''
for k, v in dis.COMPILER_FLAG_NAMES.items():
    mod_dict[""CO_"" + v] = k
'''\nAnswer: We ran across this issue in our code with the same exact AttributeError.
Turns out it was a totally unrelated file in the current directory called dis.py.",0.40816328,0.1862778,0.04923316463828087
13,"Question\nHow can I save a plot in a 750x750 px using savefig?
The only useful parameter is DPI, but I don't understand how can I use it for setting a precise size\nAnswer: I added plt.tight_layout() before savefig(), and it solved the trimming issue I had. Maybe it will help yours as well.
I also set the figure size at the begining rcParams['figure.figsize'] = 40, 12(you can set your own width and height)",0.0,0.33715636,0.11367440968751907
14,"Question\nThere is an API that I am using from another company that returns the ID-s of the last 100 purchases that have been made in their website.
I have a function change_status(purchase_id) that I would like to call whenever a new purchase has been made. I know a workaround on how to do it, do a while True loop, keep an index last_modified_id for the last modified status of a purchase and loop all purchases from the latest to the earliest and stop once the current id is the same as last_modified_id and then put a sleeper for 10 seconds after each iteration. 
Is there a better way on how to do it using events in python? Like calling the function change_status(purchase_id) when the result of that API has been changed. I have been searching around for a few days but could not find about about an event and an API. Any suggestion or idea helps. Posting what I have done is usually good in stackoverflow, but I don't have anything about events. The loop solution is totally different from the events solution.
Thank you\nAnswer: The only way to do this is to keep calling the API and watching for changes from the previous response, unless...
The API provider might have an option to call your API when something is updated on their side. It is a similar mechanism to push notifications. If they provide a method to do that, you can create an endpoint on your side to do whatever you need to do when a new purchase is made, and provide them the endpoint. However, as far as I know, most API providers do not do this, and the first method is your only option.
Hope this helps!",0.40816328,0.3144729,0.008777884766459465
15,"Question\nI've been studying python for data science for about 5 months now. But I get really stucked when it comes to matplotlib. There's always so many options to do anything, and I can't see a well defined path to do anything. Does anyone have this problem too and knows how to deal with it?\nAnswer: I think your question is stating that you are bored and do not have any projects to make. If that is correct, there are many datasets available on sites like Kaggle that have open-source datasets for practice programmers.",0.0,0.2130763,0.04540150612592697
16,"Question\nI've been studying python for data science for about 5 months now. But I get really stucked when it comes to matplotlib. There's always so many options to do anything, and I can't see a well defined path to do anything. Does anyone have this problem too and knows how to deal with it?\nAnswer: in programming in general "" There's always so many options to do anything"".
i recommend to you that read library and understand their functions and classes in a glance, then go and solve some problems from websites or give a real project if you can. if your code works do not worry and go ahead. 
after these try and error you have a lot of real idea about various problems and you recognize difference between these options and pros and cons of them. like me three years ago.",0.0,-0.019974828,0.00039899375406093895
17,"Question\nI have a OnetoOne field  with primary_key=True in a model.
Now I want to change that to a ForeignKey but cannot since there is no 'id'.
From this:

user = models.OneToOneField(User, primary_key=True, on_delete=models.CASCADE)

To this:

user1 = models.ForeignKey(User, related_name='questionnaire', on_delete=models.CASCADE)

Showing this while makemigrations:

You are trying to add a non-nullable field 'id' to historicaluserquestionnaire without a default; we can't do that (the database needs something to populate existing rows).
  Please select a fix:
   1) Provide a one-off default now (will be set on all existing rows with a null value for this column)
   2) Quit, and let me add a default in models.py

So how to do that?
Thanks!\nAnswer: The problem is that your trying to remove the primary key, but Django is then going to add a new primary key called ""id"".  This is non-nullable and unique, so you can't really provide a one-off default.
The easiest solution is to just create a new model and copy your table over in a SQL migration, using the old user_id to populate the id field.  Be sure to reset your table sequence to avoid collisions.",0.13605443,0.27282482,0.01870614103972912
18,"Question\nSo I recently finished my python project, grabbing values from an API and put it into my website.
Now I have no clue how I actually start the website (finding a host) and making it accessible to other people, I thought turning to here might find the solution.
I have done a good amount of research, tried ""pythonanywhere"" and ""google app engine"" but seem to not really find a solution. 
I was hoping to be able to use ""hostinger"" as a host, as they have a good price and a good host. Contacted them but they said that they couldn't, though I could upload it to a VPS (which they have). Would it work for me to upload my files to this VPS and therefor get it to a website? or should I use another host?\nAnswer: A VPS would work, but you'll need to understand basic linux server admin to get things setup properly.
Sounds like you don't have any experience with server admin, so something like App Engine would be great for you. There are a ton of tutorials on the internet for deploying flask to GAE.",0.0,0.1214031,0.014738712459802628
19,"Question\nI extracted some data from investing but columns values are all dtype = object, so i cant work with them...
how should i convert object to float?
(2558    6.678,08 2557    6.897,23 2556    7.095,95 2555    7.151,21 2554    7.093,34...    4       4.050,38 3       4.042,63 2       4.181,13 1       4.219,56 0       4.223,33 Name: Alta, Length: 2559, dtype: object)
What i want is :
2558   6678.08 2557    6897.23 2556    7095.95 2555    7151.21 2554    7093.34...    4       4050.38 3       4042.63 2       4181.13 1       4219.56 0       4223.33 Name: Alta, Length: 2559, dtype: float
Tried to use the a function which would replace, for.
def clean(x): x = x.replace(""."", """").replace("","",""."")
but it doesnt work cause dtype is object
Thanks!\nAnswer: That is because there is a comma between the value
Because a float cannot have a comma, you need to first replace the comma and then convert it into float
result[col] = result[col].str.replace("","","""").astype(float)",0.0,0.14611077,0.02134835720062256
20,"Question\nI need to develope a python cgi script for a server run on Windows+IIS. The cgi script is run from a web page with Windows authentification. It means the script is run under different users from Windows active directory. 
I need to use login/passwords in the script and see no idea how to store the passwords securely, because keyring stores data for a certain user only. Is there a way how to access password data from keyring for all active OS users?
I also tried to use os.environ variables, but they are stored for one web session only.\nAnswer: The only thing I can think of here is to run your script as a service account (generic AD account that is used just for this service) instead of using windows authentication. Then you can log into the server as that service account and setup the Microsoft Credential Manager credentials that way.",0.40816328,0.13735348,0.07333794981241226
21,"Question\nI have a path variable in the system variables but how do i add a path variable in the user variables section since i don't have any at the moment.
If there isn't a path variable  in the user variables will it affect in any way?
How much will values of the path variables differ from the one in environment variables to the one in user variables if there is only one user present?\nAnswer: to add a new variable in users variable

click one new button below the user variables.

2.Then a pop window will appear asking you to type new variable name and its value, click ok after entering name and value.
Thats how you can add a new variable in user variables.
You should have a path variable in user variables also because,for example while installing python you have a choice to add python path to variables here the path will be added in user variable 'path'.",0.0,0.20244509,0.04098401591181755
22,Question\nI have a program that modifies PNG files with Python's Pillow library. I was wondering how I could load binary data into a PNG image from PIL's Image object. I receive the PNG over a network as binary data (e.g. the data looks like b'\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR...'). What is the best way to accomplish this task?\nAnswer: I'd suggest receiving the data into a BytesIO object from the io standard library package. You can then treat that as a file-like object for the purposes of Pillow.,0.40816328,0.24575913,0.02637510746717453
23,"Question\nI want to know how to delete/clear all text in a file inside another python file, I looked through stack overflow and could not find a answer, all help appreciated. Thanks!\nAnswer: Try: open('yourfile.txt', 'w').close()",0.13605443,0.048925996,0.00759136350825429
24,"Question\nI am trying to train a model for autonomous driving that converts input from the front camera, to a bird's eye view image.
The input and output, both are segmentation masks with shape (96, 144) where each pixel has a range from  0 to 12 (each number represents a different class).
Now my question is how should i preprocess my data and which loss function should i use for the model (I am trying to use a Fully convolutional Network).
I tried to convert input and outputs to shape (96, 144, 13) using keras' to_categorical utility so each channel has 0s and 1s of representing a specific mask of a category. I used binary_crossentropy ad sigmoid activation for last layer with this and the model seemed to learn and loss started reducing. 
But i am still unsure if this is the correct way or if there are any better ways.
what should be the:

input and ouptput data format
activation of last layer
loss function\nAnswer: I found the solution, use categorical crossentropy with softmax activation at last layer. Use the same data format as specified in the question.",0.40816328,0.07813656,0.10891763120889664
25,"Question\nI succesfully compiled app for android, and now I want to compile python kivy app for ios using buildozer. My operation system is Windows 10, so I don't know how to compile file for ios. I downloaded ubuntu console from microsoft store, that helped me to compile apk file. How to compile file for ios? I hope you help me...\nAnswer: You can only deploy to iOS if you're working on a MacOS machine.",0.0,0.24560863,0.060323599725961685
26,"Question\nI have a dataframe with a single column ""Cntr_Number"" with x no of rows.
What i trying to achieve is using selenium to copy and paste the data into the web page textarea.
The constraint is that the web page text area only accept 20 rows of data per submission.
So how can i impplment it using while loop or other method.

Copy and paste the first 20 rows of data and click on the ""Submit""
button 
Copy and paste the next 20 rows of data and click on the
""Submit"" button

repeat the cycle until the last row.
Sorry i dont have any sample code to show but this is what I'm trying to achieve.
Appreciate if could have some sample code on how to do the implmentation.\nAnswer: The better approach will be capture all the the data in a List, Later while pasting it you can check the length of the list, and later iterate through the list and paste the data 20 at a time in the text area. I hope this will solve your problem.",0.40816328,0.07886189,0.10843940079212189
27,"Question\nTLDR: This is not a question about how to change the way a date is converted to a string, but how to convert between the two format types - This being ""%Y"" and ""YYYY"", the first having a % and the second having 4 x Y.
I have the following date format ""%Y-%M-%D"" that is used throughout an app. I now need to use this within a openpyxl NamedStyle as the number_format option. I cant use it directly as it doesn't like the format, it needs to be in ""YYYY-MM-DD"" (Excel) format.

Do these two formats have names? (so I can Google a little more)
Short of creating a lookup table for each combination of %Y or %M to Y and M is there a conversion method? Maybe in openpyxl? I'd prefer not to use an additional library just for this!

TIA!\nAnswer: Sounds like you are looking for a mapping between printf-style and Excel formatting. Individual date formats don't have names. And, due to the way Excel implements number formats I can't think of an easy way of covering all the possibilities. NamedStyles generally refer to a collection of formatting options such as font, border and not just number format.",0.40816328,0.24866855,0.025438567623496056
28,"Question\nAs far as I understood it, iterators use lazy evaluation, meaning that they don't actually save each item in memory, but just contain the instructions on how to generate the next item. 
However, let's say I have some list [1,2,3,4,5] and convert it into an iterator doing a = iter([1,2,3,4,5]). 
Now, if iterators are supposed to save memory space because as said they contain the instructions on how to generate the next item that is requested, how do they do it in this example? How is the iterator a we created supposed to know what item comes next, without saving the entire list to memory?\nAnswer: Just think for a moment about this scenario... You have a file of over a million elements, loading the memory of the whole list of elements would be really expensive. By using an iterator, you can avoid making the program heavy by opening the file once and extracting only one element for the computation. You would save a lot of memory.",0.0,0.30858076,0.09522208571434021
29,"Question\n[On a mac] 
I know I can get packages doing pip install etc.
But I'm not entirely sure how all this works.
Does it matter which folder my terminal is in when I write this command? 
What happens if I write it in a specific folder?
Does it matter if I do pip/pip3?
I'm doing a project, which had a requirements file. 
So I went to the folder the requirements txt was in and did pip install requirements, but there was a specific tensorflow version, which only works for python 3.7. So I did """"""python3.7 -m pip install requirements"""""" and it worked (i'm not sure why). Then I got jupyter with brew and ran a notebook which used one of the modules in the requirements file, but it says there is no such module. 
I suspect packages are linked to specific versions of python and I need to be running that version of python with my notebook, but I'm really not sure how. Is there some better way to be setting up my environment than just blindley pip installing stuff in random folders? 
I'm sorry if this is not a well formed question, I will fix it if you let me know how.\nAnswer: There may be a difference between pip and pip3, depending on what you have installed on your system.  pip is likely the pip used for python2 while pip3 is used for python3.  
The easiest way to tell is to simply  execute python and see what version starts.  python will run typically run the older version 2.x python and python3 is required to run python version 3.x.  If you install into the python2 environment (using pip install or python -m pip install the libraries will be available to the python version that runs when you execute python.  To install them into a python3 environment, use pip3 or python3 -m pip install.
Basically, pip is writing module components into a library path, where import <module> can find them.  To do this for ALL users, use python3 or pip3 from the command line.  To test it out, or use it on an individual basis, use a virtual environment as @Abhishek Verma said.",0.0,0.27499688,0.0756232813000679
30,"Question\nBackground: I'm using the gmaps package in Jupyter Python notebook. I have 2 points A (which is a marker) and B (which is a symbol) which is connected by a line.
Question: I want to somehow display text on this line that represents the distance between A and B. I have already calculated the distance between A and B but cannot display the text on the map. Is there any way to display text on the line?\nAnswer: I found that gmaps doesn't have this feature so I switched to folium package which has labels and popups to display text on hover and clicking the line.",0.0,0.119755626,0.014341410249471664
31,"Question\nI've just built a function that is working fine on my laptop (Mac, but I'm working on a Windows virtual machine of the office laptop), but when I pass it to a colleague o'mine, it raises a ValueError:
""You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat""
The line of the code that raises the error is a simple merge that on my laptop works perfectly:
df = pd.merge(df1, df2, on = ""x"", how = ""outer)
The input files are exactly the same (taken directly from the same remote folder).
I totally don't know how to fix the problem, and I don't understand why on my laptop it works (even if I open a new script or I restart the kernel, so no stored variables around) and in the one of my colleague it doesn't.
Thanks for your help!\nAnswer: my guess (a wild guess) is that the data from the 2 tab-separated CSV files (i.e., TSV files) is somehow converted using different locales on your computer and your colleague's computer.
Check if you have locale-dependent operations that could cause a number with the ""wrong"" decimal separator not to be recognized as a number.
This should not happen in pd.read_csv() because the decimal parameter has a well-defined default value of ""."".
But from an experience I had with timestamps in another context, one timestamp with a ""bad"" format can cause the whole column to be of the wrong type. So if just one number of just one of the two files, in the column you are merging on, has a decimal separator, and this decimal separator is only recognized as such on your machine, only on your machine the join will succeed (I'm supposing that pandas can join numeric columns even if they are of different type).",0.0,0.19952393,0.03980979695916176
32,"Question\nWe have one terraform instance and script which could create infra in azure. We would like to use same scripts to create/update/destroy isolated infra for each one of our customers on azure. We have achieved this by assigning one workspace for each client,different var files and using backend remote state files on azure.
Our intend is to create a wrapper python program that could create multiple threads and trigger terraform apply in parallel for all workspaces. This seems to be not working as terraform runs for one workspace at a time. 
Any suggestions/advice on how we can achieve parallel execution of terraform apply for different workspaces?\nAnswer: It's safe to run multiple Terraform processes concurrently as long as:

They all have totally distinct backend configurations, both in terms of state storage and in terms of lock configuration. (If they have overlapping lock configuration then they'll mutex each other, effectively serializing the operations in spite of you running multiple copies.)
They work with an entirely disjoint set of remote objects, including those represented by both managed resources (resource blocks) and data resources (data blocks).

Most remote APIs do not support any sort of transaction or mutex concept directly themselves, so Terraform cannot generally offer fine-grained mutual exclusion for individual objects. However, multiple runs that work with entirely separate remote objects will not interact with one another.
Removing a workspace (using terraform workspace delete) concurrently with an operation against that workspace will cause undefined behavior, because it is likely to delete the very objects Terraform is using to track the operation.
There is no built-in Terraform command for running multiple operations concurrently, so to do so will require custom automation that wraps Terraform.",1.0,0.5311508,0.2198195606470108
33,"Question\nI am wondering how can you get device connection string from IotHub using python in azure? any ideas? the device object produced by IoTHubRegisterManager.Create_device_with_sas(...) doesn't seem to contain the property connection string.\nAnswer: You can get a device connection string from the device registry. However, it is not recommended that you do that on a device. The reason being is that you will need the IoT hub connection string to authenticate with your hub so that you can read the device registry. If your device is doing that and it is compromised then the perpetrator now has your IoT hub connection string and could cause all kinds of mayhem. You should specifically provide each device instance with its connection string. 
Alternatively, you could research the Azure DPS service which will provide you with device authentication details in a secure manner.",0.0,0.33817232,0.1143605187535286
34,"Question\nI was doing a game in tkinter, then I make it executable with PyInstaller and sent it to my friends so they can run it and tell me how it feels.
It seems that they could download the file, but can't open it because windows forbade them telling that it's not secure and not letting them choose to assume the risk or something.
They tried to run as administrator and still nothing changed.
What should I do or what I should add to my code so that windows can open it without problem and why windows opens other executable files without saying that(current error that my executable gets)?\nAnswer: compress it as a.zip file and then it will most probably work
or install NSIS and create a windows installer for it.",0.0,0.05865258,0.003440125146880746
35,"Question\nIn django, if I want to use csrf token, I need to imbed a form with csrf token in django template. However as a backend-engineer I am co-working with a front-end engineer whose code is not available for me. So I caanot use the template. In this case, if I want still the csrf function. what should I do?\nAnswer: you should ask the coworker to embed the csrf token in the form he is sending you
you can get it from document.Cookies if he doesnt want to or cannot use the {% csrf %} tag",0.0,0.2738532,0.07499558478593826
36,"Question\nI crawled and saved the user's website usage lists.
I want to analyze the results of the crawl, but I wonder how there is a way.
First of all, what I thought was Word Cloud.
I am looking for a way to track user's personal preferences with user's computer history. 
I want a way to visualize personal tendencies, etc. at a glance. Or I'm looking for a way to find out if there's no risk of suicide or addiction as a result of the search.
thank you.\nAnswer: If you want to visualize data and make analysis on it matplotlib would be good start, again it depends a lot on your data. Matplotlib and seaborn are plotting libraries that are good for representing quantitative data and get some basic analysis at least.",0.0,0.13243991,0.017540330067276955
37,"Question\nGood evening, I'm making a platformer and would like to know when you should use one of the both.
For example for:
1)The player controlled character
2)The textured tiles that make up the level
3)The background
Should/Could you make everything with sprites?
I just want to know how you would do it if you were to work on a pygame project.
I ask this because I see lots of pygame tutorials that explain adding textures by using surfaces but then in other tutorials, they use sprite objects instead.\nAnswer: Yes you could make everything including the background with sprites. It usually does not make sense for the background though (unless you;re doing layers of some form).
The rest often make senses as sprite, but that depends on your situation.",1.0,0.14075357,0.7383044362068176
38,"Question\nI have two functions: def is_updated_database(): is checking if database is updated and the other  onedef scrape_links(database): is scraping through set of links(that it downloaded from aforementioned database).  
So what I want do is when def is_updated_database(): finds that the updated is downloaded, I want to stop def scrape_links(database): and reload it with a new function parameter(database which would be a list of new links).
My attempt: I know how to run two threads, but I have no idea how to ""connect"" them, so that if something happens to one then something should happen to another one.\nAnswer: Well, one way to solve this problem, may be the checking of database state, and if something new appears there, you could return the new database object, and after that scrape the links, probably this is losing it's multithreading functionality, but that's the way it works.
I don't think that any code examples are required here for you to understand what I mean.",0.0,0.14749086,0.021753553301095963
39,"Question\nI've to build an ML model to classify sentences into different categories. I have a dataset with 2 columns (sentence and label) and 350 rows i.e. with shape (350, 2). To convert the sentences into numeric representation I've used TfIdf vectorization, and so the transformed dataset now has 452 columns (451 columns were obtained using TfIdf, and 1 is the label) i.e. with shape (350, 452). More generally speaking, I have a dataset with a lot more features than training samples. In such a scenario what's the best classification algorithm to use? Logistic Regression, SVM (again what kernel?), neural networks (again which architecture?), naive Bayes or is there any other algorithm?
How about if I get more training samples in the future (but the number of columns doesn't increase much), say with a shape (10000, 750)?
Edit: The sentences are actually narrations from bank statements. I have around 10 to 15 labels, all of which I have labelled manually. Eg. Tax, Bank Charges, Loan etc. In future I do plan to get more statements and I will be labelling them as well. I believe I may end up having around 20 labels at most.\nAnswer: With such a small training set, I think you would only get any reasonable results by getting some pre-trained language model such as GPT-2 and fine tune to your problem. That probably is still true even for a larger dataset, a neural net would probably still do best even if you train your own from scratch. Btw, how many labels do you have? What kind of labels are those?",0.0,0.079536915,0.006326120812445879
40,"Question\nIn Python how do I write code which shifts off the last element of a list and adds a new one to the beginning - to run as fast as possible at execution?
There are good solutions involving the use of append, rotate etc but not all may translate to fast execution.\nAnswer: Don't use a list.
A list can do fast inserts and removals of items only at its end. You'd use pop(-1) and append, and you'd end up with a stack.
Instead, use collections.deque, which is designed for efficient addition and removal at both ends. Working on the ""front"" of a deque uses the popleft and appendleft methods. Note, ""deque"" means ""double ended queue"", and is pronounced ""deck"".",1.0,0.40970314,0.3484503924846649
41,"Question\nThis is my first question to here. I don't know how to set Border Radius for Tkinter Entry, Thanks for your Help!\nAnswer: There is no option to set a border radius on the tkinter or ttk Entry widgets, or any of the other widgets in those modules. Tkinter doesn't support the concept of a border radius.",0.81632656,0.1755681,0.4105713963508606
42,"Question\nI am using Django 3.0 and I was wondering how to create a new database table linked to the creation of each user. In a practical sense: I want an app that lets users add certain stuff to a list but each user to have a different list where they can add their stuff. How should I approach this as I can't seem to find the right documentation... Thanks a lot!!!\nAnswer: This is too long for a comment.
Creating a new table for each user is almost never the right way to solve a problem.  Instead, you just have a userStuff table that maintains the lists.  It would have columns like:

userId
stuffId

And, if you want the stuff for a given user, just use a where clause.",0.0,0.1520074,0.023106249049305916
43,"Question\nHow do I enable method autocompletion for discord.py in PyCharm? Until now I've been doing it the hard way by looking at the documentation and I didn't even know that autocomplete for a library existed. So how do I enable it?\nAnswer: The answer in my case was to first create a new interpreter as a new virtual environment, copy over all of the libraries I needed (there is an option to inherit all of the libraries from the previous interpreter while setting up the new one) and then follow method 3 from above. I hope this helps anyone in the future!",0.20408164,0.24328232,0.0015366931911557913
44,"Question\nCan anyone tell me how to install turicreate on windows 7? I am using python of version 3.7. I have tried using pip install -U turicreate to install but failed.
Thanks in advance\nAnswer: I am quoting from Turicreate website:
Turi Create supports:

macOS 10.12+
Linux (with glibc 2.12+)
Windows 10 (via WSL)

System Requirements

Python 2.7, 3.5, or 3.6
Python 3.7 macOS only
x86_64 architecture

So Windows 7 is not supported in this case.",0.0,0.43938184,0.1930564045906067
45,"Question\nI am relatively new to the field of NLP/text processing. I would like to know how to identify domain-related important keywords from a given text.
For example, if I have to build a Q&A chatbot that will be used in the Banking domain, the Q would be like: What is the maturity date for TRADE:12345? 
From the Q, I would like to extract the keywords: maturity date & TRADE:12345.
From the extracted information, I would frame a SQL-like query, search the DB, retrieve the SQL output and provide the response back to the user.
Any help would be appreciated.
Thanks in advance.\nAnswer: So, this is where the work comes in. 
Normally people start with a stop word list. There are several, choose wisely. But more than likely you'll experiment and/or use a base list and then add more words to that list.
Depending on the list it will take out 

""what, is, the, for,?""

Since this a pretty easy example, they'll all do that. But you'll notice that what is being done is just the opposite of what you wanted. You asked for domain-specific words but what is happening is the removal of all that other cruft (to the library).
From here it will depend on what you use. NLTK or Spacy are common choices. Regardless of what you pick, get a real understanding of concepts or it can bite you (like pretty much anything in Data Science).
Expect to start thinking in terms of linguistic patterns so, in your example:

What is the maturity date for TRADE:12345? 

'What' is an interrogative, 'the' is a definite article, 'for' starts a prepositional phrase.
There may be other clues such as the ':' or that TRADE is in all caps. But, it might not be.
That should get you started but you might look at some of the other StackExchange sites for deeper expertise. 
Finally, you want to break a question like this into more than one question (assuming that you've done the research and determined the question hasn't already been asked -- repeatedly). So, NLTK and NLP are decently new, but SQL queries are usually a Google search.",0.0,0.43271738,0.18724434077739716
46,"Question\nI'm trying to retrieve versions of all packages from specific index. I'm trying to sending GET request with /user/index/+api suffix but it not responding nothing intresting. I can't find docs about devpi rest api :( 
Has anyone idea how could I do this?
Best regards, Matt.\nAnswer: Simply add header Accept: application/json - it's working!",0.81632656,0.07873565,0.544040322303772
47,"Question\nI am trying to find out what the best tool is for my project.
I have a lighttpd server running on a raspberry pi (RPi) and a Python3 module which controls the camera. I need a lot of custom control of the camera, and I need to be able to change modes on the fly. 
I would like to have a python script continuously running which waits for commands from the lighttpd server which will ultimately come from a user interacting with an HTML based webpage through an intranet (no outside connections). 
I have used Flask in the past to control a running script, and I have used FastCGI to execute scripts. I would like to continue using the lighttpd server over rather than switching entirely over to Flask, but I don't know how to interact with the script once it is actually running to execute individual functions. I can't separate them into multiple functions because only one script can control the camera at a time. 
Is the right solution to set up a Flask app and have the lighttpd send requests there, or is there a better tool for this?\nAnswer: You have several questions merged into one, and some of them are opion based questions as such I am going to avoid answering those. These are the opinion based questions.

I am trying to find out what the best tool is for my project.
Is the right solution to set up a Flask app and have the lighttpd send requests there
Is there a better tool for this?

The reason I point this out is not because your question isnn't valid but because often times questions like these will get flagged and/or closed. Take a look at this for future referece.
Now to answer this question: 
"" I don't know how to interact with the script once it is actually running to execute individual functions""
Try doing it this way:

Modify your script to use threads and/or processes.
You will have for example a continously running thread which would be the camera.
You would have another non blocking thread listening to IO commands.
Your IO commands would be comming through command line arguments.
Your IO thread upon recieving an IO command would redirect your running camera thread to a specific function as needed.

Hope that helps and good luck!!",0.0,0.4481367,0.200826495885849
48,"Question\nI am trying to find out what the best tool is for my project.
I have a lighttpd server running on a raspberry pi (RPi) and a Python3 module which controls the camera. I need a lot of custom control of the camera, and I need to be able to change modes on the fly. 
I would like to have a python script continuously running which waits for commands from the lighttpd server which will ultimately come from a user interacting with an HTML based webpage through an intranet (no outside connections). 
I have used Flask in the past to control a running script, and I have used FastCGI to execute scripts. I would like to continue using the lighttpd server over rather than switching entirely over to Flask, but I don't know how to interact with the script once it is actually running to execute individual functions. I can't separate them into multiple functions because only one script can control the camera at a time. 
Is the right solution to set up a Flask app and have the lighttpd send requests there, or is there a better tool for this?\nAnswer: I have used Flask in the past to control a running script, and I have used FastCGI to execute scripts.

Given your experience, one solution is to do what you know.  lighttpd can execute your script via FastCGI.  Python3 supports FastCGI with Flask (or other frameworks).  A python3 app which serially processes requests will have one process issuing commands to the camera.

I would like to continue using the lighttpd server over rather than switching entirely over to Flask, but I don't know how to interact with the script once it is actually running to execute individual functions.

Configure your Flask app to run as a FastCGI app instead of as a standalone webserver.",0.20408164,0.40275353,0.03947051987051964
49,"Question\nI am using the pho MQTT client library successfully to connect to AWS. After the mqtt client is created, providing the necessary keys and certificates is done with a call to client.tls_set() This method requires file paths to root certificate, own certificate and private key file.
All is well and life is good except that I now need to provide this code to external contractors whom should not have direct access to these cert and key files. The contractors have a mix of PC and macOS systems. On macOS we have keychain I am familiar with but do not know how to approach this with python - examples/library references would be great. On the PC I have no idea which is the prevalent mechanism to solve this.
To add to this, I have no control over the contractor PCs/Macs - i.e., I have no ability to revoke an item in their keychain. How do I solve this?
Sorry for being such a noob in security aspects. No need to provide complete examples, just references to articles to read, courses to follow and keywords to search would be great - though code examples will be happily accepted also of course.\nAnswer: Short answer: you don't.
Longer answer:
If you want them to be able connect then you have no choice but to give them the cert/private key that identifies that device/user.
The control you have is issue each contractor with their own unique key/cert and if you believe key/cert has been miss used, revoke the cert at the CA and have the broker check the revocation list.
You can protect the private key with a password, but again you have to either include this in the code or give it to the contractor.
Even if the contractors were using a device with a hardware keystore (secure element) that you could securely store the private key in, all that would do is stop the user from extracting the key and moving it to a different machine, they would still be able to make use of the private key for what ever they want on that machine.
The best mitigation is to make sure the certificate has a short life and control renewing the certificate, this means if a certificate is leaked then it will stop working quickly even if you don't notice and explicitly revoke it.",0.40816328,0.3426423,0.004292997997254133
50,"Question\nim very new in programming and i learn Python.
I'm coding on mac btw.
I'd like to know how can i import some modules in VS code. 
For exemple, if i want to use the speedtest module i have to download it (what i did) and then import it to my code. But it never worked and i always have the error no module etc.
I used pip to install each package, i have them on my computer but i really don't know to import them on VS code. Even with the terminal of the IDE. 
I know it must be something very common for u guys but i will help me a lot. 
Thx\nAnswer: Quick Summary
This might not be an issue with VS Code.
Problem: The folder to which pip3 installs your packages is not on your $PATH.
Fix: Go to /Applications/Python 3.8 in Finder, and run the Update Shell Profile.command script. Also, if you are using pip install <package>, instead of pip3 install <package> that might be your problem.
Details
Your Mac looks for installed packages in several different folders on your Mac. The list of folders it searches is stored in an environment variable called $PATH. Paths like /Library/Frameworks/Python.framework/Versions/3.8/bin should be in the $PATH environment variable, since that's where pip3 installs all  packages.",0.0,0.39800924,0.15841135382652283
51,"Question\nI really don't understand how batch files work. But I made a python script for my father to use in his work. And I thought installing pip and necessary modules with a single batch file would make it a lot easier for him. So how can I do it? 
The modules I'm using in script are: xlrd, xlsxwriter and tkinter.\nAnswer: You can create a requirements.txt file then use pip install -r requirements.txt to download all modules, if you are working on a virtual environment and you only have the modules your project uses, you can use pip3 freeze >> requirements.txt This is not a batch file but it will work just fine and it is pretty easy",0.30612245,-0.35063535,0.4313308596611023
52,"Question\nI have x and y coordinates in a df  from LoL matches and i want to create a contour plot or heat map to show where the player normally moves in a match.
Does any one know how can I do it?\nAnswer: A contour plot or heat map needs 3 values. You have to provide x, y and z values in order to plot a contour since x and y give the position and z gives the value of the variable you want to show the contour of as a variable of x and y.
If you want to show the movement of the players as a function of time you should look at matplotlib's animations. Or if you want to show the ""players density field"" you have to calculate it.",0.0,0.22358024,0.04998812451958656
53,"Question\nI am developing ecommerce website in django.
I have view ( addToCart)
I want sure before add to cart if user logged in or not 
so that i use @login_required('login') before view
but when click login it show error (can't access to page ).
Note that: normal login is working\nAnswer: Please check the following
1. Add login url on settings
2. Add redirect url on login required decorator
3. If you create a custom login view make sure to check next kwargs",0.0,0.048690557,0.0023707703221589327
54,"Question\nI have a file structure that looks something like this:
Master:

First


train.py
other1.py

Second


train.py
other2.py

Third


train.py
other3.py


I want to be able to have one Python script that lives in the Master directory that will do the following when executed:

Loop through all the subdirectories (and their subdirectories if they exist)
Run every Python script named train.py in each of them, in whatever order necessary

I know how to execute a given python script from another file (given its name), but I want to create a script that will execute whatever train.py scripts it encounters. Because the train.py scripts are subject to being moved around and being duplicated/deleted, I want to create an adaptable script that will run all those that it finds.
How can I do this?\nAnswer: Which OS are you using?
If Ubuntu/CentOS try this combination:
import os
//put this in master and this lists every file in master + subdirectories and then after the pipe greps train.py 
train_scripts = os.system(""find. -type d | grep train.py  "") 
//next execute them
python train_scripts",0.13605443,0.4819323,0.11963151395320892
55,"Question\nI have a file structure that looks something like this:
Master:

First


train.py
other1.py

Second


train.py
other2.py

Third


train.py
other3.py


I want to be able to have one Python script that lives in the Master directory that will do the following when executed:

Loop through all the subdirectories (and their subdirectories if they exist)
Run every Python script named train.py in each of them, in whatever order necessary

I know how to execute a given python script from another file (given its name), but I want to create a script that will execute whatever train.py scripts it encounters. Because the train.py scripts are subject to being moved around and being duplicated/deleted, I want to create an adaptable script that will run all those that it finds.
How can I do this?\nAnswer: If you are using Windows you could try running them from a PowerShell script. You can run two python scripts at once with just this:
python Test1.py
python Folder/Test1.py
And then add a loop and or a function that goes searching for the files. Because it's Windows Powershell, you have a lot of power when it comes to the filesystem and controlling Windows in general.",0.13605443,0.19357485,0.0033085986506193876
56,"Question\nAnyone know how you get a dataframe from Quantopian to excel - I try - results.to_excel
results are the name of my dataframe\nAnswer: Try this :
Name of your DataFrame: Result.to_csv(""result.csv"")
here Result is your DataFrame Name, while to_csv() is a function",0.0,0.08908868,0.007936792448163033
57,"Question\nSo in my case, I have a class Gnome for example and I want to destroy each object of this class when its variable health reaches 0. Is there a way for me to delete each instance of Gnome when its hp is 0 or should I ""mark it for death"" and delete everything that was marked? Either way, how can I do this?\nAnswer: Unfortunately, there isn't a way to do what you're wanting.  Every Python object maintains a record of how many references there are to it.  Once the reference count reaches 0, the Python garbage collector will clean it up.
As long as you still have references to the instances, they will persist.",0.0,0.26052576,0.06787367165088654
58,"Question\nRecently i read this comment 
I like Spyder for interacting with my variables and PyCharm for editing my scripts. Alternative Solution: use both simultaneously. As I edit in PyCharm (on Mac OS), the script updates live in spyder. Best of both worlds!
i want to understand how to use them together and live update the script in Spyder?\nAnswer: After some research, I find that there is no variable explorer like Sypder option in PyCharm. To work with PyCharm and Spyder together, we need to use the two IDEs parallelly i.e., to write the code we can use the PyCharm and to view the Spyder we can just alt tab to the Spyder window and re run the code in Spyder. It will not take much time to re run the code again. We just need to press Ctrl + A and Ctrl + Enter, then the variables will get updated in the variable explorer. Spyder variable view is amazing especially data frames.
Only thing we need to remember is, we need to install the packages in both PyCharm and Sypder. If we install the package in PyCharm, it will not reflect in Spyder. So we need to install through Conda Prompt.",0.0,0.15923566,0.025355994701385498
59,"Question\nI have a project and i was wondering how can i make a button in my web that when it is being clicked it can display a string in my python terminal
Thank you in advance\nAnswer: You need to create an http server in your Python, and call it with fetch in JavaScript. You can pass data in the query parameters.",0.0,0.069936395,0.0048910994082689285
60,"Question\nI have a question about a programm with Python.
I must capture my Notebookcam with Pepper and show it on the Display from Pepper.
Now I have the Problem, the programming with Choreograph is a little bit different and I don't know how I can handle this Programm. I would be happy if you could answer.
Thanks.\nAnswer: You cannot use Choregraphe to retrieve the video remotely because the applications made using Choregraphe are run on the robot, not on your PC.
You need to write separate program on your PC to retrieve the video.",0.0,-0.030210376,0.0009126667864620686
61,"Question\nI recently switched to the new Pycharm version and in the contrary to the previous versions it seems like two underscores are no longer combined like this: __ 
Does someone know how to switch it back, so the IDE combines them?\nAnswer: Please try to enable: File - Settings - Editor - Font - Enable font ligatures",0.40816328,0.076140046,0.1102394238114357
62,"Question\nI have received a link with.ipynb  link. I am new to Python and Jupyter and I need to open the link to work on the details inside.
The link opens in my internet browser and I couldn't properly see the contents and bring it in to a Jupyter notebook. 
Could anyone please give me a tip how to handle such links for Python/Jupyter?\nAnswer: Adding to Vinzee's answer: jupyter notebook starts in your home folder and you can't move up from there; only down into subfolders.  Open jupyter to see what folder it starts in, and make sure that you put the.ipynb file in that folder or one of its subfolders.",0.0,0.06534624,0.004270131234079599
63,"Question\nI don't know how it happened, but my sys.path now apparently contains the path to my local Python project directory, let's call that /home/me/my_project. (Ubuntu).
echo $PATH does not contain that path and echo $PYTHONPATH is empty.
I am currently preparing distribution of the package and playing with setup.py, trying to always work in an virtualenv. Perhaps I messed something up while not having a virtualenv active. Though I trying to re-install using python3 setup.py --record (in case I did an accidental install) fails with insufficient privileges - so I probably didn't accidentally install it into the system python.
Does anyone have an idea how to track down how my module path got to the sys.path and how to remove that?\nAnswer: I had the same problem. I don't have the full understanding of my solution, but here it is nonetheless.
My solution
Remove my package from site-packages/easy-install.pth
(An attempt at) explanation
The first hurdle is to understand that PYTHONPATH only gets added to sys.path, but is not necessarily equal to it. We are thus after what adds the package into sys.path.
The variable sys.path is defined by site.py.
One of the things site.py does is automatically add packages from site-packages into sys.path.
In my case, I incorrectly installed my package as a site-package, causing it to get added to easy-install.pth in site-packages and thus its path into sys.path.",0.0,0.42501432,0.1806371659040451
64,"Question\nTL;DR
What's the fasted way to get near-zero loading time for a pandas dataset I have in memory, using ray?
Background
I'm making an application which uses semi-large datasets (pandas dataframes between 100MB to 700MB) and are trying to reduce each query time. For a lot of my queries the data loading is the majority of the response times. The datasets are optimized parquet files (categories instead of strings, etc) which only reads the columns it needs. 
Currently I use a naive approach that per-requests loads the require dataset (reading the 10-20 columns out of 1000 I need from the dataset) and then filter out the rows I need. 
A typical request:

Read and parse the contract (~50-100ms)    
Load the dataset (10-20 columns) (400-1200ms)    
Execute pandas operations (~50-100ms)
Serialise the results (50-100ms)

I'm now trying to speed this up (reduce or remove the load dataset step). 
Things I have tried:

Use Arrow's new row-level filtering on the dataset to only read the rows I need as well. This is probably a good way in the future, but for now the new Arrow Dataset API which is relies on is significantly slower than reading the full file using the legacy loader. 
Optimize the hell out of the datasets. This works well to a point, where things are in categories, the data types is optimized. 
Store the dataframe in Ray. Using ray.put and ray.get. However this doesn't actually improve the situation since the time consuming part is deserialization of the dataframe. 
Put the dataset in ramfs. This doesn't actually improve the situation since the time consuming part is deserialization of the dataframe.
Store the object in another Plasma store (outside of ray.put) but obviously the speed is the same (even though I might get some other benefits)

The datasets are parquet files, which is already pretty fast for serialization/deserialization. I typically select about 10-20 columns (out of 1000) and about 30-60% of the rows. 
Any good ideas on how to speed up the loading? I haven't been able to find any near zero-copy operations for pandas data",0.20408164,-0.123042345,0.10701011121273041
65,Question\nI am making a CRM web application. I am planning to do its backend in python(because I only know that language better) and I have a friend who uses flutter for frontend. Is it possible to link these two things(flutter and python backend)? If yes how can it be done...and if no what are the alternatives I have?\nAnswer: I used  $.ajax() method in HTML pages and then used request.POST['variable_name_used_in_ajax()'] in the views.py,0.0,0.0069088936,4.77328103443142e-05
66,"Question\nI am making a CRM web application. I am planning to do its backend in python(because I only know that language better) and I have a friend who uses flutter for frontend. Is it possible to link these two things(flutter and python backend)? If yes how can it be done...and if no what are the alternatives I have?\nAnswer: Yes you both can access same Django rest framework Backend. Try searching for rest API using Django rest framework and you are good to go.
Other alternatives are Firebase or creating rest API with PHP.
You would need to define API endpoints for different functions of your app like login,register etc.
Django rest framework works well with Flutter. I have tried it. You could also host it in Heroku
Use http package in flutter to communicate with the Django server.",0.0,0.17834008,0.03180518373847008
67,"Question\nI have an old Macbook Pro 3,1 running ubuntu 20.04 and python 3.8. The mac CPU doesn't have support for avx (Advanced Vector Extensions) which is needed for tensorflow 2.2 so whilst tensorflow installs, it fails to run with the error:

illegal instruction (core dumped)

I've surfed around and it seems that I need to use tensorflow 1.5 however there is no wheel for this for my configuration and I have the impression that I need to build one for myself.
So here's my question... how do I even start to do that? Does anyone have a URL to Building-Stuff-For-Dummies or something similar please? (Any other suggestions also welcome)
Thanks in advance for your help\nAnswer: Usually there are instructions for building in the repository's README.md. Isn't there such for TensorFlow? It would be odd.",0.0,0.29858357,0.08915214985609055
68,"Question\nI have an old Macbook Pro 3,1 running ubuntu 20.04 and python 3.8. The mac CPU doesn't have support for avx (Advanced Vector Extensions) which is needed for tensorflow 2.2 so whilst tensorflow installs, it fails to run with the error:

illegal instruction (core dumped)

I've surfed around and it seems that I need to use tensorflow 1.5 however there is no wheel for this for my configuration and I have the impression that I need to build one for myself.
So here's my question... how do I even start to do that? Does anyone have a URL to Building-Stuff-For-Dummies or something similar please? (Any other suggestions also welcome)
Thanks in advance for your help\nAnswer: Update: I installed python 3.6 alongside the default 3.8 and then installed tensorflow 1.5 and it looks like it works now (albeit with a few 'future warnings'.)",0.0,0.31219646,0.09746663272380829
69,"Question\nIn the xgboost documentation they refer to a copy() method, but I can't figure out how to use it since if foo is my model, neither bar = foo.copy() nor bar=xgb.copy(foo) works (xgboost can't find a copy() attribute of either the module or the model). Any suggestions?\nAnswer: It turns out that copy() is a method of the Booster object, but a (say) XGBClassifier is not one, so if using the sklearn front end, you do
bar = foo.get_booster().copy()",0.20408164,0.07801485,0.015892835333943367
70,"Question\nI'm trying to get Python and selenium to store the ""1292"" in the following html script and cant figure out why it won't work. I've tried using find_element_by_xpath as well as placing a wait before it and I keep getting this error ""Message: no such element: Unable to locate element:""
Any ideas on how else I can accomplish this? Thanks
<text x=""76.76666666666667"" y=""141.42345774407445"" style=""font-family:&quot;Lucida Grande&quot;, &quot;Lucida Sans Unicode&quot;, Verdana, Arial, Helvetica, sans-serif;font-size:12px;color:#4572A7;font-size:11px;fill:#4572A7;"" text-anchor=""middle"" zIndex=""1"">
     <tspan x=""76.76666666666667"">1292</tspan>
   </text>\nAnswer: You can try:
driver.find_element_by_xpath(""//tspan[text()='1292']"").text
to obtain the string ""1292"".",0.0,0.32306933,0.10437379777431488
71,"Question\nWe have a big C# application, would like to include an application written in python and cython inside the C#
Operating system: Win 10
Python: 2.7
.NET: 4.5+
I am looking at various options for implementation here.
(1) pythonnet - embed the python inside the C# application, if I have abc.py and inside the C#, while the abc.py has a line of ""import numpy"", does it know how to include all python's dependencies inside C#?
(2) Convert the python into.dll - Correct me if i am wrong, this seems to be an headache to include all python files and libraries inside clr.CompileModules. Is there any automatically solution? (and clr seems to be the only solution i have found so far for building dll from python.
(3) Convert.exe to.dll for C# - I do not know if i can do that, all i have is the abc.exe constructed by pyinstaller
(4) shared memory seems to be another option, but the setup will be more complicated and more unstable? (because one more component needs to be taken care of?)
(5) Messaging - zeromq may be a candidate for that. 
Requirements:
Both C# and python have a lot of classes and objects and they need to be persistent
C# application need to interact with Python Application
They run in real-time, so performance for communication does matter, in milliseconds space.
I believe someone should have been through a similar situation and I am looking for advice to find the best suitable solution, as well as pros and cons for above solution.
Stability comes first, then the less complex solution the better it is.\nAnswer: For variant 1: in my TensorFlow binding I simply add the content of a conda environment to a NuGet package. Then you just have to point Python.NET to use that environment instead of the system Python installation.",0.0,0.23179424,0.05372856929898262
72,"Question\nI have a flask app that is intended to be hosted on multiple host. That is, the same app is running on different hosts. Each host can then send a request to the others host to take some action on the it's respective system. 
For example, assume that there is systems A and B both running this flask app. A knows the IP address of B and the port number that the app is hosted on B. A gets a request via a POST intended for B. A then needs to forward this request to B. 
I have the forwarding being done in a route that simply checks the JSON attached to the POST to see if it is the intended host. If not is uses python's requests library to make a POST request to the other host. 
My issue is how do I simulate this environment (two different instance of the same app with different ports) in a python unittest so I can confirm that the forwarding is done correctly?
Right now I am using the app.test_client() to test most of the routes but as far as I can tell the app.test_client() does not contain a port number or IP address associated with it. So having the app POST to another app.test_client() seems unlikely. 
I tried hosting the apps in different threads but there does not seem to be a clean and easy way to kill the thread once app.run() starts, can't join as app.run() never exits. In addition, the internal state of the app (app.config) would be hidden. This makes verifying that A does not do the request and B does hard. 
Is there any way to run two flask app simultaneously on different port numbers and still get access to both app's app.config? Or am I stuck using the threads and finding some other way to make sure A does not execute the request and B does? 
Note: these app do not have any forums so there is no CSRF.\nAnswer: I ended up doing two things. One, I started using patch decorator from the mock library to fake the response form systems B. More specifically I use the @patch('requests.post') then in my code I set the return value to ""< Response [200]>"". However this only makes sure that requests.post is called, not that the second system processed it correctly. The second thing I did was write a separate test that makes the request that should have been sent by A and",0.0,0.14853334,0.022062154486775398
73,"Question\nI have two-time series datasets i.e. errors received and bookings received on a daily basis for three years (a few million rows). I wish to find if there is any relationship between them.As of now, I think that cross-correlation between these two series might help. I order to so, should I perform any transformations like stationarity, detrending, deseasonality, etc. If this is correct, I'm thinking of using ""scipy.signal.correlate¶"" but really want to know how to interpret the result?\nAnswer: scipy.signal.correlate is for the correlation of time series. For series y1 and y2, correlate(y1, y2) returns a vector that represents the time-dependent correlation: the k-th value represents the correlation with a time lag of ""k - N + 1"", so that the N+1 th element is the similarity of the time series without time lag: close to one if y1 and y2 have similar trends (for normalized data), close to zero if the series are independent.
numpy.corrcoef takes two arrays and aggregates the correlation in a single value (the ""time 0"" of the other routine), the Pearson correlation, and does so for N rows, returning a NxN array of correlations. corrcoef normalizes the data (divides the results by their rms value), so that he diagonal is supposed to be 1 (average self correlation).
The questions about stationarity, detrending, and deseasonality depend on your specific problem. The routines above consider ""plain"" data without consideration for their signification.",0.81632656,0.26633227,0.30249372124671936
74,"Question\nI was doing some project by using django
and I realized that I forgot to activate virtualenv.
I already made some changes and applied it not on the venv,
and created superuser on the system.

How to find any changes on the system?
how to remove superuser that I made on the system
and what are the cmd commands for that?\nAnswer: If you haven't setup an additional database for your project and you have used django-admin startproject you'll just have a standard django setup, and you will be using sqlite. With this setup, your database is stored in a file in your root directory (for the project) called db.sqlite3.
This is where the super-user you have created will be stored. So it does not matter if the virtualenv was activated or not. Your superuser will have been created in the right place. 
TLDR: No need to worry, the superuser you created will most likely be in the right place.",0.0,0.38410443,0.14753621816635132
75,"Question\nOkay please do not close this and send me to a similar question because I have been looking for hours at similar questions with no luck.
Python can search for digits using re.search([0-9])
However, I want to search for any whole number. It could be 547 or 2 or 16589425. I don't know how many digits there are going to be in each whole number.
Furthermore I need it to specifically find and match numbers that are going to take a form similar to this: 1005.2.15 or 100.25.1 or 5.5.72 or 1102.170.24 etc.
It may be that there isn't a way to do this using re.search but any info on what identifier I could use would be amazing.\nAnswer: Assuming that you're looking for whole numbers only, try re.search(r""[0-9]+"")",0.0,0.2470125,0.06101517379283905
76,"Question\nI'm using Python with cx_Oracle, and I'm trying to do an INSERT....SELECT.   Some of the items in the SELECT portion are variable values.   I'm not quite sure how to accomplish this.   Do I bind those variables in the SELECT part, or just concatenate a string?

  v_insert = (""""""\
    INSERT INTO editor_trades
      SELECT "" + v_sequence + "", "" + issuer_id, UPPER("" + p_name + ""), "" + p_quarter + "", "" + p_year +
            "", date_traded, action, action_xref, SYSDATE
      FROM "" + p_broker.lower() + ""_tmp"") """""")

Many thanks!\nAnswer: With Oracle DB, binding only works for data, not for SQL statement text (like column names) so you have to do concatenation. Make sure to allow-list or filter the variables (v_sequence etc) so there is no possibility of SQL injection security attacks.  You probably don't need to use lower() on the table name, but that's not 100% clear to me since your quoting currently isn't valid.",0.0,0.32074863,0.10287968069314957
77,"Question\nI use Heroku to host my discord.py bot, and since I've started using sublime merge to push to GitHub (I use Heroku GitHub for it), Heroku hasn't been running the latest file. The newest release is on GitHub, but Heroku runs an older version. I don't think it's anything to do with sublime merge, but it might be. I've already tried making a new application, but same problem. Anyone know how to fix this?
Edit: I also tried running Heroku bash and running the python file again\nAnswer: 1) Try to deploy branch (maybe another branch)
2) Enable automatic deploy",0.40816328,0.20425889,0.04157700017094612
78,"Question\nSay I had a PostgreSQL table with 5-6 columns and a few hundred rows. Would it be more effective to use psycopg2 to load the entire table into my Python program and use Python to select the rows I want and order the rows as I desire? Or would it be more effective to use SQL to select the required rows, order them, and only load those specific rows into my Python program.
By 'effective' I mean in terms of:

Memory Usage.
Speed.

Additionally, how would these factors start to vary as the size of the table increases? Say, the table now has a few million rows?\nAnswer: Actually, if you are comparing data that is already loaded into memory to data being retrieved from a database, then the in-memory operations are often going to be faster.  Databases have overhead:

They are in separate processes on the same server or on a different server, so data and commands needs to move between them.
Queries need to be parsed and optimized.
Databases support multiple users, so other work may be going on using up resources.
Databases maintain ACID properties and data integrity, which can add additional overhead.

The first two of these in particular add overhead compared to equivalent in-memory operations for every query.
That doesn't mean that databases do not have advantages, particularly for complex queries:

They implement multiple different algorithms and have an optimizer to choose the best one.
They can take advantage of more resources -- particularly by running in parallel.
They can (sometimes) cache results saving lots of time.

The advantage of databases is not that they provide the best performance all the time.  The advantage is that they provide good performance across a very wide range of requests with a simple interface (even if you don't like SQL, I think you need to admit that it is simpler, more concise, and more flexible that writing code in a 3rd generation language).
In addition, databases protect data, via ACID properties and other mechanisms to support data integrity.",0.6122449,0.4806875,0.017307350412011147
79,"Question\nI am working on a code that is supposed to use a while loop to determine if the number inputted by the user is the same as the variable secret_number = 777.
the following criteria are: 
will ask the user to enter an integer number;
will use a while loop;
will check whether the number entered by the user is the same as the number picked by the magician. If the number chosen by the user is different than the magician's secret number, the user should see the message ""Ha ha! You're stuck in my loop!"" and be prompted to enter a number again. 
If the number entered by the user matches the number picked by the magician, the number should be printed to the screen, and the magician should say the following words: ""Well done, muggle! You are free now.""
if you also have any tips how to use the while loop that would be really helpful. Thank you!\nAnswer: You can use while(true) to create a while loop.
Inside, set a if/else to compare the value input and secret_number. If it's true, print(""Well done, muggle! You are free now."") and break. Unless, print(""Ha ha! You're stuck in my loop"") and continue",0.0,0.021162987,0.00044787200749851763
80,"Question\nthanks for reading this. I am using macOS High Sierra. I am not very familiar with terminal or environment variables, but am trying to learn more. From reading other threads and google, it seems like I either have multiple pythons installed, or have pythons running from different paths. However I am not able to find a solution to resolving this, either by re-pathing my IDLE or deleting it entirely. 
I do have python, python launcher, and anaconda (not very sure how anaconda works, have it installed a few years back and didn't touch it) installed. I am trying to install pandas (pip install pandas), which tells me that I have it installed, but when I run it on IDLE, it says module not found. Though if i run python3 on terminal and type my code in, it works (so pandas has indeed been installed).
When i run which python on terminal, it returns
/Users/myname/anaconda3/bin/python
(when i enter into this directory from terminal, it shows that in the bin folder, I have python, python.app, python3, python3-config, python3.7, python3.7-config, python3.7m, python3.7m-config)
When i run which idle on terminal, it returns
/usr/bin/idle (im not even sure how to find this directory from the terminal) 
When i run import os; print(os.path) on IDLE, it returns module 'posixpath' from '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py'
Would really appreciate some help to figure out how to ensure that when i install modules from terminal, it would be installed into the same python as the one IDLE is using. Also, I would like to know whether it is possible for me to work on VSCode instead of IDLE. I cant seem to find suitable extensions for data science and its related modules (like statsmodels, pandas etc). Thanks a lot!\nAnswer: First: This would be a comment if I had enough reputation.
Second: I would just delete python. Everything. And reinstall it.",0.13605443,0.16184342,0.0006650721188634634
81,"Question\nthanks for reading this. I am using macOS High Sierra. I am not very familiar with terminal or environment variables, but am trying to learn more. From reading other threads and google, it seems like I either have multiple pythons installed, or have pythons running from different paths. However I am not able to find a solution to resolving this, either by re-pathing my IDLE or deleting it entirely. 
I do have python, python launcher, and anaconda (not very sure how anaconda works, have it installed a few years back and didn't touch it) installed. I am trying to install pandas (pip install pandas), which tells me that I have it installed, but when I run it on IDLE, it says module not found. Though if i run python3 on terminal and type my code in, it works (so pandas has indeed been installed).
When i run which python on terminal, it returns
/Users/myname/anaconda3/bin/python
(when i enter into this directory from terminal, it shows that in the bin folder, I have python, python.app, python3, python3-config, python3.7, python3.7-config, python3.7m, python3.7m-config)
When i run which idle on terminal, it returns
/usr/bin/idle (im not even sure how to find this directory from the terminal) 
When i run import os; print(os.path) on IDLE, it returns module 'posixpath' from '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py'
Would really appreciate some help to figure out how to ensure that when i install modules from terminal, it would be installed into the same python as the one IDLE is using. Also, I would like to know whether it is possible for me to work on VSCode instead of IDLE. I cant seem to find suitable extensions for data science and its related modules (like statsmodels, pandas etc). Thanks a lot!\nAnswer: First of all, a quick description of anaconda:
Anaconda is meant to help you manage multiple python ""environments"", each one potentially having its own python version and installed packages (with their own respective versions). This is really useful in cases where you would",0.13605443,0.7282373,0.35068055987358093
82,"Question\nthanks for reading this. I am using macOS High Sierra. I am not very familiar with terminal or environment variables, but am trying to learn more. From reading other threads and google, it seems like I either have multiple pythons installed, or have pythons running from different paths. However I am not able to find a solution to resolving this, either by re-pathing my IDLE or deleting it entirely. 
I do have python, python launcher, and anaconda (not very sure how anaconda works, have it installed a few years back and didn't touch it) installed. I am trying to install pandas (pip install pandas), which tells me that I have it installed, but when I run it on IDLE, it says module not found. Though if i run python3 on terminal and type my code in, it works (so pandas has indeed been installed).
When i run which python on terminal, it returns
/Users/myname/anaconda3/bin/python
(when i enter into this directory from terminal, it shows that in the bin folder, I have python, python.app, python3, python3-config, python3.7, python3.7-config, python3.7m, python3.7m-config)
When i run which idle on terminal, it returns
/usr/bin/idle (im not even sure how to find this directory from the terminal) 
When i run import os; print(os.path) on IDLE, it returns module 'posixpath' from '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py'
Would really appreciate some help to figure out how to ensure that when i install modules from terminal, it would be installed into the same python as the one IDLE is using. Also, I would like to know whether it is possible for me to work on VSCode instead of IDLE. I cant seem to find suitable extensions for data science and its related modules (like statsmodels, pandas etc). Thanks a lot!\nAnswer: To repeat and summarized what has been said on various other question answers:
1a. 3rd party packages are installed for a particular python(3).exe binary.
1b. To install multiple packages to multiple binaries, see the option from python -m pip -",0.0,0.05166304,0.002669069916009903
83,"Question\nhello before I say anything I would like to let you know that I tried searching for the answer but I found nothing.
whenever I use os.system('cls') it clears the screen but it prints out a zero.
is this normal, if not how do I stop it from doing that?\nAnswer: I guess you running in inside an interpreter
os.system will return:

a 16-bit number, whose low byte is the signal number that killed the process, and whose high byte is the exit status (if the signal number is zero)

So it just print the value it got, the return value of the command cls in the command line, which is 0 cause the command run successfully",0.20408164,0.39589727,0.03679323568940163
84,"Question\nI have a workspace setup in VS Code where I do python development.  I have linting enabled, pylint enabled as the provider, and lint on save enabled, but I continue to see no errors in the Problems panel. When I run pylint via the command line in the virtual environment i see a bunch of issues - so I know pylint works.  I am also using black formatting(on save) which works without issue.  I have tried using both the default pylint path as well as updating it manually to the exact location and still no results.  When I look at the Output panel for python it looks like pylint is never even running (i.e. I see the commands for black running there but nothing for pylint).
My pylint version is 2.4.4 and VS Code version 1.46
Any idea how to get this working?\nAnswer: Uninstall Python Extension
Reinstall Python Extension
And with that there will will be one more extension of ""Python Extension"" named - ""PYLANCE"" don't forget to install that too.
Reload VS Code

DONE!!",0.0,0.08524108,0.007266041822731495
85,"Question\nThis is my first time on stack overflow. I am a beginner python coder and I use the Atom text editor. I am currently learning from a book called PythonCrashCourse by Eric Matthes (second edition) and is developing a practice-project called Alien Invasion. I am currently stuck on saving a file of a spaceship image into a folder named ""images"" within my text editor. I have an ASUS chromebook. The file I am trying to save is called ship.bmp and the book instructions say ""Make a folder called images inside your main alien_invasion project folder. Save the file ship.bmp in the images folder."" I have the ship.bmp file saved but I just don't know how to transport it into a file within my text editor ""images"" folder. I have been stuck on this for quite a while and I would really appreciate it if someone could give me some advice. Thanks!\nAnswer: First of all you need to have the ship.bmp file downloaded somewhere on your computer. You then would need to move it into your project folder. I think that the easiest way for you to navigate through the files you have is to go to your ""Files"" app in the Chromebook. You should look through your Downloads folder for the ship.bmp after you download it and manually move it into the project folder that you are working on. You should be able to open your project folder and place the ship.bmp file inside the ""images"" folder.",0.0,0.31363833,0.09836900234222412
86,"Question\nI am aware of the following:

[1,2,3]<[1,2,4] is True because Python does an element-wise comparison from left to right and 3 < 4
[1,2,3]<[1,3,4] is True because 2 < 3 so Python never even bothers to compare 3 and 4

My question is how does Python's behavior change when I compare two lists of unequal length?  

[1,2,3]<[1,2,3,0] is True
[1,2,3]<[1,2,3,4] is True

This led me to believe that the longer list is always greater than the shorter list. But then:

[1,2,3]<[0,0,0,0] is False

Can someone please explain how these comparisons are being done by Python? 
My hunch is that element-wise comparisons are first attempted and only if the first n elements are the same in both lists (where n is the number of elements in the shorter list) does Python consider the longer list to be greater. If someone could kindly confirm this or shed some light on the reason for this behavior, I'd be grateful.\nAnswer: The standard comparisons (<, <=, >, >=, ==,!=, in, not in ) work exactly the same among lists, tuples and strings. 
The lists are compared element by element.
If they are of variable length, it happens till the last element of the shorter list
If they are same from start to the length of the smaller one, the length is compared i.e. shorter is smaller",0.40816328,0.466387,0.003390002064406872
87,"Question\nI need to install older versions of tensorflow to get the deepface library to work properly, however whenever I run pip install tensorflow==1.9.0, I get: ERROR: Could not find a version that satisfies the requirement tensorflow==1.9.0 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0)
Anyone else run into this issue/know how to fix it? Thanks!\nAnswer: You can install TensorFlow 1.9.0 with the following Python versions: 2.7 and 3.4 to 3.6.",0.81632656,0.5074652,0.09539534896612167
88,"Question\nI want to remove the non-ASCII Character '\xa0' while reading my CSV file using read_csv into a dataframe with python. Can someone tell me how to achieve this?\nAnswer: You can use x = txt.replace(u'\xa0', u'') for text you're reading.",0.0,0.22007167,0.04843154177069664
89,"Question\nI am using selenium python and was wondering how one effectively scrapes over 50,000 data points in under 24 hours. For example, when I search for products on the webpage 'insight.com' it takes about 3.5 seconds for the scraper to search for the product and grab its price, meaning that with large amounts of data it takes the scraper several days. A part from using threads to simultaneously look up several products at the same time, how else can I speed up this process?
I only have one laptop and will have to simultaneously scrape six other similar websites so therefore do not want too many threads and the speed at which the computer operates will slow down significantly.
How do people achieve to scrape large amounts of data in such short periods of time?\nAnswer: If you stop using the selenium module, and rather work with a much more sleek and elegant module, like requests, you could get the job done in a matter of mere minutes.
If you manage to reverse engineer the requests being handled, and send them yourself, you could pair this with threading to scrape at some 50 'data points' per second, more or less (depending on some factors, like processing and internet connection speed).",0.40816328,0.12833583,0.07830339670181274
90,"Question\nI am using selenium python and was wondering how one effectively scrapes over 50,000 data points in under 24 hours. For example, when I search for products on the webpage 'insight.com' it takes about 3.5 seconds for the scraper to search for the product and grab its price, meaning that with large amounts of data it takes the scraper several days. A part from using threads to simultaneously look up several products at the same time, how else can I speed up this process?
I only have one laptop and will have to simultaneously scrape six other similar websites so therefore do not want too many threads and the speed at which the computer operates will slow down significantly.
How do people achieve to scrape large amounts of data in such short periods of time?\nAnswer: Find an API and use that!!!  The goal of both web scraping and APIs is to access web data.
Web scraping allows you to extract data from any website through the use of web scraping software. On the other hand, APIs give you direct access to the data you’d want.
As a result, you might find yourself in a scenario where there might not be an API to access the data you want, or the access to the API might be too limited or expensive.
In these scenarios, web scraping would allow you to access the data as long as it is available on a website.
For example, you could use a web scraper to extract product data information from Amazon since they do not provide an API for you to access this data.  However, if you had access to an API, you could grab all the data you want, super, super, super fast!!!  It's analogous to doing a query in a database on prem, which is very fast and very efficient, vs. refreshing a webpage, waiting for ALL elements to load, and you can't use the data until all elements have been loaded, and then.....do what you need to do.",0.20408164,0.278395,0.005522475112229586
91,"Question\nBackground:
I have multiple asset tables stored in a redshift database for each city, 8 cities in total. These asset tables display status updates on an hourly basis. 8 SQL tables and about 500 mil rows of data in a year.
(I also have access to the server that updates this data every minute.)

Example: One market can have 20k assets displaying 480k (20k*24 hrs) status updates a day.

These status updates are in a raw format and need to undergo a transformation process that is currently written in a SQL view. The end state is going into our BI tool (Tableau) for external stakeholders to look at.
Problem:
The current way the data is processed is slow and inefficient, and probably not realistic to run this job on an hourly basis in Tableau. The status transformation requires that I look back at 30 days of data, so I do need to look back at the history throughout the query.
Possible Solutions:
Here are some solutions that I think might work, I would like to get feedback on what makes the most sense in my situation.

Run a python script that looks at the most recent update and query the large history table 30 days as a cron job and send the result to a table in the redshift database.
Materialize the SQL view and run an incremental refresh every hour
Put the view in Tableau as a datasource and run an incremental refresh every hour

Please let me know how you would approach this problem. My knowledge is in SQL, limited Data Engineering experience, Tableau (Prep & Desktop) and scripting in Python or R.\nAnswer: So first things first - you say that the data processing is ""slow and inefficient"" and ask how to efficiently query a large database.  First I'd look at how to improve this process.  You indicate that the process is based on the past 30 days of data - is the large tables time sorted, vacuumed and analyzed?  It is important to take maximum advantage of metadata when working with large tables.  Make sure your where clauses are effective at eliminating fact table block - don't rely on dimension table where clauses to select the date range.
Next look at your distribution keys and how these are impacting the need for your critical query to move large amounts of data across the network.  The internode network has the lowest bandwidth in a Redshift cluster and needlessly pushing lots of data across it will make things slow and inefficient.",0.81632656,0.26203632,0.3072376549243927
92,"Question\nI now have an HTML file and I want to send it as a table, not an attachment by using outlook. The code that I found online only sends the file as an attachment. Can anyone give me ideas on how to do it properly?\nAnswer: You can use the HTMLBody property of the MailItem class to set up the message body.",0.0,0.09057391,0.008203632198274136
93,"Question\nSo I've made a script/code in python idle and want to run it on python.exe but whenever I do this the you can see the python window pop up briefly for a second before closing, and I want to run my code using python instead of idle, how can I do this?\nAnswer: since I cant comment yet:
go to the command line and open the file location directory and type:   python filename.py",0.20408164,0.36583957,0.02616562880575657
94,"Question\nI have been trying to do web automation using selenium,Is there any way to use browser like chrome,firefox without actually installing then, like using some alternate options, or having portable versions of them.If I can use portable versions how do i tell selenium to use it?\nAnswer: If you install pip install selenium
it comes with the protable chrome browser, no need to install any browser for this.
the chrome has a tag ""chrome is controlled by automated test software"" near search bar",0.0,0.4364635,0.1905003935098648
95,"Question\nIf I have a menu with too many items to fit on the screen, how do I get one of those'more' buttons with a downward arrow at the bottom of the menu? Is that supported?\nAnswer: I solved my problem with cascading menus. I already had some, but I didn't want to use more for these particular menus items—but after closer inspection, I think it's better this way.
I'm still interested in other solutions, for scenarios where cascading menus are not a practical option, however (like if the screen is too narrow to cascade that far or something). So, I don't plan to mark this as the accepted answer anytime soon (even though in most circumstances, it's probably the best solution).",-0.35714287,0.18797171,0.2971498966217041
96,"Question\nI have an PyQt5 application to update database collections one by one using QThread and send updation signal to main thread as each collection gets updated to reflect it on GUI. It runs continuously 24X7. But somehow the data stops getting updated and also GUI stops getting signals. But the application is still running as other part are accessible and functioning properly. Also no errors are found in log file.
Mostly the application runs fine but after some random period this problem arises(first time after approximately a month, then after 2 weeks and now after 23 days). However restarting the application solves the problem.
I tried using isRunning() method and isFinished() method but no change found.
Can anyone tell what is the problem?? Thank you in advance.
Also tell how to check weather the QThread is stuck or killed?\nAnswer: If any exception occur in the thread, then thread can be finished soon.
so You should use settimeout function to calling any third party library(data update) in the thread.
That will solve your problem.",0.0,0.14393175,0.0207163468003273
97,"Question\nI am running a Flask API application, and I have an SSL Certificate.
When I run flask server on localhost the certificate is applied from Flask successfully.
But when I use Ngrok to deploy the localhost on a custom domain, the certificate is changed to *.ngrok.com, how can I change that to my certificate?.
EDIT #1:
I already have a certificate for the new hostname and I have already applied it on Flask, but ngrok is changing it.\nAnswer: You expose your service through the URL *.ngrok.com. A browser or other client will make a request to *.ngrok.com. The certificate presented there must be valid for *.ngrok.com. If *.ngrok.com presents a certificate for example.com, any valid HTTPS client would reject it because the names do not match, which by definition makes it an invalid certificate and is a flag for a potential security problem, exactly what HTTPS is designed to mitigate.
If you want to present your certificate for example.com to the client, you need to actually host your site at example.com",0.0,0.3309254,0.10951162129640579
98,"Question\nI am making a simple Python utility that shows the tempo of a song (BPM) that is playing. I record short fragments of a few seconds to calculate the tempo over. The problem is that now I want to show this on a display using a Pygame UI, but when I'm recording sound, the UI does not respond. I want to make it so that the UI will stay responsive during the recording of the sound, and then update the value on the screen once the tempo over a new fragment has been calculated. How can I implement this?
I have looked at threading but I'm not sure this is the appropriate solution for this.\nAnswer: I'd use the python threading library.
Use the pygame module in the main thread (just the normal python shell, effectively) an create a separate thread for the function that determines BPM. 
This BPM can then be saved to a global variable that can be accessed by PyGame for displaying.",0.40816328,0.31552064,0.008582658134400845
99,"Question\nI am in trouble to understand Word2Vec. I need to do a help desk text classification, based on what users complain in the help desk system. Each sentence has its own class.
I've seen some pre-trained word2vec files in the internet, but I don't know if is the best way to work since my problem is very specific. And my dataset is in Portuguese.
I'm considering that I will have to create my own model and I am in doubt on how to do that. Do I have to do it with the same words as the dataset I have with my sentences and classes?
In the frst line, the column titles. Below the first line, I have the sentence and the class. Could anyone help me? I saw Gensin to create vector models, and sounds me good. But I am completely lost. 

: chamado,classe 'Prezados não estou conseguindo gerar uma nota fiscal
  do módulo de estoque e custos.','ERP GESTÃO', 'Não consigo acessar o
  ERP com meu usuário e senha.','ERP GESTÃO', 'Médico não consegue gerar
  receituário no módulo de Medicina e segurança do trabalho.','ERP
  GESTÃO', 'O produto 4589658 tinta holográfica não está disponível no
  EIC e não consigo gerar a PO.','ERP GESTÃO',\nAnswer: Your inquiry is very general, and normally StackOverflow will be more able to help when you've tried specific things, and hit specific problems - so that you can provide exact code, errors, or shortfalls to ask about. 
But in general:

You might not need word2vec at all: there are many text-classification approaches that, with sufficient training data, may assign your texts to helpful classes without using word-vectors. You will likely want to try those first, then consider word-vectors as a later improvement.
For word-vectors to be helpful, they need to be based on your actual language, and also ideally your particular domain-of-concern. Generic word-vectors from news articles",0.0,0.7249801,0.5255961418151855
0,"Question\nI want to create a qwidgets one with raised/sunkin/groove/ridge relief similar to tkinter. I know how to do this in tkinter, but don't know the style sheet option in Pyqt5 for each one. Please find the tkinter option
Widget = Tkinter.Button(top, text =""FLAT"", relief=raised ). Hope you can help to translate to Pyqt5\nAnswer: You can do this with QFrame. you can set QFrame.setFrameShadow(QFrame.Sunken). But I couldn't find for a QWidget one.",0.0,0.23381704,0.05467040836811066
1,"Question\nI am interested in using the tensor cores from NVIDIA RTX GPUs in python to benefit from its speed up in some scientific computations. Numba is a great library that allows programming kernels for cuda, but I have not found how to use the tensor cores. Can it be done with Numba? If not, what should I use?\nAnswer:.... I have not found how to use the tensor cores. Can it be done with Numba?

No. Numba presently doesn't have half precision support or tensor core intrinsic functions available in device code.

If not, what should I use?

I think you are going to be stuck with writing kernel code in the native C++ dialect and then using something like PyCUDA to run device code compiled from that C++ dialect.",0.81632656,0.26263416,0.3065752685070038
2,"Question\ncan i run multiple python http servers on one machine to receive http post request from a webpage?
currently i am running an http server on port 80 and on the web page there is a HTML form which sends the http post request to the python server and in the HTML form i am using the my server's address like this : ""http://123.123.123.123"" and i am receiving the requests
but i want to run multiple servers on the same machine with different ports for  each server.
if i run 2 more servers on port 21200 and 21300 how do i send the post request from the HTML form on a specified port, so that the post request is received and processed by correct server??
do i need to define the server address like this : ""http://123.123.123.123:21200"" and ""http://123.123.123.123:21300""?\nAnswer: Yes can run multiple webservers on one machine.
use following commands to run on different ports:
python3 -m http.server 4000
4000 is the port number, you can replace it with any port number here.",1.0,0.13304818,0.7516054511070251
3,"Question\nI just finished my app and made a release version with buildozer and signed it but when I tried to upload my apk file to Google Play Console...It said that the API level of the app was 27 and it should be level 28. So how can I do this?
Thanks in advance\nAnswer: Find the line that says android.api = 27 in your buildozer.spec file and change it to 28.",0.0,0.31702632,0.10050568729639053
4,"Question\nI just finished my app and made a release version with buildozer and signed it but when I tried to upload my apk file to Google Play Console...It said that the API level of the app was 27 and it should be level 28. So how can I do this?
Thanks in advance\nAnswer: It should be edited in buildozer.spec file.
If you scroll down it's default to 27, change it to specification",0.0,0.094056845,0.008846689946949482
5,"Question\nMy question is very simple, as you read the title I want plugin similar to GitLens that I found in vscode. As you know with GitLens you can easily see the difference between two or multiple commits. I searched it up and I found GitToolBox but I don't know how to install it as well and I don't think that's like GitLens...\nAnswer: Open Settings on jetbrains IDE.
Go to plugins and look for git toolbox.
Install it and boom, its done!",0.0,0.23889321,0.057069964706897736
6,"Question\nI would like to know how to perform the below mentioned task
I want to upload a CSV file to a python script 1, then send file's path to another python script in file same folder which will perform the task and send the results to python script 1.
A working code will be very helpful or any suggestion is also helpful.\nAnswer: You can import the script editing the CSV to the python file and then do some sort of loop that edits the CSV file with your script 1 then does whatever else you want to do with script 2.
This is an advantage of OOP, makes these sorts of tasks very easy as you have functions set in a module python file and can create a main python file and run a bunch of functions editing CSV files this way.",0.0,0.04665059,0.0021762773394584656
7,"Question\nim new to python and i was trying to install ""time"" library on python, i typed
pip install time
but the compiler said this
C:\Users\Giuseppe\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Python 3.6>pip install time ERROR: Could not find a version that satisfies the requirement time (from versions: none) ERROR: No matching distribution found for time
i dont know how to resolve, can anyone help me? please be the more simple u can cause im not too good in py, as i said im new, thx to everyone!
P.S.
the py version is 3.6
thx everyone, im stupid xd\nAnswer: Time is a module that comes built-in with python so no need to install anything, just import it :
import time",0.13605443,-0.28606164,0.1781819760799408
8,"Question\nim just trying to use an vpn extension with selenium. I have the extension running, but i need to click in the button and enable the vpn so it can works, there's a way to do that with selenium? im thinking to use another similar option like scrapy or pyautogui...\nAnswer: No there is no way to enable the VPN on your extension
If you want to use your VPN extension you have to set a profile (otherwise selenium will create a new profile without installed extension)",0.0,0.2416048,0.05837288126349449
9,"Question\nLong story short, I messed with my Python environment too much (moving files around, creating new folders, trying to reinstall packages, deleting files etc.) My google package doesn't work anymore. Everytime I try to import the package, it says it can't find the module, even though I did a pip install.
I was wondering how I could do a hard reset/delete python off my computer and reinstall it.
Thanks.\nAnswer: I figured it out. My pip was installing to a site packages folder inside a local folder, while my jupyter notebook was trying to pull from the anaconda site packages folder.",0.0,-0.0017203689,2.9596690183097962e-06
10,"Question\nI often delete code in Colab, by accident, and for some reason when I try to do undo code deletion it does not work. So basically when I do this I want to get my cells back somehow. Is there any way to do this, like take a look at the code that Colab is running, because my cells are probably still there. Another option would be to somehow see cells that have been previously deleted. Please help me. Any other solutions would be nice.\nAnswer: You can undo deleting cell in google colab simply by typing ctrl + M Z",0.20408164,0.84574103,0.41172677278518677
11,"Question\nI have a string where I am trying to replace [""{\"" with [{"" and all \"" with "".
I am struggling to find the right syntax in order to do this, does anyone have a solid understanding of how to do this?
I am working with JSON, and I am inserting a string into the JSON properties. This caused it to put a single quotes around my inserted data from my variable, and I need those single quotes gone. I tried to do json.dumps() on the data and do a string replace, but it does not work.
Any help is appreciated. Thank you.\nAnswer: I would recommend maybe posting more of your code below so we can suggest a better answer. Just based on the information you have provided, I would say that what you are looking for are escape characters. I may be able to help more once you provide us with more info!",0.0,0.20465976,0.0418856181204319
12,"Question\nI have a string where I am trying to replace [""{\"" with [{"" and all \"" with "".
I am struggling to find the right syntax in order to do this, does anyone have a solid understanding of how to do this?
I am working with JSON, and I am inserting a string into the JSON properties. This caused it to put a single quotes around my inserted data from my variable, and I need those single quotes gone. I tried to do json.dumps() on the data and do a string replace, but it does not work.
Any help is appreciated. Thank you.\nAnswer: if its two characters you want to replace then you have to first check for first character and then the second(which should be present just after the first one and so on) and shift(shorten the whole array by 3 elements in first case whenever the condition is satisfied and in the second case delete \ from the array.
You can also find particular substring by using inbuilt function and replace it  by using replace() function to insert the string you want in its place",0.0,0.08798617,0.007741566281765699
13,"Question\nI've seen similar questions but it's shocking that I didn't see the answer I was, in fact, looking for. So here they are, both the question and the answer:
Q:
How to calculate simply the percentage in Python.
Say you need a tax calculator. To put it very simple, the tax is 18% of earnings.
So how much tax do I have to pay if I earn, say, 18 342? The answer in math is that you divide by 100 and multiply the result by 18 (or  multiply with 18 divided by 100). But how do you put that  in code?
tax = earnings / 100 * 18
Would that be quite right?\nAnswer: The answer that best fitted me, especially as it implied no import, was this:
tax = earnings * 0.18
so if I earned 18 342, and the tax was 18%, I should write:
tax = 18 342 * 0.18
which would result in 3 301.56
This seems trivial, I know, and probably some code was expected, moreover this form might be applicable not only in Python, but again, I didn't see the answer anywhere and I thought that it is, after all, the simplest.",0.0,0.15186852,0.02306404709815979
14,"Question\nI want to find out there is a p4 command that can find cl submitted in a depot branch from a cl submitted in another depot branch.
like -
if CL 123 was submitted to branch //code/v1.0/files/...
and same code changes were also submitted to another branch //code/v5.0/files/...
can i find out cl in 2nd branch from cl 123?\nAnswer: There are a few different methods; which one is easiest will depend on the exact context/requirements of what you're doing.
If you're interested in the specific lines of code rather than the metadata, p4 annotate is the best way.  Use p4 describe 123 to see the lines of code changed in 123, and then p4 annotate -c v5.0/(file) to locate the same lines of code in v5.0 and see which changelist(s) introduced them into that branch.  This method will work even if the changes were copied over manually instead of using Perforce's merge commands.
If you want to track the integration history (i.e. the metadata) rather than the exact lines of code (which may have been edited in the course of being merged between codelines, making the annotate method not work), the easiest method is to use the Revision Graph tool in P4V, which lets you visually inspect a file's branching history; you can select the revision from change 123 and use the ""highlight ancestors and descendants"" tool to see which revisions/changelists it is connected to in other codelines.  This makes it easy to see the context of how many integration steps were involved, who did them, when they happened, whether there were edits in between, etc.
If you want to use the metadata but you're trying for a more automated solution, changes -i is a good tool.  This will show you which changelists are included in another changelist via integration, so you can do p4 changes -i @123,123 to see the list of all the changes that contributed to change 123.  On the other side (finding changelists in v5.0 that 123 contributed to), you could do this iteratively; run p4 changes -i @N,N for each changelist N in the v5.0 codeline, and see which of them include 123 in the output (it may be more than one).",0.81632656,0.31627727,0.25004929304122925
15,"Question\nI am learning how to use Alpha_Vantage api and came across this line of code. I do not understand what is the purpose of [0].
SATS = ts.get_daily('S58.SI', outputsize = ""full"")[0]\nAnswer: ts.get_daily() appears to return an array.
SATS is getting the 0 index of the array (first item in the array)",0.0,0.27577424,0.07605142891407013
16,"Question\nI trained a deep learning-based detection network to detect and locate some objects. I also trained a deep learning-based classification network to classify the color of the detected objects. Now I want to combine these two networks to detect the object and also classify color. I have some problems with combining these two networks and running them together. How do I call classification while running detection?
They are in two different frameworks: the classifier is based on the Keras and TensorFlow backend, the detection is based on opencv DNN module.\nAnswer: I have read your question and from that, I can infer that your classification network takes the input from the output of your first network(object locator). i.e the located object from your first network is passed to the second network which in turn classifies them into different colors. The entire Pipeline you are using seems to be a sequential one. Your best bet is to first supply input to the first network, get its output, apply some trigger to activate the second network, feed the output of the first net into the second net, and lastly get the output of the second net. You can run both of these networks in separate GPUs.
The Trigger that calls the second function can be something as simple as cropping the located object in local storage and have a function running that checks for any changes in the file structure(adding a new file). If this function returns true you can grab that cropped object and run the network with this image as input.",0.0,0.21703255,0.04710312932729721
17,"Question\nI used auto-py-to-exe to convert a Python script into an executable file and it converts it to an executable without any problems, but when I launch the executable the following error happens:
ModuleNotFoundError: No module named 'pandas'
[11084] Failed to execute script test1
Any ideas on how to fix this? I've tried many libraries to convert the Python file to and Executable and all give me the same error. I've tried with cx_Freeze, PyInstaller, py2exe, and auto-py-to-exe. All give me a ModuleNotFoundError, but when I run the script on the IDE it runs perfectly.\nAnswer: Are you trying pip install pandas?",0.27210885,0.032404423,0.057458214461803436
18,"Question\nI used auto-py-to-exe to convert a Python script into an executable file and it converts it to an executable without any problems, but when I launch the executable the following error happens:
ModuleNotFoundError: No module named 'pandas'
[11084] Failed to execute script test1
Any ideas on how to fix this? I've tried many libraries to convert the Python file to and Executable and all give me the same error. I've tried with cx_Freeze, PyInstaller, py2exe, and auto-py-to-exe. All give me a ModuleNotFoundError, but when I run the script on the IDE it runs perfectly.\nAnswer: A script that runs in your IDE but not outside may mean you are actually working in a virtual environment. Pandas probably is not installed globally in your system. Try remembering if you had created a virtual environment and then installed pandas inside this virtual environment.
Hope it helped,
Vijay.",0.27210885,0.112077236,0.025610119104385376
19,"Question\nI used auto-py-to-exe to convert a Python script into an executable file and it converts it to an executable without any problems, but when I launch the executable the following error happens:
ModuleNotFoundError: No module named 'pandas'
[11084] Failed to execute script test1
Any ideas on how to fix this? I've tried many libraries to convert the Python file to and Executable and all give me the same error. I've tried with cx_Freeze, PyInstaller, py2exe, and auto-py-to-exe. All give me a ModuleNotFoundError, but when I run the script on the IDE it runs perfectly.\nAnswer: For cx_freeze, inlcude pandas explicitly in the packages. Like in the example below -
build_exe_options = {'packages': ['os', 'tkinter', 'pandas']}
This should include the pandas module in you build.",0.13605443,0.24068642,0.010947853326797485
20,"Question\nCurrently, I have been scouring the internet for a code that will either add this program (something.exe) to the windows task scheduler or if that is not even an option how to add it to the windows reg key for a startup. I cannot find anything in terms of Python3, and I really hope it is not an answer that is right in front of my face. Thanks!\nAnswer: Open the windows scheduler -> select ""create basic task"" -> fill out the desired times -> input the path to the script you want to execute.",0.0,0.17522162,0.0307026170194149
21,"Question\nI am trying to use sunnyportal-py. Relatively new to python, I do not understand step 2 in the README:
How to run

Clone or download the repository.
Enter the directory and run:
PYTHONPATH=../bin/sunnyportal2pvoutput --dry-run sunnyportal.config
Enter the requested information and verify that the script is able to connect to Sunny Portal.
The information is saved in sunnyportal.config and can be edited/deleted if you misstype anything.
Once it works, replace --dry-run with e.g. --output to upload the last seven days output data to pvoutput or --status to upload data for the current day.
Add --quiet to silence the output.

Could anyone help me? I have gone into a cmd.exe in the folder I have downloaded, I don't know how to correctly write the python path in the correct location. What should I paste into the command line? Thanks!
Edit : I would like to be able to do this on Windows, do tell me if this is possible.\nAnswer: The command at bullet 2 is to be typed at the commandline (You need to be in windows: cmd or powershell, Linux: bash, etc.. to be able to do this).

PYTHONPATH=../bin/sunnyportal2pvoutput --dry-run sunnyportal.config

The first part of the command code above indicates where your program is located. Go to the specific folder via commandline (windows: cd:... ; where.. is your foldername) and type the command.
The second part is the command to be executed. Its behind the ""--"" dashes. The program knows what to do. In this case:

--dry-run sunnyportal.config

running a validation/config file to see if the program code itself works; as indicated by ""dry run"".
In your case type at the location (while in cmd):

""sunnyportal2pvoutput --dry-run sunnyportal.config""

or

""sunnyportal2pvoutput.py --dry-run sunnyportal.config"" (without the environment variables (python path) set).

Note: the pythonpath is an environment variable. This can be added",0.0,-0.35303676,0.12463495135307312
22,"Question\nI am trying to run a contionus azure webjob for python.
i have 6 files where main.py is the main file, other files internally importing each other and finally everything is being called from main.py, now when i am trying to run only the first python file is getting executed, but i want that when the webjob will start only main.py will be executed not anything else. how to achieve that?\nAnswer: This is quite simple. In azure webjob, if the file name starts with run, then this file has the highest priority to execute.
So the most easiest way is just renaming the main.py to run.py.
Or add an run.py, then call the main.py within it.",0.40816328,0.28794527,0.014452369883656502
23,"Question\nI am trying to download youtube videos using python and for the code to work I need to install pytube3 library but I am very new to coding so I am not sure how to do it.\nAnswer: You could use
python3 -m pip install pytube3",0.13605443,0.089820564,0.0021375699434429407
24,"Question\nGiven any image of a scanned document, I want to check if it's not empty page.
I know I can send it to AWS Textract - but it will cost money for nothing.
I know I can use pytesseract but maybe there is more elegant and simple solution?
Or given a.html file that represents the text of the image - how to check it shows a blank page?\nAnswer: We can use pytesseract for this application by thresholding the image and passing it to tesseract. However if you have a.html file that represents text of image, you can use beautifulsoup for extracting text from it and check if it is empty.Still this is a round way approach.",0.20408164,0.23483807,0.0009459579014219344
25,"Question\nScenario
Hi, I have a collection of APIs that I run on Postman using POST requests. The flask and redis servers are set up using docker.
What I'm trying to do
I need to profile my setup/APIs in a high traffic environment. So,

I need to create concurrent requests calling these APIs

The profiling aims to get the system conditions with respect to memory (total memory consumed by the application), time (total execution time taken to create and execute the requests) and CPU-time (or the percentage of CPU consumption)


What I have tried
I am familiar with some memory profilers like mprof and time profiler like line_profiler. But I could not get a profiler for the CPU consumption. I have run the above two profilers (mprof and line_profiler) on a single execution to get the line-by-line profiling results for my code. But this focuses on the function wise results.I have also created parallel requests earlier using asyncio,etc but that was for some simple API-like programs without POSTMAN. My current APIs work with a lot of data in the body section of POSTMAN
Where did I get stuck
With docker, this problem gets trickier for me.

Firstly, I am unable to get concurrent requests

I do not know how to profile my APIs when using POSTMAN (perhaps there is an option to do it without POSTMAN) with respect to the three parameters: time, memory and CPU consumption.\nAnswer: I suppose that you've been using the embbed flask server(dev server) that is NOT production ready and,by default, it supports only on request per time. For concurrent requests should be looking to use gunicorn or some other wsgi server like uWsgi.
Postman is only a client of your API, i don't see it's importance here. If you want to do a stress test or somethin like that, you can write your own script or use known tools, like jmetter.
Hope it helps!",0.0,0.22525436,0.05073952674865723
26,"Question\nI am using chatterbot, I want to send clickable link and Mail as per message sent by the user. I cant find any link or reference on how to do this\nAnswer: Try using linkify.... pip install autolink... linkify (bot.get_response(usr_text))",0.0,0.3880936,0.1506166309118271
27,"Question\nIve got two separate models, one for mask recognition and another for face recognition. The problem now is that how do I combine these both models so that it performs in unison as  a single model which is able to :-

Recognize whether or not a person is wearing mask
Simultaneously recognize who that person is if he isn't wearing mask apart from warning about no mask.

What are the possibilities I have to solve this problem!!\nAnswer: You don't have to combine the both models and train them you have to train them seprately. And after training the model first you have to check with the mask detection model what's the probability/confidence score that there's a mask detected and if the probability is low say like 40%-45% then you have to use the other model that recognises the person.",0.20408164,0.03596902,0.028261853381991386
28,"Question\nI have been attempting to create a model that given an image, can read the text from it. I am attempting to do this by implementing a cnn, rnn, and ctc. I am doing this with TensorFlow and Keras. There are a couple of things I am confused about. For reading single digits, I understand that your last layer in the model should have 9 nodes, since those are the options. However, for reading words, aren't there infinitely many options, so how many nodes should I have in my last layer. Also, I am confused as to how I should add my ctc to my Keras model. Is it as a loss function?\nAnswer: I see two options here:

You can construct your model to recognize separate letters of those words, then there are as many nodes in the last layer as there are letters and symbols in the alphabet that your model will read.
You can make output of your model as a vector and then ""decode"" this vector using some other tool that can encode/decode words as vectors. One such tool I can think of is word2vec. Or there's an option to download some database of possible words and create such a tool yourself.
Description of your model is very vague. If you want to get more specific help, then you should provide more info, e.g. some model architecture.",0.0,0.2882787,0.08310461044311523
29,"Question\nI decided to ask this here after googling for hours. I want to create my own API endpoint on my own server.
Essentially I want to be able to just send a yaml payload to my server, when received I want to kick off my python scripts to parse the data and update the database. I'd also like to be able to retrieve data with a different call. I can code the back-end stuff, I just don't know how to make that bridge between hitting the server from outside and having the server do the things in the back-end in python.
Is django the right way? I've spent a couple days doing Django tutorials, really cool stuff, but I don't really need a website right now but whenever I search for web and python together, django pretty much always comes up. I don't need any python code help, just some direction on how to create that bridge.
Thanks.\nAnswer: DRF was what I was looking for. As suggested.",0.0,0.093413115,0.008726010099053383
30,"Question\nWhen reading events from a simple button in PySimpleGui, spamming this button with mouseclicks will generate an event for each of the clicks.
When you try to do the same with Listboxes (by setting enable_events to True for this element) it seems like there is a timeout after each generated event. If you click once every second, it will generate all the events. But if you spam-click it like before it will only generate the first event.
I'm not sure if this behavior is intended (only started learning PySimpleGui today), but is there a way to get rid of this delay? I tried checking the docs but can't find it mentioned anywhere.\nAnswer: I think the reason is that a Listbox reacts to click events, but also to double click events. A Button does not. This behavior looks like consistent.",0.0,0.30797434,0.09484819322824478
31,"Question\nI'm wondering how would I run my 2 discord bots at once from main, app.py, file.
And after I kill that process (main file process), they both would stop.
Tried os.system, didn't work. Tried multiple subprocess.Popen, didn't work.
Am I doing something wrong?
How would I do that?\nAnswer: I think the good design is to have one bot per.py file. If they both need code that is in app.py then they should 'import' the common code. Doing that you can just run both bot1.py and bot2.py.",0.0,0.26563287,0.07056082040071487
32,"Question\nI have tried all of the things here on stack and on other sites with no joy...
I'd appreciate any suggestions please.
I have installed Jupyter and Notebook using pip3 - please note that I have updated pip3 before doing so.
However when trying to check the version of both jupyter --version and notebook --version my terminal is returning no command found. I have also tried to run jupyter, notebook and jupyter notebook and I am still getting the same message.
I have spent nearly two days now trying to sort this out... I'm on the verge of giving up.
I have a feeling it has something to do with my PATH variable maybe not pointing to where the jupyter executable is stored but I don't know how to find out where notebook and jupyter are stored on my system.
many thanks in advance
Bobby\nAnswer: have you tried  locate Jupiter? It may tell you where jupyter is on your system.
Also, why not try installing jupyter via anaconda to avoid the hassle?",0.08163265,0.051741123,0.0008935034857131541
33,"Question\nI have tried all of the things here on stack and on other sites with no joy...
I'd appreciate any suggestions please.
I have installed Jupyter and Notebook using pip3 - please note that I have updated pip3 before doing so.
However when trying to check the version of both jupyter --version and notebook --version my terminal is returning no command found. I have also tried to run jupyter, notebook and jupyter notebook and I am still getting the same message.
I have spent nearly two days now trying to sort this out... I'm on the verge of giving up.
I have a feeling it has something to do with my PATH variable maybe not pointing to where the jupyter executable is stored but I don't know how to find out where notebook and jupyter are stored on my system.
many thanks in advance
Bobby\nAnswer: So to summarise this is what I have found on this issue (in my experience):
to run the jupyter app you can use the jupyter-notebook command and this works,  but why? This is because, the jupyter-notebook is stored in usr/local/bin which is normally always stored in the PATH variable.
I then discovered that the jupyter notebook or jupyter --version command will now work if I did the following:

open my./bash_profile file
add the following to the bottom of the file: export PATH=$PATH:/Users/your-home-directory/Library/Python/3.7/bin

this should add the location of where jupyter is located to your path variable.
Alternatively, as suggested by @HackLab we can also do the following:

python3 -m jupyter notebook

Hopefully, this will give anyone else having the same issues I had an easier time resolving this issue.",0.24489796,0.12889934,0.013455681502819061
34,"Question\nI wrote a Python script that allows me to retrieve calendar events from an externally connected source and insert them into my Google Calendar thanks to the Google Calendar's API. It works locally when I execute the script from my command line, but I would like to make it happen automatically so that the externally added events pop up in my Google Calendar automatically.
It appears that a cron job is the best way to do this, and given I used Google Calendar's API, I thought it might be helpful to use Cloud Functions with Cloud Scheduler in order to make it happen. However, I really don't know where to start and if this is even possible because accessing the API requires OAuth with Google to my personal Google account which is something I don't think a service account (which I think I need) can do on my behalf.
What are the steps I need to take in order to allow the script which I manually run and authenticates me with Google Calendar run every 60 seconds ideally in the cloud so that I don't need to have my computer on at all times?
Things I’ve tried to do:
I created a service account with full permissions and tried to create an http-trigger event that would theoretically run the script when the created URL is hit. However, it just returns an HTTP 500 Error.
I tried doing Pub/Sub event targets to listen and execute the script, but that doesn’t work either.
Something I’m confused about:
with either account, there needs to be a credentials.json file in order to login; how does this file get “deployed” alongside the main function? Along with the token.pickle file that gets created when the authentication happens for the first time.\nAnswer: The way a service account works is that it needs to be preauthorized.  You would take the service account email address and share a calendar with it like you would with any other user.  The catch here being that you should only be doing this with calendars you the developer control.  If these are calendars owned by others you shouldnt be using a service account.
The way Oauth2 works is that a user is displayed a consent screen to grant your application access to their data.  Once the user has granted you access and assuming you requested offline access you should have a refresh token for that users account.  Using the refresh token you can request a new access token at anytime.  So the trick here would be storing the",0.81632656,0.23986119,0.33231231570243835
35,"Question\nI am using Pycharm to code with Python FBX SDK, but I don't how to enable auto-complete. I have to look at the document for function members. It's very tedious. So, does anyone know how to enable auto-complete for Python FBX SDK in editor?
Thanks!\nAnswer: Copy these two files
[PATH_TO_YOUR_MOBU]\bin\config\Python\pyfbsdk_gen_doc.py
[PATH_TO_YOUR_MOBU]\bin\config\Python\pyfbsdk_additions.py
to another folder like
d:\pyfbsdk_autocomplete for instance.
rename the file pyfbsdk_gen_doc.py to pyfbsdk.py
add the folder to your interpreter paths in PyCharm. (Interpreter Settings, Show All, Show paths for interpreter)",0.81632656,0.36599207,0.2028011530637741
36,"Question\ni am new to Python programming language and Django. I am learning about web development with Django, however, each time I create a new project in PyCharm, it doesn´t recognize django module, so i have to install it again. Is this normal? Because i´ve installed django like 5 times. It doesn´t seem correct to me, there must be a way to install Django once and for all and not have the necessity of using 'pip install django' for each new project I create, I am sure there must be a way but I totally ignore it, I think I have to add django to path but I really don´t know how (just guessing). I will be thankful if anyone can help me :)\nAnswer: pycharm runs in a venv. A venv is an isolated duplicate (sort of) of python (interpreter) and other scripts. To use your main interpreter, change your interpreter location. The three folders (where your projects is, along with your other files) are just that. I think there is an option to inherit packages. I like to create a file called requirements.txt and put all my modules there. Comment for further help.
In conclusion, this is normal.",0.0,0.19119239,0.03655453026294708
37,"Question\nI am new to Django and trying to create an Application.
My scenario is:
I have a form on which there are many items and user can click on Add to Cart to add those item to Cart. I am validating if the user is logged in then only item should be added to Cart else a message or dialogue box must appear saying please login or sign up first.
Although I was able to verify the authentication but the somehow not able to show the message if user is not logged in.
For now I tried the below things:

Using session messages, but somehow it needs so many places to take care when to delete or when to show the message
Tried using Django Messages Framework, I checked all the configuration in settings.py and everything seems correct but somehow not showing up on HTML form

Does anyone can help me here?
I want to know a approach where I can authenticate the user and if user is not logged in a dialogue box or message should appear saying Please login or Signup. It should go when user refreshes the page.\nAnswer: If you are using render() for views.py you could add a boolean value to the context
i.e render(request ""template_name.html"", {""is_auth"": True})
Assumedly you are doing auth in the serverside so you could tackle it this way.
Not a great fix but might help.",0.0,0.07146555,0.0051073250360786915
38,"Question\nI have successfully installed opencv 4.3.0 on my Mac OS Catalina, python 3.8 is installed also, but when I try to import cv2, I get the Module not found error.
Please how do I fix this?
thanks in advance.\nAnswer: Can you try pip install opencv-python?",0.0,0.022856534,0.000522421149071306
39,"Question\nI have successfully installed opencv 4.3.0 on my Mac OS Catalina, python 3.8 is installed also, but when I try to import cv2, I get the Module not found error.
Please how do I fix this?
thanks in advance.\nAnswer: I was having issue with installing opencv in my Macbook - python version 3.6 ( i downgraded it for TF 2.0) and MacOs Mojave 10.14. Brew, conda and pip - none of the three seemed to work for me. So i went to [https://pypi.org/project/opencv-python/#files] and downloaded the.whl that was suitable for my combo of python and MacOs versions. Post this navigated to the folder where it was downloaded and executed pip install./opencv_python-4.3.0.36-cp36-cp36m-macosx_10_9_x86_64.whl",0.0,-0.075791895,0.005744411610066891
40,"Question\nI recently migrated my Python / Jupyter work from a macbook to a refurbrished Gen 8 HP rackmounted server (192GB DDR3 2 x 8C Xeon E5-2600), which I got off amazon for $400. The extra CPU cores have dramatically improved the speed of fitting my models particularly for decision tree ensembles that I tend to use a lot. I am now thinking of buying additional servers from that era (early-mid 2010s) (either dual or quad-socket intel xeon E5, E7 v1/v2) and wiring them up as a small HPC cluster in my apartment. Here's what I need help deciding:

Is this a bad idea? Am I better off buying a GPU (like a gtx 1080). The reason I am reluctant to go the GPU route is that I tend to rely on sklearn a lot (that's pretty much the only thing I know and use). And from what I understand model training on gpus is not currently a part of the sklearn ecosystem. All my code is written in numpy/pandas/sklearn. So, there will be a steep learning curve and backward compatibility issues. Am I wrong about this?

Assuming (1) is true and CPUs are indeed better for me in the short term. How do I build the cluster and run Jupyter notebooks on it. Is it as simple as buying an additional server. Designating one of the servers as the head node. Connecting the servers through ethernet. Installing Centos / Rocks on both machines. And starting the Jupyter server with IPython Parallel (?).

Assuming (2) is true, or at least partly true. What other hardware / software do I need to get? Do I need an ethernet switch? Or if I am connecting only two machines, there's no need for it? Or do I need a minimum of three machines to utilize the extra CPU cores and thus need a switch? Do I need to install Centos / Rocks? Or are there better, more modern alternatives for the software layer. For context, right now I use openSUSE on the HP server, and I am pretty much a rookie when it comes to operating systems and networking.

How homogeneous should my hardware be? Can I mix and match different frequency CPUs and memory across the machines? For example, having 1600 MHz DDR3 memory in one machine, 1333 MHz",0.40816328,0.063622,0.11870869249105453
41,"Question\nI currently converted a list of roughly 1200 items (1200 rows) and a problem arised when i looked at the date of each individual item and realised that the day and month was before the year which meant that ordering them by date would be useless. Is there any way I can reorder over 1200 dates so that they can be formatted correctly with me having to manually do it. Would I have to use python. I am very new to that and I don't know how to use it really.
Here's an example of what I get:
September 9 2016
And this is what i want:
2016 September 9
I am also using the microsoft excel if anyone was asking.\nAnswer: it must be date format.
you can split date parts in other cells and re-merge them in preferred format...",0.0,-0.1226213,0.015035982243716717
42,"Question\nI have created a calculator in Python using Tkinter  module,though I converted it to exe but I am not able to convert it to apk.please tell me how to do so?\nAnswer: I personally haven't seen anyone do that. I think it would be best to try and re-make you calculator in the Kivy framework if you want to later turn it into an APK using bulldozer.  Tkinter is decent for beginners but if you want to have nice Desktop UI's use PyQT5 and if you're interested in making mobile apps use Kivy. Tkinter is just a way to dip into using GUIs in python.",0.40816328,0.24147451,0.027785146608948708
43,"Question\nI'm running a Python script in an AWS Lambda function. It is triggered by SQS messages that tell the script certain objects to load from an S3 bucket for further processing.
The permissions seem to be set up correctly, with a bucket policy that allows the Lambda's execution role to do any action on any object in the bucket. And the Lambda can access everything most of the time. The objects are being loaded via pandas and s3fs: pandas.read_csv(f's3://{s3_bucket}/{object_key}').
However, when a new object is uploaded to the S3 bucket, the Lambda can't access it at first. The botocore SDK throws An error occurred (403) when calling the HeadObject operation: Forbidden when trying to access the object. Repeated invocations (even 50+) of the Lambda over several minutes (via SQS) give the same error. However, when invoking the Lambda with a different SQS message (that loads different objects from S3), and then re-invoking with the original message, the Lambda can suddenly access the S3 object (that previously failed every time). All subsequent attempts to access this object from the Lambda then succeed.
I'm at a loss for what could cause this. This repeatable 3-step process (1) fail on newly-uploaded object, 2) run with other objects 3) succeed on the original objects) can happen all on one Lambda container (they're all in one CloudWatch log stream, which seems to correlate with Lambda containers). So, it doesn't seem to be from needing a fresh Lambda container/instance.
Thoughts or ideas on how to further debug this?\nAnswer: Amazon S3 is an object storage system, not a filesystem. It is accessible via API calls that perform actions like GetObject, PutObject and ListBucket.
Utilities like s3fs allow an Amazon S3 bucket to be'mounted' as a file system. However, behind the scenes s3fs makes normal API calls like any other program would.
This can sometimes (often?) lead to problems, especially where files are being quickly created, updated and deleted. It can take some time for s3fs to update S3 to match what is expected from a local filesystem.
Therefore, it is not recommended to use tools like s3fs to'mount' S3 as a",0.81632656,0.41690063,0.1595410704612732
44,"Question\nI've updated conda and spyder to the latest versions.
I want to execute python scripts (using F5 hotkey) in the current console.
However, the new spyder behaves unexpectedly, for example, if I enter in a console a=5 and then run test.py script that only contains a command print(a), there is an error: NameError: name 'a' is not defined.
In the configuration options (command+F6) I've checked ""Execute in current console"" option.
I am wondering why is this happening?
Conda 4.8.2, Spyder 4.0.1\nAnswer: In the preferences, run settings, there is a ""General settings"", in which you can (hopefully still) deactivate ""Remove all variables before execution"".
I even think to remember that this is new, so it makes sense.",0.0,0.14776617,0.02183484099805355
45,"Question\nI've updated conda and spyder to the latest versions.
I want to execute python scripts (using F5 hotkey) in the current console.
However, the new spyder behaves unexpectedly, for example, if I enter in a console a=5 and then run test.py script that only contains a command print(a), there is an error: NameError: name 'a' is not defined.
In the configuration options (command+F6) I've checked ""Execute in current console"" option.
I am wondering why is this happening?
Conda 4.8.2, Spyder 4.0.1\nAnswer: I figured out the answer:
In run configuration (command+F6) there is another option that needs to be checked: ""Run in console's namespace instead of empty one""",0.0,0.2074076,0.0430179089307785
46,"Question\nI have a python list of force data that was sampled at 1280 Hz, I have to get it do exactly 240 Hz in order to match it exactly with a video that was filmed at 240 Hz. I was thinking about downsampling to 160 Hz and then upsampling through interpolation to 240 Hz. Does anyone have any ideas on how to go about doing this? Exact answers not needed, just an idea of where to look to find out how.\nAnswer: Don't downsample and that upsample again; that would lead to unnecessary information loss.
Use np.fft.rfft for a discrete Fourier transform; zero-pad in the frequency domain so that you oversample 3x to a sampling frequency of 3840 Hz. (Keep in mind that rfft will return an odd number of frequencies for an even number of input samples.) You can apply a low-pass filter in the frequency domain, making sure you block everything at or above 120 Hz (the Nyqvist frequency for 240 Hz sampling rate). Now use np.fft.irfft to transform back to a time-domain signal at 3840 Hz sampling rate. Because 240 Hz is exactly 16x lower than 3840 Hz and because the low-pass filter guarantees that there is no content above the Nyqvist frequency, you can safely take every 16th sample.",0.0,0.22299206,0.04972546175122261
47,"Question\nI have my dataset in a TensorFlow Dataset pipeline and I am wondering how can I normalize it, The problem is that in order to normalize you need to load your entire dataset which is the exact opposite of what the TensorFlow Dataset is for.
So how exactly does one normalize a TensorFlow Dataset pipeline? And how do I apply it to new data? (I.E. data used to make a new prediction)\nAnswer: You do not need to normalise the entire dataset at once.
Depending on the type of data you work with, you can use a.map() function whose sole purpose is to normalise that specific batch of data you are working with (for instance divide by 255.0 each pixel within an image.
You can use, for instance, map(preprocess_function_1).map(preprocess_function_2).batch(batch_size), where preprocess_function_1 and preprocess_function_2 are two different functions that preprocess a Tensor. If you use.batch(batch_size) then the preprocessing functions are applied sequentially on batch_size number of elements, you do not need to alter the entire dataset prior to using tf.data.Dataset()",0.20408164,0.36913413,0.027242323383688927
48,"Question\nI have this Instagram bot that is made using Python and Selenium, It log into Instagram, goes to a profile, select the last post and select the ""other x people liked this photo"" to show the complete list of the people that liked the post(it can be done with the follower of the page too).
Now I am stuck because I don't know how can i make the bot click only the profiles that have active stories and how to make it scroll down (the problem is that the way that i found to click on the profiles works just with the first one profile because when I click on the profile it opens the stories and closes the post, so when i reopen the post and the list of like on this post it will reclick on the same profile that I have already seen the stories of).
Does someone know how to do that or a similar thing maybe something even better that I didn't thinked of?
I don't think code is needed but if you need I will post it, just let me know.\nAnswer: Have you tried to use the ""back"" button on your browser window? Or open the page in a new tab, so you have still the old one to go back to.",0.40816328,0.033403814,0.14044465124607086
49,"Question\nImagine, that I have a 28 x 28 size grayscale image.. Now if I apply a Keras Convolutional layer with 3 filters and 3X3 size with 1X1 stride, I will get 3 images as output. Now if I again apply a Keras Convolutional layer with only 1 filter and 3X3 size and 1X1 stride, so how will this one 3X3 filter convolute over these 3 images and then how will we get one image..
What I think is that, the one filter will convolute over each of the 3 images resulting in 3 images, then it adds all of the three images to get the one output image.
I am using tensorflow backend of keras. please excuse my grammar, And Please Help me.\nAnswer: Answering my own question:
I figured out that the one filter convolutes over 3 images, it results in 3 images, but then these these images pixel values are added together to get one resultant image..
You can indeed check by outputting 3 images for 3 filters on 1 image. when you add these 3 images yourself (matrix addition), and plot it, the resultant image makes a lot of sense.",0.0,0.07922715,0.006276941392570734
50,"Question\nI'm working on a neural network to predict scores on how ""good"" the images are. The images are the inputs to another machine learning algorithm, and the app needs to tell the user how good the image they are taking is for that algorithm.
I have a training dataset, and I need to rank these images so I can have a score for each one for the regression neural network to train.
I created a program that gives me 2 images from the training set at a time and I will decide which one wins (or ties). I heard that the full rank can be obtained from these comparisons using SVM Ranking. However, I haven't really worked with SVMs before. I only know the very basics of SVMs. I read a few articles on SVM Ranking and it seems like the algorithm turns the ranking problem to a classification problem, but the maths really confuses me.
Can anyone explain how it works in simple terms and how to implement it in Python?\nAnswer: I did some more poking around on the internet, and found the solution.
The problem was how to transform this ranking problem to a classification problem. This is actually very simple.
If you have images (don't have to be images though, can be anything) A and B, and A is better than B, then we can have (A, B, 1). If B is better, then we have (A, B, -1)
And we just need a normal SVM to take the names of the 2 images in and classify 1 or -1. That's it.
After we train this model, we can give it all the possible pairs of images from the dataset and generating the full rank will be simple.",0.0,0.099209666,0.009842557832598686
51,"Question\nIf I have a list say l = [1, 8, 8, 8, 1, 3, 3, 8] and it's guaranteed that every element occurs an even number of times, how do I make a list with all elements of l now occurring n/2 times. So since 1 occurred 2 times, it should now occur once. Since 8 occurs 4 times, it should now occur twice. Since 3 occurred twice, it should occur once.
So the new list will be something like k=[1,8,8,3]
What is the fastest way to do this?
I did list.count() for every element but it was very slow.\nAnswer: Instead of using a counter, which keeps track of an integer for each possible element of the list, try mapping elements to booleans using a dictionary. Map to true the first time they're seen, and then every time after that flip the bit, and if it's true skip the element.",0.23809524,0.1056394,0.017544548958539963
52,"Question\nIf I have a list say l = [1, 8, 8, 8, 1, 3, 3, 8] and it's guaranteed that every element occurs an even number of times, how do I make a list with all elements of l now occurring n/2 times. So since 1 occurred 2 times, it should now occur once. Since 8 occurs 4 times, it should now occur twice. Since 3 occurred twice, it should occur once.
So the new list will be something like k=[1,8,8,3]
What is the fastest way to do this?
I did list.count() for every element but it was very slow.\nAnswer: I like using a trie set, as you need to detect duplicates to remove them, or a big hash set (lots of buckets).  The trie does not go unbalanced and you do not need to know the size of the final set.  An alternative is a very parallel sort -- brute force.",0.034013607,0.11771095,0.007005244959145784
53,"Question\nIs there a way to get  a  position of thumb in pixels in vertical scale widget relative to upper right corner of widget? I want a label with scale value to pop up next to thumb when mouse pointer hovering over it, for this I need thumb coordinates.\nAnswer: The coords method returns the location along the trough corresponding to a particular value.
This is from the canonical documentation for the coords method:

Returns a list whose elements are the x and y coordinates of the point along the centerline of the trough that corresponds to value. If value is omitted then the scale's current value is used.

Note: you asked for coordinates relative to upper-right corner. These coordinates are relative to the upper-left. You can get the width of the widget with winfo_width() and do a simple transformation.",0.0,0.39083058,0.15274854004383087
54,"Question\nI'm working on a question and answer system with django. my problem : I want the app to get a question from an ontology and according the user's answer get the next question. how can I have all the questions and user's answers displayed. i'm new to django, I don't know if I can use session with unauthenticated user and if I need to use websocket with the django channels library.\nAnswer: Given that you want to work with anonymous users the simplest way to go is to add a hidden field on the page and use it to track the user progress. The field can contain virtual session id that will point at a model record in the backend, or the entire Q/A session(ugly but fast and easy). Using REST or sockets would require similar approach.
I can't tell from the top of my mind if you can step on top of the built in session system. It will work for registered users, but I do believe that for anonymous users it gets reset on refresh(may be wrong here).",0.40816328,0.16606855,0.05860985442996025
55,"Question\nI am trying to scrape data from a mobile application (Pokemon HOME). The app shows usage statistics and other useful statistics that I want to scrape. I want to scrape this on my computer using python.
I am having trouble determining how to scrape data from a mobile application. I tried using Fiddler and an Android emulator to intercept server data but I am unfamiliar with the software to be able to understand what exactly to do.
Any help would be very beneficial. Even just suggestions for resources where I can learn how to do this on my own. Thank you!\nAnswer: It's possible but it's really a hard nut to break. There's a huge difference between Mobile app and web app
Web app is accessible through WAN,v.i.z World area network. Scraping is fairly and squarely easier.
In Python, you can bs4 to do it.
But in Mobile app, essentially and effectively, it's more about LAN.
It's installed locally.
Install an app to remote control your device from another device (usually required root)
However, whole data might not be available.",0.0,0.19265896,0.037117473781108856
56,"Question\nThe most popular python version is CPython, written in C. What i want to know is how is it possible to write a python collection using C when C arrays can only store on type of data at the same time?\nAnswer: This is not how python does it in C, but I've written a small interpreted language in Java (which also only allows arrays/lists with 1 data type) and implemented mixed type lists. I had a Value interface and a class for each type of value and those classes implemented the Value interface. I had FunctionValue class, a StringValue class, a BooleanValue class, and a ListValue class, all of which implemented the value interface. The ListValue class has a field of type List<Value> which contains the list's elements. All methods on the Value interface and its implementing classes which do stuff like numeric addition, string appending, list access, function calling, etc. initially take in Value objects and do different things based on which actual kind of Value it is.
You could do something similar in C, albeit at a lower level since it doesn't have interfaces and classes to help you manage your types.",0.0,0.14512485,0.02106122300028801
57,"Question\nSo I'm doing this python basics course and my final project is to create a card game. At the bottom of the instructions I get this

For extra credit, allow 2 players to play on two different computers that are on the same network. Two people should be able to start identical versions of your program, and enter the internal IP address of the user on the network who they want to play against. The two applications should communicate with each other, across the network using simple HTTP requests. Try this library to send requests:


http://docs.python-requests.org/en/master/


http://docs.python-requests.org/en/master/user/quickstart/


And try Flask to receive them:


http://flask.pocoo.org/


The 2-player game should only start if one person has challenged the other (by entering their internal IP address), and the 2nd person has accepted the challenge. The exact flow of the challenge mechanism is up to you.

I already investigated how flask works and kind of understand how python-requests works too. I just can't figure out how to make those two work together. If somebody could explain what should I do or tell me what to watch or read I would really appreciate it.\nAnswer: it would be nice to see how far you've come before answer (as hmm suggested you in a comment), but i can tell you something theorical about this.
What you are talking about is a client-server application, where server need to elaborate the result of clients actions.
What i can suggest is to learn about REST API, that you can use to let client and server to communicate in a easy way. Your clients will send http requests to server exposed APIs.
From what you wrote, you have a basically constraints that should be respected during client and server communication, here reasumed:

Someone search for your ip and send you a challenge request

You have received a challenge that you refuse or accept; only if you accept the challenge you can start the game


As you can see from the project specifications the entire challenge mechanism is up to you, so you can decide the best for you.
I would begin start thinking to a possible protocol that make use of REST API to start initial communication between client and server and let you define a basic challenge mechanism.
Enjoy programming :).",0.0,0.13523024,0.01828721910715103
58,"Question\nI'm writing installer for my program with python.
When everything is extracted, how can i make my program.exe file to run with Windows startup?
I want to make it fully automatic, without any user input.
Thanks.\nAnswer: You don't need to use Python for this. You can copy your.exe file and paste it in this directory:

C:\Users\YourUsername\AppData\Roaming\Microsoft\Windows\Start
Menu\Programs\Startup

It will run automatically when your computer starts.",0.0,0.2528935,0.06395512819290161
59,"Question\nWhen installing python modules, I seem to have two possible command line commands to do so.
pip install {module}
and
py -{version} -m pip install {module}
I suppose this can be helpful for selecting which version of python has installed which modules? But there's rarely a case where I wouldn't want a module installed for all possible versions.
Also the former method seems to have a pesky habit of being out-of-date no matter how many times I call:
pip install pip --upgrade
So are these separate? Does the former just call the latest version of the latter?\nAnswer: So the pip install module is callable if you have already installed the pip. The pip install pip --upgrade upgrades the pip and if you replace the pip into a module name it will upgrade that module to the most recent one. the py -{version} -m pip install {module} is callable if you have installed many versions of python - for example most of the Linux servers got installed python 2, so when you install the Python 3, and you want to install a module to version 3, you will have to call that command.",0.0,0.15907383,0.025304483249783516
60,"Question\nWhen installing python modules, I seem to have two possible command line commands to do so.
pip install {module}
and
py -{version} -m pip install {module}
I suppose this can be helpful for selecting which version of python has installed which modules? But there's rarely a case where I wouldn't want a module installed for all possible versions.
Also the former method seems to have a pesky habit of being out-of-date no matter how many times I call:
pip install pip --upgrade
So are these separate? Does the former just call the latest version of the latter?\nAnswer: TLDR: Prefer... -m pip to always install modules for a specific Python version/environment.

The pip command executes the equivalent of... -m pip. However, bare pip does not allow to select which Python version/environment to install to – the first match in your executable search path is selected. This may be the most recent Python installation, a virtual environment, or any other Python installation.
Use the... -m pip variant in order to select the Python version/environment for which to install a module.",0.6122449,0.35995632,0.06364952772855759
61,"Question\nI'm looking to build a system that alerts me when there's a package at my front door. I already have a solution for detecting when there's a package (tflite), but I don't know how to get the array of detected objects from the existing tflite process and then pull out an object's title through the array. Is this even possible, or am I doing this wrong?
Also, the tflite model google gives does not know how to detect packages, but I'll train my own for that\nAnswer: I've figured out a solution. I can just use the same array that the function that draws labels uses (labels[int(classes[i])) to get the name of the object in place i of the array (dunno if I'm using the correct terminology but whatever). hopefully this will help someone",0.0,0.05359602,0.0028725333977490664
62,"Question\nI have python 3.6 in my venv on PyCharm. However, I want to change that to Python 3.8. I have already installed 3.8, so how do I change my venv python version?
I am on windows 10.
Changing the version on the project intepreter settings seems to run using the new venv not my existing venv with all the packages I have installed. Attempting to add a new intepreter also results in the ""OK"" button being greyed out, possibly due to the current venv being not empty.\nAnswer: In pycharm you can do further steps:

Go in File-->Settings-->Python Interpreter
Select different python environment if already available from the drop down, If not click on ""Add"".
Select New Environment option, then in Base interpreter you can select 3.8 version",0.20408164,0.43735254,0.05441531166434288
63,"Question\nI am very new to python, and I am trying to create a chatbot with python for a school project.
I am almost done with creating my chatbot, but I don't know how to create a website to display it, I know how to create a website with Flask but how can I embed the chatbot code into the website?\nAnswer: In your flask code you can also embed the chatbot predict-functions into specific routes of your flask app. This would require following steps:
Just before you start the flask server you train the chatbot to ensure its predict function works propperly.
After that you can specifiy some more route-functions to your flask app.
In those functions you grab input from the user (from for example route parameters), send it through the chatbots predict function and then send the respons (probably with postprocessing if you wish) back to the requester.
Sending to the requester can be done through many different ways.
Two examples just of my head would be via display (render_template) to the webpage (if the request came in over GET-Request via usual browser site-opening request) or by sending a request to the users ip itself.
As a first hand experience i coupled the later mechanism to a telegram bot on my home-automation via post-request which itself then sends the response to me via telegram.",0.0,0.1961472,0.03847372531890869
64,"Question\nI had uninstalled python 3.8 from my system and installed 3.7.x
But after running the command where python and where python3 in the cmd I get two different locations.
I was facing issues regarding having two versions of python. So I would like to know how i can completely remove python3 located files.\nAnswer: To delete a specific python version, you can use which python and remove the python folder using sudo rm -rf <path returned from the above command>. You might also have to modify the PATH env variable to the location which contains the python executables of the version you want.
Or you can install Anaconda [https://www.anaconda.com/products/individual] which helps to manage multiple versions of python for you.",0.0,0.26116425,0.06820676475763321
65,"Question\nI have a flask site. It's specifically a note app. At the moment I am storing the user notes as plaintext. That means that anyone with access to the server which is me has access to the notes. I want to encrypt the data with the user password, so that only the user can access it using their password, but that would require the user to input his/her password each time they save their notes, retrive the notes or even updates them. I am hashing the password obviously.
Anyone has any idea how this could be done?\nAnswer: Use session to store user information, the Flask-Login extension would be a good choice for you.",-0.35714287,0.16172487,0.26922371983528137
66,"Question\nI'm using VS Code on Windows 10. I had no problems until a few hours ago (at the time of post), whenever I want to run a python program, it opens terminals outside of VS Code like Win32 and Git Bash. How do I change it back to the integrated terminal I usually had?\nAnswer: With your Python file open in VS Code:

Go to Run > Open Configurations, if you get prompted select ""Python File""
In the launch.json file, change the value of ""console"" to ""integratedTerminal""",0.40816328,0.20667732,0.04059659317135811
67,"Question\nI want to learn how to remove a virtual environment using the windows command prompt, I know that I can easily remove the folder of the environment. But I want to know if there is a more professional way to do it.\nAnswer: There is no command to remove virtualenv, you can deactivate it or remove the folder but unfortunately virtualenv library doesn't contain any kind of removal functionality.",0.40816328,0.105282426,0.0917368084192276
68,"Question\nI have a calculation that may result in very, very large numbers, that won fit into a float64. I thought about using np.longdouble but that may not be large enough either.
I'm not so interested in precision (just 8 digits would do for me). It's the decimal part that won't fit. And I need to have an array of those.
Is there a way to represent / hold an unlimited size number, say, only limited by the available memory? Or if not, what is the absolute max value I can place in an numpy array?\nAnswer: Can you rework the calculation so it works with the logarithms of the numbers instead?
That's pretty much how the built-in floats work in any case...
You would only convert the number back to linear for display, at which point you'd separate the integer and fractional parts; the fractional part gets exponentiated as normal to give the 8 digits of precision, and the integer part goes into the ""×10ⁿ"" or ""×eⁿ"" or ""×2ⁿ"" part of the output (depending on what base logarithm you use).",0.81632656,0.18115771,0.403439462184906
69,"Question\nAs mentioned in the question, I build a kivy app and deploy it to my android phone. The app works perfectly on my laptop but after deploying it the font size changes all of a sudden and become very small.
I can't debug this since everything works fine. The only problem is this design or rather the UI.
Does anyone had this issue before? Do you have a suggestion how to deal with it?
PS: I can't provide a reproducible code here since everything works fine. I assume it is a limitation of the framework but I'm not sure.\nAnswer: It sounds like you coded everything in terms of pixel sizes (the default units for most things). The difference on the phone is probably just that the pixels are smaller.
Use the kivy.metrics.dp helper function to apply a rough scaling according to pixel density. You'll probably find that if you currently have e.g. width: 50, on the desktop then width: dp(50) will look the same while on the phone it will be twice as big as before.

PS: I can't provide a reproducible code here since everything works fine.

Providing a minimal runnable example would, in fact, have let the reader verify whether you were attempting to compensate for pixel density.",0.40816328,0.27122468,0.01875218003988266
70,"Question\nIn Visual Studio Code, with git extensions installed, how do you add files or complete folders to the.gitignore file so the files do not show up in untracked changes. Specifically, using Python projects, how do you add the pycache folder and its contents to the.gitignore. I have tried right-clicking in the folder in explorer panel but the pop-menu has no git ignore menu option. Thanks in advance.
Edit: I know how to do it from the command line. Yes, just edit the.gitignore file. I was just asking how it can be done from within VS Code IDE using the git extension for VS Code.\nAnswer: So after further investigation, it is possible to add files from the pycache folder to the.gitignore file from within VS Code by using the list of untracked changed files in the'source control' panel. You right-click a file and select add to.gitignore from the pop-up menu. You can't add folders but just the individual files.",1.0,0.28566414,0.5102757215499878
71,"Question\nif I print the string in command prompt I I'm getting it i proper structure
""connectionstring""."""".""OT"".""ORDERS"".""SALESMAN_ID""
but when I write it to json, I'm getting it in below format
\""connectionstring\"".\""\"".\""OT\"".\""ORDERS\"".\""SALESMAN_ID\""
how to remove those escape characters?
when It's happening?\nAnswer: What is happening?
Json serialization and de-serialization is happening.
From wikipedia:
In the context of data storage, serialization (or serialisation) is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted (for example, across a network connection link) and reconstructed later. [...]
The opposite operation, extracting a data structure from a series of bytes, is deserialization.
In console, you de-serialize the json but when storing in file, you serialize the json.",0.0,0.30704522,0.09427677094936371
72,"Question\nAbout the data :
we have 2 video files which are same and audio of these files is also same but they differ in quality.
that is one is in 128kbps and 320kbps respectively.
we have used ffmpeg to extract the audio from video, and generated the hash values for both the audio file using the code : ffmpeg -loglevel error -i 320kbps.wav -map 0 -f hash -
the output was : SHA256=4c77a4a73f9fa99ee219f0019e99a367c4ab72242623f10d1dc35d12f3be726c
similarly we did it for another audio file to which we have to compare,
C:\FFMPEG>ffmpeg -loglevel error -i 128kbps.wav -map 0 -f hash -
SHA256=f8ca7622da40473d375765e1d4337bdf035441bbd01187b69e4d059514b2d69a
Now we know that these audio files and hash values are different but we want to know how much different/similar they are actually, for eg: like some distance in  a-b is say 3
can someone help with this?\nAnswer: You cannot use a SHA256 hash for this. This is intentional. It would weaken the security of the hash if you could. what you suggest is akin to differential cryptoanalysis. SHA256 is a modern cryptographic hash, and designed to be safe against such attacks.",0.20408164,0.26227498,0.003386464901268482
73,"Question\nI would like to ask how could I add dynamically some widgets in my application one by one and not all at once. Those widgets are added in a for loop which contains the add_widget() command, and is triggered by a button.
So I would like to know if there is a way for the output to be shown gradually, and not all at once, in the end of the execution. Initially I tried to add a delay inside the for loop, but I'm afraid it has to do with the way the output is built each time.
EDIT: Well, it seems that I hadn't understood well the use of Clock.schedule_interval and Clock.schedule_once, so what I had tried with them (or with time.sleep) didn't succeed at all. But obviously, this was the solution to my problem.\nAnswer: Use Clock.schedule_interval or Clock.schedule_once to schedule each iteration of the loop at your desired time spacing.",0.40816328,0.29909158,0.011896636337041855
74,"Question\nPer title. I do not understand why it is not valid. I understand that they mutate the object, but if you call the sort method, after it's done then you'd call the reverse method so it should be fine. Why is it then that I need to type lst.sort() then on the line below, lst.reverse()?
Edit: Well, when it's pointed out like that, it's a bit embarrassing how I didn't get it before. I literally recognize that it mutated the object and thus returns a None, but I suppose it didn't register that also meant that you can't reverse a None-type object.\nAnswer: When you call lst.sort(), it does not return anything, it changes the list itself.
So the result of lst.sort() is None, thus you try to reverse None which is impossible.",0.13605443,0.39596397,0.06755296885967255
75,"Question\nI have a boolean numpy array which I need to convert it to binary, therefore where there is true it should be 255 and where it is false it should be 0.
Can someone point me out how to write the code?\nAnswer: Let x be your data in numpy array Boolean format.
Try
np.where(x,255,0)",0.0,0.052030385,0.002707161009311676
76,"Question\nI would like more info. on the answer to the following question:

df[‘Name’] and 2. df.loc[:, ‘Name’], where:

df = pd.DataFrame(['aa', 'bb', 'xx', 'uu'], [21, 16, 50, 33], columns = ['Name', 'Age'])
Choose the correct option:

1 is the view of original dataframe and 2 is a copy of original
dataframe
2 is the view of original dataframe and 1 is a copy of
original dataframe
Both are copies of original dataframe
Both are views of original dataframe

I found more than one answer online but not sure. I think the answer is number 2 but when i tried x = df['name'] then x[0] = 'cc' then print(df) I saw that the change appeared in the original dataframe. So how the changed appeared in the original dataframe although I also got this warining:
A value is trying to be set on a copy of a slice from a DataFrame
I just want to know more about the difference between the two and weather one is really a copy of the original dataframe or not. Thank you.\nAnswer: Both are the views of original dataframe
One can be used to add more columns in dataframe and one is used for specifically getting a view of a cell or row or column in dataframe.",0.0,0.10371411,0.010756616480648518
77,"Question\nI've been reading about in-memory databases and how they use RAM instead of disk-storage.
I'm trying to understand the pros and cons of building an in-memory database with different programming languages, particularly Java and Python. What would each implementation offer in terms of speed, efficiency, memory management and garbage collection?
I think I could write a program in Python faster, but I'm not sure what additional benefits it would generate.
I would imagine the language with a faster or more efficient memory management / garbage collection algorithm would be a better system to use because that would free up resources for my in-memory database. From my basic understanding I think Java's algorithm might be more efficient that Python's at freeing up memory. Would this be a correct assumption?
Cheers\nAnswer: You choose an in-memory database for performance, right? An in-memory database written in C/C++ and that provides an API for Java and/or Python won't have GC issues. Many (most?) financial systems are sensitive to latency and 'jitter'. GC exacerbates jitter.",0.0,0.27283657,0.07443979382514954
78,"Question\nI have one simple question, is there a easy way to know the type of API's response?
Fox example:
Using requests post method to send api requests, some apis will return data format as.xml type or.json type,
how can i know the response type so i can choose not to convert to.json use json() when response type is.xml?\nAnswer: Use r.headers.get('content-type') to get the response type",0.40816328,-0.14263844,0.3033825755119324
79,"Question\nI am working with Python and currently trying to figure out the following: If I place an ellipsis of which the semi-axes, the centre's location and the orientation are known, on a pixel map, and the ellipsis is large enough to cover multiple pixels, how do I figure out which pixel covers which percentage of the total area of the ellipsis? As an example, let's take a map of 10*10 pixels (i.e. interval of [0,9]) and an ellipsis with the centre at (6.5, 6.5), semi-axes of (0.5, 1.5) and an orientation angle of 30° between the horizontal and the semi-major axis. I have honestly no idea, so any help is appreciated.
edit: To clarify, the pixels (or cells) have an area. I know the area of the ellipsis, its position and its orientation, and I want to find out how much of its area is located within pixel 1, how much it is within pixel 2 etc.\nAnswer: This is math problem. Try math.exchange rather than stackoverflow.
I suggest you to transform the plane: translation to get the center in the middle, rotation to get the ellipsis's axes on the x-y ones and dilatation on x to get a circle. And then work with a circle on rhombus tiles.
Your problem won't be less or more tractable in the new formulation but the math and code you have to work on will be slightly lighter.",0.0,0.18693942,0.034946344792842865
80,"Question\nIn a project of mine I need to create an online encyclopedia. In order to do so, I need to create a page for each entry file, which are all written in Markdown, so I have to covert it to HTML before sending them to the website. I didn't want to use external libraries for this so I wrote my own python code that receives a Markdown file and returns a list with all the lines already formatted in HTML. The problem now is that I don't know how to inject this code to the template I have in Django, when I pass the list to it they are just printed like normal text. I know I could make my function write to an.html file but I don't think it's a great solution thinking about scalability.
Is there a way to dynamically inject HTML in Django? Is there a ""better"" approach to my problem?\nAnswer: You could use the safe filter in your template! So it would look like that.
Assuming you have your html in a string variable called my_html then in your template just write
{{ my_html | safe }}
And don’t forget to import it!",0.81632656,0.30509055,0.2613622546195984
81,"Question\nI am trying to get python-utils package and utils module work in my anaconda3. However, whenever I open my Anaconda Powershell and try to install the package it fails with the comment

EnvironmentNotWritableError: The current user does not have write permissions to the target environment.
environment location: C:\ProgramData\Anaconda3

I searched for solutions and was advised that I update conda.
However, when I ran the comment below

conda update -n base -c defaults conda

it also failed with EnvironmentNotWritableError showing.
Then I found a comment that says maybe my conda isn't installed at some places, so I tried

conda install conda

which got the same error.
Then I tried

conda install -c conda-forge python-utils

which also failed with the same error.
Maybe it's the problem with setting paths? but I don't know how to set them. All I know about paths is that I can type

sys.path

and get where Anaconda3 is running.\nAnswer: I have got the same non writable error in anaconda prompt for downloading pandas,then sorted the the error by running anaconda prompt as administrator. it worked for me since i already had that path variable in environment path",0.40816328,0.07707411,0.10962003469467163
82,"Question\nI am trying to get python-utils package and utils module work in my anaconda3. However, whenever I open my Anaconda Powershell and try to install the package it fails with the comment

EnvironmentNotWritableError: The current user does not have write permissions to the target environment.
environment location: C:\ProgramData\Anaconda3

I searched for solutions and was advised that I update conda.
However, when I ran the comment below

conda update -n base -c defaults conda

it also failed with EnvironmentNotWritableError showing.
Then I found a comment that says maybe my conda isn't installed at some places, so I tried

conda install conda

which got the same error.
Then I tried

conda install -c conda-forge python-utils

which also failed with the same error.
Maybe it's the problem with setting paths? but I don't know how to set them. All I know about paths is that I can type

sys.path

and get where Anaconda3 is running.\nAnswer: Run the PowerShell as Administrator. Right Click on the PowerShell -> Choose to Run as Administrator. Then you'll be able to install the required packages.",0.81632656,0.11473787,0.4922266900539398
83,"Question\nHow do I wait for all the new elements that appear on the screen to load after clicking a specific button? I know that I can use the presence_of_elements_located function to wait for specific elements, but how do I wait until all the new elements have loaded on the page? Note that these elements might not necessarily have one attribute value like class name or id.\nAnswer: Well in reality you can't, but you can run a script to check for that.
However be wary that this will not work on javascript/AJAX elements.
self.driver.execute_script(""return document.readyState"").equals(""complete""))",0.81632656,0.2719072,0.29639244079589844
84,"Question\nDoes anyone know how to feed in an initial solution or matrix of initial solutions into the differential evolution function from the Scipy library?
The documentation doesn't explain if its possible but I know that initial solution implementation is not unusual. Scipy is so widely used I would expect it to have that type of functionality.\nAnswer: Ok, after review and testing I believe I now understand it.
There are a set of parameters that the scipy.optimize.differential_evolution(...) function can accept, one is the init parameter which allows you to upload an array of solutions. Personally I was looking at a set of coordinates so enumerated them into an array and fed in 99 other variations of it (100 different solutions) and fed this matrix into the inti parameter. I believe it needs to have more than 4 solutions or your are going to get a tuple error.
I probably didn't need to ask/answer the question though it may help others that got equally confused.",0.0,0.24008161,0.05763917788863182
85,"Question\nso im making a generator (doesn't really matter what one it is)
and im trying to make the a/ans appear before nouns correctly.
for example:
""an apple plays rock paper scissors with a banana""
and not:
""a apple plays rock paper scissors with an banana""
the default thing for the not-yet determined a/an is ""<a>""
so i need to replace the ""<a>"" with either a or an depending on if the letter after it is a vowel or not.
how would i do this?\nAnswer: Pseudo code

first find letter 'a' or 'an' in string and keep track of it
then find first word after it
if word starts with vowel: make it 'an'
Else: make it 'a'
this rules breaks with words like 'hour' or 'university' so also make exception rule(find a list of words if u can)",0.0,0.38751623,0.15016883611679077
86,Question\nI can get to the user information using the API but I cannot access course information. Can someone explain what I need to do to make the correct call for course information?\nAnswer: The easiest way to answer these questions is to try it in Postman. Highly recommended.,0.0,0.08898342,0.007918048650026321
87,"Question\nI'm reviewing the concepts of OOP, reading.
Here the book defines interface as

The set of all signatures defined by an object’s operations is called the interface to the object. (p.39)

And the abstract class as

An abstract class is one whose main purpose is to define a common interface for its subclasses. An abstract class will defer some or all of its implementation to operations defined in subclasses; hence an abstract class cannot be instantiated. The operations that an abstract class declares but doesn’t implement are called abstract operations. Classes that aren’t abstract are called concrete classes. (p.43)

And I wonder, if I define an abstract class without any internal data (variables) and concrete operations, just some abstract operations, isn't it effectively just a set of signatures? Isn't it then just an interface?
So this is my first question:

Can I say an abstract class with only abstract functions is ""effectively (or theoretically)"" an interface?

Then I thought, the book also says something about types and classes.

An object’s class defines how the object is implemented. The class defines the object’s internal state and the implementation of its operations. In contrast, an object’s type only refers to its interface—the set of requests to which it can respond. An object can have many types, and objects of different classes can have the same type. (p.44)

Then I remembered that some languages, like Java, does not allow multiple inheritance while it allows multiple implementation. So I guess for some languages (like Java), abstract class with only abstract operations!= interfaces.
So this is my second question:

Can I say an abstract class with only abstract functions is ""generally equivalent to"" an interface in languages that support multiple inheritance?

My first question was like checking definitions, and the second one is about how other languages work. I mainly use Java and Kotlin so I'm not so sure about other languages that support multiple inheritance. I do not expect a general, comprehensive review on current OOP languages, but just a little hint on single language (maybe python?) will be very helpful.\nAnswer: No.

In Java, every class is a subclass of Object, so you can't make an abstract class with only abstract methods.  It will always have the method implementations inherited from Object:  hashCode",0.81632656,0.1643041,0.4251333177089691
88,"Question\nI am building a web application with Django and I show the graphs in the website. The graphs are obtained from real time websites and is updated daily. I want to know how can I send graphs using matplotlib to template and add refresh option with javascript which will perform the web scraping script which I have written. The main question is which framework should I use? AJAX, Django REST, or what?\nAnswer: You're better off using a frontend framework and calling the backend for the data via JS. separating the front and backend is a more contemporary approach and has some advantages over doing it all in the backend.
From personal experience, it gets really messy mixing Python and JS in the same system.
Use Django as a Rest-ful backend, and try not to use AJAX in the frontend, then pick a frontend of your choice to deliver the web app.",0.40816328,0.16268015,0.06026196852326393
89,"Question\nI installed/imported streamlit, numpy, and pandas but I do not know how I can see the charts I have made. How do I deploy it on repl.it?\nAnswer: You can not deploy streamlit application within repl.it because

In order to protect against CSRF attacks, we send a cookie with each request.
To do so, we must specify allowable origins, which places a restriction on
cross-origin resource sharing.

One solution is push your code from repl.it to GitHub. Then deploy from GitHub on share.streamlit.io.",0.20408164,0.40546197,0.04055403545498848
90,"Question\nI am a data scientist use jupyter notebook a lot and also have started to do lot of development work and use Vscode for development. so how can I get Jupyter notebook theme in vscode as well? I know how to open a Jupyter notebook in vscode by installing an extension but I wanted to know how to get Jupyter notebook theme for vs code. so it gets easier to switch between both ide without training eyes\nAnswer: You can edit your VScode's settings by:
1- Go to your Jupyter extension => Extension settings => and check ""Ignore Vscode Theme"".
2- Click on File => preference=> color Theme
3-  Select the theme you need.
You can download the theme extension from VSCode's extension store, for example: Markdown Theme Kit; Material Theme Kit.
Note:
You need to restart or reload VSCode to see the changes.",0.30612245,0.2349562,0.00506463460624218
91,"Question\nSo I'm making this game with Kivy and it's a game where there's a start screen with an MDToolbar, an MDNavigationDrawer, two Images, three MDLabels and a OneLineIconListItem that says 'Start Game' and when you click on it the game is supposed to start.
The game screen contains:

Viruses
Masked man
Soap which you use to hit the viruses
Current score in an MDLabel
A button to go back to the start screen

Issues:

The background music for the game starts playing before the game screen is shown (When the start screen is shown) - ScreenManager issue
When I click the button to go back to the start screen, the button doesn't get clicked - MDFlatButton issue

I used on_touch_down, on_touch_move, and on_touch_up for this game and I know that's what's causing the MDFlatButton issue. So does anyone know how I'm supposed to have the on_touch_* methods defined AND have clickable buttons?
And I don't know how to fix the ScreenManager issue either.
I know I haven't provided any code here, but that's because this post is getting too long. I already got a post deleted because people thought the post was too long and I was providing too much code and too less details. And I don't want that to happen again. If anyone needs to view the code of my project, I will leave a Google Docs link to it.
Thanks in advance!\nAnswer: I fixed my app.
Just in case anyone had the same question, I'm gonna post the answer here.

To get a clickable button, you have to create a new Screen or Widget and add the actual screen as a widget to the new class. Then, you can add buttons to the new class. This works because the button is on top of the actual screen. So when you click anywhere in the button's area, the button gets clicked and the on_touch_* methods of the actual screen don't get called.


And to fix the ScreenManager issue, you just have to expirement.",0.0,0.39397228,0.15521416068077087
92,"Question\nhelp me please how can I use the pickle save if I have a lot of entry and I want to save all in one file and load form the file for each entry separately?\nAnswer: You can't pickle tkinter widgets. You will have to extract the data and save just the data. Then, on restart you will have to unpickle the data and insert it back into the widgets.",0.0,0.15586853,0.02429499849677086
93,"Question\nI'm writing a desktop and web app, Just need to know how can i authorize this desktop application with same open web app browser after installed?\nAnswer: if you mean to authorize your desktop app via the login of user from any web browser, you can use TCP/UDP socket or also for example, call an api every 2 seconds to check is user is loged in or not. in web browser, if user had be loged in, you can set login state with its ip or other data in database to authorize the user from desktop app.",0.0,0.0049732924,2.4733637474128045e-05
94,"Question\nI have a plot made using Python matplotlib that updates every time new sensor data is acquired. I also have a web GUI using vue. I'd like to incorporate the matplotlib figure into the web GUI and have it update as it does when running it independently. This therefore means not just saving plot and loading it as an image.
Can anyone advise how to achieve this?\nAnswer: In my opinion it's not reasonable way, There are very good visualizing tools powered by javascript, for example chart.js.
you can do your computation with python in back-end and pass data to front-end by API and plot every interactive diagrams you want using javascript.",0.40816328,0.05062127,0.12783628702163696
95,"Question\nI am using the IDE called Spyder for learning Python.
I would like to know in how to go about in installing Python packages for Spyder?
Thank you\nAnswer: I have not checked if the ways described by people here before me work or not.
I am running Spyder 5.0.5, and for me below steps worked:

Step 1: Open anaconda prompt (I had my Spyder opened parallelly)
Step 2: write - ""pip install package-name""

Note: I got my Spyder 5.0.5 up and running after installing the whole Anaconda Navigator 2.0.3.",0.0,0.3371607,0.11367734521627426
96,"Question\nI am using the IDE called Spyder for learning Python.
I would like to know in how to go about in installing Python packages for Spyder?
Thank you\nAnswer: Spyder is a package too, you can install packages using pip or conda, and spyder will access them using your python path in environment.
Spyder is not a package manager like conda,, but an IDE like jupyter notebook and VS Code.",0.1632653,0.28681433,0.015264363028109074
97,"Question\nSpecifically, I would like to know how to give input in the case of read(). I tried everywhere but couldn't find the differences anywhere.\nAnswer: read() recognizes each character and prints it.
But readline() recognizes the object line by line and prints it out.",0.20408164,0.14740348,0.0032124139834195375
98,"Question\nSpecifically, I would like to know how to give input in the case of read(). I tried everywhere but couldn't find the differences anywhere.\nAnswer: >>> help(sys.stdin.read)
Help on built-in function read:

read(size=-1, /) method of _io.TextIOWrapper instance
    Read at most n characters from stream.
    
    Read from underlying buffer until we have n characters or we hit EOF.
    If n is negative or omitted, read until EOF.
(END)

So you need to send EOF when you are done (*nix: Ctrl-D, Windows: Ctrl-Z+Return):

>>> sys.stdin.read()
asd
123
'asd\n123\n'

The readline is obvious. It will read until newline or EOF. So you can just press Enter when you are done.",0.40816328,0.26084566,0.0217024814337492
99,"Question\nIs there a generic python way to pass arguments to arbitrary functions based on specified positions? While it would be straightforward to make a wrapper that allows positional argument passing, it would be incredibly tedious for me considering how frequently I find myself needing to pass arguments based on their position.
Some examples when such would be useful:

when using functools.partial, to partially set specific positional arguments
passing arguments with respect to a bijective argument sorting key, where 2 functions take the same type of arguments, but where their defined argument names are different

An alternative for me would be if I could have every function in my code automatically wrapped with a wrapper that enables positional argument passing. I know several ways this could be done, such as running my script through another script which modifies it, but before resorting to that I'd like to consider simpler pythonic solutions.\nAnswer: For key arguments use **kwargs but for positional arguments use *args.",0.0,0.39434928,0.15551134943962097
0,"Question\nI am attempting to calculate Kendall's tau for a large matrix of data stored in a Pandas dataframe. Using the corr function, with method='kendall', I am receiving NaN for a row that has only one value (repeated for the length of the array). Is there a way to resolve it? The same issue happened with Spearman's correlation as well, presumably because Python doesn't know how to rank an array that has a single repeated value, which leaves me with Pearson's correlation -- which I am hesitant to use due to its normality and linearity assumptions.
Any advice is greatly appreciated!\nAnswer: I decided to abandon the complicated mathematics in favor of intuition. Because the NaN values arose only on arrays with constant values, it occurred to me that there is no relationship between it and the other data, so I set its Spearman and Kendall correlations to zero.",0.0,0.34103632,0.11630576848983765
1,"Question\nI have Windows 10 on my computer and when I use the cmd and check python --version, I get python 3.8.2. But when I try to find the path for it, I am unable to find it through searching on my PC in hidden files as well as through start menu. I don't seem to have a python 3.8 folder on my machine. Anybody have any ideas how to find it?\nAnswer: If you're using cmd (ie Command Prompt), and typing python works, then you can get the path for it by doing where python. It will list all the pythons it finds, but the first one is what it'll be using.",0.13605443,0.16363955,0.0007609387976117432
2,"Question\nI want to make a script in pygame where two balls fly towards each other and when they collide they should bounce off from each other but I don't know how to do this so can you help me?\nAnswer: Its pretty easy you just check if the x coordinate is in the same spot as the other x coordinate. For example if you had one of the x coordinated called x, and another one called i(there are 2 x coordinates for both of the balls) then you could just say if oh and before I say anything esle this example is fi your pygame window is a 500,500. You could say if x == 250: x -= 15. And the other way around for i. If i == 250: i += 15. Ther you go!. Obviously there are a few changes you have to do, but this is the basic code, and I think you would understand this",0.0,0.13043624,0.017013613134622574
3,"Question\nI have created an API using AWS Lambda function (using Python). Now my react js code hits this API whenever an event fire. So user can request API as many times the events are fired. Now the problem is we are not getting the response from lambda API sequentially. Sometime we are getting the response of our last request faster than the previous response of previous request.
So we need to handle our response in Lambda function sequentially, may be adding some delay between 2 request or may be implementing throttling. So how can I do that.\nAnswer: Did you check the concurrency setting on Lambda? You can throttle the lambda there.
But if you throttle the lambda and the requests being sent are not being received, the application sending the requests might be receiving an error unless you are storing the requests somewhere on AWS for being processed later.
I think putting an SQS in front of lambda might help. You will be hitting API gateway, the requests get sent to SQS, lambda polls requests concurrently (you can control the concurrency) and then send the response back.",0.13605443,0.15144497,0.00023686887288931757
4,"Question\nI have created an API using AWS Lambda function (using Python). Now my react js code hits this API whenever an event fire. So user can request API as many times the events are fired. Now the problem is we are not getting the response from lambda API sequentially. Sometime we are getting the response of our last request faster than the previous response of previous request.
So we need to handle our response in Lambda function sequentially, may be adding some delay between 2 request or may be implementing throttling. So how can I do that.\nAnswer: You can use SQS FIFO Queue as a trigger on the Lambda function, set Batch size to 1, and the Reserved Concurrency on the Function to 1. The messages will always be processed in order and will not concurrently poll the next message until the previous one is complete.
SQS triggers do not support Batch Window - which will 'wait' until polling the next message. This is a feature for Stream based Lambda triggers (Kinesis and DynamoDB Streams)
If you want to streamlined process, Step Function will let you manage states using state machines and supports automatic retry based off the outputs of individual states.",0.0,0.28871548,0.08335662633180618
5,"Question\nWould it be possible to store an image and a value together in a database? Like in a array?
So it would be like [image, value]. I’m just trying to be able to access the image to print that and then access the value later (for example a image if a multi-choice question and its answer is the value).
Also how would I implement and access this? I’m using Firebase with the pyrebase wrapper for python but if another database is more suitable I’m open to suggestions.\nAnswer: you can set your computer as a server and in database you can store like [image_path, value].",0.0,0.16743809,0.028035514056682587
6,"Question\nI have created a new Anaconda environnement for Python. I managed to add it has an optional environnement you can choose when you create a new Notebook. Hovewer, I'd like to know how can I change the environnement of an already existing Notebook.\nAnswer: open your.ipynb file on your browser. On top, there is Kernel tab. You can find your environments under Change Kernel part.",0.20408164,0.28007233,0.00577458506450057
7,"Question\nDoes anyone know if it is possible to use n_simulation = None in 'MarkovModel' algorithm in 'pychhatr' library in Python?
It throws me an error it must be an integer, but in docsting i have information like that:
'n_simulations : one of {int, None}; default=10000'
I`d like to do something like nsim = NULL in'markov_model' in 'ChannelAttribution' package in R, these two algorithms are similarly implemented.
I don`t know how does it works exactly, how many simulations from a transition matrix I have using NULL.
Could anyone help with this case?
Regards,
Sylwia\nAnswer: Out of curiosity I spent some minutes staring intensely at the source code of both pychattr module and ChannelAttribution package.
I'm not really familiar with the model, but are you really able to call this in R with ""nsim=NULL""? Unless I missed something if you omit this parameter it will use value 100000 as the default and if parameter exists, the R wrapper will complain if it's not a positive number.
Regards,
Maciej",0.0,0.33034763,0.10912955552339554
8,"Question\nDoes anyone know if it is possible to use n_simulation = None in 'MarkovModel' algorithm in 'pychhatr' library in Python?
It throws me an error it must be an integer, but in docsting i have information like that:
'n_simulations : one of {int, None}; default=10000'
I`d like to do something like nsim = NULL in'markov_model' in 'ChannelAttribution' package in R, these two algorithms are similarly implemented.
I don`t know how does it works exactly, how many simulations from a transition matrix I have using NULL.
Could anyone help with this case?
Regards,
Sylwia\nAnswer: I checked that 'pychattr' (Python) doesn`t support value None but it supports n_simulations = 0 and it sets n_simulations to 1e6 (1 000 000).
'ChannelAttribution' (R) replaces nsim = NULL and nsim = 0 to nsim = 1e6 (1 000 000) too.
In latest version of 'ChannelAttribution' (27.07.2020) we have nsim_start parameter instead of nsim and it doesn`t support 0 or NULL value anymore.
Important: default value of nsim_start is 1e5 (100 000) and from my experience it`s not enough in many cases.
Regards,
Sylwia",0.0,0.3007107,0.09042692929506302
9,"Question\nHow do you decide the critical values(alpha) and analyze with the p value
example: stats.ttest_ind(early['assignment1_grade'], late['assignment1_grade'])
(2 series with score of their assignments)
I understand the concept that if the p value is greater than the alpha value then the null hypothesis cant be neglected.
Im doing a course and instructor said that the alpha value here is 0.05 but how do you determine it.\nAnswer: The alpha value cannot be determined in the sense that there were a formula to calculate it. Instead, it is arbitrarily chosen, ideally before the study is conducted.
The value alpha = 0.05 is a common choice that goes back to a suggestion by Ronald Fisher in his influential book Statistical Methods for Research Workers (first published in 1925). The only particular reason for this value is that if the test statistic has a normal distribution under the null hypothesis, then for a two-tailed test with alpha = 0.05 the critical values of the test statistic will be its mean plus/minus 2 (more exactly, 1.96) times its standard deviation.
In fact, you don't need alpha when you calculate the p value, because you can just publish the p value and then every reader can decide whether to consider it low enough for any given purpose or not.",0.0,0.27005315,0.07292870432138443
10,"Question\nI need my Python program to do some stuff, and at a certain point give control to the user (like a normal Python shell when you run python3 or whatever) so that he can interact with it via command line. I was thinking of using pwntools's interactive() method but I' m not sure how I would use that for the local program instead of a remote.
How would I do that?
Any idea is accepted, if pwntools is not needed, even better.\nAnswer: Use IPython
If you haven't already, add the package IPython using pip, anaconda, etc.
Add to your code:
from IPython import embed
Then where you want a ""breakpoint"", add:
embed()
I find this mode, even while coding to be very efficient.",0.40816328,0.14000642,0.07190810143947601
11,"Question\nIn python I can get test coverage by coverage run -m unittest and the do coverage report -m / coverage html to get html report.
However, it does not show the actual unit test report. The unit test result is in the logs, but I would like to capture it in a xml or html, so I can integrate it with Jenkins and publish on each build. This way user does not have to dig into logs.
I tried to find solution to this but could not find any, please let me know, how we can get this using coverage tool.
I can get this using nose2 - nose2 --html-report --with-coverage --coverage-report html - this will generate two html report - one for unit test and other for coverage. But for some reason this fails when I run with actual project (no coverage data collected / reported)\nAnswer: Ok for those who end up here, I solved it with -
nose2 --html-report --with-coverage --coverage-report html --coverage./ 
The issue I was having earlier with 'no coverage data' was fixed by specifying the the directory where the coverage should be reported, in the command above its with --coverage./",0.0,-0.058713853,0.003447316586971283
12,"Question\nI am working with the John Hopkins Covid data for personal use to create charts.  The data shows cumulative deaths by country, I want deaths per day.  Seems to me the easiest way is to create two dataframes and subtract one from the other.  But the file has column names as dates and the code, e.g. df3 = df2 - df1 subtracts the columns with the matching dates.  So I want to rename all the columns with some easy index, for example, 1, 2, 3,....
I cannot figure out how to do this?\nAnswer: Thanks for the time and effort but I figured out a simple way.
for i, row in enumerate(df):
df.rename(columns = { row : str(i)}, inplace = True)
to change the columns names and then
for i, row in enumerate(df):
df.rename(columns = { row : str( i + 43853)}, inplace = True)
to change them back to the dates I want.",0.0,0.27867746,0.07766112685203552
13,"Question\nI need help understanding the security of JWT tokens used for login functionality. Specifically, how does it prevent an attack from an attacker who can see the user's packets? My understanding is that, encrypted or not, if an attacker gains access to a token, they'll be able to copy the token and use it to login themselves and access a protected resource. I have read that this is why the time-to-live of a token should be short. But how much does that actually help? It doesn't take long to grab a resource. And if the attacker could steal a token once, can't they do it again after the refressh?
Is there no way to verify that a token being sent by a client is being sent from the same client that you sent it to? Or am I missing the point?\nAnswer: how does it prevent an attack from an attacker who can see the user's packets?

Just because you can see someone's packets doesn't mean that you can see the contents. HTTPS encrypts the traffic so even if someone manages to capture your traffic, they will no be able to extract JWT out of it. Every website that is using authentication should only run through HTTPS. If someone is able to perform man-in-the-middle attack then that is a different story.

they'll be able to copy the token and use it to login themselves and access a protected resource

Yes but only as the user they stole the token from. JWT are signed which means that you can't modify their content without breaking the signature which will be detected by the server (at least it is computationally infeasible to find the hash collision such that you could modify the content of the JWT). For highly sensitive access (bank accounts, medical data, enterprise cloud admin accounts...) you will need at least 2-factor authentication.

And if the attacker could steal a token once, can't they do it again after the refressh?

Possibly but that depends on how the token has been exposed. If the attacked sits on the unencrypted channel between you and the server then sure they can repeat the same process but this exposure might be a result of a temporary glitch/human mistake which might be soon repaired which will prevent attack to use the token once it expires.

Is there no way to verify that a token being sent by a client is being sent from the same client that you sent it to?

If the attacker successfully performs",0.20408164,0.06773561,0.01859023980796337
14,"Question\nHere is the situation.
Trying to run a Python Flask API in Kubernetes hosted in Raspberry Pi cluster, nodes are running Ubuntu 20. The API is containerized into a Docker container on the Raspberry Pi control node to account for architecture differences (ARM).
When the API and Mongo are ran outside K8s on the Raspberry Pi, just using Docker run command, the API works correctly; however, when the API is applied as a Deployment on Kubernetes the pod for the API fails with a CrashLoopBackoff and logs show'standard_init_linux.go:211: exec user process caused ""exec format error""'
Investigations show that the exec format error might be associated with problems related to building against different CPU architectures. However, having build the Docker image on a Raspberry Pi, and are successfully running the API on the architecture, I am unsure this could the source of the problem.
It has been two days and all attempts have failed. Can anyone help?\nAnswer: Fixed; however, something doesn't seem right.
The Kubernetes Deployment was always deployed onto the same node. I connected to that node and ran the Docker container and it wouldn't run; the ""exec format error"" would occur. So, it looks like it was a node specific problem.
I copied the API and Dockerfile onto the node and ran Docker build to create the image. It now runs. That does not make sense as the Docker image should have everything it needs to run.
Maybe it's because a previous image build against x86 (the development machine) remained in that nodes Docker cache/repository. Maybe the image on the node is not overwritten with newer images that have the same name and version number (the version number didn't increment). That would seem the case as the spin up time of the image on the remote node is fast suggesting the new image isn't copied on the remote node. That likely to be what it is.
I will post this anyway as it might be useful.

Edit: allow me to clarify some more, the root of this problem was ultimately because there was no shared image repository in the cluster. Images were being manually copied onto each RPI (running ARM64) from a laptop (not running ARM64) and this manual process caused the problem.
An image build on the laptop was based from a base image incompatible with ARM64; this was manually copied to all RPI's in",0.81632656,0.08720034,0.5316250324249268
15,"Question\nI have issue with scraping page and getting json from it.
<script type=""text/x-magento-init""> inside is json that I'am trying to get but when I try with.find('script',{'type':'text/x-magento-init'})I recive first json that I don't need. My question is how to find the 8th json with that same name? There is no other name/id etc.\nAnswer: you need to use re.findall(), that will return to you a list of matches. Then to get the 8th element you can go by result[7]",0.0,0.17521653,0.03070083074271679
16,"Question\nSo when developing an app, it's considered good practice to specify the  minimal (least restrictive) needed dependency versions in setup.py's, install_requires. Well, how do I know which versions of my dependencies my project actually depends on?
Is there any way to automatically determine this? If not, is there maybe a nice way to test the upper and lower bounds of the dependency ranges I specify?
Ideally, I'd like to focus on actual development more than manually tracking every new version of my dependencies and sifting through release histories to find out when the features I used were first introduced.\nAnswer: The ""Correct"" way would just be to keep track of what features you use in each dependency, and then the minimum versions for each would be the minimum version that has those features, or has some important update (security, speed, etc).
The ""If it works, it works"" way would be to write a script that brute forces the versions for each individual dependency to get a range for each by installing it and seeing if it works. You could probably do this easily using github actions (Although you'd have to pay if your repo is private)",0.20408164,0.35905302,0.0240161269903183
17,"Question\nI have a python virtual environment (conda) where I’ve installed CUDA toolkit 10.1.243 and tensorflow-gpu 2.3.0rc0. My CUDA driver is 11.0.
In order to test if tensorflow was installed to GPU correctly, I ran a series of commands from within the venv:
tf.test.is_built_with_cuda()
True
tf.config.list_physical_devices(‘GPU’)
Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Quadro M2000M computeCapability: 5.0
[PhysicalDevice(name=’/physical_device:GPU:0’, device_type=‘GPU’)]
python -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000,1000])))""
tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid
I am not sure how to troubleshoot this. I have a feeling that it is related to modifying the compilation such that tensorflow supports the compute capability of my device (5.0), but I am not sure how to proceed. Thank you!!\nAnswer: i recommend pip install tensorflow-gpu==2.2",0.13605443,0.17027473,0.0011710295220836997
18,"Question\nI wanna know is it possible Running two server Django and React.js together?
every time I should to run Backend with python manage.py run server and then I should go to Frontend and Run npm Start
actually I have One Fullstack project with Subfolders Backend And Frontend
my question is we have any code to rune both servers together or Not?\nAnswer: As I know,unfortunately,there is no way to run them both with just a line of code.This can be wrong though.",0.20408164,0.08604455,0.013932754285633564
19,"Question\nPyCharm was being stupid so I decided to use Atom. Before, I had stupidly downloaded Python from the official website and the Microsoft store, so it caused errors on PyCharm which meant I had to use Atom.
However, recently I managed to fix that issue by uninstalling Python from the Microsoft store and installing the latest version of Python with the PATH properly configured, but it only fixed PyCharm but not Atom.
However, whenever I try to run a script on Atom, I get this error message:
Python was not found but can be installed from the Microsoft Store: https://go.microsoft.com/fwlink?linkID=2082640
I have Python installed from the website but not the MS store. Any ideas how to fix this?\nAnswer: It should not matter whether you download Python from the Microsoft Store or from python.org. What matters is that Python is installed on your system and that any application using that version of the interpreter can access it. Typically, this means that you have any entry on your PATH environment variable to the directory of your Python interpreter.
From what you've described, it sounds like you may have had more than one version on your system at the same time, and then you removed one and made some modifications to your PATH variable. I would confirm that your PATH variable is correct and that you can run the Python interpreter from a Command Prompt window by invoking it directly. If you can do that, then Atom ought to be able to find your Python interpreter. If it cannot, then I would recommend uninstalling and reinstalling Atom rather than uninstalling and reinstalling Python.
I would like to help more, but would need more info about your system to provide a more specific steps.",0.0,0.09263635,0.00858149304986
20,"Question\nHere is a chronological summary of what happened.

My local copy of a branch, let's call it featurebranch is fully up to date with the copy of this branch on github
On my local machine, I delete a file called test.csv.
I call ""git pull origin featurebranch"" on my local machine, but the test.csv does not show up on my local machine
I call ""git status"" and output is ""Changes not staged for commit: deleted test.csv""
I call ""git add test.csv""
I call ""git status"" and output is ""Changes to be committed: deleted test.csv""
I call ""git commit - m ""delete test file""""
I call ""git pull origin featurebranch"" and output is ""Already up to date""

It's not up to date though. test.csv still exists on github but not on my local machine. How do I get this file back on my local machine? I assume there is something about how git works that I do not realize. Thank you for reading.\nAnswer: git pull is used to ""sync"" your branch with the latest commits in the remote repository (GitHub). It is not used to sync files.
Since featurebranch was never updated in the remote with new commits that you don't have, git pull has nothing to do - there are no new commits to fetch and sync to. Your commit is the most up-to-date version of that branch.
If you want your file back, you can either git reset <older_commit> the branch to some older commit that contained the branch. Or you can git revert <the_commit_that_deleted_the_file> the commit that deleted that file. Or you can git checkout <some_other_commit> -- test.csv to selectively restore the file from <some_other_commit> that contains the file. Or you can git checkout  or git switch to some other branch/commit to change your working directory to a commit that actually had that file.",0.0,0.55716044,0.3104277551174164
21,"Question\nFor my project, I have a virtual currency, which you can get by staying on the page. (every 15 minutes you get 0.1 coins) I did some research, and I did not find anything about it in Flask and I have no clue on how to see how long a user has been on a page. Is there anyone who could help me?
Thanks,
Robert S.\nAnswer: There is no direct way as http is stateless.
That means that once a page is loaded, the connection between browser and server gets disconnected.
Maybe there are ways to workaround this limitation, eg by regularly send a message to the server via Javascript or with websockets.",0.40816328,0.21935016,0.03565039485692978
22,"Question\nI have a task to perform classification on a data set in which the feature names are anonymous like var1,var2 etc. There is mixed continuous and one hot encoded features. I am bit stuck how to make new features out of the existing features and perform EDA on it.\nAnswer: There are many approaches to feature engineering, which could be also used on anonymized data:

transformation of continuous variables: log, power, normalization;
aggregations: `df.groupby(['cat_column']).agg({'continuous_column': ['min','max','mean','std'])
interactions of continuous variables: addition, subtraction, multiplications, division
And so on.",0.40816328,0.34240532,0.004324109293520451
23,"Question\njust wondering how to convert a set of code into comments, without having to go through each individual line of code and put a # at the beginning. Was thinking that maybe there was a quick way to highlight the code and then convert it into comment, atm i'm just cutting it off the code and then re-pasting it in later on.
Much appreciated :)
P.S. i m using Pycharm as my IDE\nAnswer: You can use the ("""""") symbol before and after the text you want to comment out. It is not exactly a comment and more of a text constant, but either way it is often used for documentation, so you can use it for multiline comments except for some cases when it causes unexpected errors
To avoid such cases you could simply use multiple single line comments by selecting the desired lines and using ctrl+/ to comment them out",0.0,-0.10343337,0.010698461905121803
24,"Question\njust wondering how to convert a set of code into comments, without having to go through each individual line of code and put a # at the beginning. Was thinking that maybe there was a quick way to highlight the code and then convert it into comment, atm i'm just cutting it off the code and then re-pasting it in later on.
Much appreciated :)
P.S. i m using Pycharm as my IDE\nAnswer: In jupyter notebook, we select lines of code and press ctrl and / key simultaneously to convert a set of code into comments. Also same for vice versa.
You can try it in Pycharm.",0.20408164,0.06174785,0.020258907228708267
25,"Question\nI am currently working on my Bachelor Thesis. I want to predict the heartrate of people with video data. I build my network similar to the paper ""RhythmNet: End-to-End Heart Rate Estimation
From Face via Spatial-Temporal Representation"".
My problem:
My network does not learn to predict the HR properly instead it learns the mean HR. Could you give me some tipps how to improve the variety of my Regression output? Do you have some general tipps how to improve the network performance?
Thx in advance\nAnswer: According to Page 8 of the Paper, the output of the network is the mean Heart Rate. The output is computed with a mean pooling layer. Maybe you can get the single values from the single heart rates before the mean pooling layer?",0.0,-0.05224383,0.0027294177561998367
26,"Question\nMy python version is Python 2.7.17
I usually use pip to install other modules like shutil and itertool etc. I can't find the package name for os, getopt and sys modules.
Any help will be appreciated.
Thank you.\nAnswer: They are installed with your python as part of the standard library, if they are missing, something is wrong with your Python installation and you should reinstall it",0.0,0.05085942,0.0025866806972771883
27,"Question\nAlembic doesn't support replaceable objects like views so we often have to write a custom function in revision that executes creation of them. Is there a way to share these functions across revisions instead of copy-pasting over and over?\nAnswer: After looking, you can either reference code in a separate package or use env.py. Creating your own module is probably easier",0.0,0.30018902,0.09011344611644745
28,"Question\nI tried using Scikit Learn's classification_report() to find other details why I got a certain score but instead I got an error.

ValueError: continuous is not supported

I just want to know what it is and how to resolve it?\nAnswer: Probably you are trying to classify a continuous value. When the values are continuous you need to perform a regression. If you want to perform a classification you then perform a classifier in which you assign the features to a class (group).",0.40816328,0.27864468,0.016775067895650864
29,"Question\nIs there a way to find out what width a Label would be for a given text and font, without having to add it to the graphical elements currently being displayed.
I need to get the information without any effect on the GUI, however momentary that effect may be. And, in any case, packing appears to work in a top-down fashion, meaning that controls may expand to fill others higher in the hierarchy but I can't get them to adjust those higher to allow for their actual size.
I'm more used to QML where I can just give an off-screen element the text and font details then just read out the bounding box.
Does anyone know how to do something similar in Tkinter?\nAnswer: You can create the Label without calling pack() / grid() / place(), then call winfo_reqwidth() and winfo_reqheight() on the label to get its size.",0.81632656,0.21687895,0.3593374490737915
30,"Question\nUsing Scikit-Learn's RandomizedSearchCV module, how do you guarantee a certain set of hyper-parameter settings will all be tested?
My goal is to guarantee that, using a randomized search for optimal estimator hyper-parameters, all available activation functions for sklearn MLPClassifier are tested. Still, I would also like an answer that applies to Python machine learning models/estimators in general. I believe I could test which activation function was the best by running RandomizedSearchCV or GridSearchCV using 3 different instances of MLPClassifier. The problem is, what if I want to test all available activation functions and all weight ""solvers"" among other parameters such as the number of neurons and layers? Is there any way to do this using a Python library?\nAnswer: I am only posting this answer since the comment below the question is the best answer for me.
Comment by desertnaut on Aug. 08, 2020:

You cannot; RandomizedSearchCV provides absolutely no such guarantees. You should revert to GridSearchCV if you want to be sure that certain combinations will be tested.

The best solution for now is to use a combination of both RandomizedSearchCV and GridSearchCV.",0.0,0.3350764,0.11227618902921677
31,"Question\nSo I know what recursive functions are, but every time I have a task that seems like recursion is the way to go, I am not sure where to start.  What are some good ways to think about how to approach the problem?  For example, should I start from the last case scenario (the scenario that causes the recursion to stop) or the first case scenario?
Any references or guides that explain the approach (as opposed to what is recursion itself) would be helpful.  Or maybe it's not that hard and I'm just overthinking it.\nAnswer: Recursion is generally about solving a problem by dividing it into subproblems, until you reach a base case (or as you said, a ""last"" case). Once you have the base case(s) and the general case, you have solved the recursive problem.
The general case is usually the trickier bit. For some problems it can be more obvious, but a good tactic is to start with the simplest input you can think of (which may be your base case), and then work your way up to more complicated inputs.
For example, if you want to process a non-empty array in some manner recursively, you first solve the problem for arrays of length 1, then arrays of length 2, etc. until you see the general solution.
Practice is often most helpful. Look at solutions to example problems (after attempting them yourself) until you get a better feel for it. Good luck!",0.40816328,0.26944947,0.019241521134972572
32,"Question\nI'm trying to connect Google Colab to a local runtime (Windows 10). As part of that I need to add the jupyter_http_over_ws extension to Jupyter Notebook. py -m pip install jupyter_http_over_ws worked as expected but when I tried to enable it with py -m notebook serverextension enable --py jupyter_http_over_ws, it errored.
[E 13:43:13.400 NotebookApp] Support for specifying --pylab on the command line has been removed.
[E 13:43:13.400 NotebookApp] Please use %pylab jupyter_http_over_ws or %matplotlib jupyter_http_over_ws in the notebook itself.
Does anyone know how I can get this to work?\nAnswer: Uninstall the extention and try pip3 install install jupyter_http_over_ws. It worked fine for me.",0.0,0.18974686,0.03600386902689934
33,"Question\nI'm trying to connect Google Colab to a local runtime (Windows 10). As part of that I need to add the jupyter_http_over_ws extension to Jupyter Notebook. py -m pip install jupyter_http_over_ws worked as expected but when I tried to enable it with py -m notebook serverextension enable --py jupyter_http_over_ws, it errored.
[E 13:43:13.400 NotebookApp] Support for specifying --pylab on the command line has been removed.
[E 13:43:13.400 NotebookApp] Please use %pylab jupyter_http_over_ws or %matplotlib jupyter_http_over_ws in the notebook itself.
Does anyone know how I can get this to work?\nAnswer: Run jupyter-serverextension enable jupyter_http_over_ws",-0.35714287,0.17533809,0.2835359573364258
34,"Question\nI am wondering if there is a method to connect 3ds Max and Maya. For example, I want to use a Maya plugin to send a maxscript to 3ds Max to import an fbx in 3ds max. I think it just like send to Max function in Maya, and it seems like the OneClickDispatch does similar work, but I do not how OneClickDispatch sends the script to 3ds max and execute it. Does anyone know how to do it?
Thanks!\nAnswer: For example SublimeEditor is doing it by finding 3dsMax window handle, then is finding handle of childwindow ""Scintilla"" - maxscript listener, send text there, like filein @""X:\location\to\your\temp\script"" and then return char - should be easy to snitch from Sublime, otherwise you have to be quite familiar with Windows ShellApi",0.0,0.1517868,0.023039234802126884
35,"Question\nI'm writing Rasa source code for my chatbot, using Python 3.7.7, pip 20.1.1 and conda 4.5.12.
I use the command: conda install tensorflow to install this framework, version 2.1.0.
However when trying to execute rasa train to train my chatbot, I came up against an error with process of importing tensorflow.
The Error: ImportError: DLL load failed: The specified module could not be found.
I still find the tensorflow folder, so what I need to do to fix this problem?\nAnswer: You can Use pip install tensorflow",0.0,0.15527761,0.02411113679409027
36,"Question\nfirst of all apologies for the vague question, I thought I would clarify it in the body.
So basically, I am looking to design a website for my society, as a person with a fair amount of software development know how(or at-least I would like to think so), how can I go about designing a web application that can be edited, from the front end(I know this breaks alot of MVC principles), I want to make it so, that an administrator can login to the administrator account on the web app, and upload/delete a new blog and make aesthetic changes to the application. Is there any way to build/design a web applications this way.
So basically the program flow would look something like this

Administrators decides to update the blog
Administrator logins into the admin account
Edits the blog on the website
Saves the blog, the edited blog now appears on the front end, visible to all visitors.

Secondly, if the first option is not recommended, what frameworks can I use(preferably python) so that I can ensure the website is as maintainable as possible(after I finish university, I will not be maintaining it ).
Would a common framework such as Django, Flask suffice?
This must all be completed within 4-5 weeks, in a three developer team.\nAnswer: I would go with Flask, you can do almost anything you want with it.
When you say ""edit blog"" you mean ""edit posts of the blog"" like postbody-text, photos, etc?
These tasks can be done easily with Flask.
You can do the same with Django too, but Django is more heavy and is more suitable for big websites
In practice, you will create some routes in the Flask app with the functionality you want (edit posts, replace photos, etc) that will be accessed by the admin account only",0.0,0.06452882,0.004163968842476606
37,"Question\nI have text ""Number 2169/B/PK/Pjk/2019"", 
I want to match the following characters or strings /Pjk/ that begins and ends with a forward slash and whose intervening characters are not forward slashes from that text.


I already use code 

re.search(r""(\b/).*(/\b)"", text)

which are \b/ for search the character start that with /, /\b for search the character that end with /, and.* as AND operator to combine those both conditions. 

But I still don't know how to add the condition to limit length of characters at least 5 characters {5,} from that code above. 

Anyone have an idea?\nAnswer: To get 3 chars between / / you can do r""(\b/).{3}(/\b)"".
Simpler can be r""/[^/]{3}/"" and it will not get / between / /
For 3 or more chars you can add, (comma) r""/[^/]{3,}/""",0.81632656,0.3688035,0.20027688145637512
38,"Question\nI have Energy Meter connected through RTU and i am able to get holding registers data through simple RTU Code.
Now i want to make Convert this RTU to TCP through Forwarder. I want to send data to TCP which forwards the command to RTU and fetches data of RTU connected device for me.
I have implement the Forwarder code just dont know how to fetch the holding register of RTU through it.\nAnswer: If you are working with real device, make sure you are explicitly initialising the remote slave context with the unit Id.
context = RemoteSlaveContext(serial_client, unit=<unit-id-of-slave>)
The default is 0 which works fine with simulated slaves but would be considered as a broadcast address with the real devices generally and no response would be returned back.",0.0,0.1648202,0.027165696024894714
39,"Question\nSay I have a string only containing “A”, “B” and “C”.
For example, “ABBCAABBCABBBCBBA”.
How do I find the longest substring such that the substring does not contain “A”?
I know that itertools.groupby() can be used to find longest consecutive sequence of an element, but how do I adapt this to find the longest sequence such that an element isn’t present?\nAnswer: The most efficient method would be a single pass, stepping through each element and adding it to the ""current"" substring if it isn't an ""A"" and then just keeping track of the longest substring found.
Alternatively, you could just.split(""A"") and get the longest substring in the resulting list.",0.06802721,0.25542116,0.035116493701934814
40,"Question\nSay I have a string only containing “A”, “B” and “C”.
For example, “ABBCAABBCABBBCBBA”.
How do I find the longest substring such that the substring does not contain “A”?
I know that itertools.groupby() can be used to find longest consecutive sequence of an element, but how do I adapt this to find the longest sequence such that an element isn’t present?\nAnswer: You can keep using your itertools.groupby solution, by passing it a key function that checks for equality with ""A"".",0.06802721,0.39061195,0.10406091809272766
41,"Question\nI am writing a Python program with two threads. One displays a GUI and the other gets input from a scanner and saves data in an online database. The code works fine on my raspberry pi but if I try it on my MacBook Pro (Catalina 10.15.2), I get the above mentioned warning followed by my code crashing.
Does anyone have an idea how to get it working or what causes the problem?\nAnswer: You likely use different Python versions. Your Python on your Raspberry PI still allows invalidating NSWindow drag regions outside the Main thread, while your Python in your MacBook Pro already stopped supporting this. You will likely need to refactor your code so that NSWindow drag regions will only be invalidated on the Main thread.
You need to localize where NSWindow drag regions are invalidated and make sure that those happen in the Main thread.
EDIT
The asker explained that according to his/her findings, NSWindow drag regions only apply to Mac.",0.81632656,0.3266729,0.23976069688796997
42,"Question\nIs it possible + legal to have a website where when people register, a personal google drive is created for them? Trying to create a website where people can upload audio files in a post, which would be displayed for other users. Can the drive be used as a hosting platform for the uploaded files? If yes, then how can this be done?\nAnswer: Is it possible + legal to have a website where when people register, a personal google drive is created for them?

You cant programmaticlly create a google account for another user.  You could request access to their google drive account and access things there but the issue will be if you want to share their files with someone else.

Trying to create a website where people can upload audio files in a post, which would be displayed for other users.

You could use a service account to upload the files to an account that you the developer own and some how associate what was uploaded by each user.  Probably storing each users data in a directory linked to there user id in your system worked be the most logical.
The issue being here is you are going to be footing the bill for the storage space.

Can the drive be used as a hosting platform for the uploaded files? If yes, then how can this be done?

The google drive api is really just a file storage system.  If you want the users to be able to view theses files after they will need to download them to their own machine.  Drive api wasn't really meant to host that will be viewed.",0.40816328,0.3967973,0.00012918550055474043
43,"Question\nI am using Google Cloud Document AI's Form Parser API. After i do the request to the API, I get a response with type google.cloud.documentai.v1beta2.types.document.Document. I tried to write it to JSON using json.dumps()  but it gives JSONDecodeError because JSON.dumps() dont know how to serialize object of type google.cloud.documentai.v1beta2.types.document.Document.
I am confused how to convert this to JSON
Any Help Appreciated!\nAnswer: I solved my problem.
basically you have to write a function that explores the Document object, and then assemble entire JSON yourself by code.",0.0,-0.020829737,0.00043387795449234545
44,"Question\nI have a discord bot I need to scale.
The main features of the bot is to fetch data from a 3rd party website and also keep a database with member info.
These 2 operations are quite time consuming and I wanted to have a separate worker/process for each of them.
My constraints:

There is a limit of GET's per min with the 3rd party website.
The database can't be accessed simultaneously for same guild.

I've been researching online for the best way to do this but I come into several libraries/ways to implement this kind of solution. What are the options I have and their strengths and weaknesses?\nAnswer: Since there is a limit on the amount of requests from the host, I would first try to run a synchronous program and check whether the limit is reached before the minute ends. If it does then there would be no need to concurrently run other workers. However if the limit is not reached, then I would recommend you use both asyncio and aiohttp to asynchronously get the requests. There's a ton of information out there on how to get started using these libraries.
The other option would be to use the good old threading module (or concurrent.futures for a higher level use case). Both options have their pros and cons. What I would do is first try the concurrent.futures (namely, the ThreadPoolExecutor context manager) module since you only have to add like one line of code. If it does not get the job done, then remember: use asyncio if you have to, and threading if you must. Both of these modules are easy to use and understand as well, but they do need to follow a general structure, which means you'll most likely have to change your code.",0.0,0.08236319,0.006783694960176945
45,"Question\nI’m looking to essentially use two devices: raspberry pi 3 and Mac 10.15. I am using the pi to capture video from my web cam and I want to use my Mac to kind of extend to the pi so when I use cv2.videocapture I can capture that same video in preferably real-time or something close. I’m programming this using python on bout devices. I thought of putting it on a local server and retrieving it but I have no idea how I could use that with opencv. If someone could provide and explain a useful example, I would greatly appreciate it. Thank you.\nAnswer: To transfer a video stream, you could use instead of a custom solution a RTMP server on the source machine feeding it with the cam source and the target opens the stream and processes it.
A similar approach to mine is widely implemented into IP cameras: They run a RTMP server to make the stream available for phones and PC.",0.0,0.30331135,0.09199777245521545
46,"Question\nI have a large structure of primitive types within nested dict/list. The structure is quite complicated and doesn't really matter.
If I represent it in python's built-in types (dict/list/float/int/str) it takes 1.1 GB, but if I store it in protobuf and load it to memory it is significantly smaller. ~250 MB total.
I'm wondering how can this be. Are the built-in types in python inefficient in comparison to some external library?
Edit: The structure is loaded from json file. So no internal references between objects\nAnswer: This is normal and it's all about space vs. time tradeoff. Memory layout depends on the way how a particular data structure is implemented, which in turn depends on how it is going to be used.
A general-purpose dictionary is typically implemented with a hashtable. It has a fixed-size list of buckets that store key-value pairs. The number of items in a dictionary can be smaller, equal or bigger that number of buckets. If smaller, space is wasted. If bigger, dictionary operations take a long time. A hashtable implementation usually starts with a small initial bucket list, then grow it as new items are added to keep the performance decent. However, resizing also requires rehashing which is computationally very expensive, so whenever you do it, you want to leave some room for growth. General-purpose dictionaries are a trade-off between space and time because they don't ""know"" how many elements they are supposed to contain and because there is no perfect hash function. But in a good-enough case, a general-use hashtable will give you near-O(1) performance.
When data is serialized it's a different story. Data in transit does not change, you are not doing lookups with it, it is not subjected to garbage collection, boundary alignment and so on. This means you can simply pack keys and values one after another for space efficiency. You need virtually no metadata and no control structures as long as the values can be reconstructed. On the downside, manipulating packed data is very slow because all operations take O(n) time.
For this reason, you will almost always want to:

convert data from time-efficient into space-efficient format before sending it
convert data from space-efficient into time-efficient format after receiving it.

If you are using nested dictionaries (or lists,",0.0,0.39248437,0.15404397249221802
47,"Question\nI intend to have a program detect when a login failure occurs. I intend to use this program on mac, in case that is important. Could this solution detect how many login failures occur, or would I use a variable to determine how many have occurred?\nAnswer: As mentioned by @JaniniRami, you can scrape the data from the macOS logs located in /var/log/asl.log (except for macOS High Sierra), and see if there were any authentication failures in the log (such as warnings or errors)",0.0,0.31938267,0.10200528800487518
48,"Question\nI have a class with some simple methods, and I want to create a sub-class of this which builds on these methods and adds some more functionality.
So, I inherit from this class, and build on the simple methods in the parent class in my child class.
Now, how do I ensure that you can't call the (simple) methods in the parent class from an instance of the child class from outside the class? I want the parent class to be used for the simple methods and the child class to be used for the more complex methods.\nAnswer: Python's philosophy is ""we are all consenting adults"". If a class has a method, then that method can be called on any object of that class, or of a child class of that class. Therefore, what you're asking isn't really possible.
There's still something you could do. If you had a method on the parent class, you could override that method on the child class and have it do nothing. In this way, creating an instance of the child class and calling one of the parent class methods will instead call the overridden method you wrote and not actually do anything. Yet, I'd consider the above bad practice; it's definitely not recommended.",0.0,0.3171665,0.10059459507465363
49,"Question\nI'm brand new to Github but experienced using Python and just wondered how to use both so that I can upload and export scripts for others to use! I have created a repository to start with! Any help would be great.\nAnswer: for a beginner with GitHub as you, it will be great to use GitHub desktop.
Download it and clone the repository you created. Now move your files into the repository folder. In GitHub desktop add a quick summary about what you changed and click the ""commit"" button. With this you created a timestamp in the history of your repository, but it is only available on your local machine. Now click the ""push origin"" button on the top in GitHub desktop. This will push the changes to the repository. When your repository will become more popular, people might want to commit changes to it. To do this, they will open a pull request and you just need to approve it and their changes will be merged into your repository. I hope this helped. If you don't understand anything about GitHub, feel free to ask. :)",0.0,0.21389034,0.045749079436063766
50,"Question\nAfter updating macOS to Catalina, I can no longer import modules like Pandas, Numpy or Scipy--I get a ModuleNotFoundError. Interestingly, import os and import sys do not throw this error.
Does anyone know why this is and how to fix it?
Thanks\nAnswer: Probably after update you have another default Python interpreter.
You either should find where ""the old one"" is located or install again all libraries needed (e.g. pip install pandas)",0.0,0.39475662,0.15583278238773346
51,"Question\nScenario: Buildozer packaged python apk works fine on Android Emulator and shows Login screen. On hitting Login button I am getting details of logged in user from Mysql database
MySql database server is a Ubuntu chromebook. Android Emulator is on Windows machine.
I can access the database via from Windows machine using HeidiSql - i.e ip address and user name / password @ port 3306.
However the app running on the emulator gives a permission denied error
Please advise how I can find root cause of the issue and rectify it\nAnswer: The issue was happening because the buildozer spec file was missing the option
android.permissions = INTERNET
After putting this the sql queries started working.",0.0,0.0948118,0.008989277295768261
52,"Question\nI have seen PyCharm suggest to me to use a function named copyright(). I have never written such function / imported anything which means it is a python standard library function. could not find any docs about it neither description in PyCharm itself. The function don't have any arguments which seems odd as I would infer it's goal is to set some part of code to be copyrighted. What is this function for and how to use it?\nAnswer: copyright() is a Python builtin class.
You can CTRL + click on the function name in Pycharm so it will lead you to the actual source definition, which should belong to your standard python installation.
The function will print the actual Python copyright, which is contained in sys.copyright 
It's the same as credits function",0.40816328,0.24321258,0.027208732441067696
53,"Question\nI had the idea to synchronise the equal scripts from a python program
that Im running on different computers in parallel, by creating a
global timestamp based of the script that startet first.
So for a better explanation, it should work like this:
The Script runs on PC 1 and creates a timestamp.
Everything the script has to do after that will only be executed once
the time in the timestamp + 5 minutes is reached.
For the script on PC 2 or more, basically the same is happening but without
creating a new timestamp, since the starting time wouldnt be the
same anymore then.
Now I already build a Python webserver that creates the timestamp and I am
also able to retrieve the time for my script. But I have to make sure that I
can reach the webserver without beeing in my own network and
using ""localhost:..."" as an adress. So now I wonder how I can make the webserver
public in the most easy way possible. The other option would be an
already public site, that can create custom timestamps on the basis of my configuration, so that I only create 1 timestamp for the first visitor on the site.
Do you by chance know any of these sites or can tell me how I can make my HTML web server public?
Thanks\nAnswer: You could use an amazon service like elastickbeanstalk its free to some extent and will make your server public",0.10204082,-0.31241077,0.1717701256275177
54,"Question\nI had the idea to synchronise the equal scripts from a python program
that Im running on different computers in parallel, by creating a
global timestamp based of the script that startet first.
So for a better explanation, it should work like this:
The Script runs on PC 1 and creates a timestamp.
Everything the script has to do after that will only be executed once
the time in the timestamp + 5 minutes is reached.
For the script on PC 2 or more, basically the same is happening but without
creating a new timestamp, since the starting time wouldnt be the
same anymore then.
Now I already build a Python webserver that creates the timestamp and I am
also able to retrieve the time for my script. But I have to make sure that I
can reach the webserver without beeing in my own network and
using ""localhost:..."" as an adress. So now I wonder how I can make the webserver
public in the most easy way possible. The other option would be an
already public site, that can create custom timestamps on the basis of my configuration, so that I only create 1 timestamp for the first visitor on the site.
Do you by chance know any of these sites or can tell me how I can make my HTML web server public?
Thanks\nAnswer: You could create a proxy server on the same network with the pc you have the script running on and connect to it from the other pcs",0.10204082,0.09595105,3.7085290387040004e-05
55,"Question\nI have a trading bot that trades multiple pairs (30-40). It uses the previous 5m candle for the price input. Therefore, I get 5m history for ALL pairs one by one. Currently, the full cycle takes about 10 minutes, so the 5m candles get updated once in 10m, which is no good.
Any ideas on how to speed things up?\nAnswer: Just to follow up on that answer. You can see the candle closing as the websocket return data for every tick has a boolean property for if the candle is closed or not i.e. on a 5min timeframe if the candle closed on the 5min mark",0.20408164,0.27213013,0.004630597308278084
56,"Question\nWhen running a PyTorch training program with num_workers=32 for DataLoader, htop shows 33 python process each with 32 GB  of VIRT and 15 GB of RES.
Does this mean that the PyTorch training is using 33 processes X 15 GB = 495 GB of memory? htop shows only about 50 GB of RAM and 20 GB of swap is being used on the entire machine with 128 GB of RAM. So, how do we explain the discrepancy?
Is there a more accurate way of calculating the total amount of RAM being used by the main PyTorch program and all its child DataLoader worker processes?
Thank you\nAnswer: Does this mean that the PyTorch training is using 33 processes X 15 GB = 495 GB of memory?

Not necessary. You have a worker process (with several subprocesses - workers) and the CPU has several cores. One worker usually loads one batch. The next batch can already be loaded and ready to go by the time the main process is ready for another batch. This is the secret for the speeding up.
I guess, you should use far less num_workers.
It would be interesting to know your batch size too, which you can adapt for the training process as well.

Is there a more accurate way of calculating the total amount of RAM being used by the main PyTorch program and all its child DataLoader worker processes?

I was googling but could not find a concrete formula. I think that it is a rough estimation of how many cores has your CPU and Memory and Batch Size.
To choose the num_workers  depends on what kind of computer you are using, what kind of dataset you are taking, and how much on-the-fly pre-processing your data requires.
HTH",0.20408164,0.15171784,0.0027419673278927803
57,"Question\nI am setting up a discord game which uses folders as profiles, how do I fix the problem that if someone changes their username the bot can no longer access their profile?\nAnswer: Use the UserID instead of the Username.
This never changes unless they change to a different account, so you won’t have to worry about usernames changing.",0.20408164,0.3159532,0.012515245005488396
58,"Question\nI would like to use PandasGui to show intermediate DataFrame results of my app. It works perfectly in Jupyter. However, once I create the executable (using PyInstaller) the app does not work - just cmd shows for a second. Generaly I have no problem with other GUIs after converting to exe (EasyGUI, tkinter, etc.). So this must be something very specific.
Any ideas how to make it work?
Or any alternative to PandasGui which works correctly?\nAnswer: What exactly was the pyinstaller command that you tried for compiling the executable?
Also, you may want to consider tkinter or PyQt5 if this problem persists and you're unable to get a solution here since at least the former has worked for you in the past.",0.20408164,0.26606178,0.0038415382150560617
59,"Question\nTo give an example of what I'm trying to do, let's say there is a website that displays results of a lottery every hour. The webpage itself is static with the surrounding text staying the same and only the numbers changing (input by human not updated dynamically).
Something like The lucky number is: X where X indicates a different number each hour.
Now I want to run a python script that parses the number(s) each hour, and then at the end of the day would print out all the numbers in a nice format.
I know how to get the webpage content and get only the text parts of it without html tags etc by using the BeautifulSoup and requests libraries, however I'm not quite sure how to get the target number.
I was thinking something like a regex which would find a static word from the text e.g. 'number is:' in this case and then grab the word (number) right after it.
Is this doable? and if yes, how?
Thank you in advance.\nAnswer: It's possible with regex but if you know the string already and if it's static, use simple split on that string.
Let's say
var='The lucky number is: 123'
Out= int(var.split(':')[1])
Out will be 123",0.0,0.059369504,0.0035247381310909986
60,"Question\nWhen using cv2.HOGDescriptor().detectMultiScale, what is the starting size of the detection window used? Is it the same size as my training data?
For example, if my training data are all 64*128 images then the detection window starts at 64*128?
and how is the scaling factor used? For example, If I want to detect humans on an image of size 640*512, and I set scale=1.05, how is this 1.05 used?\nAnswer: The detection window is always 64 x 128 by default.  To accommodate for the multiscale, the image is progressively scaled to create an image pyramid while keeping the detection window of 64 x 128 the same.  This achieves the effect of searching for humans at larger sized search windows in order to keep the search window the same size.  The image pyramid is constructed by progressively decreasing the image size by the scale factor until the 64 x 128 search window can no longer fit inside the rescaled image.  Therefore, if your search images already consist of 64 x 128 images then there will only be one scale.
This moves to your next question where if scale=1.05, we produce an image pyramid by progressively resizing the input image rows and columns by rows / (scale ** i) and cols / (scale ** i) where i = 0, 1, 2,... to provide an image pyramid.  For each image in the pyramid, we use the 64 x 128 search window to look for the object of interest.",0.40816328,0.3884585,0.00038827871321700513
61,"Question\nI have an array of log values temp= [4,4.05......9]
These are actually from 10^4 to 10^9
for example, log(10^4)=4 and so on
I need to take the antilog of temp array to get these numbers from 10^4 to 10^9 but when I calculate np.exp(6.89) it gives me 991
So how to take antilog correctly?
enter code hereEve_gradual_mr24_23=np.exp(np.mean(new_q3[340:350]))\nAnswer: np.exp is the exponential in base e. If you want the exponential in base 10, you could use scipy.special.exp10 or simply 10.0**your_array",0.81632656,0.40747246,0.16716167330741882
62,"Question\nI've trained my langue model with cmu sphinx and now I want to use it in speech recognition using python script. How to change the default language model or how to use the trained model in recognize_sphinx().\nAnswer: c = r.recognize_sphinx(audio, language='zh-cn')",0.40816328,0.1802665,0.05193694308400154
63,"Question\nI have 150 GB file in s3,I would like to unzip and upload the each file back to s3.what is the best approach to do with python and EC2? I appreciate your response.\nAnswer: Download it on your system
Unzip the folder and it will create a normal folder say ""Unzipped_Folder""
Assuming you are using Windows, install aws-cli in that.
Create an IAM User with S3 write access to that bucket and create a Access and Secret Access Key.
In aws-cli add the credentials from command prompt.
$ aws configure
Now run the following command to send files to S3 bucket
$ aws s3 cp your_directory_path s3://your_bucket_name --recursive",0.0,-0.16102576,0.025929296389222145
64,"Question\nI use Anaconda with Spyder that was installed in Anaconda to learn python.
The problem: when I'm trying to save python scripts by extending the file name to.py, the python script automatically gets saved in the Windows notepad. And when I try to open that again in Spyder it doesn't open.
In other words, in Spyder I saved the script by doing ""file - save as - 'filename.py'""
and I tried to open the saved script by doing ""file - open - 'filename.py'"" in Spyder.
However, it doesn't work.
I two questions about this.

Is it normal for python scripts to be saved on windows notepad? If not, what is wrong here and how do you normally save python scripts with Anaconda?

Why doesn't Spyder open the notepad file, the python script I saved? I added the extention.py when I saved it but it is not opening in Anaconda Spyder regardless.


I realize this is a basic question to many and the answer may exist somewhere but I have a very hard time finding it and I'm hoping someone can help me solve this problem. Thanks.\nAnswer: Answer to the First question
Is it normal for python scripts to be saved on windows notepad? If not, what is wrong here and how do you normally save python scripts with Anaconda?

Check what default program is set
for opening.py file in your system you can change it by
rightclicking on.py file >expand open with > choose open with another
app and tick the option 'always use this for.py file'

Second question
Why doesn't Spyder open the notepad file, the python script I saved? I added the extention.py when I saved it but it is not opening in Anaconda Spyder regardless.

Check type of file your saving ""the field below filename it should be
allfiles(*)"" probably your script is saving ""filename.py.txt"" which is
not a proper format for python file",0.40816328,0.24234825,0.02749462239444256
65,"Question\nI would like to install pip for the default installation of Python on Mac OS.
Please don't recommend brew, I already have it and installed Python 3 with it, but it seems that Automator only knows how to use the default version of Python located in /usr/bin/python That's the reason behind my specific request
I did my homework first, or tried to, before asking the question, but what I found confusing is that the recommended method seems to be using get-pip.py, but the pip documentation says

Warning Be cautious if you are using a Python install that is managed
by your operating system or another package manager. get-pip.py does
not coordinate with those tools, and may leave your system in an
inconsistent state.

This threw me off, as I don't want to risk breaking the default Python on Mac OS, as I understood that might mess my system.
I also didn't want to use the deprecated easy_install.
And I couldn't find an answer to my question, as usually the answers just recommend installing a different version of Python with brew.\nAnswer: Problem
Seems like Automator isn’t loading /usr/local/Cellar/bin into your PATH. You can echo $PATH in Automator to confirm this.
Solution
Reinstall using brew and ensure that you run brew link python.
You can export PATH=... before running your script or move /usr/bin/python to /usr/bin/pythonx.x where x is the default version installed, then symlink /usr/bin/python to your brew installed python in /usr/local/bin/.",0.0,0.36977863,0.13673624396324158
66,"Question\nI have a Python codebase with some tests that I am able to run with the python -m unittest... command however, when I run the same tests with bazel test, the tests just get stuck and time out. One thing to note is that the code is using python's multiprocessing, and also makes a bunch of post requests to an external service.
Using bazel run and logging entry points in several parts of the code verifies the code is randomly stuck.
top also does not show a lot of resources being used.
Any ideas on how to debug this? Most of the tests are set to size=large and have the ""exclusive"" tag.\nAnswer: The bazel sandbox by default blocks all network requests. You can specify that a given test requires network access by adding the tag requires-network.
Alternatively you can add the tag no-sandbox to disable the sandbox completely for the given test / action.
It is also possible that network access is disabled by a bazelrc option such as --modify_execution_info=TestRunner=+block-network so you might want to check your bazelrc too if the requires-network tag doesn't fix the timeouts in the tests.",0.0,0.36852682,0.13581201434135437
67,"Question\nI have installed git, github desktop, python 3.7.0 and 3.8.1 and have py.exe in my c://windows folder. Nevertheless, when I try to type a command in power shell in github desktop for a repo that I have cloned, it says that the ""pip"" is not recognized. Any idea how to fix this?\nAnswer: Executables are searched om your PATH environment variable, which probably doesn't include c:\windows. You need to edit this environment variable and add c:\windows to it.",0.0,0.24104053,0.058100536465644836
68,"Question\nI wanted to read this article online and something popped and I thought that I want to read it offline after I have successfully extracted it... so here I am after 4 weeks of trials and all the problem is down to is I the crawler can't seem to read the content of the webpages even after all of the ruckus...
the initial problem was that all of the info was not present on one page so is used the button to navigate the content of the website itself...
I've tried BeautifulSoup but it can't seem to parse the page very well. I'm using selenium and chromedriver at the moment.
The reason for crawler not being able to read the page seems to be the robot.txt file (the waiting time for crawlers for a single page is 3600 and the article has about 10 pages, which is bearable but what would happen if it were to say 100+)and I don't know how to bypass it or go around it.
Any help??\nAnswer: If robots.txt puts limitations then that's the end of it. You should be web-scraping ethically and this means if the owner of the site wants you to wait 3600 seconds between requests then so be it.
Even if robots.txt doesn't stipulate wait times you should still be mindful. Small business / website owners might not know of this and by you hammering a website constantly it could be costly to them.",0.40816328,0.20528734,0.04115864634513855
69,"Question\nI am using skmultilearn library to solve a multi-label machine learning problem. There are 5 labels with binary data (0 or 1). Sklearn logistic regression is being used as base classifier. But I need to set label specific features for each classifier. The label data of one classifier to be used as feature of another classifier.
I am not able to figure out on how to do that.
Any help appreciated.\nAnswer: One-vs-Rest is the method of solving the multi-label problem you are trying to address, it is the transformation type. You just need to generate a different training set for each simple classifier so that you have all the combinations between the original attributes and each of the labels. Pandas can be useful for the manipulation of the data and the generation of the different datasets for each simple classifier. Note that using this strategy in its original form ignores the relationships between the tags.",0.0,0.18674618,0.03487413749098778
70,"Question\nI have refactored my app and project names, but after that Django doesn't create migrations folder in my app and doesn't actually apply my models migrations.
Even after migrations (with no warning nor error) I have no tables with my objects.
Does anybody know how to force django to do those migrations?\nAnswer: There can multiple reasons behind it. Please check following if you are missing something.

The app must have migrations/__init__.py folder. It automatically creates but if you did code refactoring. You can miss this.

Check INSTALLED_APPS in settings.py it should have the same app name as in admin.py.",0.0,0.4706577,0.22151868045330048
71,"Question\nI am relatively new to Machine Learning and Tensorflow, and I want to try and implement mini-batch gradient descent on the MNIST dataset. However, I am not sure how I should implement it.
(Side note: the training images (28px by 28px) and labels are stored in Numpy arrays)
At the moment, I can see 2 different ways to implement it:

My training images are in a Numpy array of [60000,28,28]. Reshape this into a [25 (num batches), 2400 (num images in batch), 28,28] and then use a for loop to call each batch and pass it the model.compile() method. The only thing that I am worried about with this method is that for loops are inherently slow, and a vectorised implementation would be much quicker.

Combine the images and labels into a tensorflow dataset object, and then call the Dataset.batch() method and Dataset.prefetch() method, and then pass the data to the model.compile() method. The only problem with this is that my data doesn't remain as a Numpy array, which I feel have more flexibility than tensorflow dataset objects.


Which of these 2 methods would be best to implement, or is there a third way that is best that I am not aware of?\nAnswer: Keras has an inbuilt batch_size argument to its model.fit method (since you tagged this question with keras I assume that you're using it). I believe that this will probably be the best optimised method to achieve what you're looking for.",0.81632656,0.27070242,0.2977057099342346
72,"Question\nI know very little regarding python,I'm using it to get data from twitter.
my old laptop has python version 3.7.3 it works fine, but I didn't set it up.
I installed python on my new laptop, but I think I also need to install some packages from my old python (laptop)
the error shows in my new laptop ""no module named twitter""
I don't know how this works.\nAnswer: First, you need to cd to your python project and then activate your virtual environment and run
pip install twitter",0.0,0.10791722,0.01164612639695406
73,"Question\nCan you please tell me how can I integrate the python automation in my flutter app. Such that suppose I have a text field and I have entered a youtube channel name. Then the app will automatically go to that channel and play the latest video for me as we do in desktops using selenium.
Yes, something like that. Also, I want to perform many such tasks using python automation. Please tell me if you have any reference links or knowledge regarding this.\nAnswer: Whatever data you have automated in Python, you can expose as an API in Flask, then make HTTP requests to that endpoint from your Flutter app.",0.40816328,0.13897479,0.07246244698762894
74,"Question\nOne of my friends who works on JAVA asked me how do I handle checked and unchecked exceptions in Python. I haven't heard these terms before, so I googled around to find out what is checked and unchecked exception. I didn't find anything related to this kind of exception in Python.

Do we have a concept of checked and unchecked exceptions in Python? If
no, then by default all the exceptions are checked or unchecked?

Thank you in advance!\nAnswer: Java have checked and unchecked exceptions because Java is a complied programming language, checked exception comes in compiling. In python, there is no such exception because Python is an interpreted language.",-0.35714287,0.22448659,0.33829283714294434
75,"Question\nI have an existing flask application (having a bunch of REST apis) which is  hosted by Gunicorn. I also have to send and receive messages from AmazonMQ in the same application. I have functions using stomp.py to send/receive messages from AmazonMQ, but I am not sure where to place them, or how should I use them so as to keep existing flow intact\nAnswer: Hey thanks for help @AKX,
I resolved the issue by triggering the whole process of messaging (Creation of connection, subscription to channel etc) by a method call from
on_starting(server)
method in gunicorn_config.py",0.0,0.4306273,0.1854398548603058
76,"Question\nThere is a game where rectangles falls and we have to avoid them, if we collide once speed decreases by 1, second time speed decreases by 2 and so on
How to find out how many times we collided\nAnswer: You can create a variable which gets added to every time you collide and another variable which is set to true once it collides then you can make it false and send the rectangle somewhere else.",0.40816328,0.16897434,0.05721134692430496
77,"Question\nI am building a website where user come and login, after login user publish articles with admin approval. I do not know how to do it. I made a user authentication system where user can login. But i do not know how to make him post data with admin approval.\nAnswer: That's a Good one. You can enable this with adding a new column to your database like onapproval Set it as an boolean variable like 0 or 1 either true or false. Then check for it. If it's true you can set the status as approved and if it is not set it as not approved. The same process will also takes place in admin panel too.",0.0,0.1500107,0.022503212094306946
78,"Question\nI have a python functionality in Jupyter notebook/ Google Colab which checks current stock prices and other computations. I'm planning to set up some alerts based on some triggers. So for being able to be running throughout the business day, how and where do I run the code?
i. How do I run the python code 24x7? Do I need to have a designated server that runs 24/7?
ii. Should I be changing my code to python scripts rather than using notebooks?
iii. Is there a good reference for the python project structure?
iv. Any suggestions on python libraries for setting up email/sms/Whatsapp alerts?
v. I'm planning to add ML features in the future. Should I be using cloud functions vs local GPU?\nAnswer: One design solution would be to use a cloud provider like google cloud platform or amazon web services to schedule the execution of a python script. The involved gcp services would be Pub/Sub, Cloud Scheduler, Cloud Functions, Storage and Cloud Source Repositories.

Pub/Sub := an asynchronous messaging service
Cloud Scheduler := a cronjob scheduler
Cloud Functions := Functions-as-a-Service, deploys automatically a Compute Engine and Storage to execute the python script and deletes the Compute Engine and Storage afterwards.
Cloud Repository := private Git repository


You create a topic/channel with Pub/Sub.
You set up a cronjob via Cloud Scheduler to send a message periodically to the previously created pub/sub topic.
You connect your GitHub repository which consists the python script with the Cloud Source Repository (Cloud Source Repository will from now on sync automatically with your GitHub Repository)
You create a function with Cloud Functions and use your Cloud Source Repository as the source code for the function. As a trigger for this function you set any message which is send to the previously created Pub/Sub topic.
The python script could include the yfinance library to request the stock data, the google cloud libraries to handle the data storage via GCP Storage and then you could potentially extend it with other google cloud libraries to include ML services.",0.40816328,0.2721917,0.018488269299268723
79,"Question\nI am using Open CV2 face detection in Python.      It works very well, but often finds faces that, although they really are faces, are so blurry as to be useless.      It succeeds in finding faces that can't be recognized as male or female, adult or child, but still clearly human faces.
Detecting a face that can't be recognized is not a useful result, but I don't know how to programmatically block these results, or determine that they should be ignored.
The only approach I currently have is to ignore any face smaller than a certain threshold, but I still get some large blurry faces.
Any suggestions?       I am using the haarcascade_frontalface_alt_tree.xml for detection.\nAnswer: If your problem is to detect faces of Male, female or child you need to feed the images of the genders and train your program. It involves a lot of programming, but can be solved easily with opencv. You need to train your model(project) with thousands of images for accuracy.
If you want to detect certain faces only, you need to do the same but train your model with the images of faces you want to detect.....",0.0,0.18069363,0.032650187611579895
80,"Question\nI wrote a python script which is deployed in a EC2 Instance and lets say this EC2 reside in AWS account A1. Now my script from A1 want to access 10 other AWS account.
And remember I don't have any AWS_ACCESS_KEY or SECRET_KEY of any account cause using AWS_ACCESS_KEY or SECRET_KEY is strictly prohibited here.
I can easily do that if I have access key. But I can't figure it out how can I do that without access key?
Is there any possible way to do that?\nAnswer: The EC2 should assume an IAM Role.
Then log in to all your 10 other accounts and create roles there. These roles should give cross account access to the EC2 instance role. It is also in these roles that you define what permissions the EC2 instance should have.",0.27210885,0.19084084,0.006604489870369434
81,"Question\nI just updated my python3 using homebrew and my python3 was messed up badly. I followed the instructions on other threads and was able to cleanly install python3 but some linking still exists that I am unable to figure out.
Problem:
python3 -version
dyld: Library not loaded: /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Python
Referenced from: /Users/abcd/.ansible/py3/bin/python3
Reason: image not found
zsh: abort      python3 -version
Python paths:
abcd@abcd-ltm Cellar % which python
/Users/abcd/.ansible/py3/bin/python
abcd@abcd-ltm Cellar % which python3
/Users/abcd/.ansible/py3/bin/python3
echo $PATH:
/Users/abcd/.ansible:/Users/abcd/.ansible/py3/bin:/Users/abcd/.ansible/bin:/Users/abcd/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/abcd/.ansible:/Library/Apple/usr/bin
I dont know from where it is still refering and getting this error at any python3 command
dyld: Library not loaded: /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Python
I can confirm there is no folder named python inside /usr/local/Cellar/. There is one newly created python@3.8 though which should be correct. Any pointers how I can find where is it picking up the incorrect python path and how I can fix it?\nAnswer: It appears you have an ansible installation that built a virtualenv associated with the Python that was upgraded in Homebrew. You should rebuild those virtualenvs or remove them from your PATH if you don't need them.",0.40816328,0.06565595,0.1173112764954567
82,"Question\nUsing beautifulsoup it's easy to fetch URLs that follow a certain numeric order. However how do I fetch URL links when it's organized otherwise such as https://mongolia.mid.ru/en_US/novosti where it has articles like
https://mongolia.mid.ru/en_US/novosti/-/asset_publisher/hfCjAfLBKGW0/content/24-avgusta-sostoalas-vstreca-crezvycajnogo-i-polnomocnogo-posla-rossijskoj-federacii-v-mongolii-i-k-azizova-s-ministrom-energetiki-mongolii-n-tavinbeh?inheritRedirect=false&redirect=https%3A%2F%2Fmongolia.mid.ru%3A443%2Fen_US%2Fnovosti%3Fp_p_id%3D101_INSTANCE_hfCjAfLBKGW0%26p_p_lifecycle%3D0%26p_p_state%3Dnormal%26p_p_mode%3Dview%26p_p_col_id%3Dcolumn-1%26p_p_col_count%3D1?
Websites such as these are weird because once you first open the link, you have » Бусад мэдээ  button to go to the next page of articles. But once you click there, now you have Previous or Next button which is so unorganized.
How do I fetch all the news articles from websites like these (https://mongolia.mid.ru/en_US/novosti or https://mongolia.mid.ru/ru_RU/)?\nAnswer: It seems that the » Бусад мэдээ  button from https://mongolia.mid.ru/ru_RU/ just redirects to https://mongolia.mid.ru/en_US/novosti. So why not start from the latter?
To scrape all the news just go page through page using the link from the Next button.
If you want it to be more",0.40816328,0.3435776,0.0041713109239935875
83,"Question\nas I use the holoviews library in conjunction with the bokeh backend, I would like to create a 3D surface plot, too. In the documentation I only found matplotlib and plotly backends.
Can somebody share a code snippet how to use holoviews/bokeh for 3D surface plots?
Thank you\nAnswer: Bokeh is a 2d plotting library, there is no built-in support for 3d plots. It's possible to wrap third-party 3d JS plotting tools as Bokeh custom extensions, but AFAIK Holoviews has not done this.",0.0,0.2735262,0.07481657713651657
84,"Question\nI am currently working on automating commands for a Docker container with a Python script on the host machine. This Python script for now, builds and runs a docker-compose file, with the commands for the containers written into the docker-compose file and the Dockerfile itself.
What I want to do is have the Python script action all commands to run within the container, so if I have different scripts I want to run, I am not changing the container. I have tried 2 ways.
First was to run os.system() command within the Python script, however, this works only as far as opening the shell for the container, the os.system() command does not execute code in the Docker container itself.
The second way uses CMD within the Dockerfile, however, this is limited and is hard coded to the container. If I have multiple scripts I have to change the Dockerfile, I don't want this. What I want is to build a default container with all services running, then run Python scripts on the host to run a sequence of commands on the container.
I am fairly new to Docker and think there must be something I am overlooking to run scripted commands on the container. One possible solution I have come across is nsenter. Is this a reliable solve and how does it work?  Or is there a much simpler way? I have also used docker-volume to copy the python files into the container to be run on build, however, I can still not find a solve to automate the accessing and running these python scripts from the host machine.\nAnswer: If the scripts need to be copied into a running container, you can do this via the docker cp command.  e.g. docker cp myscript.sh mycontiainer:/working/dir.
Once the scripts are in the container, you can run them via a docker exec command.  e.g docker exec -it mycontainer /working/dir/myscript.sh.
Note, this isn't a common practice. Typically the script(s) you need would be built (not copied) into container image(s).  Then when you want to execute the script(s), within a container, you would run the container via a docker run command.  e.g. docker run -it mycontainerimage /working/dir/myscript.sh",0.40816328,0.09213364,0.09987473487854004
85,"Question\nCMD:
C:>pip list
Package        Version

beautifulsoup4 4.0.0b4
imgkit         1.0.2
pip            19.2.3
setuptools     41.2.0
wheel          0.34.2
But how can I use this packages like imgkit and beautifulsoup4 in Pycharm?\nAnswer: You can write the command ""pip install <PACKAGE_NAME>"" in the command line or just import it and Pycharm will ask you if you like to install this package.
Make sure that the python interpreter of your Pycharm project is the same running on your command line, if not it will not sync its kind of a headache, from experience.",0.13605443,0.21193808,0.005758329294621944
86,"Question\nIf I'm in the Python IDLE editor and the shell is not open, is there some way to open the shell without running a program? I expect it's something simple that I just can't find.
Thanks\nAnswer: For windows:

Win+R to open run window
cmd to open, well, the command line
python to run python. Make sure you've added the python.exe file to PATH",0.40816328,0.47890592,0.005004520528018475
87,"Question\nwhen i try to import tensorflow from jupyter notebooks. I'm facing a error No module named 'tensorflow'.
But i have installed tensorflow using pip command, and it available in this path c:\program files\python38\lib\site-packages.
please tell me how to access packages installed via pip from jupyter notebooks?\nAnswer: When you installed tensorflow you had a specific environment active and that is where tensorflow was installed. If you are using Anaconda and did not specify which environment to make active it installed it in the base environment. If you want to install tensorflow to a specific environment (lets call it tf) then start the anaconda prompt and enter the text conda activate tf. Then install tensorflow with pip in the same window. My recommendation is to install tensorflow with conda versus pip. conda installs tensorflow and also installs the cuda toolkit and the proper version of cuDNN. pip does not do that. If you install tensorflow with conda I believe it installs version 2.1, cuda toolkit version 10.1.243 and cuDNN version 7.6.5.",0.40816328,0.25426745,0.023683924227952957
88,"Question\nI installed a Python package using pip from git. In order to achieve this, I added the command
-e git+<link>#<egg> to my requirements.txt. Pip installs this package, but not in the way I would like to have it.
First problem: I use a virtual environment. Usually, packages end up in <env>/Lib/site-packages. This one does not, it ends up in <env>/src. This makes it more to difficult to import.
Second problem: The src folder also gets created within my actual project, the one I am working on.
Both things are different to installing packages from other sources like PyPI. Is there a way to install my own packages via git in the same way? I actual built a wheel and a tar.gz, but I don't know how to install them...\nAnswer: @AnthonySottile's comment pointed me into the right direction: The -e option makes the installed package editable, which is not what I wanted. It actually works without it!
So if you want to avoid having src folders appearing everywhere - download from git without the -e option:
git+<link>#<egg>
Unfortunately, I didn't find any documentation for this... so feel free to post a more profound answer or add to this one.",0.0,0.18757784,0.035185448825359344
89,"Question\nI want to subset anndata on basis of clusters, but i am not able to understand how to do it.
I am running scVelo pipeline, and in that i ran tl.louvain function to cluster cells on basis of louvain. I got around 32 clusters, of which cluster 2 and 4 is of my interest, and i have to run the pipeline further on these clusters only. (Initially i had the loom file which i read in scVelo, so i have now the anndata.)
I tried using adata.obs[""louvain""] which gave me the cluster information, but i need to write a new anndata with only 2 clusters and process further.
Please help on how to subset anndata. Any help is highly appreciated. (Being very new to it, i am finding it difficult to get)\nAnswer: If your adata.obs has a ""louvain"" column that I'd expect after running tl.louvain, you could do the subsetting as
adata[adata.obs[""louvain""] == ""2""]
if you want to obtain one cluster and
adata[adata.obs['louvain'].isin(['2', '4'])]
for obtaining cluster 2 & 4.",1.0,0.20953494,0.6248350143432617
90,"Question\nI'm just wondering if anybody knows how I should approach the problem of automating extracting attachments from emails, putting the attachment through an excel macro, and then sending an email out! I have experience working with Python and R for data science but don't have much straight coding experience. Anybody got any resources I could read up on/scripts that exist on GitHub for something similar?\nAnswer: well you can use beautiful soup to automatically scrape csv/xlsx files from your email and use pandas to extract needed data and then use smptlib to send emails out.",0.0,0.0040775537,1.6626445358269848e-05
91,"Question\nWe're using the DocuSign SDK (version 3.3.0) and utilizing webhooks to receive the various envelope and recipient events.
Similar to DocuSign, we have given a option to delete an envelope in our application. Since there is no webhook notification once a envelope is moved to deleted bin. And also there is a field called deleted_date_time in get_envelope method (which provides the envelope details) and there is no status change, which are not updated once the envelope is being removed from the deleted bin.
Can anyone please help me in how to make sure the envelope has been removed from the deleted bin?\nAnswer: There's no means to permanently delete and envelope or remove it from the recycle bin folder. You cannot do that from the DocuSign Web App either as far as I can tell.
Voiding an envelope is and deleting it is as good as it gets. There's no need to worry about it anymore.",0.40816328,0.29730725,0.012289058417081833
92,"Question\nI created five functions in AWS which are being triggered by an ALB.
The lambdas are written in Python and I need to create a single swagger doc for them.
I considered using flask_swagger_ui but the only way I found to make it work is to use a single lambda (instead of five) and let flask do the routing inside that lambda (A solution which is not good for me)
Any ideas on how this can be achieved?\nAnswer: You can create an additional Lambda, which is just for documentation purpose.
The functions in that Lambda are just annotated placeholders, and the ALB listener routes only the documentation path requests to this lambda, and the other requests to the ""real"" Lambdas.",0.0,0.27000695,0.07290375232696533
93,"Question\nCurious, if there is way to avoid skipping messages sent from Telegram Bot while web-server that accepts Webhooks is down (because of redeploy, failure or maintenance).
When you use polling - Telegram API sends messages starting from last retrieved and no message are skipped.
But how to be with Webhooks? Use polling or there are some special mechanism for that?\nAnswer: I had the same problem recently but I just resolved it by when the server starts save the started time to a variable and then use Telegrambot.Message.date and compare the time if it was sent before the server start time or not.",0.0,0.17293489,0.02990647591650486
94,"Question\nI want to integrate external data into a Django app. Let's say, for example, I want to work with GitHub issues as if they were formulated as normal models within Django. So underneath these objects, I use the GitHub API to retrieve and store data.
In particular, I also want to be able to reference the GitHub issues from models but not the other way around. I.e., I don't intend to modify or extend the external data directly.
The views would use this abstraction to fetch data, but also to follow the references from ""normal objects"" to properties of the external data. Simple joins would also be nice to have, but clearly there would be limitations.
Are there any examples of how to achieve this in an idiomatic way?
Ideally, this would be would also be split in a general part that describes the API in general, and a descriptive part of the classes similar to how normal ORM classes are described.\nAnswer: I would suggest to just use normal OOP principles, Polymorphism, Association etc. to get a similar feel to real models.
But I'm not sure I would try to simulate behavior as close as I could, because the ORM is specifically designed for database interaction. I would just write my custom methods.",0.0,0.20990276,0.04405916854739189
95,"Question\ni have an NI USB Data Logger. when i plug-in the device, device sends same 10 byte data every 100 ms before opening the software. i captured this data using an USB packet sniffer software.(assume this data is device id). my question is how the device can send data without its software being open? and how can i find out with which endpoint this packets are sending?
thank you\nAnswer: I can't answer your first question (why a particular device could be sending 10-byte packets before its accompanying application software is launched). There are numerous possibilities that are in line with the USB spec.
To your 2nd question, you may use a tracing software like WireShark or Microsoft Event Analyzer (the latter has recently reached EOL, though) to find out what endpoints do the transfers belong to and even decode the content to produce an easily-readable outputs (for devices belonging to standard classes).",0.0,0.22252274,0.04951636865735054
96,"Question\nIn a terminal emulator with readline support, I can use key binding Ctrl-X Ctrl-E to bring up $EDITOR to edit a command.
How do I do that in iPython to bring up $EDITOR to edit half-finished code?
P.S. My $EDITOR is set to ""vim -u "".\nAnswer: shortcuts used:
'g' to launch gvim with the content of current cell (you can replace gvim with whatever text editor you like).
so, when you want to edit the cell with your preferred editor, hit 'g', make the changes you want to the cell, save the file in your editor (and then quit), then press 'u'.",0.40816328,0.30703413,0.010227103717625141
97,"Question\nI would like to know how to stop lightgbm logging.
What kind of settings should I use to stop the log?
Also, is there a way to output only your own log with the lightgbm log stopped?\nAnswer: Follow these points.

Use ""verbose= False"" in ""fit"" method.
Use ""verbose= -100"" when  you call the classifier.
Keep ""silent = True"" (default).",0.0,0.4790175,0.2294577658176422
98,"Question\nI am wondering how to make an entry point to call a python script, similar to black.
e.g.:
black my_script.py
Say I have a python file called fix_newline.py.
Instead of calling python fix_newline.py path/to/my_script.py in the directory of fix_newline.py, I'd like to assign the name fix_newline to python path/to/fix_newline.py.
The ultimate goal is to call fix_newline from anywhere in my directory tree, as long as I am in the same environment (e.g. ~/.bash_profile).\nAnswer: Add path/to to your PATH variable.bash_profile. (If you have a lot of scripts, consider installing them in a fixed location like ~/bin/, so that you don't add a lot of unnecessary directories to your PATH.

Make sure you script is executable and has an appropriate shebang.

Drop the.py from the script name.",0.0,0.33443022,0.11184357106685638
99,"Question\nWhen writing a Python package, I know how to specify other required Python packages in the setup.py file thanks to the field install_requires from setuptools.setup.
However, I do not know how to specify external system dependencies that are NOT Python packages, i.e. a commands such as git or cmake (examples) that my package could call via subprocess.call or subprocess.Popen?
Do I have to manually check the availability of the commands in my setup.py file, or is there a fancy way to specify system requirements?
Edit: I just want to be able to check if the external tools are available, and if not invite the user to install them (by themself). I do not want to manage the installation of external tools when installing the package.
Summary of contributions: it seems that setuptools has no support for this, and it would be safer to do the check at runtime (c.f. comments and answers).\nAnswer: My recommendation would be to check for the presence of those external dependencies not at install-time but at run-time. Either at the start of each run, or maybe at the first run.
It's true that you could add this to your setup.py, but the setup.py is not always executed at install-time: for example if your project is packaged as a wheel then it doesn't even contain the setup.py file at all. And even if you do not distribute your project as a wheel, if I am not mistaken pip tends to build a wheel locally anyway and reuse it for the subsequent installations.
So although it would be possible to do such checks as part of the setup script at install time (provided that you can guarantee the presence and execution of setup.py), I would say run-time is a safer bet.",1.0,0.2284795,0.5952438712120056
0,"Question\nI have a set of co-ordinates(latitudes and longitudes) of different buildings of a city. The sample size is around 16,000. I plan to use these co-ordinates as the central point of their locality/neighbourhood, and do some analysis on the different neighbourhoods of the city. The ""radius/size"" for each neighbourhood is still undecided as of now.
However, a lot of these co-ordinates are too close to each other. So, many of them actually represent the same locality/neighbourhood.
As a result, I want to select a smaller sample(say, 3-6k) of co-ordinates that will be more evenly spread out.
Example:- If two of the co-ordinates are representing two neighbouring buildings, I don't want to include both as they pretty much represent the same area. So we must select only one of them.
This way, I was hoping to reduce the population to a smaller size, while at the same time being able to cover most of the city through the remaining co-ordinates.
One way I was imagining the solution is to plot these co-ordinates on a 2D graph(for visualisation). Then, we can select different values of ""radius"" to see how many co-ordinates would remain. But I do not know how to implement such a ""graph"".
I am doing this analysis in Python. Is there a way I can obtain such a sample of these co-ordinates that are evenly distributed with minimal overlap?
Thanks for your help,\nAnswer: It seems like for your use case, you might need clustering instead of sampling to reduce your analysis set.
Given that you'd want to reduce your ""houses"" data to ""neighborhoods"" data, I'd suggest exploring geospatial clustering to cluster houses that are closer together and then take your ~3-4K clusters as your data set to begin with.
That being said, if your objective still is to remove houses that are closer together, you can obviously create an N*N matrix of the geospatial distance between each house vs. others and remove pairs that are within (0, X] where X is your threshold.",0.0,0.1341263,0.0179898664355278
1,"Question\nCan someone please tell how to get a usb device’s parent information in Python?
I tried WMI, win32com, and pyusb without any lucks.
Thanks!\nAnswer: The usb device has a child device com-port. I can find the child device info using serial.tools.list_ports.comports(). From the tool usbtreeview.exe, I can see the child device has a parent with class type usb. How do I retrieve the parent’s info using Python?",0.0,0.1478107,0.021848002448678017
2,"Question\nI am interested in trying to make a machine translation for language accents and is curious for methods avaialable to collect data or how to make your own corpus with unlimited resource. Any good reference i could refer to or ideas?\nAnswer: What kind of realization you need? if it just shell programm, it is easy? if you want GUI(Tkinter) or WEB (Djano) app?",-0.71428573,0.12759513,0.7087633609771729
3,"Question\nMy deep learning topic is classifying images into 5 different categories. I used the ImageDataGenerator library to split my dataset into train and test.  I've successfully developed a model architecture following the CNN method and evaluated the performance of my model on a test dataset, which gave me an accuracy of 83%.
Is it possible to apply sklearn evaluation metrics such as precision, recall, f1_score, etc. to evaluate my test results? If yes, how can I do it?\nAnswer: Yes you can do it as long as your model is giving out either the class labels or probabilities as it predictions.
If your model is predicting the encoded (integer) labels then you can use
sklearn.metrics.precision_score(y_true, model.predict(test_x))
On the other hand if the  model is predictiong the probabilies which is norammly the case then you have to fist convert them into class labels using argmax. So if you have a batch of test_x data then you can use
sklearn.metrics.precision_score(y_true, np.argmax(model.predict(test_x), axis=1))",0.40816328,0.4492704,0.001689795171841979
4,"Question\nI am using bokeh server to plot the result from my streaming frames of a video. I see Bokeh provide add_periodic_callback as an auto-update on the server, but I don't know how to pause it. Any suggestion?\nAnswer: I have solved by passing some global value to track the result of each update. If the global values don't change, then I will pause the update",0.0,-0.12119663,0.014688622206449509
5,"Question\nI had a DataFrame whose memory usage was 159.7 MB. When I used.to_csv method to write it in storage the written file was about 400 MB. And when I loaded this file its memory usage was 159.7 MB. Is there an explanation for this difference in sizes and how to write it so that it takes less space in the hard drive? Thank you for your help\nAnswer: The easiest way to reduce the size of the csv is to compress it when writing, using the compression parameter in to_csv. For example df.to_csv(compression='gzip').
There are a variety of reasons the memory usage could be so different from the size of the csv on disk, it's a little hard to say without knowing any specifics about the data you're working with.
One generic recommendation is to check the precision of any floating point values in your dataframe, if you're writing a bunch of numbers with 15 decimal points of precision or something that will take up a lot of space. Try truncating these values to the precision you need.",0.0,0.28272808,0.07993516325950623
6,"Question\nI'm new to python GUI and I'm writing a simple game that involves moving an image from one grid cell to another and I don't have to stack an image on top of another image. Therefore to place an image I have to check whether the new location is empty. In addition the image can only be moved either vertically or horizontally. The user clicks the image to be moved and then the location into which to place the image.  I placed each image in a label on an 8x8 board. To check if the move is horizontal or vertical, I check if the row numbers or column numbers of the source and destination are the same respectively. If row number is the same, for instance from position (row=2,column=1) to (row =2, column =4), I want to loop through positions from column =2 to column = 4 and check if there an image or not. If there is an image in any of those cells i.e. (2,2,), (2,3) or (2,4),  then the move is cancelled. But all the functions about grid only show how to get the column and row numbers but don't give any information on how to get the value if I already know the row and column numbers. Can anyone help on how to go about problem?
Thank you\nAnswer: I believe the grid method you're looking for is grid_slaves(row, column).

w.grid_slaves(row=None, column=None)
Returns a list of the widgets managed by widget w. If no arguments are provided, you will get a
list of all the managed widgets. Use the row= argument to select only the widgets in one row, or
the column= argument to select only the widgets in one column.

Note that although the documentation states to use row= or column=,
in practice, you can specify both and get the widget(s) in a single grid cell.
Being frame the container widget (where the grid is located), calling frame.grid_slaves(row=3, column=2) will give you a list of widgets in that grid cell (or a list with a single item, in case there is only one widget there) or an empty list, if there is none.",0.0,0.27193445,0.07394834607839584
7,"Question\nI created a Twitter App in order to control a specified account (post tweet, etc).
I created this app with my personal account so I can only post tweets on my personal account.
I'd like to share permissions with my other account so I can tweet on it.
I heard about OAuth but I don't understand how to use it.\nAnswer: You can use twurl to authenticate to another account. The account token and secret will be stored in the.twurlrc file in your home directory, you can use those to post from another account.",0.0,0.13681841,0.018719276413321495
8,"Question\nI am still getting used to PVLIB and figuring out how to use the methods associated with it. I'd like to be able to model the growth of localised hotspots in a PV module due to current mismatch that results from partial shading.
I'd be surprised if I was the first person to do this, so I'm wondering what other solutions exist to do this in a straightforward way from PVLIB.\nAnswer: You can use the single-diode model functions from pvlib to build up the electrical simulation for this scenario, and thereby determine how much electrical power the affected cell absorbs.
There isn't a thermal model in pvlib to tell you how hot it would get, but as a first approximation you could adapt one of the existing module/cell temperature functions quite easily.  There is a local variable called heat_input to which you can add the electrical power.",0.20408164,0.2637471,0.003559966804459691
9,"Question\nI installed python via anaconda on an EC2 Ubuntu Instance.
The command which python returns */home/ubuntu/anaconda3/bin/python*
Jenkins is instead installed in */var/lib/jenkins*
I am trying to run a simple ""Hello World"" script saved on a file named *test.py* and located within the */home/ubuntu/scripts/* folder.
While running *python /home/ubuntu/scripts/test.py* works on terminal, it fails as an ""Execute shell"" build step in Jenkins.
Why and how do I configure Jenkins to run python scripts step by step?\nAnswer: The issue was that the anaconda python installation was only available to the user ""ubuntu"". For Jenkins to be able to run python scripts, the ""jenkins"" user needs to use that installation.
To solve the problem, this is what I did:

Logged in as jenkins with the command sudo su -s /bin/bash jenkins
Edited the python install location as export PATH=/home/ubuntu/anaconda3/bin:$PATH
Checked that the path is correct through which python
Logged back as ubuntu user
Restarted Jenkins through sudo service jenkins restart (not sure if necessary)

Now I can run python scripts through Jenkins.",0.0,0.08293742,0.006878615356981754
10,"Question\nIn PyCharm, I know that you can use the'split pane' option to open one file in two panes.
But how do you concurrently open two files side by side?  E.g. stack.py and test_stack.py?\nAnswer: First, use split panel. After that, you can move any open files from one panel to the other by dragging and/or you can put the cursor in one panel and open a file as usual. The file will be opened in that panel.
If you split using either ""split vertically"" or ""split horizontally"" the file will be in both panels (you can close one of them) but there is also ""split and move down"" and ""split and move right"" actions.
Good shortcuts to know for this is Ctrl+Shift+A for actions (then write ""split"" and choose what you want) and Ctrl+Shift+N for easily opening files.",0.6122449,0.42238498,0.036046791821718216
11,"Question\nIn PyCharm, I know that you can use the'split pane' option to open one file in two panes.
But how do you concurrently open two files side by side?  E.g. stack.py and test_stack.py?\nAnswer: In PyCharm:

Open two files, say stack.py, and test_stack.py.  They will open in two tabs.
Let's say we want to open test_stack.py in the right pane.  Right-click on the test_stack.py tab, and choose ""Split and Move Right"".",0.20408164,0.34258282,0.01918257772922516
12,"Question\nqs
when run locust --master and client locust --master-host,
locust can not run api stress,why?
env
server: locust -f locustfile.py --master (centos)
client: locust -f locustfile.py --worker --master-host 10.200.6.1 (mac)
status
visit webUI,worker show 1 worker,but start test,
it will stop immediately after three seconds,
rps and result still show None.
cmd log
error log : lewis-test/INFO/locust.main: Starting Locust 1.2.3
passlog :
lewis-test/INFO/locust.main: Starting Locust 1.2.3
lewis-test/INFO/locust.runners: Spawning 100
users at the rate 100 users/s (0 users already running)...
lewis-test/INFO/locust.runners: All users spawned: MyUser: 100 (0 already running\nAnswer: it work
diferent locust version,ths",0.0,0.11716092,0.013726680539548397
13,"Question\nMy dataset contains columns describing abilities of certain characters, filled with True/False values. There are no empty values. My ultimate goal is to make groups of characters with similar abilities. And here's the question:

Should i change True/False values to 1 and 0? Or there's no need for that?
What clustering model should i use? Is KMeans okay for that?
How do i interpret the results (output)? Can i visualize it?

The thing is i always see people perform clustering on numeric datasets that you can visualize and it looks much easier to do. With True/False i just don't even know how to approach it.
Thanks.\nAnswer: In general there is no need to change True/False to 0/1. This is only necessary if you want to apply a specific algorithm for clustering that cannot deal with boolean inputs, like K-means.
K-means is not a preferred option. K-means requires continuous features as input, as it is based on computing distances, like many clustering algorithms. So no boolean inputs. And although binary input (0-1) works, it does not compute distances in a very meaningful way (many points will have the same distance to each other). In case of 0-1 data only, I would not use clustering, but would recommend tabulating the data and see what cells occur frequently. If you have a large data set you might use the Apriori algorithm to find cells that occur frequently.
In general, a clustering algorithm typically returns a cluster number for each observation. In low-dimensions, this number is frequently used to give a color to an observation in a scatter plot. However, in your case of boolean values, I would just list the most frequently occurring cells.",0.81632656,0.48597568,0.10913170129060745
14,"Question\nI'm trying to create a guitar amp simulator.
I want to be able to read audio as input, and simultaneously play it, like a real guitar amp.
Couldn't find a lead on how to do this.
What python library is able to do that?
Thanks\nAnswer: Have you tried the Mido or a similar MIDI library?",0.0,0.08396202,0.007049621548503637
15,"Question\nI build this chat website, and I'd like to test this functionnality:

if you send a message and the user is online, send message via websocket: Tested
if you send a message and the user is offline, send a push notification (it's a REST call).

Obviously, in my test, I don't want to do the REST call. I'd like to mock the function ""push_notif"".
But, when I use unittest.patch, the function is not mocked in the consumer (probably because of some async stuff). How can I mock this ""push_notif"" function\nAnswer: Out of solution, I made a decorator for my function ""push_notif"". If in test, it writes in a file ""func.__name__,args,kwargs"", then I read this file in my test to see if the right call was passed.
Ugly, but getting things done",0.0,-0.048499346,0.0023521864786744118
16,"Question\nSo basically I am creating an app with BeeWare(Python). And I wanted to know whether it is possible to change the default icon of beeware and toga and use my own icons. If it is possible, then please someone provide a solution for it.
Thanks.\nAnswer: If you have made an android app and it is in your PC then to do so you will need winrar application just right click on it and choose open with and select winrar (or whatever unzipping tool you have) and browse to the resources folder and there are many folders there, there in the last 4 to 5 folders there is an image you have to replace them with an image of same format and resolution, if you replace then with some other format or resolution your application would not work..
Thanks",-0.35714287,0.107406616,0.21580621600151062
17,"Question\nI am currently trying to run a program with Scipy, and I want to use the load_npz module.
Whenever I tried to run it, the compiler would say that that module doesn't exist.
I ran scipy.__version__ and got 0.13.0b1, which makes sense as to why it couldn't find the module as it doesn't exist in that version, but I am confused as I have 1.5.2 installed in both pip and brew yet it keeps defaulting to the oldest version which is very frustrating.
Does anybody know how to get rid of this version? I have tried uninstalling from pip and brew, along with finding the path of the imported scipy with the outdated version yet it still is causing issues.
I do have a lot of packages installed (numpy, matplotlib, etc.) so could it be a dependency that keeps reinstalling an old version?
Strangely, even if I delete scipy from both brew and pip, it will still show the old version but throw an error on a different local file that also uses scipy saying the module does not exist (which is expected as I deleted it).\nAnswer: I figured it out, I just deleted all my possible scipy locations and then just downloaded Anaconda and I'm using that as my python interpreter.",0.0,0.10857636,0.011788825504481792
18,"Question\nAll the time that I've worked with python and anaconda, I have never wondered how actually virtual envs are useful except for version controlling. When I looked it up, I found a lot of articles on how to create and use custom envs, but not exactly why they are so awesome. Why is it dangerous to install new libraries into the original installation? Are virtual envs useful for anything other than versioning?\nAnswer: PROS:

You can use any version of python you want for a specific environment without having to worry about collisions.
Your main python package directory does not get flooded with unnecessary python packages.
You can organize your packages much better and know exactly the packages you need to run.
Anyone can run your code on their machine.
Your project is easier to deploy.
Your application runs faster.
Ease of maintenance.

CONS:

storage space?",1.0,0.33668,0.4399934411048889
19,"Question\nIf I have a multivariable function such as
F= lambda x,y: x**2+y**2
and if I need to use the input x0=np.array([1,1])
May I know how I should use x0 to get the value from F?
I understand that I could use something like F(x0[0],x0[1])
But I would like to know whether there is a way that I can directly use x0 rather than calling each cordinate manually
Appreciate your help\nAnswer: Python lets you do this by doing F(*x0), which expands the array into the parameters. In other languages this is sometimes called ""splatting"".",1.0,0.024801254,0.9510126113891602
20,"Question\nI have created an environment in Anaconda by running:

conda create --name myenv

switched to that environment after creation and added conda-forge to the anaconda channels.
Then I ran

conda install pycotools

Installation seem to be successful.
When I try to import pycotools in Python

from pycotools import models

I get the error message:

ModuleNotFoundError: No module named 'pycotools'

To verify installation of pycotools, I run

conda list pycotools

But this gives nothing back
However, when I run

conda list

I find pycotools in the list.
How can I verify that I have pycotools installed?
And if no, how do I properly install pycotools in my conda environment?
by\nAnswer: Try to execute pip install pycotools.",0.0,0.14096195,0.019870270043611526
21,"Question\nI got two versions of Python on Windows, and try to set Python 3.6 as default. Does anyone know how can I set it? Many thanks!\nAnswer: This is most likely due to the path variable. You can see this in a command window by typing
Path
at your prompt.
To update the settings,
Open the 'System' properties.
System Properties
Open Environment Variables
Path Variables
Highlight the 'Path' Variable and click edit.
Edit Path Variable
Edit the values for the Python entries, to point to the desired python version.
enter image description here
OK on all boxes, close any CMD windows open, and open new one. Python command should now reference the correct location.",0.0,0.2425223,0.0588170662522316
22,"Question\nI'm working on a project on recommender systems written by python using the Bayesian Personalized Ranking optimization. I am pretty confident my model learns the data I provided well enough, but now it's time I found out the exact model hyperparameters and try to avoid overfitting. Since the movielens dataset only provided me with 5-fold train-test datasets without validation sets, I want to split the original dataset by myself to validate my model.
Since the movielens dataset holds 943 user data with each user guaranteed to have ranked at least 20 movies, I'm thinking of splitting the data so that both TRAIN and TEST datasets contain the same number of users(e.g. 943), and distributing 80% of the implicit feedback data to TRAIN, and the other to TEST. After training validation will take place using the mean value of Recall at k precision through all 943 users.
Is this the right way to split the dataset? I am curious because the original movielens test dataset doesn't seem to contain test data for all 943 users. If a certain user doesn't have any test data to predict, how do I evaluate using recall@k -- when doing so would cause a zero division? Should I just skip that user and calculate the mean value with the rest of the users?
Thanks for the lengthy read, I hope you're not as confused as I am.\nAnswer: How I would split it is the whole data set on 80% (train)- 10% (validation) - 10% (test). It should work out :)",0.40816328,0.20793205,0.04009254276752472
23,"Question\nI want to replace a Noun in a sentence with its pronoun. I will be using this to create a dataset for a NLP task. for example if my sentences are -->

""Jack and Ryan are friends. Jack is also friends with Michelle.""

Then I want to replace the second Jack(in italics and bold ) with ""He"".
I have done the POS tagging to find the Nouns in my sentences. But I do not know how to proceed from here.
If I have a list of all possible pronouns that can be used, Is there a corpus or system that can tell me the most appropriate pronoun for the word?\nAnswer: You can almost do this with tools in Stanford CoreNLP. If you run the ""coref"" annotator, then it will attempt to determine the reference of a pronoun to other entity mentions in the text. There is also a ""gender"" annotator, which can assign a  (binary) gender to an English name (based just on overall frequency statistics). (This gender annotator can at present only be accessed programmatically; its output doesn't appear in our standard output formats.)
However, both coreference resolution and automated gender assignment are tasks with mediocre accuracy, and the second has further assumptions that make it generally questionable. I find it hard to believe that doing this automatically will be a useful strategy to automatically produce data for an NLP task.",0.81632656,0.24142289,0.33051422238349915
24,"Question\ni am trying to install opencv.i downloaded  ""opencv3-3.1.0-py35_0.tar"" and extracted it after that copy the file in a system that hv no internet. i run the conda install PATH_TO_FILE/opencv3-3.1.0-py35_0.tar through conda prompt....
but it is not helping & showing ""http error""
how to import libraries offline?\nAnswer: Try unzipping  the file, then open a terminal on that directory, run- pip install your-unzipped-folder/filename.
you can also use wheel- before the previous command- run - pip install wheel- first.",0.0,0.11584777,0.013420704752206802
25,"Question\nSo I tried to use the command python -m twine upload --repository testpypi dist/* and after I press enter, it appears a prompt asking for my username, I tried entering my test pypi username, __ token __, or the name of my token, but non of them works. If I enter anything, press enter, it will just go onto the next line and never does anything.
Am I missing any steps? Or what am I doing wrong? I am following the Pypi docs btw\nAnswer: Ok so I solved the problem by switching to a different terminal. Thanks!",0.0,0.18460763,0.03407997637987137
26,"Question\npython version is displayed as Python 2.7.14 when checked in cmd (strange because I have always installed python 3.6 onward). But python shell in IDLE shows Python 3.7.6. I was notified of this when using f'' strings got errors.
Also python path in environment variables is set to python37. I was wondering why this was happening and how to change it.
P.S: I have not tried reinstalling python yet.\nAnswer: Check if your environment variable is set to the correct Python executable path.
You can find your active executable with where python on windows and with which python on Linux. I guess you have an old executable active on your machine.
happy coding,
breadberry",0.0,-0.009196997,8.458474621875212e-05
27,"Question\nI'm looking for a solution at the following problem:
I have my neural network with 8 features and 6 outputs.
The network predicts some info on a graph, where a record is related to a node in the graph.
It works perfectly when I have all the examples (nodes) with the same degree of neighbours.
Now I would need it to work also for nodes with less neighbours, but on a general form, meaning that I do not know a priori how many they are.
Thus I tried putting a zero when the the neighbour does not exist, but the predictions where all strange, not correct at all.
So my questions is: is there a way such that during training I can switch off some input neurons looking at how many effective features I have on that example?
To be more precise: the first example taken by the NN has the full neighbour, thus I have 8 values; the second has 6 values, so I would like to switch off 2 input neurons and so on.
Any ideas?\nAnswer: A simple solution which might work would be to declare a feature as non relevant. So you simply specify a value, which makes the model learn that it should not be used in this iteration. So when you set the values to zero, the model wll just think that its a zero, so the meaning is ambiguus. However if you set it to -1 e.G. but -1 does not occur in your data at all, the model might be able to learn that -1 represents a non existent feature and puts more attention on the others.",0.40816328,0.13403064,0.07514870166778564
28,"Question\nI want to execute fastapi in shell.
for example, we can do it with below code in django:
python manage.py shell
how can I do this in fastapi?\nAnswer: Simple answer, You can not.
manage.py does the same thing as django-admin but also sets the DJANGO_SETTINGS_MODULE environment variable so that it points to your project’s settings.py file. In FastAPI we don't have admin utility, because there is no out-of-box config, environment management etc. That's the main difference between a microframework and a high-level framework.
FastAPI does not have any administration utilities out-of-box.",0.20408164,0.5015525,0.08848891407251358
29,"Question\nI have a python library in a folder which is not recognised by Anaconda Jupyter Notebook, so if I import <module name> I get No module named '<module name>'.
How can I add the folder to the path? Spyder has the pythonpath manager in which I can just add a folder, but I cannot see how to do it in Jupyter Notebook.
Followup question: is there a way to add a folder to the python path, such that all Anaconda applications, Spyder, Jupyter, etc., recognise it? Or dies it have to be done for each application separately?\nAnswer: PYTHONPATH is an environment variable that you can set to add additional directories where python will look for modules and packages. The how to depends on your OS. In Windows just search for environment variables.
The only reason to set PYTHONPATH is to access directories with custom libraries that you do not want to install in the default location, i.e. the site-packages directory.",0.20408164,0.2312457,0.0007378860027529299
30,"Question\nI want to generate a Dataframe which involves three industries, 100 investors, and 2000 firms in 50000 different deals. So, I have 50000 rows and four columns, including deal's date.
I randomly assigned 100 investors and 2000 firm in this dataset for all deals.
My problem is that I need to determine the industry for each deal by considering a couple of conditions that should hold for this process. 1) If a firm randomly assigned to an industry, this firm should hold in the same industry in the whole dataset. 2) All investors invest at least in one industry, 15 investors invest at least in two industries, and four investors invest in all three industries. I don't know how I can do this.\nAnswer: This seems like 2 separate questions
For 1, one idea is to just hash the firm name and then that will give you a deterministic way of setting which firm goes to which industry.
For 2, one idea is to use a permutation function. For example, you can use numpy.shuffle where the first 4 belong to all 3, then the next 15 belong to a random subset of 2... etc, etc.
You could then generate tables off of all this information and use joins to concatenate your final table, where you initially start off with just trade dates, firms, and deal info.
(This is just a starting idea)",0.0,0.22161359,0.049112580716609955
31,"Question\nI want the bot to send an inline button of ""sendPicture"" and after clicking it the user can able to click picture from the camera and send it. In this process users not able to send the picture from their gallery. Is telethon support it or is there any other methods to implement this scenario?\nAnswer: No. You can just send them text asking nicely.",0.81632656,0.08428627,0.5358830094337463
32,"Question\nI've recently joined a research group and I'm trying to  figure out how to program in Python to set up an ADwin Gold II to speed up data gathering and processing the results. I'm very rusty with coding haven't done any in a year or so, and finding the docs on ADwin very hard to follow.
If anybody could explain how to used the python ADwin commands from the official python addon, or show me to any material that may be useful. This would be  enormously appreciated.
Thanks in advance.\nAnswer: Its very hard to find, but on the installation folder, in the software folder, you can find a python Manual. There was no mention of this anywhere in the manual I stumbled across it by pure chance.
Any other poor soul out there, may this be an aid in your suffering.",0.0,0.12985933,0.016863444820046425
33,"Question\nI'm studying and practicing Python right now. I'm kinda scared by the concept of classes in it and I'm stuck wondering how to implement data structures like Linked Lists, Graphs and Trees.
I've heard from many that these are the most important data structures asked in interviews and coding competitions.
So my question is, Is there a way to implement all the said data structures without using classes and just by using predefined data structures like lists, dictionaries etc?\nAnswer: If we are being pedantic, everything in python is a class, so you can't avoid them. If you are concerned about everything that goes into creating your own class, like which methods should be defined where, that's something we can focus on. In fact, there is no general consensus on the boundaries to any given class and popular programs like C and Go don't even have them.
An alternative is to just use a dict to hold key/value pairs. Roughly, a class is just a dictionary with associated methods anyway. Dictionary keys can hold a wide variety of objects (as long as they are hashable) whereas class attributes must be strings and are further restricted to fit lexicographically in a program. A linked list for instance could be { ""next object"":obj, ""previous object"":obj, ""item"":obj } or even a list [obj, obj, obj] and your code remembers what those indexes are.
But classes are very convenient, especially when implementing other data structures. It makes sense that methods manipulating a linked list node would be on the node itself. There isn't much to gain avoiding classes when they are reasonable data structures to use.
There are plenty of modules out there that implemente linked lists, trees and graphs. Unless this is an exercise in learning data structures, some time spent with your favorite search engine is the best option of all.",0.40816328,0.4709072,0.003936801105737686
34,"Question\nIf the dependent variable has both positive and negative values, the model.score of any regression models are not accurate when train and test score are compared. I tried converting the target column into log but since there are negative values as well, it seems this is not right. Please advise\nAnswer: I can't think of any machine learning algorithms that can't handle both positive and negative values. The scoring should still be accurate. If you would like to take the log transform of a column with negative values, then you can add some large constant to the column that makes all values positive and then take the log.",0.0,0.17520326,0.030696183443069458
35,"Question\nI've seen many options online, but none of them seem to support Python 3.8. Is there any way to convert a python project to a.exe anymore? If so, how can I do that?
EDIT:
I've tried PyInstaller multiple times, both before it was recommended here and afterwards, but I keep getting huge error messages that I don't know how to make sense of.\nAnswer: Use pyinstaller.
Example: pyinstaller yourfile.py -F --onefile
This will turn your code into a.exe file on Windows.
To install PyInstaller, you simply use pip, thus pip install PyInstaller or pip3 install PyInstaller.
You can also make sure you have the latest development version of pyinstaller: pip install https://github.com/pyinstaller/pyinstaller/archive/develop.tar.gz",0.40816328,0.26633286,0.020115867257118225
36,"Question\nmac os Catalina, python3.8.2
Hey All,
I know there are similar questions about this, but they didn't seem to help me.  I installed a module (quandl) using pip3.8 install. Then I try to import quandl in a.py file and get a ModuleNotFoundError.  I do have multiple versions of python installed, can't figure out how to get rid of them safely, hence why I used pip3.8 install to make sure it points to the write place.  Still no dice.  I am not using a venv.
From the terminal, this is the pip3.8 install path: /usr/local/lib/python3.8/site-packages
From the.py file via VScode, I am interpreting using: /usr/local/bin/python3.8
Can someone educate me on what I am doing wrong?
I was previously interpreting from /Library/Frameworks/Python.framework/Versions/3.8/bin/python3, but I noticed pip was not installing there.\nAnswer: At the bottom of your VSCode window, there's an orange status bar.  Starting at the far left, you'll see your git repo info, the git status, and then the Python environment you're using.  If you click that, VSCode will slide in a modal dialog window that'll let you choose your python environment -- my VSCode defaulted to /usr/bin/python3, but my pip3 (and python3) are in /usr/local/bin.  Once I picked the correct interpreter, I stopped getting the ModuleNotFoundError",0.0,0.3744496,0.1402125060558319
37,"Question\nI have a huge distance matrix.
Example: (10000 * 10000)..
Is there an effective way to find a inverse matrix?
I've tried numpy's Inv() but it's too slow.
Is there a more effective way?\nAnswer: You probably don't actually need the inverse matrix.
There are a lot of numeric techniques that let people solve matrix problems without computing the inverse.  Unfortunately, you have not described what your problem is, so there is no way to know which of those techniques might be useful to you.
For such a large matrix (10k x 10k), you probably want to look for some kind of iterative technique.  Alternately, it might be better to look for some way to avoid constructing such a large matrix in the first place -- e.g., try using the source data in some other way.",0.0,0.27232563,0.07416125386953354
38,"Question\nI am trying to find eigenvalues and eigenvector in python without Numpy but I don't know how to calculate det(A-lemda*I) for finding eigenvalue.\nAnswer: If you really want to avoid using numpy.linalg (why? This must be some sort of a learning problem, surely), you can e.g.  implement a variant of a qr algorithm: factorize A into the product of Q@R, multiply R@Q, repeat until convergence.
However, if it's indeed a learning exercise, your best bet is to pick a textbook on numerical linear algebra.
And if it is not, then keep in mind that you are very unlikely to outperform (in any definition of performance) the tried-and-tested lapack routines that numpy.linalg wraps.",0.0,0.3215369,0.10338597744703293
39,"Question\nI want to generate 50 random coordinates in an unit square such that each point has minimum Euclidian  distance of 0.1 from other points. How can I generate such in python? Actually I need to know how to put condition in random number. But I could not find any way. One way I was using if command with that condition in it. But I need to generate 50 such points. Thanks for any help.\nAnswer: A cheap solution would be to keep generating random points in the square. If the generated point is not too close to the other points, then keep it, otherwise throw it away. Once you've reached 50 points, you can stop generating random points.",0.5442177,0.06861389,0.2261989861726761
40,"Question\nI want to generate 50 random coordinates in an unit square such that each point has minimum Euclidian  distance of 0.1 from other points. How can I generate such in python? Actually I need to know how to put condition in random number. But I could not find any way. One way I was using if command with that condition in it. But I need to generate 50 such points. Thanks for any help.\nAnswer: you could pick 50 points from the regular grid (0.1*i, 0.1*j) for i=0..10 and j=0..10",0.27210885,-0.042851895,0.09920027107000351
41,"Question\nIt seems that non-python resources are included in python distribution packages one of 4 ways:

Manifest.in file (I'm not sure when this is preferred over package_data or data_files)
package_data in setup.py (for including resources within python import packages)
data_files in setup.py (for including resources outside python import packages)
something called setuptools-scm (which I believe uses your version control system to find resources instead of manifest.in or something)


Which of these are accessible from importlib.resources?
(It is my understanding that importlib.resources is the preferred way to access such resources.) If any of these are not accessible via importlib.resources, then how could/should one access such resources?

Other people online have been scolded for the suggestion of using __file__ to find the path to a resource because installed wheel distributions may be stored as zip files and therefore there won't even be a proper path to your resources. When are wheels extracted into site-packages and when do they remain zipped?\nAnswer: All of (1)-(3) will put files into your package (don't know about (4)).
At runtime, importlib.resources will then be able to access any data in your package.
At least with Python 3.9, which can access resources in subdirectories.
Before, you had to make each subdirectory a package by adding a __init__.
As for why not use __file__: Pythons import system has some weird ways to resolve packages. For example it can look them up in a zip file if you use Zipapp.
You may even have a custom loader for a package you are asked to load some resources from.
Who knows where those resources are located? Ans: importlib.resources.
Afaik, wheels are not a contender as those are unpacked.",0.81632656,0.47768167,0.11468036472797394
42,"Question\nNew to python - I need some help figuring out how to write a tokenizer method in python without using any libraries like Nltk. How would I start? Thank you!\nAnswer: use gensim instead.
tokenized_word = gensim.utils.simple_preprocess(str(sentences ), deacc=True) # deacc=True removes punctuations",0.0,-0.091982126,0.008460711687803268
43,"Question\nI am new to OpenCV and I do not understand how to traverse and change all the pixels of black with colour code exact RGB(0,0,0) to white colour RGB(255,255,255).
Is there any function or way to check all the pixel and if RGB(0,0,0) the make it to RGB(255,255,255).\nAnswer: Subtract 255 from each Pixel and get the positive values only
For grayscale and black and white images
sub_array = 255*np.ones(28, dtype = int)
img_Invert = np.abs(np.subtract(img,sub_array))",0.0,0.17363048,0.03014754131436348
44,"Question\nFor my current web (reactJS) app, i need to implement a push notification system. Backend APIs are written in  Python on Django Framework. So how should i  implement socket, should I write in python, or may I write in node or node for the client-side and python for the server-side?\nAnswer: Are you looking for Web push or mobile app? The term push is really synonymous with 'push notifications' meaning the client will get them even if the app / website is open or not.
If that is what you are looking for then you need to be looking at service workers and push notification servers.
If you want to simply send alerts to users on your page you can for sure use a simple websocket or long poll done in JS with a node server to emit to.",0.40816328,0.27638972,0.017364270985126495
45,"Question\nI am trying to read back ciphertext in a file, for example, \t""\x87]\xb6^,\xa7G\xf7\x99<\xb2-\x06\xc8 however when I make it a bytearray for CBC decryption, I get b'\\t""\\x87]\\xb6^,\\xa7G\\xf7\\x99<\\xb2-\\x06\\xc8' which ultimately fails as this is not a 16 byte multiple for CBC to decrypt. I have tried decoding with unicode escape, however it doesn't retain it's data type as a byte. I cannot seem to work out how to get this to decrypt normally again?
Thanks.\nAnswer: For everyone with this issue:
If you are writing ciphertext to a file that contains a \, instead of reading it as a string and converting to bytes (which adds an extra \ escape character), use the ""b"" modifier in the open() statement.
Example: with open(""test.txt"",""rb"") - notice the b? That means everything from the file is automatically read as a byte. Don't write or deal with strings. Write and read bytes directly and then your encrypt/decrypt functions will now work. No extra \.",0.0,0.44925147,0.20182688534259796
46,"Question\ndoes anyone know how a program can find dead pixels on a phone screen?
I found a lot of dead pixel finding tools and all of their implementations looks similar. It's showing a single-color screen, such as red, green, blue, white and black. Then, we need to check if there's any dots with different color (which is dead pixels) manually.
Is it possible to find the dead pixels automatically?\nAnswer: No, it isn't.  Rendering the pixels is a ""write-only"" process, much like sending a message to the console.  There is no feedback from the physical pixels to indicate whether they turned on as expected.  Think of it like a TV screen: the broadcast studio has no way to tell whether you got the program.  They simply send out the signal; you pick it up or ignore it, depending on how you set your hardware state.",0.81632656,0.25321233,0.31709763407707214
47,"Question\nI am trying to run two pip packages that require separate version requirements. They would run well in separate virtual environments but not in the same.
I want to use these pip packages in the same app - a flask app. Any idea on how to go about this?\nAnswer: This is not possible.
Quite often some libraries have overly specific restrictions on dependencies.
I recommend to open an issue at their issue tracker and ask for less restrictive permissions.",0.0,0.28008783,0.07844918966293335
48,"Question\nI currently have python versions 2.7, 3.8, and 3.9 on my Mac and it just causes problems in package installing, etc. I do not know how to remove all of them and reinstall python from the beginning in a clean way this time. What should I delete?\nAnswer: First, delete the folder /Library/Frameworks/Python.framework/ using this command:
sudo rm -r /Library/Frameworks/Python.framework/

Second, remove everything related to python (python2.7, python3.8, etc.) in this dir: /usr/local/bin/

NOTE: You need admin permissions to do this on your computer.",-0.71428573,0.40911186,1.2620221376419067
49,"Question\nI had some strings in my dataframe (female x male, region and so on) and I wanted to fit a decision tree. Therefore, I applied one hot encoder in all these categorical features - which returned a bunch of new columns with 0 and 1.
However, the the default of features' threshold in decision tree are 0.5. That doesn't make sense for these categorical columns.
Does someone know how I can change the threshold for lots of columns at once (without having to input the name of each column) in python?
I want something like: column female split in 0 and 1. I want to do all these before calculating statistics (AUC, ROC, ACC, etc).
ps: I also have some numerical data (income, for example), so I can't change the threshold for all the columns
ps2: The categorical 1 and 0 are in columns 6 to 30.\nAnswer: Well, a threshold of 0.5 for a binary feature does make sense.
It just means that when the feature takes a value > 0.5 (which is thus 1) then the split is made to (say) the right, and of the feature takes a value < 0.5 (i.e. 0) the decision tree takes the other path (left).
There is no sense in changing the threshold value: any value between 0 and 1 has the same effect as 0.5 and values lower than 0 or larger than 1 mean that no split is done and all observations go to the same child node.",0.0,0.19808078,0.03923599421977997
50,"Question\nI'm currently making a program which fetches the prices and discounts etc. off the Steam sales search page, however all the prices are in $ and I need them to be in £. I'm relatively new to Python and Selenium, and have been unable to fix the issue myself, and I haven't seen anyone else with a similar problem.
I'm using the Selenium webdriver, and don't know how to configure its region, but I'm assuming that it'll probably need to be something inputted into Chrome Options, but I don't know.
Thanks in advance.\nAnswer: Add &cc=US or &currency=1 to url. I can't tell which will work if you dont give specific url.
Can you give an example url?",0.0,0.061394393,0.0037692715413868427
51,"Question\nSo i know that in pymongo we create dictionaries to represent documents.
Here is the data I need to store into mongodb
""Dog Snails Zebra Horse Fox"".split()
Pet    = [Dog,        Snails,   Fox,          Horse,       Zebra     ]
^^ How would I go about converting either of these into a dictionary so I can store the data in mongoDB?
As I know you have to have the format
emp_rec1 = {
""_id"": 1
""name"":""Mr.Geek"",
""eid"":24,
}
But as I only have one key (pets) and many values (dogs, cats, snails, Fox, Horse, Zebra) how would I do this?\nAnswer: In MongoDB, data is stored as documents. These documents are stored in MongoDB in JSON (JavaScript Object Notation) format. So you can insert as {
""pet"":[
""Dog"",
""Snails"",
""Fox"",
""Horse"",
""Zebra""
]
}",0.0,-0.052857637,0.00279392977245152
52,"Question\nI have a question; that's as to how to make a discord.py bot send a message by itself upon on_ready():?
Its only in one server and it doesn't matter what channel at the moment, i couldn't find anything in the documentation-
any help is appreciated ^^!\nAnswer: I don't believe the on_ready object has access to send. So I don't think it is possible via the on_ready object. You would have to set it via a different action.",0.0,0.18055803,0.03260120004415512
53,"Question\nI'm using python selenium to automate user login for a website. However, this specific website uses a 2-step authentication when logging in: After clicking login, it then sends a code to your email address, which you have to enter into the website in order to authenticate. Any ideas what I should do or how I can access that code using python selenium?\nAnswer: You should log in to the website in the traditional way with selenium.
Now the site will ask for the verification -> So you need to read your mails somehow.

I see 2 good ways to solve this:

Login to the email service with selenium in other window or tab -> Open emails-> Select the right email -> Read out the verification code with a regex argument from the email content.

OR

You can read your emails with IMAP -> Select the correct email with IMAP -> Once again: Read out the verification code with a regex argument from the email content.


Now you got the verification code simply write it in and log in.",0.0,0.17006308,0.028921451419591904
54,"Question\nI am working through a Udemy course. I am a Python beginner.
I am trying to change the directory in the Python terminal (in both Python 3.8 and PyCharm, same result). My current directory is C:\Users\*username*\PycharmProjects\Woops\venv\Python For Beginners and when I try to change directory using the following command:
cd C:\Users\*username*\PycharmProjects\Woops\venv\Python For Beginners>python myparser.py
I receive the message:

""Access is denied.""

I've gone into every folder and sub folder containing this item, and clicked on properties,  general and security and ensured that I have full permission. I've done the same for Python and PyCharm. I've gone into advanced security settings and the auditing and effective access headers and made sure my username has full access.
Finally, I've even opened PyCharm as an Administrator, and no matter what, when I enter the command:
cd C:\Users\*username*\PycharmProjects\Woops\venv\Python For Beginners>python myparser.py
it returns ""Access is denied.""
Anyone have an idea what is happening here, and how to execute the cd command?\nAnswer: The problem is that you can't change the directory into a python file you can only change directories to a folder like for example
Example: cd C:\Users\username\PycharmProjects\Woops\venv\Python For Beginners
To run the code write python3 or python myparser.py",0.0,-0.09366995,0.00877405982464552
55,"Question\nI have an image loaded on-page and it's loaded from the database and I want to remove that object from the database when the tab is close how can I do that?
framework- Django\nAnswer: I don't think it's doable with Django, but to fulfil your requirement you can rather store that image in session instead of a database.",0.81632656,0.28171992,0.28580424189567566
56,"Question\nDoes anyone know how can i use tesseract on Windows without using the.exe
I want to use pytesseract for a Proof of concept on my company's system where i don't have access to install the executable.
Please suggest any alternative here?
Thanks.\nAnswer: You can use the tesseract-ocr-data python package,
tho it is quite big.",0.0,0.16759878,0.028089351952075958
57,"Question\nI have a question about manifest.yml files, and the command argument. I am trying to run multiple python scripts, and I was wondering if there was a better way that I can accomplish this?
command: python3 CD_Subject_Area.py python3 CD_SA_URLS.py
Please let me know how I could call more than one script at a time. Thanks!\nAnswer: To run a couple of short-term (ie. run and eventually exit) commands you would want to use Cloud Foundry tasks. The reason to use tasks over adding a custom command into manifest.yml or a Procfile is because the tasks will only run once.
If you add the commands above, as you have them, they may run many times. This is because an application on Cloud Foundry will run and should execute forever. If it exits, the platform considers it to have crashed and will restart it. Thus when your task ends, even if it is successful (i.e. exit 0), the platfrom still thinks it's a crash and will run it again, and again, and again. Until you stop your app.
With a task, you'd do the following instead:

cf push your application. This will start and stage the application. You can simply leave the command/-c argument as empty and do not include a Procfile[1][2]. The push will execute, the buildpack will run and stage your app, and then it will fail to start because there is no command. That is OK.

Run cf stop to put your app into the stopped state. This will tell the platform to stop trying to restart it.

Run cf run-task <app-name> <command>. For example, cf run-task my-cool-app ""python3 CD_Subject_Area.py"". This will execute the task in it's own container. The task will run to completion. Looking at cf tasks <app-name> will show you the result. Using cf logs <app-name> --recent will show you the output.


You can then repeat this to run any number of other tasks commands. You don't need to wait for the original one to run. They will all execute in separate containers so one task is totally separated from another task.

[1] - An alternative is to set the command to sleep 99999 or something like that, don't map a route, and set",0.0,0.08945352,0.008001931942999363
58,"Question\nI cannot seem to figure out how to resize images and gifs using discord.py
If anyone could tell me how exactly to resize images and gifs using discord.py it would be greatly appreciated as it powers core features of my bot.\nAnswer: Use a module like Pillow (import PIL)",-0.71428573,0.27143902,0.9716532826423645
59,"Question\nI am new in this domain. I am looking for codes for late fusion. I have text features but I don't know how to implement late fusion. I read articles on this but a working code will help me to understand the implementation properly.\nAnswer: I solved this problem. I am not expecting an answer to this question. If any of you face a similar situation for them I am explaining here. Let you have texture features of two classes. Classify those features based on two/multiple different classifiers, for example, SVM, KNN, RF, Adaboost, etc. Compute probability scores using each classifier. Now, concatenate the probability scores of all classifiers and finally again classify the using any classifiers. In this step, you may use previously used classifiers.",0.40816328,0.030818045,0.1423894315958023
60,"Question\nif I execute my py script with 'import psutil' I get the error ""ImportError: No module named psutil"". I realized if I use pip3 install psutil it installs it to /lib64/ and my other modules to /lib/ so I used --target and put in the path the other modules were in and removed it from /lib64/ but it still does not work. I am working on Linux.\nAnswer: Problem was that I executed my script with 'python...' and not 'python3...'
It works now",0.0,-0.25598383,0.06552772223949432
61,"Question\nI am learning how to use python.
For the project I am working on, I have hundreds of datasheets containing a City, Species, and Time (speciesname.csv).
I also have a single datasheet that has all cities in the world with their latitude and longitude point (cities.csv).
My goal is to have 2 more columns for latitude and longitude (from cities.csv) in every (speciesname.csv) datasheet, corresponding to the location of each species.
I am guessing my workflow will look something like this:
Go into speciesname.csv file and find the location on each line
Go into cities.csv and search for the location from speciesname.csv
Copy the corresponding latitude and longitude into new columns in speciesname.csv.
I have been unsuccessful in my search for a blog post or someone else with a similar question.  I don't know where to start so anyone with a starting point would be very helpful.
Thank you.\nAnswer: You can achieve it in many ways.
The simplest way I can think of to approach this problem is:

collect all cities.csv data inside a dictionary {""cityname"":(lat,lon),...}
read line by line your speciesname.csv and for each line search by key (key == speciesname_cityname) in the dictionary.
when you find a correspondence add all data from the line and the lat & lon separated by comma to a buffer string that has to end with a ""\n"" char
when the foreach line is ended your buffer string will contains all the data and can be used as input to the write to file function",0.0,0.33999348,0.11559556424617767
62,"Question\nI am new to working with APIs, and I recently started a Python project making use of the Google Calendar API that has been put on github. As such, to protect the API keys I created a.env file and stored the keys as environment variables.
I followed guides that told me to make sure to.gitignore the.env file. However, I don't understand how a user who downloads my app and uses it would be able to access the API key values in the.env file, if the.env file is not in the git repo to begin with.
The values in my.env file are essential to authorizing the user's Google account (via OAuth) for use with the app.
What steps would I take to make sure a user of the app is able to retrieve the variables in the ignored.env file?\nAnswer: I see, so I would need to provide the values to them somehow, and have them configure it manually?

Yes, but if those values are sensitive, there should not be stored in the Git repository in the first place.
Which means your README (in that git repository) should include instructions in order for a user to:

fetch those values
build the env file",0.40816328,0.40818852,6.371871918986471e-10
63,"Question\nI've been trying to create a pyramid command in twitch chat for about 2 weeks now however I'm struggling on how to repeat a word after a specific keyword for example if someone types +pyramid (emote) in chat I want to be able to repeat that specific message after they type +pyramid, im not sure how i can start to do this because I've only found out how to replace words when they are defined e.g replace test with yo. Any help would be greatly appreciated.\nAnswer: you could replace ""+pyramid"" with """" (nothing) and that would leave you with your emote. Now you can do a for loop and times the string by the iterator variable to make a pyramid, printing on each loop.",0.0,0.15234303,0.02320840023458004
64,"Question\nI refer to frameworks such as aiohttp, tornado, gevent, quart, fastapi, among others.
If you look for tutorials on how to use celery with django and flask to do things like background and periodic tasks, for example, to send an email when a user registers to confirm their account, you'll find a lot of content about it. But not with the ones above, or they are very few and talk about other topics than perform background or periodic tasks. Does this mean that with these frameworks I don't need celery because since they are asynchronous I can do the same?\nAnswer: Celery (and similar like Huey or RQ) has different purpose than the frameworks you have listed. No matter which framework you pick, in order to distribute execution of tasks among (potentially hundreds) nodes you would need to implement the whole system yourself. Things would get even more complicated if you want to implement something like Celery workflow...
So the answer ca be both YES and NO.
NO: you do not need Celery if you want to implement all the functionality that Celery provides out-of-box by yourself.
YES: more pragmatic - you use your async framework to implement your (web) service(s), but whenever you need to distribute execution of some either CPU intensive or long-running tasks, you use Celery, as that is what it is made for.",0.81632656,0.47219074,0.11842945963144302
65,"Question\nI am trying to push code review +1 using 'on_behalf_of' in the Body long with Label field. but i am getting response code 403, it giving below response message
Response:
'not permitted to modify label ""Code-Review"" on behalf of """"
below is my post body
URL:- 'http:///base-url/changes//revisions/current/review'
data: {'on_behalf_of': 'user-name@mail.com', 'labels': {'Code-Review': 1}}
User-name is Service account or faceless ID, I don't have credential of it.
Can anyone suggest  how to push code-review using Group/Service-account id? where do we get the permission for this post?
Regards\nAnswer: To isolate all permission issues:
Apart from the command executer is having ""Label Code-Review (On Behalf Of)"" permission, the user in the context of 'on_behalf_of' should have permission to vote for Code-Review.",0.0,0.10120952,0.010243367403745651
66,"Question\nI have a game written in Pygame and was trying to debug it. I am new to debuggers so I am not even sure which one to use but I downloaded Pycharm. Anyway, my game starts up and shows the game screen along with a ""Play"" button in the middle of the screen. When you click on the Play button, the Play button is supposed to disappear and three other buttons are supposed to appear so that you can choose your skill level: Easy, Medium and Hard. When you choose your skill level, the game is supposed to begin.
The game is beginning when I click the Play button. The other three buttons are not showing. I want to use a debugger to see what is happening in the code when the Play button is clicked.
The problem that I am having is that when I try and debug the code, the game starts and takes up the full screen. I cannot see the debugger. When I click the Play button, I get the same bug as already mentioned earlier but I am unable to see the debugger to know what is happening in the code.
I need some advice. Which of the following paths should I head down?:
Path A: Try and figure out how to get the game to open in a window different from and next to the debugger window.
Path B: Try and figure out how to simulate clicking on the Play button, in the debugger, without actually running the game.
Path C: Follow a different path that I have not even thought of but someone on Stack Overflow is about to explain to me. :)
PS: I am not asking for help with the code. I can probably figure out the problem without a debugger. I am asking for help with how to use a debugger so that I can debug in the future and not have to ask so many questions.
clipovich\nAnswer: I think path A would be the best. On windows and some linux distros, you can try pressing flag+left or right arrow. Then click the debugger and press flag+left or right arrow (opposite of pygame window side). This will allow you to see both windows at once",0.40816328,0.27602848,0.017459604889154434
67,"Question\nI try to get text features for stylometry task in identifying the author of the given text.
I check for :

text length by word
text length by character
punctuation count
unique word count
etc...
but in classifying this feature give unreasonable results, so:
i want to check more features...
I have two questions:


Is there any good feature that I forget to extract from text to help classifying result
I have a data frame like this:
text,                  author,    pos
i go to school  ,        x ,       [N,V,...]
..
we are good  ,     y     ,    [N,V,ADj]

my question is that how can I get good feature from pos column? for example the ratio of ""N"" to other? or some thing like this...
how to improve this multi_input single_lable multi_class classification result? with which feature? is there any good source?\nAnswer: Many studies have shown that the frequency and distribution of FUNCTION WORDS is significant and indicative of individual style. FUNCTION WORDS are those with little or no LEXICAL MEANING: articles THE, AN, A, conjunctions, personal pronouns, auxiliary verbs. etc.",0.40816328,0.22623134,0.033099230378866196
68,"Question\nI am using pytesseract to create searchable pdfs. Since it is important to keep the original look of the images, I want to create the PDF using the original image. However, to improve the OCR result I need to apply some preprocess to the images (deskew, binarization, etc..).
here is how I get a pdf using the processed image.
pdf = pytesseract.image_to_pdf_or_hocr(adaptive_threshold, lang=""frk+deu"", config=config)
So, how could I apply an alto_xml output, for exemple:
alto= pytesseract.image_to_alto_xml(adaptive_threshold, lang=""frk+deu"", config=config)
to the original image and convert it to pdf? Like:
original_image + alto -> convert to a searchable pdf.
Thank you!\nAnswer: There could not be a way to this directly in pyhton because pytesseract is just a python wrapper for the command line version of Tesseract OCR",-0.71428573,0.32859623,1.0876028537750244
69,"Question\nI created an exe file using PyInstaller and it works on my PC with Windows 8.1 and laptop with Windows 10, but on computers with Windows 7 it has error
""error loading python37 dll ""
and something about dynamic linked libraries.
EDIT:
Error loading Python DLL 'C:\Users\Dell\Appdata\Local|Temp|_MEI16442\python37.dll'. LoadLibrary: Procedure of initialize dynamic linked library (DLL) failed.
It is translated from Polish
Do you know maybe how can I fix it?
I was reading about static linked dll but I dont know how to do it. I am working on Windows only, I dont know Linux/Mac.\nAnswer: This used to happen to me all the time, and it was always because I tried to run the executable file from the build folder while the one that works is in the dist folder.",0.0,0.070916355,0.005029129330068827
70,"Question\nI created an exe file using PyInstaller and it works on my PC with Windows 8.1 and laptop with Windows 10, but on computers with Windows 7 it has error
""error loading python37 dll ""
and something about dynamic linked libraries.
EDIT:
Error loading Python DLL 'C:\Users\Dell\Appdata\Local|Temp|_MEI16442\python37.dll'. LoadLibrary: Procedure of initialize dynamic linked library (DLL) failed.
It is translated from Polish
Do you know maybe how can I fix it?
I was reading about static linked dll but I dont know how to do it. I am working on Windows only, I dont know Linux/Mac.\nAnswer: I had this same issue while compiling the executable with a Pyinstaller command. To fix it, I added the --noupx option and everything worked fine.",0.0,0.15585446,0.02429061383008957
71,"Question\nI currently am working on a bioinformatics project that currently involves a dictionary corresponding to about 10million unique keys, which each return a subset of categorical strings.
I currently use unpickle a dictionary object, but my main issue is that unpickling takes a very long time. I also need to iterate through a file, generating a set of keys(~200) for each row, lookup the keys, appending the list to a list-of-lists, and then subsequently flattening the list to generate a counter object of value frequencies for each row, and I have heard that a SQL database like structure would end up trading load times for lookup times.
The file that has keys typically contain about 100k rows and so this was my best solution, however it seems like even on faster pcs with increased ram, num of cores, and NVME storage that the time spent on loading the database is extremely slow.
I was wondering what direction (different database structure, alternatives to pickle such as shelves or mashall, parallelizing the code with multiprocess) would provide an overall speed up (either through faster loading times, faster lookup, or both) to my code?
Specifically: Need a create databases of the format key -> (DNA sub-sequence) : value ->[A,B,C,Y,Z] on the order of 1e6/1e7 entries.
When used, this database is loaded, and then given a query file (1e6 DNA sequences to query), perform a lookup of all the sub sequences in each sequence do the following.
For each query:

slice the sequence into subsequences.
Lookup each subsequence and return the list of categoricals for each subsequence
Aggregate lists using collections.Counter

I was wondering how to either:

Speed up the loading time of the database, either through a better data structure, or some optimization
Generally improve the speed of the run itself (querying subsequences)\nAnswer: I'm not sure there is a right answer here since there are some tradeoff, BUT.
two options come to mind:
1st. consider using panads.DataFrame for the data-stucture.
It will allow serialization/deserialization to many formats (I believe CSV should be the fastest but would give SQL a try). as for query time, it should be much faster than a dict for the complex queries.
2",0.0,-0.25861257,0.06688046455383301
72,"Question\nHope everybody is safe and happy.
I have given a AWS task as follow.
Requirement: As soon as file gets uploaded in s3 bucket,it should be zipped and must be uploaded back to another s3 bucket.
I am able to complete this task using lambda(python) but it involves lots of disk IO. So,i am looking for a solution where i will not require to store incoming s3 object at /tmp/ folder. As we all know, lambda provides very less memory and storage(500MB). Hence, wants to avoid this approach.
So, does anybody aware of how to zip the incoming s3 file on the fly? I am only aware that, it possible to do with the help of streaming of s3 object. But not able to find how it can be achieved end to end using python language.
Thanks in advance.\nAnswer: You can use python generators in the Lambda to zip the s3 files and put the zip file back in another s3 as the generators doesn't occupy memory.",0.0,-0.0016465187,2.7110238534078235e-06
73,"Question\nHi am trying to calculate a vector of the major axis through a 3d mesh in python (using open3d library to interact with the mesh).
I have turned the mesh into a pointcloud using a poisson distribution (1000 points in a numpy array) and have thought about using scikit learn and its PCA functionality to try and get the value of this vector.
From googling around I think I'm on the right tract but have little idea about how to use the PCA function to get what I want.
I think I need to extract the largest eigenvalue from the pointcloud and its accompanying eigenvector - which should hopefully be what I'm looking for.
Have little idea how to do this as I am completely unfamiliar with scikit learn.
Any help please?\nAnswer: Have found a solution using trimesh library:
used the principal_inertia_vectors function to find the 3 largest eigenvalues and corresponding eigenvectors. The eigen vectors correspond to the 3 axes of the mesh.
This functions runs straight off the mesh therefore not requiring conversion to a point cloud.",0.0,0.27952474,0.07813408225774765
74,"Question\nI want to login to site: 'https://portal.librus.pl/rodzina/synergia/loguj' with requests in python, unfortunately if you inspect and go to network settings it shows main method without 'form data', in addition it has csrftoken placed in script like this:
script type=""text/javascript"">
var csrfTokenName = ""requestkey"";
var csrfTokenValue = ""MC44MDU4NjQwMCAxdjA0MDk5NtjdKdm32NdksMkfmuMTU0NzZjMTNi1WE1ZmJdfmUdfOpNIlhMDQx"";
</script
So i don't really know how to get the content out of this regard you got hundreds of script tags in html file
So the question is how to login to this site?
(i've watched all tutorials on internet, and non of them work)\nAnswer: I've found the request inside network tab and this is the things that I saw (I might have missed some information cause I got error for not having account).


In network tab if you try to login with non empty values you will have a POST request with name 'action'.
Inside this request you will find the URL you want ""https://portal.librus.pl/rodzina/login/action""
And last part is the form data in the bottom",0.0,0.067944944,0.0046165152452886105
75,"Question\nLet’s say one saves a Tensorflow model created using large data and GPUs. If one wanted to then use the saved model to do a single prediction with one small piece of data, does one still need the huge computers that created the model?
I’m wondering more generally how the size and computing resources needed to generate a deep learning model relate to using the model to make predictions.
This is relevant because if one is using Google Cloud Compute it costs more money if one has to use the huge computers all the time. If one could just use the huge computers to train the model and then more modest ones to run their app that make predictions it would save a lot of money.\nAnswer: Resources needed for prediction depend on the model size - not on the training device.
If the model has 200 bln variables - you will not be able to run it on workstation (because you have not enough memory).
But you can use model with 10 mln variables with no problems even if it was trained on GPU or TPU.
Every variable takes 4 to 8 bytes. If you have 8 GB of memory - you will probably be able to run a model with hundreds million variables.
Prediction is fast (assuming you have enough memory). Resources needed to train model quickly. It is efficient to train on GPU/TPU even if your model is small.",1.0,0.6152313,0.14804697036743164
76,"Question\nI'm working on a research project where the data is stored in a remote Windows desktop. The desktop does not have Python, it only has Rstudio, but  most of the research conducted in this topic was in Python. I want to benchmark already existing implementations, but I can't run the code on the data because there's no Python and this will not change. As far as I understand, tools like reticulate still need an underlying Python interpreter in the system to work and I am not allowed to do that.
Has anyone come up with a smart solution that does not involve me manually translating the python code to R? Any R packages that can read python? Any other out-of-the-box ideas on how to get that code to run on the data?\nAnswer: Say you have (1) the remote Windows Desktop that does not have Python and (2) the local computer you work on.
Use the remote Windows Desktop (1) as a git repository, clone that repository on your local machine, make changes to it and run locally using Python, then submit the code to the remote repository again.",0.0,0.14994574,0.022483723238110542
77,"Question\nI have a physical model with one independent variable X and three parameters A, B and C. The dependent variable is Y=F(X,A,B,C) where F is the model's function which is smooth and continuous.
I have many files with data points (X,Y) that I want to fit (each file) with the model to study how the parameters change. The model is non-linear with respect to the independent variable and all the parameters.
To fit the model, I use scipy.optimize curve_fit in Python inside a for loop throughout the files with the data points. For each file, I get the best fitting parameters (A,B,C) and the model fits great to all the files. However, when I check the parameter errors after the fittings, I notice that the errors in A are really large, even two orders of magnitude higher than the corresponding values.
The experience tells me that these large variances might be due to the fact that the parameter A could be correlated to any of the others. To check this assumption, I plotted the obtained values of A against the values of B and C. I see that, in fact, a power-law relation of the form A=a*B^b arises, which is physically acceptable.
Is there a way to tell curve_fit to take care of this correlation without having to introduce new parameters (say a and b) in order to fit the model with the lowest variance possible?\nAnswer: Try to use method = 'trf' as a parameter for your fit. This avoids using least squares and works with pseudoinverse of the Jacobian. I believe a Jacobian with a zero eigenvalue then should not be a problem.",0.0,0.10185766,0.01037498377263546
78,"Question\nI am trying to make a business out of my discord.py skills. I have made a discord.py bot and want to export it from pycharm to be able to sell it on a site like fiverr. Does someone know how I could do that? (I am an absolute noob at this, pls help me anyone...)\nAnswer: So you're exporting the source code? If so, you could copy & paste, but it isn't the most professional way of doing things, you could send them a link to a private github repo, or else send them the folder with the src files etc. There's plenty of ways of doing it. But find out what works for you. :)",0.0,0.20973498,0.04398876056075096
79,"Question\nIs there a way to be able to read your current position in an audio file using FFmpeg?
I want to make it so my discord bot plays an audio clip and then on exit, it saves the current position and then when it rejoins the VC it can resume at that point. I know how to get it to resume at the correct time using -ss but I am not sure how to get the timestamp at the point ti leaves.
Any help would be appreciated :)!\nAnswer: I figured out a better way to do it if anyone is interested. if you set a global variable to the time since epoch and set that as start, then when the bot is disconnected set the time since epoch to end. You can then subtract start from end and you are left with the amount of time into the audio clip you are. this can then be stored and retrieved at a later date.",0.40816328,0.16007662,0.061546992510557175
80,"Question\nI have been successfully using a virtual environment created by pipenv for months, located at ~/.virtualenvs. However, today when I tried to activate it using ""pipenv shell"" (while in the proper directory) pipenv creates a new.venv file in the current directory instead of loading the environment from ~/.virtualenvs. My main concern is: how do I redirect pipenv to the existing virtual environment? Out of curiosity, any ideas about what could suddenly cause this behavior?\nAnswer: just go to that project/venv bin folder and then do source activate it will as activate as it is just a wrapper around virtualenv",0.0,0.16585597,0.02750820480287075
81,"Question\nI've been running GDAL through python on anaconda for years with no problem. Today when I tried to import gdal I got this error code:

On Windows, with Python >= 3.8, DLLs are no longer imported from the
PATH. If gdalXXX.dll is in the PATH, then set the
USE_PATH_FOR_GDAL_PYTHON=YES environment variable to feed the PATH
into os.add_dll_directory().

I've been looking for a solution to this but can't seem to figure out how to fix this. Anybody has a solution?\nAnswer: use:
from osgeo import gdal
instead of:
import gdal",0.0,-0.2807268,0.078807532787323
82,"Question\nI have been able to successfully detect an object(face and eye) using haar cascade classifier in python using opencv. When the object is detected, a rectangle is shown around the object. I want to get coordinates of mid point of the two eyes. and want to store them in a array. Can any one help me? how can i do this. any guide\nAnswer: So you already detected the eye? You also have a bounding box around the eye?
So your question comes down to calculatiing the distance between 2 bounding boxes and then dividing it by 2?
Or do I misunderstand?
If you need exact the center between the two eyes a good way to go about that would be to take the center of the 2 boxes bounding the 2 eyes.
Calculate the distance between those two points and divide it by 2.
If you're willing to post your code I'm willing to help more with writing code.",0.0,0.035615146,0.0012684386456385255
83,"Question\nI have been able to successfully detect an object(face and eye) using haar cascade classifier in python using opencv. When the object is detected, a rectangle is shown around the object. I want to get coordinates of mid point of the two eyes. and want to store them in a array. Can any one help me? how can i do this. any guide\nAnswer: I suppose you have the coordinates for the bounding boxes of both eyes.
Something like X1:X2 Y1:Y2 for both boxes.
You just have to find the center of these boxes: (X2-X1)/2+X1 and (Y2-Y1)/2+Y1
You'll get two XY coordinates from this, basically just do the above again with these coordinates, and you'll get the center point",0.0,-0.22199613,0.04928228259086609
84,Question\nI'm really new to web scraping. Is there anyone that could tell me how to search on google.com with Selenium in Python?\nAnswer: Selenium probably isn't the best. other libraries/tools would work better. BeautifulSoup is the first one that comes to mind,0.13605443,0.39982766,0.06957631558179855
85,"Question\nI need some advise in one of my usecase regarding Cloudtrail and Python boto3.
I have some cloudtrail events like configured and i need to send the report of all those events manually by downloading the file of events.
I am planning to automate this stuff using python boto3. Can you please advise how can i use boto3 to get the cloudtrail events for some specific date i should paas at runtime along with the csv or json files downloaded and sent over the email. As of now i have created a python script which shows the cloudtrail event but not able to download the files. Please advise\nAnswer: My suggestions is to simply configure the deliver of those events to an S3 bucket, and you have there the file of events. This configuration is part of your trail configuration and doesn't need boto3.
You can then access events files stored on S3 using boto3 (personally the best way to interact with AWS resources), and manipulate those files as you prefer.",0.0,0.21547323,0.04642871394753456
86,"Question\nI read through the documentation, but something wasn't clear for me: if I coded a custom layer and then used it in a model, can I just save the model as SavedModel and the custom layer automatically goes within it or do I have to save the custom layer too?
I tried saving just the model in H5 format and not the custom layer. When I tried to load the model, I had an error on the custom layer not being recognized or something like this. Reading through the documentation, I saw that saving to custom objects to H5 format is a bit more involved. But how does it work with SavedModels?\nAnswer: If I understand your question, you should simply use tf.keras.models.save_model(<model_object>,'file_name',save_format='tf').
My understanding is that the 'tf' format automatically saves the custom layers, so loading doesn't require all libraries be present. This doesn't extend to all custom objects, but I don't know where that distinction lies. If you want to load a model that uses non-layer custom objects you have to use the custom_objects parameter in tf.keras.models.load_model(). This is only necessary if you want to train immediately after loading. If you don't intend to train the model immediately, you should be able to forego custom_objects and just set compile=False in load_model.
If you want to use the 'h5' format, you supposedly have to have all libraries/modules/packages that the custom object utilizes present and loaded in order for the 'h5' load to work. I know I've done this with an intializer before. This might not matter for layers, but I assume that it does.
You also need to implement get_config() and save_config() functions in the custom object definition in order for 'h5' to save and load properly.",0.0,0.41250676,0.17016182839870453
87,"Question\nHow can one surelly tell that function retuns an iterable object, which calculates results on demand, and not an iterator, which returns already calculated results?
For e.g. function filter() from python's documentation says:

Construct an iterator from those elements of iterable for which function returns true

Reading that I cat tell that this function returns an object which implements iterable protocol but I can't be sure it won't eat up all my memory if use it with generator which reads values from 16gb file untill I read further and see the Note:

Note that filter(function, iterable) is equivalent to the generator expression (item for item in iterable if function(item))

So, how does one can tell that function calculates returned results on demand and not just iterating over temporary lists which holds already calculated values? I have to inspect sources?\nAnswer: If the doc says that a function returns an iterator, it's pretty safe to assume it calculates items on the fly to save memory. If it did calculate all its items at once, it would almost certainly return a list.",0.40816328,0.2669649,0.019936978816986084
88,"Question\nhow do you get only the whole number of a non-integer value without the use of rounding-off? I have searched for it and I seem to be having a hard time.
For example:

w = 2.20
w = 2.00

x = 2.50
x = 2.00

y = 3.70
y = 3.00

z = 4.50
z = 4.00

Is it as simple as this or that might get wrong in some values?
x = 2.6 or x = 2.5 or x = 2.4
x = int(x)
x = 2

Is it really simple as that? Thanks for answering this stewpid question.\nAnswer: you can   just divided it into (1)
but use (//) like this:
x = x // 1",0.81632656,-0.095962375,0.8322710394859314
89,"Question\nI am trying to install OpenCV in a docker container (CentOS).
I tried installing python first and then tried yum install opencv-contrib but it doesn't work.
Can someone help me out as to how to install OpenCV in Docker (CentOS)?\nAnswer: To install OpenCV use the command: sudo yum install opencv opencv-devel opencv-python
And when the installation is completed use the command to verify: pkg-config --modversion opencv",0.0,0.47501254,0.22563691437244415
90,"Question\ni'm solliciting you today because i've a problem with selenium.
my goal is to make a full automated bot that create an account with parsed details (mail, pass, birth date...) So far, i've managed to almost create the bot (i just need to access to gmail and get the confirmation code).
My problem is here, because i've tried a lot of things, i have a Failed to load resource: the server responded with a status of 429 ()
So, i guess, instagram is blocking me.
how could i bypass this?\nAnswer: The answer is in the description of the HTTP error code. You are being blocked because you made too many requests in a short time.
Reduce the rate at which your bot makes requests and see if that helps. As far as I know there's no way to ""bypass"" this check by the server.
Check if the response header has a Retry-After value to tell you when you can try again.",0.0,0.19578558,0.03833199292421341
91,"Question\ni'm solliciting you today because i've a problem with selenium.
my goal is to make a full automated bot that create an account with parsed details (mail, pass, birth date...) So far, i've managed to almost create the bot (i just need to access to gmail and get the confirmation code).
My problem is here, because i've tried a lot of things, i have a Failed to load resource: the server responded with a status of 429 ()
So, i guess, instagram is blocking me.
how could i bypass this?\nAnswer: Status code of 429 means that you've bombarded Instagram's server too many times,and that is why Instagram has blocked your ip.
This is done mainly to prevent from DDOS attacks.
Best thing would be to try after some time ( there might be a Retry-After header in the response).
Also, increase the time interval between each request and set the specific count of  number of requests made within a specified time (let's say 1 hr).",0.0,0.1115545,0.012444407679140568
92,"Question\nI'm currently learning how to use the Tweepy API, and is there a way to filter quoted Tweets and blocked users? I'm trying to stop search from including quoted Tweets and Tweets from blocked users. I have filtered Retweets and replies already.
Here's what I have:
for tweet in api.search(q = 'python -filter:retweets AND -filter:replies', lang = 'en', count = 100):\nAnswer: To filter quotes, use '-filter:quote'",0.0,0.4588222,0.21051780879497528
93,"Question\nHow can I read a csv file from s3 without few values.
Eg: list [a,b]
Except the values a and b. I need to read all the other values in the csv. I know how to read the whole csv from s3. sqlContext.read.csv(s3_path, header=True)  but how do I exclude these 2 values from the file and read the rest of the file.\nAnswer: You don't.  A file is a sequential storage medium.  A CSV file is a form of text file: it's character-indexed.  Therefore, to exclude columns, you have to first read and process the characters to find the column boundaries.
Even if you could magically find those boundaries, you would have to seek past those locations; this would likely cost you more time than simply reading and ignoring the characters, since you would be interrupting the usual, smooth block-transfer instructions that drive most file buffering.
As the comments tell you, simply read the file as is and discard the unwanted data as part of your data cleansing.  If you need the file repeatedly, then cleanse it once, and use that version for your program.",0.20408164,0.25818288,0.0029269445221871138
94,"Question\nI has been forced to develop python scripts on Windows 10, which I have never been doing before.
I have installed python 3.9 using windows installer package into C:\Program Files\Python directory.
This directory is write protected against regular user and I don't want to elevate to admin, so when using pip globally  I use --user switch and python installs modules to C:\Users<user>\AppData\Roaming\Python\Python39\site-packages and scripts to C:\Users<user>\AppData\Roaming\Python\Python39\Scripts directory.
I don't know how he sets this weird path, but at least it is working. I have added this path to %Path% variable for my user.
Problems start, when I'm trying to use virtual environment and upgrade pip:

I have created new project on local machine in C:\Users<user>\Projects<project> and entered the path in terminal.
python -m venv venv
source venv\Scrips\activate
pip install --upgrade pip

But then I get error:
ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access denied: 'C:\Users\\AppData\Local\Temp\pip-uninstall-7jcd65xy\pip.exe'
Consider using the --user option or check the permissions.
So when I try to use --user flag I get:
ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.
So my questions are:

why it is not trying to install everything inside virtual enviroment (venv\Scripts\pip.exe)?
how I get access denied, when this folder suppose to be owned by my user?

When using deprecated easy_install --upgrade pip everything works fine.\nAnswer: I recently had the same issue for some other modules. My solution was simply downgrade from python 3.9 to 3.7. Or make an virtual environment for 3.7 and use that and see how it works.",0.40816328,0.13664979,0.0737195760011673
95,"Question\nI coded to open webcam video on a new window using OpenCV cv2.VideoCapture(0).
You can display text on webcam video using cv2.putText() command. But it displays string values only.
How to put varying values in the webcam video that is being displayed on a new window?
For example, if value of variable p is changing all the time, you can easily display it on the command window by writing print(p).
But how can we display values of p over the webcam video?\nAnswer: You can also show changing variables using cv2.putText() method. Just need to convert the variable into string using str() method. Suppose you want to show variable x that is for example an integer and it is  always changing. You can use cv2.putText(frame, str(x), org, font, fontScale, color, thickness, cv2.LINE_AA)  to do it (You should fill org,font, etc.).",0.0,0.501184,0.2511853873729706
96,"Question\nI've done some project using PyQt library for my class assignmnet.
And I need to check my application working before I submit it.
Today, 3 hours ago I updated my Mac book OS to Big Sur.
And I found out that PyQt library doesn't work. It doesn't show any GUI.
Are there someone know how to fix it?\nAnswer: Related to this, after upgrading to BigSur my app stopped launching its window...I am using the official Qt supported binding PySide2/shiboken2
Upgrading from PySide2 5.12 to 5.15 fixed the issue.
Steps:

Remove PySide2/shiboken2
pip3 uninstall PySide2
pip3 uninstall shiboken2

Reinstall
pip3 install PySide2",0.0,0.39590287,0.15673908591270447
97,"Question\nI've done some project using PyQt library for my class assignmnet.
And I need to check my application working before I submit it.
Today, 3 hours ago I updated my Mac book OS to Big Sur.
And I found out that PyQt library doesn't work. It doesn't show any GUI.
Are there someone know how to fix it?\nAnswer: Rolling back to PyQt5==5.13.0 fixed the issue for me!
you should uninstall PyQt5 and then install it using
pip install PyQt5==5.13.0",0.6122449,0.13509971,0.22766754031181335
98,"Question\nI have a flask application that uses flask-socketio and python-socketio to facilitate communication between a socketio server in the cloud and a display device via a hardware device.
I have a display namespace which exposes the display facing events, and also uses a separate client class which connects and talks to the server in the cloud. This works well as designed, but now I want to trigger the connection method in my client class from a different namespace. So far I have not been able to get this to work.
What I have tried is adding the display namespace class to the flask context, then passing that into the socketio.on_namespace() method. Then from the other namespace I am grabbing it from current_app and trying to trigger the connection to the cloud server. This returns a 'RuntimeError: working outside of application context' error.
So at this point I'm still researching how to do this correctly, but I was hoping someone has dealt with something like this before, and knows how to access methods on one namespace from a different one.\nAnswer: I found a solution. Instead of instantiating my client class from the display namespace, I instantiate it before I add the namespaces to socketio. Then I pass the client object into both namespaces when I call the socketio.on_namespace() method.",0.0,0.29688895,0.08814305067062378
99,"Question\nAfter pip install package_name from my recently uploaded pypi package
It imports python filename directly after installing,
I wanted to use like below
import package_name or from package_name import python_file
but this doesnt work instead this works
import python_file even package is installed name is package_name
pypi package name package_name and
My directory structure is below

package_name

setup.py

folder1

python_file





In setup.py, i've used package_dir={'': 'folder_1'}
but even import folder_1 or from folder_1 import python_file  didnt worked.
I tried if adding __init__.py inside folder_1, it didnt solved.
I've been following Mark Smith - Publish a (Perfect) Python Package on PyPI,
which told this way, but any idea what is happening, how can i solve it??\nAnswer: So what you actual did is to tell python that the root folder is folder_1.
This is not what you want.
You just need to tell that folder_1 (or actually replace it by package_name, see below) is a package and to declare it using:
packages = {'folder1'}.
Usually, people don't do it but let the function find_packages() to do the work for them by packages=find_packages()
In addition package folder should contain a __init__.py.
to conclude you need a folder structure like below and use find_packages().
It is OK and even popular choice that the project name and it single main package have the same name.

project_name

setup.py
package_name

__init__.py
python_file.py",0.40816328,0.37854695,0.00087712676031515
0,"Question\nWhat it is the best way to make a chessboard for checkers using Kivy framework?
I have board.png, white.png, black.png, white_q.png, black_q.png files already. I wonder how to assign to each black tile on my board.png its own coordinate. Should I create  32 transparent widgets placed on black tiles of board.png or it is impossible? And what widget to use for 24 checkers? Any ideas or it is too complicated using Kivy and I should use tkinter?\nAnswer: There are many ways you could do this. It isn't complicated, it's very easy. The best way depends more on how you want to structure your app than anything else.

I wonder how to assign to each black tile on my board.png its own coordinate

Set the pos attribute of a widget to control its position, or better in this case use a layout that does what you want. For instance, adding your squares to a GridLayout with the right number of columns will have the right effect without you needing to worry more about positioning them.

Should I create 32 transparent widgets placed on black tiles of board.png or it is impossible?

I don't understand what you're asking here. You can make transparent widgets if you want but I don't know why you'd want to.

And what widget to use for 24 checkers?

The real question is, what do you want the widget to do? e.g. if you want it to display an image then inherit from Image.
Overall this answer is very generic because your question is very generic. I suggest that if you're stuck, try to ask a more specific question about a task you're struggling with, and give a code example showing where you are now.",0.40816328,0.18703145,0.04889928549528122
1,"Question\nI have a local computer A and remote computer B. Computer A has script client.py Computer B has server.py Script client.py has a variable port. Let's say port = 5535.
I am running client.py on Computer A, which is using the port number for socket communication. I need to change the port number to another port number while the client.py is running so it will switch to another server at runtime after notifying the client to change the port number. I am using pyzmq to send data from the client to the server sending a continuous stream of data.
Is this scenario possible and how can I do it?\nAnswer: Yes, it's possible. You may design / modify the (so far unseen) code so as to PUSH any such need to change a port# on-the-fly to the PULL-side, to release the 5535 and use another one.
The PULL-side shall then call.disconnect() and.close() methods, so as to release the said port 5535 ( plus notify that it has done so, perhaps by another PUSH/PULL to the.bind()-locked party, that it can now unbind and close the.bind()-locked port# 5535 too) and next setup a new connection to an announced ""new_port#"", received from the initial notification ( which ought have been already.bind()-locked on the PUSH-side, ought it not? :o) ).
That easy.",0.40816328,0.15584224,0.06366590410470963
2,"Question\nis there any way to sum odd numbers from 1 to n but without any loops and if there isn't a way how can i create this by fast algorithm to do this task in less than n loops.\nAnswer: You can try the one below, which loop through from 1 to n, stepping 2
sum(range(1,n,2))",0.0,0.18748498,0.035150617361068726
3,"Question\nSo am working in a group project, we are using python and of the code is on GitHub. My question is how do I activate the virtual environment? Do I make one on my own using the ""python virtual -m venv env"" or the one that's on the repo, if there is such a thing. Thanks\nAnswer: virtual env is used to make your original env clean. you can pip install virtualenv and then create a virtual env like virtualenv /path/to/folder then use source /path/to/folder/bin/activate to activate the env. then you can do pip install -r requirements.txt to install dependencies into the env. then everything will be installed into /path/to/folder/lib
alteratively, you can use /path/to/folder/bin/pip install or /path/to/folder/bin/python without activating the env.",0.20408164,0.16214341,0.0017588151386007667
4,"Question\nSo am working in a group project, we are using python and of the code is on GitHub. My question is how do I activate the virtual environment? Do I make one on my own using the ""python virtual -m venv env"" or the one that's on the repo, if there is such a thing. Thanks\nAnswer: Yes, you'll want to create your own with something like: python -m venv venv. The final argument specifies where your environment will live; you could put it anywhere you like. I often have a venv folder in Python projects, and just.gitignore it.
After you have the environment, you can activate it. On Linux: source venv/bin/activate. Once activated, any packages you install will go into it; you can run pip install -r requirements.txt for instance.",0.0,0.3482203,0.12125737220048904
5,"Question\nI know how to use scikit-learn and pandas to encode my categorical data. I've been using the category codes in pandas for now which I later will transform into an OneHot encoded format for ML.
My issues is that I need to create a pre-processing pipeline for multiple files with the same data format. I've discovered that using the pandas category codes encoding is not consistent, even if the categories (strings) in the data are identical across multiple files.
Is there a way to do this encoding lexicographically so that it's done the same way across all files or is there any specific method that can be used which would result in the same encoding when applied on multiple files?\nAnswer: The LabelEncoder like all other Sklearn-Transformers has three certain methods:

fit(): Creates the labels given some input data
transform(): Transforms data to the labels of the encoder instance. It must have called fit() before or will throw an error
fit_transform(): That's a convenience-method that will create the labels and transform the data directly.

I'm guessing you are calling fit_transform everywhere. To fix this, just call the fit-method once (on a superset of all your data because it will throw an error if it encounters a label that was not present in the data you called fit on) and than use the transform method.",0.0,0.34497803,0.1190098449587822
6,"Question\nGenerally to create a Virtual Camera we need to create a C++ application and include DirectShow API to achieve this. But with the modules such as
win32 modules and other modules we can use win32 api which lets us use these apis in python.
Can anyone Help sharing a good documentation or some Sample codes for doing this?\nAnswer: There is no reliable way to emulate a webcam on Windows otherwise than supplying a driver. Many applications take simpler path with DirectShow, and emulate a webcam for a subset of DirectShow based applications (in particular, modern apps will be excluded since they don't use DirectShow), but even in this case you have to develop C++ camera enumation code and connect your python code with it.",0.40816328,0.2085225,0.0398564413189888
7,"Question\nI am working out of R Studio and am trying to replicate what I am doing in R in Python. On my terminal, it is saying that I have xlrd already installed but when I try to import the package (import xlrd) in R Studio, it tells me: ""No module named 'xlrd'"". Does anyone know how to fix this?\nAnswer: I have solved this on my own. In your terminal, go to ls -a and this will list out applications on your laptop. If Renviron is there, type nano.Renviron to write to the Renviron file. Find where Python is stored on your laptop and type RETICULATE_PYTHON=(file path where Python is stored). ctrl + x to exit, y to save and then hit enter. Restart R studio and this should work for you.",0.40816328,0.27501202,0.01772925816476345
8,"Question\nBasically as you all know we can backtest our strategies in Zipline, the problem is that Zipline is developed for stock markets and the minimum order of an asset that can be ordered is 1 in those markets but in crypto markets we are able to order a fraction of a Crypto currency.
So how can I make zipline to order a fraction of Bitcoin base on the available capital?\nAnswer: You can simulate your test on a smaller scale, e.g. on Satoshi level (1e8).
I can think of two methods:

Increase your capital to the base of 1e8, and leave the input as is. This way you can analyse the result in Satoshi, but you need to correct for the final portfolio value and any other factors that are dependent on the capital base.
Scale the input to Satoshi or any other level and change the handle_data method to either order on Satoshi level or based on your portfolio percentage using order_target_percent method.

NOTE: Zipline rounds the inputs to 3 decimal points. So re-scaling to Satoshi turns prices that are lower than 5000 to NaN (not considering rounding errors for higher prices). My suggestion is to either use 1e5 for Bitcoin or log-scale.",0.0,0.37037683,0.13717898726463318
9,"Question\nBeen searching for a while in order to understand how to do this basic task without any success which is very strange.
I have a dataset where some of the rows contain '-', I have no clue under which columns these values lie.
How do I search in the whole dataset (including all columns) for '-' and drop the rows containing this value?
thank you!\nAnswer: This is a bit more robust than wwnde's answer, as it will work if some of the columns aren't originally strings:
df.loc[~df.apply(lambda x: any('-' in str(col) for col in x), axis = 1)]
If you have data that's stored as datetime, it will display as having -, but will return an error if you check for inclusion without converting to str first. Negative numbers will also return True once converted to str. If you want different behavior, you'll have to do something more complicated, such as
df.loc[~df.apply(lambda x: any('-' in col if isinstance(col, str) else False for col in x), axis = 1)]",0.0,0.2877562,0.0828036367893219
10,"Question\nI have a depth map image which was obtained using a kinect camera.
In that image I have selected a region of size [400,400] and stored it as another image.
Now, I would like to know how to resize this image into a size of [x,y] in python.\nAnswer: I don't recommend to reduce resolution of depth map the same way like it is done for images. Imagine a scene with a small object 5 m before the wall:

Using bicubic/bilinear algorithms you will get depth of something between the object and the wall. In reality there is just a free space in between.
Using nearest-neighbor interpolation is better but you are ignoring a lot of information and in some cases it may happed that the object just disappears.

The best approach is to use the Mode function. Divide the original depth map into windows. Each window will represent one pixel in the downsized map. For each of them calculate the most frequent depth value. You can use Python's statistics.mode() function.",0.0,0.34686607,0.12031607329845428
11,"Question\nI was writing code to make a facial recognition, but my code did not work because I was writing on verison 3, do you know how to download python 3 on the raspberry pi?\nAnswer: Linux uses package managers to download packages or programing languages
,raspberry pi uses apt(advanced package tool)
This is how you use APT to install python3:
sudo apt-get install python3
OR
sudo apt install python3
and to test if python3 installed correctly type:
python3
If a python shell opens python3 has been installed properly",0.0,0.279827,0.07830315083265305
12,"Question\nWhile I'm using Django as my backend and flutter as my front end. I want only the flutter app to access the data from django server. Is there any way to do this thing?
Like we use allowed host can we do something with that?\nAnswer: You can use an authentication method for it. Only allow for the users authenticated from your flutter app to use your backend.",0.40816328,0.21008271,0.0392359122633934
13,"Question\nin my mongodb, i have a collection where the docs are created not using ObjectId, how can I get the timestamp (generation_time in pymongo) of those docs? Thank you\nAnswer: If you don't store timestamps in documents, they wouldn't have any timestamps to retrieve.
If you store timestamps in some other way than via ObjectId, you would retrieve them based on how they are stored.",0.0,0.44113111,0.19459666311740875
14,"Question\nI have a dataframe created from an excel sheet (the source).
The excel sheet will not have a header row.
I have a table in mysql that is already created (the target). It will always be the exact same layout as the excel sheet.
source_data = pd.read_excel(full_path, sheet_name=sheet_name, skiprows=ignore_rows, header=None)
db_engine = [function the returns my mysql engine]
source_data.to_sql(name=table_name, con=db_engine, schema=schema_name, if_exists='append', index=False)
This fails with an error due to pandas using numbers as column names in the insert statement..
[SQL: INSERT INTO [tablename] (0, 1) VALUES (%(0)s, %(1)s)]
error=(pymysql.err.OperationalError) (1054, ""Unknown column '0' in 'field list'
how can i get around this? Is there a different insert method i can use? do i really have to load up the dataframe with the proper column names from the table?\nAnswer: Maybe after importing the data into Pandas, you can rename the columns to something that is not a number, e.g. ""First"", ""Second"", etc. or [str(i) for i in range(len(source_data))]
This would resolve the issue of SQL being confused by the numerical labels.",0.0,0.2280516,0.052007533609867096
15,"Question\nI have a dataframe created from an excel sheet (the source).
The excel sheet will not have a header row.
I have a table in mysql that is already created (the target). It will always be the exact same layout as the excel sheet.
source_data = pd.read_excel(full_path, sheet_name=sheet_name, skiprows=ignore_rows, header=None)
db_engine = [function the returns my mysql engine]
source_data.to_sql(name=table_name, con=db_engine, schema=schema_name, if_exists='append', index=False)
This fails with an error due to pandas using numbers as column names in the insert statement..
[SQL: INSERT INTO [tablename] (0, 1) VALUES (%(0)s, %(1)s)]
error=(pymysql.err.OperationalError) (1054, ""Unknown column '0' in 'field list'
how can i get around this? Is there a different insert method i can use? do i really have to load up the dataframe with the proper column names from the table?\nAnswer: Found no alternatives.. went with adding the column names to the data frame during the read..
So first i constructed the list of column names
sql = (""select [column_name] from [table i get my metadata from];"")
db_connection = [my connection for sqlalchemy]
result = db_connection.execute(sql)
column_names = []
for column in result:
    column_names.append(column[0])
And then i use that column listing in the read command:
source_data = pd.read_excel(full_path, sheet_name=sheet_name, skiprows=ignore_rows,header=None, names=column_names) 
the to_sql statement then runs without error.",0.0,0.17334938,0.0300500076264143
16,"Question\nI'm having trouble finding any sort of documentation or instruction for pykinect, specifically for the xbox 360 version of the kinect. how do I get skeletal data or where do I find the docs?? if I wasn't clear here please let me know!\nAnswer: To use python with the kinect 360 you need the follwing:
python 2.7
windows kinect sdk 1.8
pykinect   - NOT pykinect2",-0.71428573,0.21528172,0.8640956282615662
17,"Question\nLooking to create a GUI based 25-key keyboard using PYQT5, which can support MIDI controller keyboards. However, I don’t know where to start (What libraries should I use and how do I go about finding a universal method to supporting all MIDI controller keyboards). I plan to potentially use the Mido Library, or PyUSB but I am still confused as to how to make this all function. Any starting guides would be much appreciated.\nAnswer: MIDI is a universal standard shared by all manufacturers, so you don't have to worry about ""supporting all MIDI controller keyboards"", you just have to worry about supporting the MIDI studio of your system.
You'll have to scan your environment to get the existing MIDI ports. With the list of existing ports you can let the user choose to which port he wants to send the events generated by your keyboard and/or from which port he wants to receive events that will animate the keyboard (for instance from a physical MIDI keyboard connected to your computer), possibly all available input ports.
To support input events, you'll need a kind of callback prepared to receive the incoming notes on and off (which are the main relevant messages for a keyboard) at any time. That also means that you have to filter the received events that are not of those types because, in MIDI, a stream of events is subject to contain many kinds of other events mixed with the notes (pitch bend, controllers, program change, and so on).
Finally notice that MIDI doesn't produce any sound by itself. So if you plane to hear something when you play on your keyboard, the produced MIDI events should be send to a device subject to produce the sound (for instance a synthesizer or virtual instrument) via a port that this device receives.
For the library, Mido seems to be a pretty good choice : it has all the features needed for such a project.",0.81632656,0.22402728,0.3508184254169464
18,"Question\nI'm working on a very basic Web Application (built using flask and flask_restful) with unrelated views split into different blueprints.
Different blueprints deal with a different instance of a class.
Now I want to design a page with status(properties and value) of all the classes these blueprints are dealing with. The page is a kind of a control panel of sorts.
For this I want to call all the status routes (defined by me) in different blueprints from a single route(status page route) in a different blueprint. I have been searching for a while on how to make internal calls in Flask / Flask_restful, but haven't found anything specifically for this. So....

I would love to find out how to make these internal calls.
Also, is there any problem or convention against making internal calls.
I also thought of making use of the requests calls using Requests module, but that feels more like a hack. Is this the only option I got??? If yes, is there a way I dont have to hard code the url in them like using something close to url_for() in flask??

Thanks..  :)\nAnswer: I would love to find out how to make these internal calls.


Ans: use url_for() or Requests module, as u do for any other post or get method.


Also, is there any problem or convention against making internal calls?


Ans: I didn't find any even after intensive searching.


I also thought of making use of the requests calls using Requests module, but that feels more like a hack. Is this the only option I
got??? If yes, is there a way I don't have to hard code the url in
them like using something close to url_for() in flask??


Ans: If you don't wanna use Requests module, url_for() is the simplest and cleanest option there is. Hard coded path is the only option.",0.0,0.6346296,0.4027547240257263
19,"Question\nEvery time I do a: python manage.py runserver
And I load the site, python gets data and puts this in my database.
Even when I already filled some info in the database. Enough to get a view of what I am working on.
Now it is not loading the information I want and instead putting in new information to add to the database so it can work with some data.
What is the reason my data in the database is not being processed?
And how do I stop new data being loaded into the database.\nAnswer: May be it is happening due to migration file first sometimes when you migrate models into database query language with same number
python manage.py makemigrations 0001
This ""0001"" has to be changed everytime
To solve your problem once delete the migrations file and then again migrate all models and then try
Tell if this work",0.0,0.17849943,0.0318620465695858
20,"Question\nI find many examples of passing a list of images, and returning a stitched image, but not much information about how these images have beeen stitched together.
In a project, we have a camera fixed still, pointing down, and coveyers pass underneath. The program detects objects and start recording images. However some objects do not enter completely in the image, so we need to capture multiple images and stich then together, but we need to know the position of the stitched image because there are other sensors synchronized with the captured image, and we need to also synchronize their readings within the stitched image (i.e. we know where the reading is within each single capture, but not if captures are stitched together).
In short, given a list of images, how can we find the coordinates of each images relative to each other?\nAnswer: Basically while stiching correspondence between two (or more) images are setup. This is done with some constant key points. After finding those key points the images are warped or transformed & put together, i.e. stitched.
Now those key points could be set/ noted as per a global coordinate system (containing all images). Then one can get the position after stitching too.",0.0,0.22590923,0.05103498324751854
21,"Question\nI import an Excel file with pandas and when I try to convert all columns to float64 for further manipulation. I have several columns that have a type like:
0
column_name_1 float64
column_name_1 float64
dtype: object
and it is unable to do any calculations. May I ask how I could change this column type to float64?\nAnswer: I just solved it yesterday and it is because I have two same columns in the Data frame and it causes that when I try to access pd['something'] it automatically combine two columns together and then it becomes an object instead of float64",0.0,-0.15173936,0.02302483282983303
22,"Question\nWhat I mean is that I have a py file which I have converted to an exe file. So I wanted to know in case I decide to update the py file then how do I make it if I have sent it to someone the same changes occur in his file as well whether the exe or py file.\nAnswer: Put your version of the program on a file share, or make it otherwise available in the internet and build in an update check in the program. So that it checks the URL for a new version everytime it is started.
I guess this is the most common way to do something like that.",0.0,0.08694595,0.007559598423540592
23,"Question\nI'm making a matching game where there are several cards faced upside down and the user has to match the right pairs. The cards faced upside down are all turtle objects.
For eg. if there are 8 faced down cards, there are 8 turtle objects.
I'm having some trouble figuring out how to select the cards since I don't know which turtle is associated with the particular card selected by the user. I do have a nested list containing all turtles and those with similar images are grouped together. Is there any way to return the turtle object selected by the user?\nAnswer: If i got your question, one way to do so is that you should provide some id attribute to each turtle which will identify it. Then you can check easily which turtle was selected by the user.",0.0,0.06208408,0.0038544328417629004
24,"Question\nI installed Nativescript successfully and it works when running ns run android.
However, when I try to use ns run ios I get the ominous WARNING: The Python'six' package not found.-error
Same happens, when I try to use ns doctor.
I tried EVERYTHING that I found on the web. Setting PATH, PYTHONPATH, re-install python, six and everything - nothing helped.
Re-install of six tells me Requirement already satisfied.
Any ideas how to make this work???
I'm on MacOS Catalina.\nAnswer: It seems I have a total mess with paths and python installations on my Mac.
I found like 6 different pip-paths and like 4 different python paths.
Since I have no idea which ones I can delete, I tried installing six with all pip-versions I found and that helped.
How to clean up this mess is likely a subject for another thread :)",0.0,0.20715833,0.04291457310318947
25,"Question\nI have different excel files in the same folder, in each of them there are the same sheets. I need to select the last sheet of each file and join them all by the columns (that is, form a single table). The columns of all files are named the same. I think it is to identify the dataframe of each file and then paste them. But I do not know how\nAnswer: Just do what Recessive said and use a for loop to read the excel file one by one and do the following:

excel_files = os.listdir(filepath)


for file in excel_files:

read excel file sheet




save specific column to variable




end of loop



concatenate each column from different variables to one dataframe",0.0,-0.005366385,2.8798087441828102e-05
26,"Question\nIm trying to make a script that sent an email with python using smtp.smtplib, almost of examples i found while googling shows how to call this function with only smtpserver and port parameters.
i want to added other paramaters : domain and binding IP
i tried this : server = smtplib.SMTP(smtpserver, 25,'mydomain.com',5,'myServerIP')
I got this as error : TypeError: init() takes at most 5 arguments (6 given)
Can you suggest a way to do this?\nAnswer: This error is likely because the parameters are invalid (there is one too many). Try looking at the smtplib docs to see what parameters are valid",0.0,0.03270787,0.0010698047699406743
27,"Question\nI want to delete/tab several lines of code at the same time in Jupiter notebook. how could i do that? Is there hot keys for that?\nAnswer: While in the notebook, click to the left of the grey input box where it says In []: (You'll see the highlight color go from green to blue)
While it's blue, hold down shift and use your up arrow key to select the rows above or below
Press D twice
Click back into the cell and the highlight will turn back to green.",0.40816328,0.17070997,0.05638407543301582
28,"Question\nI want to make a new column by calculating existing columns.
For example df
df
no    data1    data2
1      10        15
2      51        46
3      36        20
......
i want to make this
new_df
no    data1    data2    data1/-2    data1/2    data2/-2    data2/2
1      10       15         -5          5         -7.5        7.5
2      51       46        -25.5      25.5        -23         23
3      36       20         -18        18          -9         9
but i don't know how to make this as efficient as possible\nAnswer: To create a new df column based on the calculations of two or more other columns, you would have to define a new column and set it equal to your equation. For example:
df['new_col'] = df['col_1'] * df['col_2']",0.0,-0.026462793,0.0007002794300206006
29,"Question\nI need user_password plaintext using Django. I tried many ways to get plaintext in user_password. but It's not working. So, I analyzed how the Django user password is generated. it's using the make_password method in the Django core model. In this method generating the hashed code using( pbkdf2_sha256) algorthm. If any possible to decrypt the password.
Example:
pbkdf2_sha256$150000$O9hNDLwzBc7r$RzJPG76Vki36xEflUPKn37jYI3xRbbf6MTPrWbjFrgQ=\nAnswer: As you have already seen, Django uses hashing method like SHA256 in this case. Hashing mechanisms basically use lossy compression method, so there is no way to decrypt hashed messages as they are irreversible. Because it is not encryption and there is no backward method like decryption. It is safe to store password in the hashed form, as only creator of the password should know the original password and the backend system just compares the hashes.
This is normal situation for most backend frameworks. Because this is made for security reasons so far. Passwords are hashed and saved in the database so that even if the malicious user gets access to the database, he can't find usefull information there or it will be really hard to crack the hashes with some huge words dictionary.",0.40816328,0.25409275,0.023737726733088493
30,"Question\nI parse pcap file with scapy python, and there is TCP packet in that pcap that I want to know what is the answer of this pcaket, How can I do that?
For example : client and server TCP stream
client-> server : ""hi""
server-> client : ""how are you""
When I get ""hi"" packet (with scapy) how can I get ""how are you""?\nAnswer: Look at the TCP sequence number of the message from the client.  Call this SeqC.
Then look for the first message from the client whose TCP acknowledgement sequence is higher than SeqC (usually it will be equal to SeqC plus the size of the client's TCP payload).  Call this PacketS1.
Starting with PacketS1, collect the TCP payloads from all packets until you see a packet sent by the server with the TCP PSH (push) flag set.  This suggests the end of the application-layer message.  Call these payloads PayloadS1 to PayloadSN.
Concatenate PayloadS1 to PayloadSN.  This is the likely application-layer response to the client message.",0.81632656,0.26676154,0.30202171206474304
31,"Question\nFor, example If a button click turns the background blue, or changes the button's text, how do I make sure that change stays even after i go to other frames?\nAnswer: One way to go is to create a configuration file (e.g. conf.ini) where you store your changes or apply them to other dialogs. It will allow you to keep changes after an app restarted.",0.0,0.2575549,0.06633452326059341
32,"Question\nI am currently working on a project in which I am using a webcam attached to a raspberry pi to then show what the camera is seeing through a website using a client and web server based method through python, However, I need to know how to link the raspberry pi to a website to then output what it sees through the camera while then also outputting it through the python script, but then i don't know where to start
If anyone could help me I would really appreciate it.
Many thanks.\nAnswer: So one way to do this with python would be to capture the camera image using opencv in a loop and display it to a website hosted on the Pi using a python frontend like flask (or some other frontend). However as others have pointed out, the latency on this would be so bad any processing you wish to do would be nearly impossible.
If you wish to do this without python, take a look at mjpg-streamer, that can pull a video feed from an attached camera and display it on a localhost website. The quality is fairly good on localhost. You can then forward this to the web (if needed) using port forwarding or an application like nginx.
If you want to split the recorded stream into 2 (to forward one to python and to broadcast another to a website), ffmpeg is your best bet, but the FPS and quality would likely be terrible.",0.0,0.2302633,0.05302118510007858
33,"Question\nI'm new to python MNE and EEG data in general.
From what I understand, MNE raw object represent a single trial (with many channels). Am I correct? What is the best way to average data across many trials?
Also, I'm not quite sure what the mne.Epochs().average() represents. Can anyone pls explain?
Thanks a lot.\nAnswer: From what I understand, MNE raw object represent a single trial (with many channels). Am I correct?

An MNE raw object represents a whole EEG recording. If you want to separate the recording into several trials, then you have to transform the raw object into an ""epoch"" object (with mne.Epochs()). You will receive an object with the shape (n_epochs, n_channels and n_times).

What is the best way to average data across many trials? Also, I'm not quite sure what the mne.Epochs().average() represents. Can anyone pls explain?

About ""mne.Epochs().average()"": if you have an ""epoch"" object and want to combine the data of all trials into one whole recording again (for example, after you performed certain pre-processing steps on the single trials or removed some of them), then you can use the average function of the class. Depending on the method you're choosing, you can calculate the mean or median of all trials for each channel and obtain an object with the shape (n_channels, n_time).
Not quite sure about the best way to average the data across the trials, but with mne.epochs.average you should be able to do it with ease. (Personally, I always calculated the mean for all my trials for each channel. But I guess that depends on the problem you try to solve)",0.0,0.36194062,0.13100101053714752
34,"Question\ni have a 3d point clouds of my object by using Open3d reconstruction system ( makes point clouds by a sequence of RGBD frames) also I created a 3d bounding box on the object in point clouds
my question is how can I have 2d bounding box on all of the RGB frames at the same coordinates of 3d bounding box?
my idea Is to project 3d bb to 2d bb but as it is clear, the position of the object is different in each frame, so I do not know how can i use this approach?
i appreciate any help or solution, thanks\nAnswer: calculate points for the eight corners of your box
transform those points from the world frame into your chosen camera frame
project the points, apply lens distortion if needed.

OpenCV has functions for some of these operations and supports you with matrix math for the rest.
I would guess that Open3d gives you pose matrices for all the cameras. you use those to transform from the world coordinate frame to any camera's frame.",0.0,0.17433149,0.030391467735171318
35,"Question\nProblem statement
I would like to achieve the following:
(could be used for example to organize some sort of a speeddating event for students)
Create a schedule so people talk to each other one-on-one and this to each member of the group.
but with restrictions.

Input: list of people. (eg. 30 people)
Restrictions: some of the people should not talk to each other (eg. they know each other)
Output: List of pairs (separated into sessions) just one solution is ok, no need to know all of the possible outcomes

Example
eg. Group of 4 people

John
Steve
Mark
Melissa

Restrictions: John - Mellisa -> NO
Outcome
Session one

John - Steve
Mark - Melissa

Session two

John - Mark
Steve - Melissa

Session three

Steve - Mark

John and Mellisa will not join session three as it is restriction.
Question
Is there a way to approach this using Python or even excel?
I am especially looking for some pointers how this problem is called as I assume this is some Should I look towards some solver? Dynamic programming etc?\nAnswer: Your given information is pretty generous, you have a set of all the students, and a set of no-go pairs (because you said it yourself, and it makes it easy to explain, just say this is a set of pairs of students who know each other). So we can iterate through our students list creating random pairings so long as they do not exist in our no-go set, then expand our no-go set with them, and recurse on the remaining students until we can not create any pairs that do not exist already in the no-go set (we have pairings so that every student has met all students).",0.0,0.28281444,0.07998400926589966
36,"Question\nI am relatively new to the python's subprocess and os modules. So, I was able to do the process execution like running bc, cat commands with python and putting the data in stdin and taking the result from stdout.
Now I want to first know that a process like cat accepts what flags through python code (If it is possible).
Then I want to execute a particular command with some flags set.
I googled it for both things and it seems that I got the solution for second one but with multiple ways. So, if anyone know how to do these things and do it in some standard kind of way, it would be much appreciated.\nAnswer: In the context of processes, those flags are called arguments, hence also the argument vector called argv. Their interpretation is 100% up to the program called. In other words, you have to read the manpages or other documentation for the programs you want to call.
There is one caveat though: If you don't invoke a program directly but via a shell, that shell is the actual process being started. It then also interprets wildcards. For example, if you run cat with the argument vector ['*'], it will output the content of the file named * if it exists or an error if it doesn't. If you run /bin/sh with ['-c', 'cat *'], the shell will first resolve * into all entries in the current directory and then pass these as separate arguments to cat.",0.0,0.2925592,0.0855908915400505
37,"Question\nI am just trying to calculate the percentage of one column against another's total, but I am unsure how to do this in Pandas so the calculation gets added into a new column.
Let's say, for argument's sake, my data frame has two attributes:

Number of Green Marbles
Total Number of Marbles

Now, how would I calculate the percentage of the Number of Green Marbles out of the Total Number of Marbles in Pandas?
Obviously, I know that the calculation will be something like this:

(Number of Green Marbles / Total Number of Marbles) * 100

Thanks - any help is much appreciated!\nAnswer: df['percentage columns'] = (df['Number of Green Marbles']) / (df['Total Number of Marbles'] ) * 100",0.0,0.30753762,0.09457938373088837
38,"Question\nSo right now, I'm making a sudoku solver. You don't really need to know how it works, but one of the checks I take so the solver doesn't break is to check if the string passed (The sudoku board) is 81 characters (9x9 sudoku board). An example of the board would be: ""000000000000000000000000000384000000000000000000000000000000000000000000000000002""
this is a sudoku that I've wanted to try since it only has 4 numbers. but basically, when converting the number to a string, it removes all the '0's up until the '384'. Does anyone know how I can stop this from happening?\nAnswer: There is no way to prevent it from happening, because that is not what is happening. Integers cannot remember leading zeroes, and something that does not exist cannot be removed. The loss of zeroes does not happen at conversion of int to string, but at the point where you parse the character sequence into a number in the first place.
The solution: keep the input as string until you don't need the original formatting any more.",0.81632656,0.14569527,0.44974634051322937
39,"Question\nI'd like to modify the Extensions that I send in the client Hello packet with python.
I've had a read of most of the source code found on GitHub for urllib3 but I still don't know how it determines which TLS extensions to use.
I am aware that it will be quite low level and the creators of urllib3 may just import another package to do this for them. If this is the case, which package do they use?
If not, how is this determined?
Thanks in advance for any assistance.\nAnswer: The HTTPS support in urllib3 uses the ssl package which uses the openssl C-library. ssl does not provide any way to directly fiddle with the TLS extension except for setting the hostname in the TLS handshake (i.e. server_name extension aka SNI).",0.0,0.31034106,0.09631157666444778
40,"Question\nI'm using Pycharm on Windows 10.
Python version: 3.8.6
I've checked using the CMD if I have tkinter install python -m tkinter. It says I have version 8.6
Tried:

import tkinter.
I get ""No module named 'tkinter' ""

from tkinter import *.
I get ""Unresolved reference 'tkinter'""

Installed future package but that didn't seem to change the errors.


Any suggestions on how to fix this issue?
Thank you!\nAnswer: You can try ""pip install tkinter"" in cmd",-0.35714287,0.28097004,0.4071880877017975
41,"Question\nI'm using Pycharm on Windows 10.
Python version: 3.8.6
I've checked using the CMD if I have tkinter install python -m tkinter. It says I have version 8.6
Tried:

import tkinter.
I get ""No module named 'tkinter' ""

from tkinter import *.
I get ""Unresolved reference 'tkinter'""

Installed future package but that didn't seem to change the errors.


Any suggestions on how to fix this issue?
Thank you!\nAnswer: You just verify in the project settings, sometimes Pycharm doesn't use the same interpreter.",-0.35714287,0.19487906,0.3047282099723816
42,"Question\nI have a string like: string = ""[1, 2, 3]""
I need to convert it to a list like: [1, 2, 3]
I've tried using regular expression for this purpose, but to no avail\nAnswer: Try
[int(x) for x in arr.strip(""[]"").split("", "")], or if your numbers are floats you can do [float(x) for x in arr.strip(""[]"").split("", "")]",0.27210885,0.25045824,0.00046874902909621596
43,"Question\nI have a numpy ndarray train_data of length 200, where every row is another ndarray of length 10304.
However when I print np.shape(train_data), I get (200, 1), and when I print np.shape(train_data[0]) I get (1, ), and when I print np.shape(train_data[0][0]) I get (10304, ).
I am quite confused with this behavior as I supposed the first np.shape(train_data) should return (200, 10304).
Can someone explains to me why this is happening, and how could I get the array to be in shape of (200, 10304)?\nAnswer: I'm not sure why that's happening, try reshaping the array:
B = np.reshape(A, (-1, 2))",0.0,0.14098012,0.01987539604306221
44,"Question\nI have two python programs. Program 1 displays videos in a grid with multiple controls on it, and Program 2 performs manipulations to the images and sends it back depending on the control pressed in Program 1.
Each video in the grid is running in its own thread, and each video has a thread in Program 2 for sending results back.
I'm running this on the same machine though and I was unable to get multiple socket connections working to and from the same address (localhost). If there's a way of doing that - please stop reading and tell me how!
I currently have one socket sitting independent of all of my video threads in Program 1, and in Program 2 I have multiple threads sending data to the one socket in an array with a flag for which video the data is for. The problem is when I have multiple threads sending data at the same time it seems to scramble things and stop working. Any tips on how I can achieve this?\nAnswer: Regarding If there's a way of doing that - please stop reading and tell me how!.
There's a way of doing it, assuming you are on Linux or using WSL on Windows, you could use the hostname -I commend which will output an IP that looks like 192.168.X.X.
You can use that IP in your python program by binding your server to that IP instead of localhost or 127.0.0.1.",0.0,0.13145292,0.017279868945479393
45,"Question\nGood day everybody.  I'm still learning parsing data with Python.  I'm now trying to familiarize myself with Chrome Developer Tools.  My question is when inspecting a directory website like TruePeopleSearch.com, how do I copy or view the variables that holds the data such as Name, Phone, and Address?  I tried browsing the tool, but since I'm new with the Developer tool, I'm so lost with all the data.  I would appreciate if the experts here points me to the right direction.
Thank you all!\nAnswer: Upon further navigating the Developer Console, I learned that these strings are located in these variables, by copying the JS paths.
NAME & AGE
document.querySelector(""#personDetails > div:nth-child(1)"").innerText
ADDRESS
document.querySelector(""#personDetails > div:nth-child(4)"").innerText
PHONE NUMBERS
document.querySelector(""#personDetails > div:nth-child(6)"").innerText
STEP 1
From the website, highlight are that you need to inspect and click ""Inspect Element""
STEP 2
Under elements, right-click the highlighted part and copy the JS path
STEP 3
Navigate to console and paste the JS path and add.innerText and press Enter",0.0,0.32930905,0.108444444835186
46,"Question\nI am new to Deep Learning. I finished training a model that took 8 hours to run, but I forgot to plot the accuracy graph before closing the jupyter notebook.
I need to plot the graph, and I did save the model to my hard-disk. But how do I plot the accuracy graph of a pre-trained model? I searched online for solutions and came up empty.
Any help would be appreciated! Thanks!\nAnswer: What kind of framework did you use and which version? In the future problem, you may face, this information can play a key role in the way we can help you.
Unfortunately, for Pytorch/Tensorflow the model you saved is likely to be saved with only the weights of the neurons, not with its history. Once Jupyter Notebook is closed, the memory is cleaned (and with it, the data of your training history).
The only thing you can extract is the final loss/accuracy you had.
However, if you regularly saved a version of the model, you can load them and compute manually the accuracy/loss that you need. Next, you can use matplotlib to reconstruct the graph.
I understand this is probably not the answer you were looking for. However, if the hardware is yours, I would recommend you to restart training. 8h is not that much to train a model in deep learning.",0.0,0.21809244,0.0475643128156662
47,"Question\nI appear to be missing some fundamental Python concept that is so simple that no one ever talks about it.  I apologize in advance for likely using improper description - I probably don't know enough to ask the question correctly.
Here is a conceptual dead end I have arrived at:
I have an instance of Class Net, which handles communicating with some things over the internet.
I have an instance of Class Process, which does a bunch of processing and data management
I have an instance of Class Gui, which handles the GUI.
The Gui instance needs access to Net and Process instances, as the callbacks from its widgets call those methods, among other things.
The Net and Process instances need access to some of the Gui instances' methods, as they need to occasionally display stuff (what it's doing, results of queries, etc)
How do I manage it so these things talk to each other?  Inheritance doesn't work - I need the instance, not the class.  Besides, inheritance is one way, not two way.
I can obviously instantiate the Gui, and then pass it (as an object) to the others when they are instantiated.  But the Gui then won't know about the Process and Net instances.  I can of course then manually pass the Net and Process instances to the Gui instance after creation, but that seems like a hack, not like proper practice.  Also the number of interdependencies I have to manually pass along grows rather quickly (almost factorially?) with the number of objects involved - so I expect this is not the correct strategy.
I arrived at this dead end after trying the same thing with normal functions, where I am more comfortable.  Due to their size, the similarly grouped functions lived in separate modules, again Net, Gui, and Process.  The problem was exactly the same.  A 'parent' module imports 'child' modules, and can then can call their methods.  But how do the child modules call the parent module's methods, and how do they call each other's methods?  Having everything import everything seems fraught with peril, and again seems to explode as more objects are involved.
So what am I missing in organizing my code that I run into this problem where apparently all other python users do not?\nAnswer: The answer to this is insanely simple.
Anything that needs to be globally available to other modules can be stored its own module, global_param for",0.40816328,0.22176477,0.03474440425634384
48,"Question\nIf I have a single GPU with 8GB RAM and I have a TensorFlow model (excluding training/validation data) that is 10GB, can TensorFlow train the model?
If yes, how does TensorFlow do this?
Notes:

I'm not looking for distributed GPU training.  I want to know about single GPU case.
I'm not concerned about the training/validation data sizes.\nAnswer: No you can not train a model larger than your GPU's memory. (there may be some ways with dropout that I am not aware of but in general it is not advised). Further you would need more memory than even all the parameters you are keeping because your GPU needs to retain the parameters along with the derivatives for each step to do back-prop.
Not to mention the smaller batch size this would require as there is less space left for the dataset.",0.0,0.28987247,0.0840260460972786
49,Question\nthis is kind of a dumb question but how would I make a discord.py event to automatically react to a message with a bunch of different default discord emojis at once. I am new to discord.py\nAnswer: You have to use on_message event. Its a default d.py function. It is an automatic function.,0.0,0.44541344,0.19839313626289368
50,"Question\nI am creating a multiplayer game and I would like the communication between my server program (written in python) and the clients (written in c# - Unity) to happen via UDP sockets.
I recently came across the concept of UDP Multicast, and it sounds like it could be much better for my use case as opposed to using UDP Unicast, because my server needs to update all of the clients (players) with the same content every interval. So, rather than sending multiple identical packets to all the clients with UDP unicast, I would like to be able to only send one packet to all the clients using multicast, which sounds much more efficient.
I am new to multicasting and my questions are:
How can I get my server to multicast to clients across the internet?
Do I need my server to have a special public multicast IP address? If so how do I get one?
Is it even possible to multicast across the internet? or is multicasting available only within my LAN?
And what are the pros and cons with taking the multicast approach?
Thank you all for your help in advance!!\nAnswer: You can't multicast on the Internet. Full stop.
Basically, multicast is only designed to work when there's someone in charge of the whole network to set it up. As you noted, that person needs to assign the multicast IP addresses, for example.",0.40816328,0.08886111,0.10195387899875641
51,"Question\nProblem statement:
I have a python 3.8.5 script running on Windows 10 that processes large files from multiple locations on a network drive and creates.png files containing graphs of the analyzed results.  The graphs are all stored in a single destination folder on the same network drive.  It looks something like this
Source files:
\\drive\src1\src1.txt
\\drive\src2\src2.txt
\\drive\src3\src3.txt
Output folder:
\\drive\dest\out1.png
\\drive\dest\out2.png
\\drive\dest\out3.png
Occasionally we need to replot the original source file and examine a portion of the data trace in detail.  This involves hunting for the source file in the right folder.  The source file names are longish alphanumerical strings so this process is tedious.  In order to make it less tedious I would like to creaty symlinks to the orignal source files and save them side by side with the.png files. The output folder would then look like this
Output files:
\\drive\dest\out1.png
\\drive\dest\out1_src.txt
\\drive\dest\out2.png
\\drive\dest\out2_src.txt
\\drive\dest\out3.png
\\drive\dest\out3_src.txt
where \\drive\dest\out1_src.txt is a symlink to \\drive\src1\src1.txt, etc.
I am attempting to accomplish this via
os.symlink('//drive/dest/out1_src.txt', '//drive/src1/src1.txt')
However no matter what I try I get

PermissionError: [WinError 5] Access is denied

I have tried running the script from an elevated shell, enabling Developer Mode, and running
fsutil behavior set SymlinkEvaluation R2R:1
fsutil behavior set SymlinkEvaluation R2L:1
but nothing seems to work.  There is absolutely no problem creating the symlinks on a local drive, e.g.,
os.symlink('C:/dest/out1_src.txt', '//drive/src1/src1.txt')
but that does not",0.0,-0.32733232,0.10714644938707352
52,"Question\nI'm trying to install PyAudio but it needs a Python 3.6 installation and I only have Python 3.9 installed. I tried to switch using brew and pyenv but it doesn't work.
Does anyone know how to solve this problem?\nAnswer: You may install multiple versions of the same major python 3.x version, as long as the minor version is different in this case x here refers to the minor version, and you could delete the no longer needed version at anytime since they are kept separate from each other.
so go ahead and install python 3.6 since it's a different minor from 3.9, and you could then delete 3.9 if you would like to since it would be used over 3.6 by the system, unless you are going to specify the version you wanna run.",0.40816328,0.20617163,0.04080062732100487
53,"Question\nI am making a small program in which I need a few functions to check for something in the background.
I used module threading and all those functions indeed run simultaneously and everything works perfectly until I start adding more functions. As the threading module makes new threads, they all stay within the same process, so when I add more, they start slowing each other down.
The problem is not with the CPU as it's utilization never reaches 100% (i5-4460). I also tried the multiprocessing module which creates a new process for function, but then it seems that variables can't be shared between different processes or I don't know how. (newly started process for each function seems to take existing variables with itself, but my main program cannot access any of the changes that function in the separate process makes or even new variables it creates)
I tried using the global keyword but it seems to have no effect in multiprocessing as it does in threading.
How could I solve this problem?
I am pretty sure that I have to create new processes for those background functions but I need to get some feedback from them and that part I don't know to solve.\nAnswer: I ended up using multiprocessing Value",0.0,-0.14254701,0.020319649949669838
54,"Question\nI'm running CentOS 8 that came with native Python 3.6.8. I needed Python 3.7 so I installed Python 3.7.0 from sources. Now, python command is unknown to the system, while commands python3 and python3.7 both use Python 3.7.
All good until now, but I can't seem to get pip working.
Command pip returns command not found, while python3 -m pip, python3.7 -m pip, python3 -m pip3, and python3.7 -m pip3 return No module named pip. Only pip command that works is pip3.
Now whatever package I install via pip3 does not seem to install properly. Example given, pip3 install tornado returns Requirement already satisfied, but when I try to import tornado in Python 3.7 I get ModuleNotFoundError: No module named 'tornado'. Not the same thing can be said when I try to import it in Python 3.6, which works flawlessly. From this, I understand that my pip only works with Python 3.6, and not with 3.7.
Please tell me how can I use pip with Python 3.7, thank you.\nAnswer: I think the packages you install will be installed for the previous version of Python. I think you should update the native OS Python like this:

Install the python3.7 package using apt-get
sudo apt-get install python 3.7
Add python3.6 & python3.7 to update-alternatives:
sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 1
sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 2
Update python3 to point to Python 3.7:
`sudo update-alternatives --config python3
Test the version:
python3 -V",0.0,0.007040262,4.956529301125556e-05
55,"Question\nI'm running CentOS 8 that came with native Python 3.6.8. I needed Python 3.7 so I installed Python 3.7.0 from sources. Now, python command is unknown to the system, while commands python3 and python3.7 both use Python 3.7.
All good until now, but I can't seem to get pip working.
Command pip returns command not found, while python3 -m pip, python3.7 -m pip, python3 -m pip3, and python3.7 -m pip3 return No module named pip. Only pip command that works is pip3.
Now whatever package I install via pip3 does not seem to install properly. Example given, pip3 install tornado returns Requirement already satisfied, but when I try to import tornado in Python 3.7 I get ModuleNotFoundError: No module named 'tornado'. Not the same thing can be said when I try to import it in Python 3.6, which works flawlessly. From this, I understand that my pip only works with Python 3.6, and not with 3.7.
Please tell me how can I use pip with Python 3.7, thank you.\nAnswer: It looks like your python3.7 does not have pip.
Install pip for your specific python by running python3.7 -m easy_install pip.
Then, install packages by python3.7 -m pip install <package_name>
Another option is to create a virtual environment from your python3.7. The venv brings pip into it by default.
You create venv by python3.7 -m venv <venv_name>",0.20408164,0.5167443,0.09775794297456741
56,"Question\nI try to communicate with Cylon device (UC32) by Bacnet protocol (BAC0) but I can not discover any device. And I try with Yabe and it does not have any result.
Is there any document describing how to create my communication driver?
Or any technique which can be uswed to connect with this device?\nAnswer: (Assuming you've set the default gateway address - for it to know where to return it's responses, but only if necessary.)
If we start with the assumption that maybe the device is not (by default) listening for broadcasts or having some issue sending it - a bug maybe (although probably unlikely), then you could send a unicast/directed message, e.g. use the Read-Property service to read back the (already known) BOIN (BACnet Object Instance Number), but you would need a (BACnet) client (application/software) that provides that option, like possibly one of the 'BACnet stack' cmd-line tools or maybe via the (for most part) awesome (but advanced-level) 'VTS (Visual Test Shell)' tool.
As much as it might be possible to discover what the device's BOIN (BACnet Object Instance Number) is, it's better if you know it already (- as a small few device's might not make it easy to discover - i.e. you might have to resort to using a round-robin bruteforce approach, firing lots of requests - one after the other with only the BOIN changed/incremented by 1, until you receive/see a successful response).",0.40816328,0.36811632,0.001603759010322392
57,"Question\nI'm running a few programs (NodeJS and python) in my server (Ubuntu 20.04). I use PM2 CLI to create and manage processes. Now I want to manage all process through an echo system file. But when I run pm2 ecosystem, it just creates a sample file. I want to save my CURRENT PROCESSES to the echo system file and modify it. Anyone know how to save pm2 current process as echo system file?\nAnswer: If you use pm2 startup pm2 creates a file named ~/.pm2/dump.pm2 with all running processes (with too many parameters, as it saves the whole environment in the file)
Edit:
This file is similar to the output of the command pm2 prettylist",0.40816328,0.4300254,0.00047795227146707475
58,"Question\nSo I was trying to host a simple python script on Heroku.com, but encountered this error. After a little googling, I found this on the Heroku's website: git, Heroku: pre-receive hook declined, Make sure you are pushing a repo that contains a proper supported app ( Rails, Django etc.) and you are not just pushing some random repo to test it out.
Problem is I have no idea how these work, and few tutorials I looked up were for more detailed use of those frameworks. What I need to know is how can i use them with a simple 1 file python script. Thanks in advance.\nAnswer: Okay I got it. It was about some unused modules in requirements.txt, I'm an idiot for not reading the output properly ‍♂️",0.0,0.28220654,0.07964053004980087
59,"Question\nHow do I display the user's Name + Discord Tag? As in:
I know that;
f""Hello, <@{ctx.author.id}>""
will return the user, and being pinged.
(@user)
And that;
f""Hello, {ctx.author.name}""
will return the user's nickname, but WITHOUT the #XXXX after it.
(user)
But how do I get it to display the user's full name and tag?
(user#XXXX)\nAnswer: To get user#XXXX you can just do str(ctx.author) (or just put it in your f-string and it will automatically be converted to a string). You can also do ctx.author.discriminator to get their tag (XXXX).",0.20408164,0.33567542,0.017316922545433044
60,"Question\nI have made a Scrapy web crawler which can scrape Amazon. It can scrape by searching for items using a list of keywords and scrape the data from the resulting pages.
However, I would like to scrape Amazon for large portion of its product data. I don't have a preferred list of keywords with which to query for items. Rather, I'd like to scrape the website evenly and collect X number of items which is representative of all products listed on Amazon.
Does anyone know how scrape a website in this fashion? Thanks.\nAnswer: I'm putting my comment as an answer so that others looking for a similar solution can find it easier.
One way to achieve this is to going through each category (furniture, clothes, technology, automotive, etc.) and collecting a set number of items there. Amazon has side/top bars with navigation links to different categories, so you can let it run through there.
The process would be as follows:

Follow category urls from initial Amazon.com parse
Use a different parse function for the callback, one that will scrape however many items from that category
Ensure that data is writing to a file (it will probably be a lot of data)

However, such an approach would not be representative in the proportions of each category in the total Amazon products. Try looking for a ""X number of results"" label for each category to compensate for that. Good luck with your project!",0.40816328,0.29769623,0.012202968820929527
61,"Question\nThe error messages printed by pip in my Windows PowerShell are dark red on dark blue (default PowerShell background). This is quite hard to read and I'd like to change this, but I couldn't find any hint to how to do this. Even not, if this is a default in Python applied to all stderr-like output, or if it's specific to pip.
My configuration: Windows 10, Python 3.9.0, pip 20.2.3.
Thanks for your help!\nAnswer: Coloring in pip is done via ANSI escape sequences. So the solution to this problem would be, to either change the way, PowerShell displays ANSI colors or the color scheme pip uses. Pip provides though a command-line switch '--no-color' which can be used to deactivate coloring the output.",0.0,0.15670162,0.024555400013923645
62,"Question\nI will create python api using Django
now I trying to verify phone number using firebase authentication  end send SMS to user but I don't know how I will do\nAnswer: The phone number authentication in Firebase is only available from it's client-side SDKs, so the code that runs directly in your iOS, Android or Web app. It is not possible to trigger sending of the SMS message from the server.
So you can either find another service to send SMS messages, or to put the call to send the SMS message into the client-side code and then trigger that after it calls your Django API.",0.40816328,0.1005913,0.09460052102804184
63,"Question\nI'm fully aware of the previous post regarding this error. That issue was with scikit-learn < 0.20. But I'm having scikit-learn 0.23.2 and I've tried uninstall and reinstall 0.22 and 0.23 and I still have this error.
Followup: Although pip list told me the scikit-learn version is 0.23.2, but when I ran sklearn.__version__, the real version is 0.18.1. Why and how to resolve this inconsistency? (Uninstall 0.23.2 didn't work)\nAnswer: [RESOLVED]
It turned out that my Conda environment has different sys.path as my jupyter environment. The jupyter environment used the system env, which is due to the fact that I installed the ipykernel like this: python -m ipykernel install without use --user flag. The correct way should be to do so within the Conda env and run pip install jupyter",0.0,0.098213136,0.009645819664001465
64,"Question\nI have a list which has 8 elements and all of those elements are arrays whose shape are (3,480,364).Now I want to transform this list to array as (8,3,480,364).When I use the array=nd.array(list) this command,it will takes me a lot of time and sometimes it will send 'out of memory' error.When I try to use this command array=np.stack(list.aixs=0),when I debug the code,it will stay at this step and can't run out the result.So I wonder how can I transform a list to array quickly when I use the Mxnet framework?\nAnswer: Your method of transforming a list of lists into an array is correct, but an 'out of memory' error means you are running out of memory, which would also explain the slowdown.
How to check how much RAM you have left:
on Linux, you can run free -mh in the terminal.
How to check how much memory a variable takes:
The function sys.getsizeof tells you memory size in bytes.
You haven't said what data type your arrays have, but, say, if they're float64, that's 8 bytes per element, so your array of 8 * 3 * 480 * 364 = 4193280 elements should only take up 4193280 * 8 bytes = about 30 Mb. So, unless you have very little RAM left, you probably shouldn't be running out of memory.
So, I'd first check your assumptions: does your list really only have 8 elements, do all the elements have the same shape of (3, 480, 364), what is the data type, do you create this array once or a thousand times? You can also check the size of a list element: sys.getsizeof(list[0]).
Most likely this will clear it up, but what if your array is really just too big for your RAM?
What to do if an array doesn't fit in memory
One solution is to use smaller data type (dtype=np.float32 for floating point, np.int32 or even np.uint8 for small integer numbers). This will sacrifice some precision for floating point calculations.
If almost all elements in the array are zero, consider using a sparse matrix.
For training a neural net, you can use a batch training algorithm and only load data into memory in small batches.",0.0,0.28319287,0.08019820600748062
65,"Question\nI made a cool little project for my friend, basically a timer using tkinter, but I am confused on how to let them access this project without having vscode or pycharm. Is it possible for them to just see the Tkinter window or something like that?  Is there an application for this? Sorry if this is a stupid question.\nAnswer: You can just built an.exe (Application) of your project. Then just share the application file and anyone can use the application through.exe. You can use pyinstaller to convert your python code to exe.
pip install pyinstaller
then cd to the project folder then run the following command
pyinstaller --onefile YourFileName.py
if you want to make exe without console showing up then use this command
pyinstaller --onefile YourFileName.py --noconsole",0.81632656,-0.09080207,0.822882354259491
66,"Question\nI have 2 python files that do Web scraping using Selenium and Beautifulsoup and store the results in separate CSV files say file1.csv and file2.csv. Now, I want to deploy these files on the Azure cloud, I know Azure function apps will be ideal for this. But, I don't know how Functions app will support Selenium driver on it.
Basically, I want to time trigger my 2 web scraping files and store the results in two separate files file1.csv and file2.csv that will be stored in blob storage on Azure cloud. Can someone help me with this task?
How can I use the selenium driver on Azure functions app?\nAnswer: Deploying on virtual machines or EC2 is the only option that one can use to achieve this task.
Also, with Heroku, we will be able to run selenium on the cloud by adding buildpacks. But when it comes to storing the files, we will not be able to store files on heroku as heroku does not persist the files. So, VMs or EC2 instances are the only options for this task.",0.81632656,0.17281199,0.41411101818084717
67,"Question\nI need to calculate EMA for a set of data from csv file where dates are in descending order.
When I apply pandas.DataFrame.ewm I get EMA for the latest (by date) equal to the value. This is because ewm starts observation from top to bottom in DataFrame.
So far, I could not find option to make it reverse for ewm. So I guess, I will have to reverse all my dataset.
Maybe somebody knows how to make ewm start from bottom values?
Or is it recommended to always use datetimeindex sorted chronologically? From oldest values on top to newest on the bottom?\nAnswer: From pandas' documentation:

Times corresponding to the observations. Must be monotonically increasing and datetime64[ns] dtype.

I guess, datetimeindex must be chronological..",0.0,0.20794162,0.043239716440439224
68,"Question\nI have a data science project in Python and I wonder how to manage my data. Some details about my situation:

My data consists of a somewhat larger number of football matches, currently around 300000, and it is supposed to grow further as time goes on. Attached to each match are a few tables with different numbers of rows/columns (but similar column formats across different matches).
Now obviously I need to iterate through this set of matches to do some computations. So while I don’t think that I can hold the whole database in memory, I guess it would make sense to have at least chunks in memory, do computations on that chunk, and release it.
At the moment I have split everything up into single matches, gave each match an ID and created a folder for each match with the ID as folder name. Then I put the corresponding tables as small individual csv files into the folder that belongs to a given match. Additionally, I have an „overview“ DataFrame with some „metadata“ columns, one row per match. I put this row as a small json into each match folder for convenience as well.
I guess there would also be other ways to split the whole data set into chunks than match-wise, but for prototyping/testing my code with a small number of matches, it actually turned out to be quite handy to be able to go to a specific match folder in a file manager and look at one of these tables in a spreadsheet program (although similar inspections could obviously also be made from code in appropriate settings). But now I am at the point where this huge number of quite small files/folders slows down the OS so much that I need to do something else.
Just to be able to deal with the data at all right now, I simply created an additional layer of folder hierarchy like „range-0“ contains folders 0-9999, „range-1“ contains 10000-19999 etc. But I‘m not sure if this is the way to go forward.
Maybe I could simply save one chunk - whatever one chunk is - as a json file, but would lose some of the ease of the manual inspection.
At least everything is small enough, so that I can do my statistical analyses on a single machine, such that I think I can avoid map/reduce-type algorithms.
On another note, I have close to zero knowledge about database frameworks (I have",0.40816328,0.271274,0.018738673999905586
69,"Question\nI am using spyder & want to install finplot. However when I did this I could not open spyder and had to uninstall & reinstall anconda.
The problem is to do with PyQt5 as I understand. The developer of finplot said that one solution would be to install PyQt5 version 5.9.

Error: spyder 4.1.3 has requirement pyqt5<5.13; python_version >= ""3"", but you'll have pyqt5 5.13.0 which is incompatible

My question is how would I do this? To install finplot I used the line below,

pip install finplot

Is there a way to specify that it should only install PyQt5?\nAnswer: As far as I understand you just want to install PyQT5 version 9.0.You can try this below if you got pip installed on your machine

pip install PyQt5==5.9

Edit: First you need to uninstall your pyQT5 5.13

pip uninstall PyQt5",0.81632656,0.47862256,0.11404399573802948
70,"Question\nI have a notebook that %run another notebook under JupyterLab. They can call back and forth each other functions and share some global variables.
I now want to convert the notebooks to py files so it can be executed from the command line.
I follow the advice found on SO and imported the 2nd file into the main one.
However, I found out that they can not call each other functions. This is a major problem because the 2nd file is a service to the main one, but it uses continuously functions that are part of the main one.
Essentially, the second program is non-GUI and it is driven by the main one which is a GUI program. Thus whenever the service program needs to print, it checks to see if a flag is set that tells it that it runs in a GUI mode, and then instead of simple printing it calls a function in the main one which knows how to display it on the GUI screen. I want to keep this separation.
How can I achieve it without too much change to the service program?\nAnswer: I ended up collecting all the GUI functions from the main GUI program, and putting them into a 3rd file in a class, including the relevant variables.
In the GUI program, just before calling the non GUI program (the service one) I created the class and set all the variables, and in the call I passed the class.
Then in the service program I call the functions that are in the class and got the variables needed from the class as well.
The changes to the service program were minor - just reading the variables from the class and change the calls to the GUI function to call the class functions instead.",0.0,0.23420477,0.054851874709129333
71,"Question\nI trained a model using Tensorflow object detection API using Faster-RCNN with Resnet architecture. I am using tensorflow 1.13.1, cudnn 7.6.5, protobuf 3.11.4, python 3.7.7, numpy 1.18.1 and I cannot upgrade the versions at the moment. I need to evaluate the accuracy (AP/mAP) of the trained model with the validation set for the IOU=0.3. I am using legacy/eval.py script on purpose since it calculates AP/mAP for IOU=0.5 only (instead of mAP:0.5:0.95)
python legacy/eval.py --logtostderr --pipeline_config_path=training/faster_rcnn_resnet152_coco.config --checkpoint_dir=training/ --eval_dir=eval/
I tried several things including updating pipeline config file to have min_score_threshold=0.3:
eval_config: {
num_examples: 60
min_score_threshold: 0.3
..
Updated the default value in the protos/eval.proto file and recompiled the proto file to generate new version of eval_pb2.py
// Minimum score threshold for a detected object box to be visualized
optional float min_score_threshold = 13 [default = 0.3];
However, eval.py still calculates/shows AP/mAP with IOU=0.5
The above configuration helped only to detect objects on the images with confidence level < 0.5 in the eval.py output images but this is not what i need.
Does anybody know how to evaluate the model with IOU=0.3?\nAnswer: I finally could solve it by modifing hardcoded matching_iou_threshold=0.5 argument value in multiple method arguments (especially def __init) in the../object_detection/utils/object_detection_evaluation.py",0.0,0.24187547,0.05850374326109886
72,"Question\nI just learned how to use the machine learning model Random Forest; however, although I read about the random_state parameter, I couldn't understand what it does. For example, what is the difference between random_state = 0   and   random_state = 300
Can someone please explain?\nAnswer: In addition, most people use the number 42 when we use random_state.
For example, random_state = 42 and there's a reason for that.
Below is the answer.
The number 42 is, in The Hitchhiker's Guide to the Galaxy by Douglas Adams, the ""Answer to the Ultimate Question of Life, the Universe, and Everything"", calculated by an enormous supercomputer named Deep Thought over a period of 7.5 million years. Unfortunately, no one knows what the question is",0.0,0.18069696,0.0326513946056366
73,"Question\nI just learned how to use the machine learning model Random Forest; however, although I read about the random_state parameter, I couldn't understand what it does. For example, what is the difference between random_state = 0   and   random_state = 300
Can someone please explain?\nAnswer: Random forests introduce stochasticity by randomly sampling data and features. Running RF on the exact same data may produce different outcomes for each run due to these random samplings. Fixing the seed to a constant i.e. 1 will eliminate that stochasticity and will produce the same results for each run.",0.0,0.25789636,0.06651053577661514
74,"Question\nI just learned how to use the machine learning model Random Forest; however, although I read about the random_state parameter, I couldn't understand what it does. For example, what is the difference between random_state = 0   and   random_state = 300
Can someone please explain?\nAnswer: train_test_split splits arrays or matrices into random train and test subsets. That means that everytime you run it without specifying random_state, you will get a different result, this is expected behavior.
When you use random_state=any_value then your code will show exactly same behaviour when you run your code.",0.0,0.28687763,0.08229877799749374
75,"Question\nI am trying to make a server info command and I want it to display the server name, boost count, boost members and some other stuff as well.
Only problem is I have looked at the docs and searched online and I cant find out how to find the boost information.
I dont have any code as Ive not found any code to try and use for myself
Is there any way to get this information?\nAnswer: Guild Name - guild_object.name
Boost count - guild_object.premium_subscription_count
Boosters, the people who boosted the server - guild_object.premium_subscribers
If your doing this in a command as I assume, use ctx.guild instead of guild_object. For anything further, you can re-read the docs as all of the above information is in it under the discord.Guild",0.81632656,0.17942077,0.40564897656440735
76,"Question\nI were given a list of intervals, for example [[10,40],[20,60]] and a list of position [5,15,30]
we should return the frequency of position appeared in the list, the answer would be [[5,0],[15,1],[30,2]] because 5 didn't cover by the interval and 15 was covered once, 30 was covered twice.
If I just do a for loop the time complexity would be O(m*n) m is the number of the interval, n is the number of position
Can I preprocess the intervals and make it faster? I was thinking of sort the interval first and use binary search but I am not sure how to implement it in python, Can someone give me a hint? Or can I use a hashtable to store intervals? what would be the time complexity for that?\nAnswer: You can use a frequency array to preprocess all interval data and then query for any value to get the answer. Specifically, create an array able to hold the min and max possible end-points of all the intervals. Then, for each interval, increment the frequency of the starting interval point and decrease the frequency of the value just after the end interval. At the end, accumulate this data for the array and we will have the frequency of occurrence of each value between the min and max of the interval. Each query is then just returning the frequency value from this array.

freq[] --> larger than max-min+1 (min: minimum start value, max: maximum end value)
For each [L,R] --> freq[L]++, freq[R+1] = freq[R+1]-1
freq[i] = freq[i]+freq[i-1]
For any query V, answer is freq[V]

Do consider tradeoffs when range is very large compared to simple queries, where simple check for all may suffice.",0.0,0.19276011,0.03715645894408226
77,"Question\nI try to convert this String to only the link: {""link"":""https://i.imgur.com/zfxsqlk.png""}
I'm trying to create a discord bot, which sends random pictures from the API https://some-random-api.ml/img/red_panda.
With imageURL = json.loads(requests.get(redpandaurl).content) I get the json String, but what do I have to do that I only get the Link like this https://i.imgur.com/zfxsqlk.png
Sorry if my question is confusingly written, I'm new to programming and don't really know how to describe this problem.\nAnswer: What you get from json.loads() is a Python dict. You can access values in the dict by specifying their keys.
In your case, there is only one key-value pair in the dict: ""link"" is the key and ""https://i.imgur.com/zfxsqlk.png"" is the value. You can get the link and store it in the value by appending [""link""] to your line of code:
imageURL = json.loads(requests.get(redpandaurl).content)[""link""]",0.0,0.34513572,0.11911866068840027
78,"Question\nI want a security profiler for python. Specifically, I want something that will take as input a python program and tell me if the program tries to make system calls, read files, or import libraries. If such a security profiler exists, where can I find it? If no such thing exists and I were to write one myself, where could I have my profiler 'checked' (that is, verified that it works).
If you don't find this question appropriate for SO, let me know if there is another SE site I can post this on, or if possible, how I can change/rephrase my question. Thanks\nAnswer: Usually, python uses an interpreter called CPython. It is hard to say for python code by itself if it opens files or does something special, due a lot of python libraries and interpreter itself are written in C, and system calls/libc calls can happen only from there. Also python syntax by itself can be very obscure.
So, by answering your suspect: I suspect this would need specific knowledge of the python programming language, it does not look like that, due it is about C language.
You can think it is possible to patch CPython itself. Well it is not correct too as I guess. A lot of shared libraries use C/C++ code as CPython itself. Tensorflow, for example.
Going further, I guess it is possible to do following things:

patch the compiler which compiles C/C++ code for CPython/modules, which is hard I guess.
just use an usual profiler, and trace which files, directories and calls are used by python itself for operation, and whitelist them, due they are needed, which is the best option by my opinion (AppArmor for example).
maybe you can be interested in the patching of CPython itself, where it is possible to hook needed functions and calls to external C libraries, but it can be annoying due you will have to revise every added library to your project, and also C code is often used for performance (e.g. json module), which doesn't open too much things.",0.6122449,0.27222216,0.11561546474695206
79,"Question\nI have an ModelAdmin with a set of fields in list_display.
I want the user to be able to click a checkbox in order to add or remove these fields.
Is there a straightforward way of doing this?  I've looked into Widgets but I'm not sure how they would change the list_display of a ModelAdmin\nAnswer: To do this I had to

Override an admin template (and change TEMPLATES in settings.py).  I added a form with checkboxes so user can set field
Add a new model and endpoint to update it (the model stores the fields to be displayed, the user submits a set of fields in the new admin template)
Update admin.py, overriding get_list_display so it sets fields based on the state of the model object updated",0.0,-0.12018967,0.014445556327700615
80,"Question\nhow to understand difference between a+=1 and a=+1 in Python?
it seems that they're different. when I debug them in Python IDLE both were having different output.\nAnswer: a+=1 is a += 1, where += is a single operator meaning the same as a = a + 1.
a=+1 is a = + 1, which assigns + 1 to the variable without using the original value of a",0.20408164,0.5610818,0.1274491250514984
81,"Question\nhow to understand difference between a+=1 and a=+1 in Python?
it seems that they're different. when I debug them in Python IDLE both were having different output.\nAnswer: It really depends on the type of object that a references.
For the case that a is another int:
The += is a single operator, an augmented assignment operator, that invokes a=a.__add__(1), for immutables. It is equivalent to a=a+1 and returns a new int object bound to the variable a.
The =+ is parsed as two operators using the normal order of operations:

+ is a unary operator working on its right-hand-side argument invoking the special function a.__pos__(), similar to how -a would negate a via the unary a.__neg__() operator.
= is the normal assignment operator

For mutables += invokes __iadd__() for an in-place addition that should return the mutated original object.",0.10204082,0.54469204,0.19594009220600128
82,"Question\nI would like to know please, how can I define variables in a python file and share these variables with their values with multiple python files?\nAnswer: To do this, you can create a new module specifically for storing all the global variables your application might need. For this you can create a function that will initialize any of these globals with a default value, you only need to call this function once from your main class, then you can import the globals file from any other class and use those globals as needed.",0.20408164,0.17580235,0.000799718196503818
83,"Question\nI would like to know please, how can I define variables in a python file and share these variables with their values with multiple python files?\nAnswer: You can create a python module
Create a py file inside that module define variables and import that module in the required places.",0.0,0.1162017,0.013502834364771843
84,"Question\nI am making a website. And I want to know how to connect React js to my Flask back end. I have tried searching online but unfortunately it was not what I am looking for. If you know how to do it please recomend me some resources. And I also want to know the logic of how Flask and React work together.\nAnswer: Flask is a backend micro-service and react is a front-end framework. Flask communicates with the database and makes the desired API hit points. The backend listens for any API request and sends the corresponding response as a JSON format. So using React you can make HTTP requests to the backend.
For testing purposes have the backend and frontend separated and communicate only using the REST APIs. For production, use the compiled js of React as static files and render only the index.html of the compiled react from the backend.
P.S: I personally recommend Django rest framework over flask if you are planning to do huge project.",0.81632656,0.3906095,0.18123501539230347
85,"Question\nI am trying to encrypt a bitstream data or basically a list of binary data like this [1,0,1,1,1,0,0,1,1,0,1,1,0,1] in python using AES encryption with block size of 128bit, the problem is that i want the output to be binary data as well and the same size as the original binary data list, is that possible?how do i do that?\nAnswer: Yes, there are basically two ways:

You have a unique value tied to the data (for instance if they are provided in sequence then you can create a sequence number) then you can simply use the unique value as nonce and then use AES encryption in counter mode. Counter mode doesn't expand the data but it is insecure if no nonce is supplied. Note that you do need the nonce when decrypting.

You use format preserving encryption or FPE such as FF1 and FF3 defined by NIST. There are a few problems with this approach:

there are issues with these algorithms if the amount of input data is minimal (as it seems to be in your case);
the implementations of FF1 and FF3 are generally hard to find;
if you have two unique bit values then they will result in identical ciphertext.



Neither of these schemes provide integrity or authenticity of the data obviously, and they by definition leak the size of the plaintext.",0.40816328,0.39138317,0.0002815720217768103
86,"Question\nAs I was working on a project the topic of code obfuscation came up, as such, would it be possible to encrypt python code using either RSA or AES and then de-code it on the other side and run it?. And if it's possible how would you do it?. I know that you can obfuscate code using Base64, or XOR, but using AES or RSA would be an interesting application. This is simply a generic question for anyone that may have an idea on how to do it. I am just looking to encrypt a piece of code from point A, send it to point B, have it decrypted at point B and run there locally using either AES or RSA. It can be sent by any means, as long as the code itself is encrypted and unreadable.\nAnswer: Yes this is very possible but would require some setup to work.
First off Base64 is an encoder for encoding data from binary/bytes to a restricted ascii/utf subset for transmission usually over http. Its not really an obfuscator, more like a packager for binary data.
So here is what is needed for this to work.

A pre-shared secret key that both point A and point B have. This key cannot be transmitted along with the code since anyone who gets the encrypted code would also get the key to decrypt it.

There would need to be an unencrypted code/program that allows you to insert that pre-shared key to use to decrypt the encrypted code that was sent. Can't hardcode the key into the decryptor since again anyone with the decryptor can now decrypt the code and also if the secrey key is leaked you would have to resend out the decryptor to use a different key.

Once its decrypted the ""decryptor"" could save that code to a file for you to run or run the code itself using console commands or if its a python program you can call eval or use importlib to import that code and call the function within.
WARNING: eval is known to be dangerous since it will execute whatever code it reads. If you use eval with code you dont trust it can download a virus or grab info from your computer or anything really. DO NOT RUN UNTRUSTED CODE.


Also there is a difference between AES and RSA. One is a symmetric cipher and the other is asymmetric. Both will work for what you want but they require different things for encryption and decryption",0.0,0.11695504,0.013678481802344322
87,"Question\nI am working on a project that needs to do the following:

[C++ Program] Checks a given directory, extracts all the names (full paths) of the found files and records them in a vector<string>.
[C++ Program] ""Send"" the vector to a Python script.
[Python Script] ""Receive"" the vector and transform it into a List.
[Python Script] Compares the elements of the List (the paths) against the records of a database and removes the matches from the List (removes the paths already registered).
[Python Script] ""Sends"" the processed List back to the C++ Program.
[C++ Program] ""Receives"" the List, transforms it into a vector and continues its operations with this processed data.

I would like to know how to send and receive data structures (or data) between a C ++ Script and a Python Script.
For this case I put the example of a vector transforming into a List, however I would like to know how to do it for any structure or data in general.
Obviously I am a beginner, that is why I would like your help on what documentation to read, what concepts should I start with, what technique should I use (maybe there is some implicit standard), what links I could review to learn how to communicate data between Scripts of the languages ​​I just mentioned.
Any help is useful to me.\nAnswer: If the idea is to execute the python script from the c++ process, then the easiest would be to design the python script to accept input_file and output_file as arguments and the c++ program should write the input_file, start the script and read the output_file.
For simple structures like list-of-strings, you can simply write them as text files and share, but for more complex types, you can use google-protocolbuffers to do the marshalling/unmarshalling.
if the idea is to send/receive data between two already stared process, then you can use the same protocol buffers to encode data and send/receive via sockets between each other. Check gRPC",0.0,0.43397146,0.1883312314748764
88,Question\nI am trying to use a LabJack product U3 using Python and I am using PyCharm for development of my code. I am new to both Python and PyCharm FYI.  In the LabJack documentation they say to run python setup.py install in the directory I down loaded there Python links for using there device. I did this and when run under straight Python console can get the import u3 to run and am able to access the U3 device.  Yet when I run this in PyCharm I can not get it to run.  It always tells me module not found.  I have asked LabJack for help but they do not know PyCharm.  I have looked on the net but I can seem to see how to get the module properly under PyCharm.  Could i please get some help on how to do this properly?\nAnswer: First Yll download that module inside of pycharm settings if it's still not working then import module in terminal of pycharm then try to run you're python script,0.0,0.13605618,0.01851128600537777
89,"Question\nI have a doubt about classification algorithm comparation.
I am doing a project regarding hyperparameter tuning and classification model comparation for a dataset.
The Goal is to find out the best fitted model with the best hyperparameters for my dataset.
For example: I have 2 classification models (SVM and Random Forest), my dataset has 1000 rows and 10 columns (9 columns are features) and 1 last column is lable.
First of all, I splitted dataset into 2 portions (80-10) for training (800 rows) and tesing (200rows) correspondingly. After that, I use Grid Search with CV = 10 to tune hyperparameter on training set with these 2 models (SVM and Random Forest). When hyperparameters are identified for each model, I use these hyperparameters of these 2 models to test Accuracy_score on training and testing set again in order to find out which model is the best one for my data (conditions: Accuracy_score on training set < Accuracy_score on testing set (not overfiting) and which Accuracy_score on testing set of model is higher, that model is the best model).
However, SVM shows the accuracy_score of training set is 100 and the accuracy_score of testing set is 83.56, this means SVM with tuning hyperparameters is overfitting. On the other hand, Random Forest shows the accuracy_score of training set is 72.36 and the accuracy_score of testing set is 81.23. It is clear that the accuracy_score of testing set of SVM is higher than the accuracy_score of testing set of Random Forest, but SVM is overfitting.
I have some question as below:
_ Is my method correst when I implement comparation of accuracy_score for training and testing set as above instead of using Cross-Validation? (if use Cross-Validation, how to do it?
_ It is clear that SVM above is overfitting but its accuracy_score of testing set is higher than accuracy_score of testing set of Random Forest, could I conclude that SVM is a best model in this case?
Thank you!\nAnswer: I would suggest splitting your data into three sets, rather than two:

Training
Validation
Testing

Training is used to train the model, as you have been doing. The validation set is used to evaluate the performance of a model trained with a",0.0,0.25073487,0.06286796927452087
90,"Question\nI have an RGB image and I converted to Lab colorspace. Now, I want to convert the image in LAB space to grayscale one. I know L NOT = Luminance.
So, any idea how to get the equivalent gray value of a specific color in lab space?
I'm looking for a formula or algorithm to determine the equivalent gray value of a color given the LAB values.\nAnswer: The conversion from Luminance Y to Lightness L* is defined by the CIE 1976 Lightness Function. Put another way, L* transforms linear values into non-linear values that are perceptually uniform for the Human Visual System (HVS). With that in mind, your question is now dependent on what kind of gray you are looking for, if perceptually uniform and thus non-linear, the Lightness channel from CIE Lab* is actually that of CIE 1976 and is appropriate. If you need something linear, you  would have to convert back to CIE XYZ tristimulus values and use the Y channel.",0.40816328,0.21605337,0.03690622001886368
91,"Question\nI'm looking to create the below JSON file in python. I do not understand how I can have multiple dictionaries that are not separated by commas so when I use the JSON library to save the dictionary to disk, I get the below JSON;
{""text"": ""Terrible customer service."", ""labels"": [""negative""], ""meta"": {""wikiPageID"": 1}}
{""text"": ""Really great transaction."", ""labels"": [""positive""], ""meta"": {""wikiPageID"": 2}}
{""text"": ""Great price."", ""labels"": [""positive""], ""meta"": {""wikiPageID"": 3}}
instead of a list of dictionaries like below;
[{""text"": ""Terrible customer service."", ""labels"": [""negative""], ""meta"": {""wikiPageID"": 1}},
{""text"": ""Really great transaction."", ""labels"": [""positive""], ""meta"": {""wikiPageID"": 2}},
{""text"": ""Great price."", ""labels"": [""positive""], ""meta"": {""wikiPageID"": 3}}]
The difference is, in the first example, each line is a dictionary and they are not in a list or separated by commas.
Whereas in the second example, which is what I'm able to come up with is a list of dictionaries, each dictionary separated by a comma.
I'm sorry if this a stupid question I have been breaking my head over this for weeks, and have not been able to come up with a solution.
Any help is appreciated.
And thank you in advance.\nAnswer: The way you want to store the Data in one file isn't possible with JSON.
Each JSOn file can only contain one Object. This means that you can either have one Object defined within curly braces, or an Array of objects as you mentioned.
If you want to store each Object as a JSON object you should use separate files each containing a single Object.",0.0,0.3536352,0.12505784630775452
92,"Question\nI am a kivy n00b, using python, and am not sure if this is the right place to ask.
Can someone please explain how a user can input data in an Android app, and how/where it is stored (SQL table, csv, xml?). I am also confused as to how it can be extended/used for further analysis.
I think it should be held as a SQL table, but do not understand how to save/set up a SQL table in an android app, nor how to access it. Similarly, how to save/append/access a csv/xml document, nor how if these are made, how they are secure from accidental deletion, overwriting, etc
In essence, I want to save only the timestamp a user enters some data, and the corresponding values (max 4).
User input would consist of 4 variables, x1, x2, x3, x4, and I would write a SQL statement along the lines: insert into data.table timestamp, x1, x2, x3, x4, and then to access the data something along the lines of select * from data.table and then do/show stuff.
Can someone offer suggestions on what resources to read? How to set up a SQL Server table in an android app?\nAnswer: This works basically the same way on android as on the desktop: you have access to the local filesystem to create/edit files (at least within the app directory), so you can read and write whatever data storage format you like.
If you want to use a database, sqlite is the simplest and most obvious option.",0.40816328,0.15441233,0.06438954174518585
93,"Question\nEg i have a chat application,
however, i realised that  for my application, as long as you have the link to the chat, you can enter. how do I prevent that, and make it such that only members of the group chat can access the chat. Something like password protected the url to the chat, or perhaps something like whatsapp. Does anyone have any suggestion and reference material as to how I should build this and implement the function? Thank you!\nAnswer: I am in the exact same condition as you.What I am thinking of doing
is
Store group_url and the respective user_ids (which we get from django's authentication) in a table(with two columns group_url and allowed_user_ids) or in Redis.
Then when a client connects to a channel,say chat/1234 (where 1234 is the group_url),we get the id of that user using self.scope['user'].id and check them in the table.
If the user_id  is in the respected group_url,we accept the connection.Else reject the connection. I am new to this too.Suggest me if you find a better approach",0.40816328,-0.10864496,0.2670907974243164
94,"Question\nHow do I get the user/member object in discord.py with only the Name#Discriminator? I searched now for a few hours and didn't find anything. I know how to get the object using the id, but is there a way to convert Name#Discriminator to the id?
The user may not be in the Server.\nAnswer: There's no way to do it if you aren't sure they're in the server. If you are, you can search through the servers' members, but otherwise, it wouldn't make sense. Usernames/Discriminators change all the time, while IDs remain unique, so it would become a huge headache trying to implement that. Try doing what you want by ID, or searching the server.",0.0,0.3213603,0.1032724380493164
95,"Question\nI want the person who used the command to be able to delete the result. I have put the user's ID in the footer of the embed, and my question is: how do I get that data from the message where the user reacted to.
reaction.message.embed.footer doesn't work. I currently don't have code as I was trying to get that ID first.
Thanks in advance!\nAnswer: discord.Message object has no attribute embed, but it has embeds. It returns you a list of embeds that the message has. So you can simply do: reaction.message.embeds[0].footer.",0.40816328,0.3874032,0.00043098130845464766
96,"Question\nhow can I make a login form that will remember the user so that he does not have to log in next time.\nAnswer: Some more information would be nice but if you want to use a database for this then you would have to create a entry for the user information last entered.
And then on reopening the programm you would check if there are any entrys and if yes load it.
But I think that writing the login information to a file on you pc would be a lot easier. So you run the steps from above just writing to a file instead of a database.
I am not sure how you would make this secure because you can't really encrypt the password because you would need a password or key of some type and that password or key would be easy to find in the source code especially in python. It would be harder to find in other compiler based programming languages but also somewhere. And if you would use a database you would have a password for that but that would also lay on the hardrive if not encrypted otherwise but there we are where we started.
So as mentioned above a database would be quite useless for a task like this because it doesn't improve anything and is a hassle for beginners to setup.",0.0,0.16235918,0.02636050246655941
97,"Question\nI need your advice on something that I'm working on as a part of my work.
I'm working on automating the Aurora Dump to S3 bucket every midnight. As a part of it, I have created a ec2 instance that generates the dump and I have written a python script using boto3 which moves the dump to S3 bucket every night.
I need to intimate a list of developers if the data dump doesn't take place for some reason.
As of now, I'm posting a message to SNS topic which notifies the developers if the backup doesn't happen. But I need to do this with Cloudwatch and I'm not sure how to do it.
Your help will be much appreciated.! Thanks!\nAnswer: I have created a custom metric to which I have attached a Cloudwatch alarm and it gets triggered if there's an issue in data backup process.",0.0,0.08164656,0.006666161119937897
98,"Question\nI use Python Anaconda and Visual Studio Code for Data Science and Machine Learning projects.
I want to learn how to use Windows Subsystem for Linux, and I have seen that tools such as Conda or Git can be installed directly there, but I don't quite understand the difference between a common Python Anaconda installation and a Conda installation in WSL.
Is one better than the other? Or should I have both? How should I integrate WSL into my work with Anaconda, Git, and VS Code? What advantages does it have or what disadvantages?
Help please, I hate not installing my tools properly and then having a mess of folders, environment variables, etc.\nAnswer: If you use conda it's better to install it directly on Windows rather than in WSL. Think of WSL as a virtual machine in your current PC, but much faster than you think.
It's most useful use would be as an alternate base for docker. You can run a whole lot of stuff with Windows integration from WSL, which includes VS Code. You can lauch VS code as if it is run from within that OS, with all native extension and app support.
You can also access the entire Windows filesystem from WSL and vice versa, so integrating Git with it won't be a bad idea",0.0,0.025724709,0.0006617606268264353
99,"Question\nSo I am developing a Bot using discord.py and I want to get all permissions the Bot has in a specific Guild. I already have the Guild Object but I don't know how to get the Permissions the Bot has. I already looked through the documentation but couln't find anything in that direction...\nAnswer: From a Member object, like guild.me (a Member object similar to Bot.user, essentially a Member object representing your bot), you can get the permissions that member has from the guild_permissions attribute.",0.40816328,0.35722327,0.0025948842521756887
0,"Question\nI am building my website which contains a python(.py) file, html, css and JS file. I want to know that how can I run my python script in siteground from my hosting account so that it can scrape data from a site and output a JSON file to Javascript file which can display it on the webpage.\nAnswer: I would use cron jobs to run jobs in the foreground",0.0,-0.13309544,0.017714396119117737
1,"Question\nI'm studying python and there's a lab I can't seem to crack. We have a line e.g. shacnidw, that has to be transformed to sandwich. I somehow need to iterate with a for loop and pick the letters with odd indexes first, followed by backward even indexes. Like pick a letter with index 1,3,5,7,8,6,4,2.
It looks pretty obvious to use a list or slices, but we aren't allowed to use these functions yet. I guess the question is just how do I do it?\nAnswer: Programming is all about decomposing complex problems into simpler ones. Try breaking it down into smaller steps.

First, can you generate the numbers 1,3,5,7 in a for loop?
Next, can you generate 8,6,4,2 in a second loop?

Tackling those two steps ought to get you on the right track.",0.27210885,0.105547726,0.02774260938167572
2,"Question\nI have a collection of text documents. I've been asked to show each document in tf-idf vector space and in ntc form and then, train a svm model based on documents' vectors in python. What does ntc exactly mean here?
I Found that it's the same as tf-idf weights with one step of normalization which is called ""cosine normalization"". But i can't find information about such thing. I found ""cosine similarity"" which is in my idea different from ""cosine normalization"". Are they the same? And how can i create this vector in python?\nAnswer: I suggest the sklearn.feature_extraction.text.TfidfVectorizer,
scikit learn is a bib in python used for training machine learning model,
it is easy and very useful,",0.0,0.093621224,0.008764933794736862
3,"Question\nI am using the Selenium standalone server for a remote web driver. One thing I am trying to figure out is how to start/stop it effectively. On their documentation, it says
""the caller is expected to terminate each session properly, calling either Selenium#stop() or WebDriver#quit.""
What I am trying to do is figure out how to programmatically close the server, but is that even necessary? In other words, would it be okay to have the server up and running at all times, but to just close the session after each use with something like driver.quit()? Therefore when I'm not using it the server would be up but there would be no sessions.\nAnswer: you were right. Use seleniums driver.quit() as it properly closes all browser windows and ends driver's session/process. Especially the latter is what you want, because you most certainly run the script headless.
I have a selenium script running on as Raspberry Pi (hourly cron job) headless. That script calls driver.quit() at the end of each iteration. When i do a -ps A (to list al active processes under unix), no active selenium/python processes are shown anymore.
Hope that satisfies your question!",0.0,0.31701434,0.10049808770418167
4,"Question\nIn my code I need to ask the user to input items of a shopping list and then sort that list into descending order by price.
For example if the user where to enter

: Butter 1.70, Coffee 4.99, Milk 0.45,
Kitchen Towel 1.75, Washing powder 6.20

The output should be:

Washing powder 6.20, Coffee 4.99, Kitchen Towel
1.75, Butter 1.70, Milk 0.45

However I am completely stuck on how to actually do this. Any help is welcome, thanks in advance.\nAnswer: Here's some high level steps I'd start with, lmk if you need help with any particular step.

split up list with something like input.split(',').strip()

For each element, split by space item.split(' ')

Assume the number is the last 'word' of each element. Convert that word to a number.

Do the sort.",0.0,-0.01924491,0.0003703665279317647
5,"Question\nI recently did a project where I have to get the following input from the user:
Date in DD form, Month in MM form, Year in YY form, Century (19, 20 0r 21) and the date format(MM-DD-YYYY, DD-MM-YYYY, YYYY-MM-DD).
And I have to print the date in the format chosen by the user... Further I have to ask the user the number of days to be added to the date that was printed previously and then add the no. of days and then print it and if the user should be asked if he wants to continue adding days, if they choose yes.. again they'll be asked the no. of days to be added, if they select no then program ends there....
Now I have made a HTML files for that... The problem is I don't know how to combine the Python and HTML files... Please help me in this regard.\nAnswer: The keywords you should be looking are a web framework to host your application such as Flask, Django, and a template language to combine python and HTML to use it via these frameworks, such as Jinja2 or Django's own template language. I suggest Flask with Jinja2 since it's a micro framework and easy to start with.",0.20408164,0.04499656,0.025308063253760338
6,"Question\nI've tried to import a model on Kaggle:
from statsmodels.tsa.arima.model import ARIMA
but it returned this error:
AttributeError: module 'numpy.linalg.lapack_lite' has no attribute '_ilp64'
There's numpy version 1.18.5. Could you please tell how to fix this error?\nAnswer: I've just back up statmodels library to the version 0.11.0:
pip uninstall statsmodels -y
pip install statsmodels==0.11.0
It seems like default version 0.11.1 has a bug",0.40816328,-0.0154057145,0.17941069602966309
7,"Question\nI’m building a gui on python with tkinter, i’ve made a login, register and change password screens, hosting the data on mysql, i want to make an option that if user already choose password “x” for example, he will be able to repeat the password only 1 more time, after that it will not give him an option to choose and repeat the same password on the change password screen, any clue how to do it?\nAnswer: Create a table with (user, passwords_used) as columns. Each time a user changes their password, check it against this table. If that (user, password) pair isn't in the table, add it to the table and change the password. Otherwise reject it as reused.",0.0,0.18531466,0.03434152156114578
8,"Question\nIs there a way in Visual Studio Code to clear the previous code in the terminal every time I execute the code. It's very annoying to type clear every time I wanna run the code again (or to run another file).\nAnswer: To clear Terminal in VS Code simply press Ctrl + Shift + P key.
type command Terminal: Clear.
go to View in taskbar upper left corner of vs code and open Command palette.
I think Ctrl + K should do the trick too if you are in windows, or else you can make shortcuts for clearing the terminal using VS Code shortcuts(keybindings file).",0.40816328,0.18652415,0.04912390187382698
9,"Question\nI am using python 32-bits but I want to upgrade it to 64-bit to use some advanced modules.
But I don't want to lose my 32-bit projects, suggest help, please.\nAnswer: Pure Python code is neither 32 nor 64 bits, because Python is a very high level programming language. When you run a Python program, the Python interpreter quickly compiles your source code to machine code and executes it.
It doesn't matter whether you use a 32 bit or 64 bit Python interpreter to execute your pure Python program, because the result should be the same. However, if your program uses libraries that contain non-Python code, you might have to reinstall the 64 bit version of those libraries.
So to conclude, don't be afraid to download and install the 64 bit version of Python because your Python programs will all run perfectly on it!",0.40816328,0.32912135,0.00624762661755085
10,"Question\nI need to generate an executable from a python project containing multiple folders and files.
I tried to work with library cx_Freeze, but only worked for a single file project.
Can you tell me how to do please?\nAnswer: use pyinstaller. just run
pip install pyinstaller and then open the folder the file is located in with shell and run pyinstaller --onefile FILE.py where file is the name of the python file that should be run when the exe is run",0.0,0.26108706,0.06816644966602325
11,"Question\nI need to generate an executable from a python project containing multiple folders and files.
I tried to work with library cx_Freeze, but only worked for a single file project.
Can you tell me how to do please?\nAnswer: Running pyinstaller on your ""main"" python file should work, as PyInstaller automatically imports any dependencies (such as other python files) that you use.",0.13605443,0.15878445,0.0005166539340279996
12,"Question\nUsing tkinter we can use either widget.grid(row,col) or widget.pack() to place a widget.
Since row,col corresponds to the row/col-index in a given Frame/Window - how do we know how many columns the Frame/Window consists of? E.g if I want to place a widget in the midle or to the very right\nAnswer: Rows and columns are just concepts, not actual things.  There is effectively an infinite number of rows and columns, all with a width or height of zero until they contain a widget or are configured to have a minimum width or height. From a practical standpoint, there are as many rows and columns as there are pixels in the window.
In reality, the number of rows and columns is entirely up to you. A widget can have a single row and column or it can have several. It all depends on what you add to the window.
A frame starts out with nothing in it, so there are no rows and columns, just empty space. When you add a widget to a row and column, it now has one row and one column plus maybe some empty space. Even if you place your first widget at row 50 and column 20, there is still just one row (50) and one column (20).
There are simple techniques to put something in the middle, or along the right size. For example, because you can define how many columns, you can configure the master to use three columns and the place your widget in the middle column. You can use columnconfigure to cause the last column to take up all extra space with the weight option. This will move any widgets in the last column to the right edge.",0.81632656,0.3371383,0.22962139546871185
13,"Question\nDearpygui used to work just fine, till I upgraded Pycharm and Python from 3.6 to 3.9. Than this error occurred.
Went back to 3.6. Abandoned Pycharm. Removed Pycharm, Pip and Python. Installed Python 3.6, 3.7, 3.9. Started in  new environment. Upgraded pip. Tried all Python versions.... for all versions pip successfully installed Dearpygui to the site-packages...  but all failed to load DLL on Python-'import'-command.
I've seen others with similar problems with other packages, but found no useful suggestion...
It looks as if something is changed in the settings/environment of Window-10... (by Python?)
dearpygui-0.6.123,  pip 20.3.3,  python 3.7 (64-bit) a.o.,  windows 10 Pro 2014
(PS. although 'the specified module' can not be found upon the Python-'import'-command, PyCharm has somehow full access to the Dearpygui-class-information...)\nAnswer: What is the name of your file? Would it happen to be dearpygui.py?",-0.71428573,0.008138835,0.5218972563743591
14,"Question\nI have X lists of elements, each list containing a different number of elements (without repetitions inside a single list). I want to generate (if possible, 500) sequences of 3 elements, where each element belongs to a different list, and sequences do not repeat. So something like:
X (in this case 4) lists of elements: [A1,A2], [B1,B2,B3,B4], [C1], [D1,D2,D3,D4,D5]
possible results: [A1,B2,D2], [B3,C1,D2], [A1,B2,C1]... (here 500 sequences are impossible, so would be less)
I think I know how to do it with a nasty loop: join all the lists, random.sample(len(l),3) from the joint list, if 2 indices belong to the same list repeat, if not, check if the sequence was not found before. But that would be very slow. I am looking for a more pythonic or more mathematically clever way.
Perhaps a better way would be to use random.sample([A,B,C,D], 3, p=[len(A), len(B), len(C), len(D)]), then for each sequence from it randomly select an element from each group in the sequence, then check if a new sequence generated in this way hasn't been generated before. But again, a lot of looping.
Any better ideas?\nAnswer: Check itertools module (combination and permutation in particular).
You can get a random.choice() from the permutations of 3 elements from the X lists (thus selecting 3 lists), and for each of them get a random.choice() (random module).",0.0,0.11822063,0.01397611666470766
15,"Question\nSo I am using a venv (virtual environment) for one of my python projects and need to install PyAudio for it. I am using PyCharm for the venv.
When I try to install PyCharm usually it comes up with 'Command errored out with exit status: 1'
How can I install PyAudio into my venv properly (I know PyAudio needs to be installed differently I just don't know how to do it in the venv)\nAnswer: So I fixed my problem. I found the PyAudio file in my normal python (Python 3.9 64-bit) and transferred it to my venv (Python 3.8 64-bit) and now it works fine!",0.0,0.13756114,0.018923068419098854
16,"Question\nI have to generate a matrix (propagator in physics) by ordered multiplication of many other matrices. Each matrix is about the size of (30,30), all real entries (floats), but not symmetric. The number of matrices to multiply varies between 1e3 to 1e5. Each matrix is only slightly different from previous, however they are not commutative (and at the end I need the product of all these non-commutative multiplication). Each matrix is for certain time slice, so I know how to generate each of them independently, wherever they are in the multiplication sequence. At the end, I have to produce many such matrix propagators, so any performance enhancement is welcomed.
What is the algorithm for fastest implementation of such matrix multiplication in python?
In particular -

How to structure it? Are there fast axes and so on? preferable dimensions for rows / columns of the matrix?
Assuming memory is not a problem, to allocate and build all matrices before multiplication, or to generate each per time step? To store each matrix in dedicated variable before multiplication, or to generate when needed and directly multiply?
Cumulative effects of function call overheads effects when generating matrices?
As I know how to build each, should it be parallelized? For example maybe to create batch sequences from start of the sequence and from the end, multiply them in parallel and at the end multiply the results in proper order?
Is it preferable to use other module than numpy? Numba can be useful? or some other efficient way to compile in place to C, or use of optimized external libraries? (please give reference if so, I don't have experience in that)

Thanks in advance.\nAnswer: I don't think that the matrix multiplication would take much time. So, I would do it in a single loop. The assembling is probably the costly part here.
If you have bigger matrices, a map-reduce approach could be helpful. (split the set of matrices, apply matrix multiplication to each set and do the same for the resulting matrices)
Numpy is perfectly fine for problems like this as it is pretty optimized. (and is partly in C)
Just test how much time the matrix multiplication takes and how much the assembling. The result should indicate where you need to optimize.",0.40816328,0.16278839,0.06020883470773697
17,"Question\nI am making a bot that is using cogs and has a couple different commands. I want this bot to only reply in one of the two bot commands channels of the server I’m using it on. I have seen that i can use ctx.channel.id = whatever the Id is, but i would prefer the bot to not be able to respond in the channel at all, including to.help commands. I have seen people do this with on_message, but I’m not sure how I would do that with cogs. Any help would be much appreciated. My intended result is basically to have the bot only respond in two channels, the two bot channels that i specify, to any commands including the.help command. Thanks!\nAnswer: The easiest way to do this is not actually via code but via permissions on the server. On your server you should find a role with the same name as your bot, whose permissions (including send messages) you can change for seperate channels.",0.20408164,0.21005613,3.5694487451110035e-05
18,"Question\nI have this problem when I try to import Pycryptodome.
Traceback (most recent call last): File ""C:\Users\me\Documents\Python\Python 3.8\file.pyw"", line 17, in <module> from Crypto.Cipher import AES File ""C:\Users\me\AppData\Local\Programs\Python\Python38\lib\site-packages\pycrypto-2.6.1py3.8-win-amd64.egg\Crypto\Cipher\AES.py"", line 50, in <module> from Crypto.Cipher import _AES
And then:
ImportError: DLL load failed while importing _AES: %1 is not a valid Win32 application.
I'm using Windows 64 bit with Python 64 bit 3.8.7. I installed Pycryptodome (version 3.9.9) with pip install pycryptodome. But when I tried to import AES from Pycryptodome, it errors out with the error above. Can anyone please tell me how to fix it? FYI, this is my first post on Stack Overflow, so if the post is missing anything, please tell me. Thanks!\nAnswer: oh silly me, need to install pycryptodome 3.8.2. Dumb mistake lol.",0.0,0.06392908,0.0040869275107979774
19,"Question\nI am creating and training a TensorFlow model in Google Cloud in their JupyterLab AI Notebooks but for some reason I cannot find a way to save my model after it's been created.
Typically, I'd use created_model.save(str('/saved_model_file')) in Colab to save to the local directory.
But JuptyerLab in Google Cloud responds with a ""Permission Denied"" error, I've tried giving all possible maximum permissions in AIM, I'm the only person on the count. But the error persists.
But I do seem capable of saving blobs to Buckets by using blob.upload_from_filename(source_file_name) or blob.upload_from_string(source_file_name) which saving to buckets seems like a more appropriate strategy.
But neither one of these will take the trained model created by TensorFlow since it's more of a function and not a file type they seem to be looking for. The tutorials seem to casually mention that you should save your model to a bucket but completely neglect to provide any simple code examples, apparently I'm the only guy on earth who wasn't born knowing how to do this.
Would be a great if someone could provide some code examples on how to save a TensorFlow model to a bucket. I also need for this function to be done automatically by the python code. Thanks!\nAnswer: It seems you just need to create the path with the OS module first then the TF function will work. This seems kind of odd, other platforms I've used let the TF function create the path by its self. Apparently Google Cloud doesn't like that, perhaps there's some user permission buried in a hierarchy somewhere that is causing this problem.

MODEL_NAME ='model_directory'
model_version = int(time.time())
model_path = os.path.join(MODEL_NAME, str(model_version))
os.makedirs(model_path)
tf.saved_model.save(model, model_path)",0.0,0.25485712,0.06495214998722076
20,"Question\nI have Android application made in java and I need to call Python script and put there some parameters to its function. When I make apk. of this androdid app. how can I make Android device execute python script in it?\nAnswer: You can make a server out of python as backend with flask or django, and then call the server whenever your app needs to run the python script.",0.0,0.1739561,0.03026072308421135
21,"Question\nI am building a django app in which the user has to write a sentence in a text box. This sentence gets then sent to the server and received by it. After that the user has to click on continue and gets on a another html page. Here the user has to record an audio of a word he sais. The word is then turned into a string and after that sent to the server. Now I would like the function in views.py to find out if the word the user said is in the sentence the user wrote before. But the sentence is only in the first function that receives the sentence after it is sent. I know I could first store the sentence but is there also another way?\nAnswer: yes, at least there is two ways first using a model to store the value. or a file maybe.
second using some html magic(? I'm not sure of magic). using an input type=""hidden"".
your first function receives the text, redirects user to another page but with an argument the text!, then inside that template store that text in a hidden input and by clicking the button send both voice and hide value text to the new functon.",0.40816328,0.11461192,0.08617240190505981
22,"Question\nI am looking to delete a photo sent by my bot (reply_photo()), I can't find any specific reference to doing so in the documentation, and have tried delete_message(), but don't know how to delete a photo. Is it currently possible?\nAnswer: It's currently possible in Telegram API, not the Bot API unfortunately. It's a shame :(",0.0,0.17229474,0.029685476794838905
23,"Question\nI am looking to delete a photo sent by my bot (reply_photo()), I can't find any specific reference to doing so in the documentation, and have tried delete_message(), but don't know how to delete a photo. Is it currently possible?\nAnswer: You need to have the chat_id and the message_id of that message sent by bot, then you can delete using context.bot.delete_message(chat_id, message_id).
Note: Bot cannot delete a message if it was sent more than 48 hours ago.",0.0,0.52850765,0.2793203294277191
24,"Question\nI hope you can help me. I have a static website hosted on Heroku and I would like to implement a Python Script to be executed when a button is clicked. So, just as a reference you would have:

A text field
A button
Another text field

The idea is that you enter some text in the first text field, you click the button calling the Python Script, and then print the result coming from the Python Script in the second text field.
How would you implement such technology? Which services should be used to achieve the result?
I think that the script should be hosted somewhere and be called via an API but I do not really know how to do it. I hope you can help me.
Thanks!\nAnswer: You have to use backend for your purpose. When a user clicks your button some data would be collected by your backend, handled and showed to user with the help of API. You can start with learning a little bit of Flask and learning Django later for some bigger projects.",0.13605443,0.14264512,4.343724867794663e-05
25,"Question\nI hope you can help me. I have a static website hosted on Heroku and I would like to implement a Python Script to be executed when a button is clicked. So, just as a reference you would have:

A text field
A button
Another text field

The idea is that you enter some text in the first text field, you click the button calling the Python Script, and then print the result coming from the Python Script in the second text field.
How would you implement such technology? Which services should be used to achieve the result?
I think that the script should be hosted somewhere and be called via an API but I do not really know how to do it. I hope you can help me.
Thanks!\nAnswer: I should use Flask or Django. In Flask you simply use the: name = ""your_variable"" command in your HTML code and then you can simply use the code request.form [""your_variable""] in your python script.",0.13605443,0.17713195,0.0016873630229383707
26,"Question\nIf a run a python script where i declare 6 processes using multiprocessing, but i only have 4 CPU cores, what happens to the additional 2 processes which can find a dedicated CPU core.

How are they executed?
If the two additional processes run as separate threads on the existing Cores, will GIL not stop their execution?

#Edit 1 - 21st Jan 2021
I have mixed up threads and processes in the question I asked. Since I have better clarity on the concept, I would rephrase question 2 as follows(for any future reference):
If the two additional processes run in parallel with two other processes in existing Cores, will GIL not stop their execution?
Ans: GIL does NOT affect the processes, GIL allows only one thread to run at a time, there is no restriction on processes, however. The system scheduler manages how the additional two processes will run on the existing cores.\nAnswer: First you are mixing up threads and processes: in Python only threads not processes have to share a lock on their interpreter.
If your are using the multiprocessing library then, your are using Python processes which have their own interpreter.
When you are using Python processes, their execution is managed by your operating system scheduler, in the same manner as every other processes in your computer.
If you have more processes than CPU cores then the extra processes are waiting in background to be scheduled.
This usually happen when an other process terminates, wait on an IO, or periodically with clock interrupts.",0.81632656,0.33566797,0.23103268444538116
27,"Question\nI noticed any exe application I make are treated like viruses upon download. This is terrible, how do I make them legitimate? I read something about self-signing, but I still don't get it. What is the process of self signing and how do I do it? If it helps I am using pygame, python, on pycharm, with pyinstaller, on Windows 10.\nAnswer: You cant, or it is hard, to fix that your program treats as virus at antiviruses because av's use euristhic analysis and machine learning so they could make your program false positive. You have to go to av's forums and ask why your app is false positive.",0.0,0.100544274,0.010109150782227516
28,"Question\nI know the function 'ord' can convert character to number.
but I just want to know how to convert without 'ord'
C can convert it, but is it impossible in Python?\nAnswer: You can use len() function to solve it
input(""any value?"")
print(len(""value""))input(""any value you want"") print(len(""enter your value""))",-0.35714287,-0.050642073,0.09394273906946182
29,"Question\nI  am asked to normalize a probability distribution P=A(x^2)(e^-x) within 0 to infinity by finding the value for A. I know the algorithms to calculate the Numerical value of Integration, but how do I deal with one of the limits being Infinity.\nAnswer: The only way I have been able to solve this problem with some accuracy (I got full accuracy, indeed) is by doing some math first, in order to obtain the taylor series that represents the integral of the first.
I have been looking here for my sample code, but I don't find it.  I'll edit my post if I get a working solution.
The basic idea is to calculate all the derivatives of the function exp(-(x*x)) and use the coeficients to derive the integral form (by dividing those coeficients by one more than the exponent of x of the above function) to get the taylor series of the integral (I recommend you to use the unnormalized version described above to get the simple number coeficients, then adjust the result by multiplying by the proper constants)  you'll get a taylor series with good convergence, giving you precise values for full precision (The integral requires a lot of subdivision, and you cannot divide an unbounded interval into a finite number of intervals, all finite)
I'll edit this question if I get on the code I wrote (so stay online, and dont' change the channel :) )",0.0,0.2282823,0.05211281031370163
30,"Question\nI tried makemigrations, migrate, and even many methods stated in stack overflow but nothing is happening. Please tell me the reason why this happens and how can i solve it?\nAnswer: I think this can help you :-
Delete all the migrations in your app except init.py file. AND then again type python manage.py make makemigrations and type python manage.py migrate",0.0,0.26996642,0.07288186997175217
31,"Question\nI have tried installing nni packages in windows command prompt,it installed successfully.but when i tried to command,
""nnictl create --config nni\example/trails\mnist-pytorch\config_windows.yml""
in windows command prompt.it says,
'nnictl' is not recognized as an internal or external command,operable program or batch file.
how can i fix this error?\nAnswer: It is solved! I just installed anaconda3 and tried installing nni packages in it.And also it recognized 'nnictl' command in anaconda3 prompt automatically by setting path variables.Thanks:)",0.0,0.13079482,0.0171072855591774
32,"Question\nI'm working on Ubuntu by remoteSSH, and I updated python kernel in my vitual environment named nn form 3.7.9 to 3.8.5, however, I still find the old kernel standing in the jupyter kernel list. I want to know how to delete the old kernel name from the kernel list.
I've replaced python 3.7.9 and python3.6.4 with python 3.8.5, but the old kernels didn't disappear, I want to delete them manually.
Moreover, I can't select Python 3.8.5 from the kernel list.\nAnswer: I had the same problem and the following might help someone else encountering the issue:

Reload VS Code Window by Ctrl+Shift+P and selecting Reload Window.

Reload the Python and Jupyter extensions under the Extensions in the Side Bar.

Quit and relaunch VS Code.


It seems that VS Code is not that quick to update the interpreter list.",0.6802721,0.25427377,0.18147458136081696
33,"Question\nI am implementing a system that checks the plagiarism of documents.
our stack is vuejs, nodejs/express and flask for python.
My question is that i have a page which the user will upload his documents for checking and the vue ui will send a request to the backend apis with the user file to check the similarity, while this process is running a loading overlay is displayed in the same page.
I want to update this page with live steps from the backend side like ""extracting"", ""searching"", ""comparing"", ""generating report"".
noting that the request sent with the user file have only one response.
so any ideas how can i achieve this step??
Thanks,,\nAnswer: You can return a request_id and after that you can use the id to check on the status/stage of the request.",0.0,0.09330517,0.008705854415893555
34,"Question\ni am trying to program a simple script and i would like to know if anyone has the answer to this question
when i have a module for example WMI from 'pip install wmi' in the form as 'import wmi'  in my code, how do i get the pyinstaller module to compile the wmi module with the exe file
i have tried importing from the source code in a folder example 'from wmi import wmi' but i got no luck when launching the exe file only in the raw python file, also just to note when i compile the script i do the command 'pyinstaller vb.py --onefile'\nAnswer: remove --onefile since wmi couns as a file (or multiple if its a package)",0.0,0.19567883,0.03829020634293556
35,"Question\nI am trying to install jupyterlab via the command terminal and it gave me the following warning:
WARNING: The script jupyter-server.exe is installed in 'C:\Users\Benedict\AppData\Roaming\Python\Python39\Scripts' which is not on PATH.
Consider adding this directory to PATH or, if you prefer to suppress thus warning, use  --no-warn-script-location.
Please how do I add the directory to PATH? Someone help me please. Thank you\nAnswer: As i can see, you haven't put that in path, for doing that follow the following step:-

Open the advance system settings
select environment variable
Then click on path and press edit.
Click on new and enter the you path and then your path to python script directory.
Press okay and reopen the jupyter.
That's it",0.0,0.04065901,0.0016531550791114569
36,"Question\nI have been trying to link a main module with another module, both need each other to perform a specific task, so I imported them to each other, but I get an error about circular importation. I've been trying to avoid this but it keeps raising the error, please how do I correct this??\nAnswer: Simply create a 3rd module, from which you can then import the functions, classes and variables to the 2 other scripts. Normally issues like these are signs of poor code structuring. 2 separate scripts should never both depend on one another at the same time. Avoid such design blunders in the future.",0.0,0.24976283,0.06238147243857384
37,"Question\nI have a small python code that uses min(list) multiple times on the same unchanged list, this got me wondering if I use the result of functions like min(), len(), etc... multiple times on an unchanged list is it better for me to store those results in variables, does it affect memory usage / performance at all?
If I had to guess I'd say that if a function like min() gets called many times it'd be better for performance to store it in a variable, but I am not sure of this since I don't really know how python gets the value or if python automatically stores this value somewhere as long as the list isn't changed.\nAnswer: If you are only using it 1-5 times, it doesn't really matter. But if you are going to call it anymore, and really less too, it is best to just save it as a variable. It will take next to no memory, and very little time to do so and to pull it from memory.",0.0,0.10520971,0.011069082655012608
38,"Question\nIs there a possibility to see all pip installed packages in Pycharm?
Because I have the Problem: I write in PyCharm and it works fine, but now I want to move the project to a server... And now I don't know how can I quickly export this\nAnswer: Use the command pip freeze >requirements.txt locally to import the environment you need into the file,
then  use the command pip install -r requirements.txt  on the server to install the required environment",0.13605443,-0.022859037,0.02525348961353302
39,"Question\nI wrote a program that automatically navigates me threw a website, but how do I copy my current URL.
Context: I am attempting to code a watch2gether bot that automatically creates a watch2gether room\nAnswer: I don't know which programming language you use, but in Python3 its simply driver.current_url",0.20408164,0.36259073,0.025125131011009216
40,"Question\nI'm a newbie to the django framework and trying to make a watchlist for stocks. I've already made the crux of the webapp, where-in, a user can search for a quote and add it to their watchlist, along with relevant data about that quote.
What I want to do now is, to save the separate watchlists that different users are creating (after creating an account on my site) and upon logging in to my site, they can view their personalized watchlist and edit it.
I'm using a model for storing the data for the watchlist quotes and looking for a way to provide the different personalized watchlists depending upon the logged in user.
Can anyone give me a lead on how to employ the logic for this? Do I need to use two data bases - one for the data of the users and the other one for storing the respective user watchlists? If yes, how do I connect everything?
EDIT: Ever used a stock investment app? The way every user/customer can log in to their account and make/edit and save their watchlists in the app - that is the functionality I want to implement. How/Where do I store so many watchlists?\nAnswer: use'request.user' from your view, to know the user who sent the request and return the corresponding watchlist",0.40816328,0.039558172,0.13586972653865814
41,"Question\nI am doing binary classification for a time series with keras LSTM. How could I extract the final output from the model? By this I mean, how can I get a list containing zero and one values from the final model?\nAnswer: You should attach right after a LSTM layer a Dense layer with as much neurons as you consider (that depends upon the LSTM output itself), and on top of that one add a final Dense classification layer with a single neuron, that'd be the binary output.",0.0,0.10657519,0.011358271352946758
42,"Question\nI am new on Medical Imaging. I am dealing with MRI images, namely T2 and DWI.
I uploaded both images with nib.load, yet each sequence image has a different number of slices (volume, depth?). If I select one slice (z coordinate), how can get the corresponding slice on the other sequence image? ITK does it correctly, so maybe something in the NIFTI header could help?
Thank you so much for reading! I also tried interpolation, but it did not work.\nAnswer: Reading the NIFTI, namely affine, and extracting the translation Transformation Matrix to create a mapping from the T2 to the DWI voxels.
Hint: nibabel.",0.20408164,3.2305717e-05,0.04163613170385361
43,"Question\nI'm trying to reinstall Jupyter-Lab with conda completely. I mean, when I run uninstall jupyterlab and install it again, the system already comes with configuration I had previously, such as extensions installed. Therefore, there is something that is still present after the uninstall.
Hence, how do I completely remove jupyter-lab and install it again from scratch?\nAnswer: When jupyterlab is installed use jupyter --paths to see where the configuration, data and runtime is stored. After removing the corresponding files and directories you will be able to perform a clean install without any traces of the old extensions.
Remember to use it in the right environment.",0.40816328,0.10473442,0.09206907451152802
44,"Question\nI have built a Socket TCP / IP server that listens on a specific port and then, with that data, makes a rest query to another server, and that response is returned through the port where it received it.
All Socket server is made in Python 3.8 and works great.
I need to know how to implement this code from my Socket server to an Azure Functions, so that it provides permanent service?
I appreciate the goodwill of anyone who can offer an answer.
Thanks Total.\nAnswer: Simple answer: you cannot do that. Azure Functions are Event-based (such as an HTTP call). If you need to provide TCP socket, maybe hosting your python code in a container, e.g. Azure Container Instances, might be a good way to go.",0.0,0.22112858,0.04889785125851631
45,"Question\nI'm hoping this is a fairly simple question with a simple answer.
In PostgreSQL I have a table with a Answer column that is a jsonb.
As of right now, the data that can be stored in the column can be empty or quite varied. Some examples include:

{""Answer"":""My name is Fred""}
{""Answer"":[{""text"": ""choice 1"", ""isActive"": true}, {""text"": ""choice 2"", ""isActive"": false}]}

Yes, we store a field called Answer in our column called Answer. Not sure why, but that is how it is.
I want to be able to test if the JSON attribute Answer contains a string or an array. But I don't know how, and I must be wording my searches incorrectly. I'm not finding anything concrete. I already know how to check if Answer exists. Just can't tell what it contains.
Does anyone know how I would do this? Or if there isn't a way, what I need to do instead to query this data?\nAnswer: func.jsonb_typeof(<TableNameHere>.answer.op('->')('Answer')) == ""string"" seems to do the job.",0.0,0.23611528,0.055750422179698944
46,"Question\nI want to implement a simple source code that DROPs all RST packets that come into the computer using Python. What should I do?
Linux servers can be easily set up using the iptables command, but I want to make it Python for use on Mac, Linux, and Windows systems.\nAnswer: Dropping RST packets is a function of the networking firewall built into your operating system.
There is only one way to do it on Linux: with iptables. You could use Python to instruct iptables.
Windows has its own way to add firewall rules. MacOS also has its own way, and each of them is different from the other.
There is no single common way to do this. Therefore, there is no single common way to do this with Python.",0.0,0.47573233,0.2263212502002716
47,"Question\nI am using bulk_create to upload some data from excel to django db. Since the data is huge I had to use bulk_create instead of.create and.save. But the problem is that I need to show the user how many duplicate data has been found and has not been uploaded due to integrity error. Is there a way to get the number of errors or duplicate data while using bulk upload?\nAnswer: After, Reading data from csv file.
First create a list before inserting data to system.
Then convert that list to set after then again sort the data which is in set.
Here, you gets every data exactly one time in sorted manner.",-1.0,0.23428237,1.5234529972076416
48,"Question\nI have Django model with Image field, but sometimes I don't need to actually upload file and store path to it, but I need to store only path. Especially url. I mean, I have web client, that receives both foreign urls like sun1-17.userapi.com and url of my own server, so sometimes I don't need to download but need to store url. Is it possible, to store url in ImageField, or I need to make CharField and save files via python? If its impossible, how do I save file in python3, having one, sent me via multipart?\nAnswer: The URL field in the ImageField is ReadOnly, so you cannot write it. You should probably use in addition a URLField (better than a CharField) to save the URLs.
You can allow null values on both and use only the appropriate one according to your scenario.",0.0,0.21154296,0.04475042596459389
49,"Question\nHow can I an image as the background in my ursina project. I know I can change the color of the background by using window.color = color.light_gray for example. But how do I use an image?\nAnswer: Try
Sky(texture=""texture_name"")",0.0,0.31010497,0.09616509079933167
50,"Question\nI am trying to make an app with kivy and kivymd but I can't figure out how I can make the setup screen show up only the first time. This is how the application is going to work: User launches the application after installation and is being shown the sign up/log in screen, And once the user is done with the setup, the setup screens will never appear again unless the user reinstalls the application.
How can I make this happen?
Please help and thanks SO much in advance!\nAnswer: I fixed this problem by creating and reading a ""text"" file.My ""text"" file has '0' as a boolean variable.Once the user is done with signing up / logging in, I change that ""text"" file to '1',and in the __init__ func, I check if that file equals to '0' or '1'.
I'm not sure if this is the correct way or not,but this worked for me.",0.40816328,0.14688551,0.06826607137918472
51,"Question\nI am wondering how best to feed back the changes my DQN agent makes on its environment, back to itself.
I have a battery model whereby an agent can observe a time-series forecast of 17 steps, and 5 features. It then makes a decision on whether to charge or discharge.
I want to includes its current state of charge (empty, half full, full etc) in its observation space (i.e. somewhere within the (17,5) dataframes I am feeding it).
I have several options, I can either set a whole column to the state of charge value, a whole row, or I can flatten the whole dataframe and set one value to the state of charge value.
Is any of these unwise? It seem a little rudimentary to me to set a whole columns to a single value, but should it actually impact performance? I am wary of flattening the whole thing as I plan to use either conv or lstm layers (although the current model is just dense layers).\nAnswer: You would not want to add in unnecessary features which are repetitive in the state representation as it might hamper your RL agent convergence later when you would want to scale your model to larger input sizes(if that is in your plan).
Also, the decision of how much of information you would want to give in the state representation is mostly experimental. The best way to start would be to just give in a single value as the battery state. But if the model does not converge, then maybe you could try out the other options you have mentioned in your question.",0.40816328,0.14715856,0.068123459815979
52,"Question\nSorry to my bad english.
So i use a automate with MERVIS software and i use a Bacnet server to have my variable in my IHM (weintek panel pc with Easybuilder Pro).
So all i make is good and work but i'm not happy to EasyBuilder pro and i want make my own HMI. I decide to make my application with QT in C++.
But i'm physicien at the begining so learn little bit by little bit( i have base of python,c++, structur text). I know nothing about how build a bacnet client and do you have idea where can i find some simple exemple to communicate with my PLC because i find nothing and i need to learn and make this to my project.
So i have my PLC, link in ethernet to my PC where i make my hmi. In the future i want put this application in PANEL PC tactil work in window and link to my PLC with MERVIS software.\nAnswer: If I'm clear on the question, you could checkout the 'BACnet Stack' project source code or even the 'VTS' source code too - for a C/C++ (language) reference.
Otherwise YABE is a good project in C# (language), but there's also a BACnet NuGet package available for C# too - along with the mechanics that underpin the YABE tool.",0.0,0.17880529,0.03197133168578148
53,"Question\nhello guys I'm trying to make another one to access my Django website I host it on my localhost by type
python manage.py runserver 0.0.0.0:8000
and i set the ALLOWED_HOSTS = ['*']
when I trying to connect with my IP it's says refused to connect.
can someone help me\nAnswer: you are only hosting your server in your local network therefore no-one outside of this network can access your server. To make them access it you would have to make it accessible over the internet for example via hosting it on aws or another cloud hoster.",-0.71428573,0.09841108,0.6604760885238647
54,"Question\nLet's imagine, I have a simple tkinter program: only tkinter.Entry(), where I can write down some text. The main goal I have set to this tkinter.Entry() is to make next: when I try to input there some symbol, it is immediately deleted from tkinter.Entry(). So the question is how to make tkinter.Entry() delete every symbol, when it have been just input there?
I hope the problem is fully described. Thanks in advance for your help.

I apologize, but it seems to me that this question has lost its former relevance for me. Sorry for letting you take all of your precious time. I took all the answers and tips into account. I will delete the question soon. Thank you for your attention to me\nAnswer: From what I deduced, you're trying to delete the content from the entry widget.
tkinter.Entry.delete('0',END)
This should do it.",0.40816328,0.28494728,0.015182183124125004
55,"Question\nI have a Tkinter app which I have converted to.app and.exe, but after giving this app to others if I have to update the app how should I do it(like in play store update)? And also if I package this app and distribute with an installer then how to send update to the app?\nAnswer: I don't think that it's possible to update the app like that. Android apps usually are made with Java, and iOS apps are made with Xcode, Swift, and Objective-C. I don't usually make apps with Python, unless they are for myself, because once they are made into apps, they cannot be updated (as far as I know). If I wanted to update my Python app, I would remove the first app, then use Pyinstaller to make the updated app.
Hope this helps, and have a good day. :)",0.81632656,0.40423495,0.16981950402259827
56,"Question\nI'm using Task Scheduler to execute python run.py. It works, but the Python interpreter pops up. I want to run run.py in the background without any interpreter popping up.
How can I do this? In Linux I'd just do python run.py & to get it to run in the background silently, but I'm not sure how to achieve the same in Windows with Task Scheduler.\nAnswer: You can just change.py extension to.pyw
and the python file will run in background.
And if you want to terminate or check if it actually running in background,
simply open Task manager and go to Processes you will see python32 running
there.
EDIT
As you mentioned, this doesn't seem like working from command line because changing the file's.extension simply tells your system to open the file with pythonw application instead of python.
So when you are running this via command line as python.\run.pyw even with the.pyw this will run with python.exe instead of pythonw.exe.
Solution:
As you mentioned in the comments, run the file as pythonw.\run.pyw or.\run.py
or just double click the run.pyw file.",0.0,0.3721341,0.13848377764225006
57,"Question\nI had developed a standalone application on Windows for Deep Learning / Computer vision in Python 3.X using various standard python libraries such as pandas, numpy, TensorFlow, Keras, Yolo, PyQt...etc. I want to deliver this application to my client but without source code.
Can you please help me how to do this?\nAnswer: ""I want to deliver this application to my client but without source code.""
Can you clarify this process?
If you just want to deliver this service to your client you can just use HTTP/POST to let users upload their data to you, then you run these data on your network on the server, and finally, just return the prediction result to them.",0.0,0.0016762614,2.809852276186575e-06
58,"Question\nI just uninstalled and reinstalled python on my Windows machine. Before I uninstalled my previous version I was able to just double-click on a python script and it would open the command prompt, run the script, and close automatically. After re-installing with the newest version (3.9), I am no longer able to execute the script like that with a double-click.
Clearly I had done something special last time to set that up for myself, but I don't remember what it was. Any idea how I can get that double-click deal going again?\nAnswer: There will be an option of ""Open With"" after right-click on the file go and choose CMD. I hope it helps if not then sorry. Because I use Parrot OS",0.0,0.23728466,0.05630401149392128
59,"Question\nI have built a snake game using Turtle graphics module of Python, and now I wish to convert it into an apk.
I have tried kivy. It builds the apk, but the app crashes as soon as I open it in android. When using adb logact -s python, it says that the tkinter module is not available.
On further researching, I got to know that turtle graphics is based upon tkinter module and tkinter is not supported by python-for-android. The solutions suggest to rewrite my code in Kivy, but I don't know how to do so.
Any suggestions on how can I run my turtle graphics game on android?\nAnswer: The solutions suggest to rewrite my code in Kivy, but I don't know how to do so.


Any suggestions on how can I run my turtle graphics game on android?

It looks like you've already found the solution: rewrite your graphics in Kivy, or another python module that works on android. Recent pygame releases might.
If you don't know how to do so, you need to learn. If you try to do so but have problems with any specific question, that would be a better target for a stackoverflow question.",0.0,0.30449492,0.09271715581417084
60,"Question\nI have VRP problem. I have vehicles starting positions and I have distance matrix. I want solution to be terminated/finished when certain locations are visited.
So I don't want it to actually visit each index of location_matrix but if visiting different index beside ""MUST VISITS"" make for better solution then I have no problem. Because you know sometimes going from directly 1 to 3 is slower than 1-2-3. (visiting 2 which is not necessary but make it for shortcut)
I defined a dummy depot which cost 0, I used this for end because if you use starts you have to define ends. And I put ends 000 which are basically ending position. You might ask why you didnt put your ""JOB"" locations. But this means they have to end there. So it doesn't seem optimal because  example one vehicle could be really near to both of ""JOB"" locations but if it terminates / ends because it has END defined vehicle would stop.
I have no idea how to make this work. Basically what I want that if certain locations are visited once just terminate - that's the solution. So if jobs are (1,3,5) and  Vehicle 1  visited 1,3 and Vehicle 2 just visited 2 it should be finished.
If I use solver in ortools It will be like TSP problem which will try to visit each location in distance_matrix. I don't exactly want this. It could visit if results better solution(does it make sense?) but it should be focusing on ""JOB"" locations and how to go them faster\nAnswer: Potential approach: Compute a new distance matrix with only the ""MUST VISIT"" locations, and run a VRP with this matrix.

Compute a distance matrix with only the ""MUST VISIT"" locations.
Each cell contains the shortest path between two ""MUST VISIT"" locations.
Store all the pairwise shortest paths found.
Run a regular VRP on this distance matrix.
Reconstruct the full path using the shortest paths found earlier.",0.40816328,0.31400657,0.008865486830472946
61,"Question\nI'm dealing with a highly imbalanced dataset for my project rn, for the simplicity, I will give a simple example here: a dataset has a number of 20 '0's and 80 '1's so the total is 100.
Suppose I have already used X_train, X_test,y_train,y_test = train_test_split(X, y,stratify=y,random_state=42) to make a stratified split (X_train.shape is 80 and X_test.shape is 20), so my question is how to achieve under-sampling with K-fold validation in the train dataset at the same time.
My initial thought is use from imblearn.under_sampling import RandomUnderSampler to get 16 '0's and 16 '1's (total is 32) to make equal distributed dataset, and do the K-fold cross-validation on that 32 dataset and discard the rest of 48 in the X_train. Use the model to predict the X_test. So I was wondering if this is correct procedure to deal with.\nAnswer: You can use RandomUnderSampler method to achieve it. Put random states and ratio value in the arguments and try to see if this works.",0.0,0.20968163,0.0439663864672184
62,"Question\nI've built an IB TWS application in python. All seems to work fine, but I'm struggling with one last element.
TWS requires a daily logout or restart. I've opted for the daily restart at a set time so I could easily anticipate a restart of my application at certain times (at least, so I thought.)
My program has one class, called InteractiveBrokersAPI which subclasses the ECClient and EWrapper. Upon the start of my program, I create this instance and it successfully connects to and works with TWS. Now, say that TWS restarts daily at 23:00. I have implemented logic in my program that creates a new instance of my InteractiveBrokersAPI, and calls run() on it af 23:15. This too seems to work. I know this because upon creation, InteractiveBrokersAPI calls reqAccountUpdates() and I can see these updates coming in after my restart. When I try to actually commit a trade the next day, I get an error that it's not connected.
Does anyone else have experience in how to handle this? I am wondering how others have fixed this issue. Any guidance would be highly appreciated.\nAnswer: Well, this doesnt exactly answer your question, but have you looked at ib_insync",0.0,0.14095649,0.01986873149871826
63,"Question\nMy code (test.py) looks like this (simplified):
from app.utils import conversion 
(code) 
When I try to make an executable using PyInstaller, the.exe works when I import generic modules. However, I get the following error message when I use ''from app.utils import conversion'' at the beginning of my code:
ModuleNotFoundError: No module named 'app' 
and the.exe won't run.
My project is structured this way (simplified):
project/app/test.py 
project/app/utils/conversion.py 
The instruction I put in the console is:
pyinstaller --onefile test.py 
Any idea why and how to overcome this? Thanks!\nAnswer: Here is how I fixed my problem:
in the.spec file, added the missing app module in hiddenimports:
hiddenimports=[""app""]
Then to compile the executable, I run the.spec file instead or the.py file.
pyinstaller --onefile test.spec",0.0,0.18356574,0.03369637951254845
64,"Question\nLet me describe it as briefly and clearly as possible:
I have 10 different copies of a node JS based program running on 10 different desktops. I want to create a Node JS based (or any other technology) web app deployed on a server which will check if these 10 programs are online or not.
Any suggestions as to how I can implement this?
Note: The node JS based desktop apps are running on electron.\nAnswer: You can use 2 most probable ways.

if you want to know immediately whether out of 10 programs, if any of them goes offline then you should use Socket.io


Your server nodejs program will act as server and your 10 desktop program will work as client. Your 10 client socket connection will connect with server socket connection and server socket can check whether socket client is still connected or not based on Ping/Pong concept of socket.

in brief, Ping/pong technique in which server sends Ping event on socket connection and client will receive server's ping event and send Pong event back to server.
if client is not sending Pong event back in predefined time interval on getting Ping event then that client is offline or disconnected.

You can periodically (say every 1/5/10 minutes etc ) call simple HTTP request and check if response status is 200 or not. If any of the 10 desktop program is offline then you will know it by response status whether it is 200 or not.",0.0,0.35740393,0.12773756682872772
65,"Question\nLet me describe it as briefly and clearly as possible:
I have 10 different copies of a node JS based program running on 10 different desktops. I want to create a Node JS based (or any other technology) web app deployed on a server which will check if these 10 programs are online or not.
Any suggestions as to how I can implement this?
Note: The node JS based desktop apps are running on electron.\nAnswer: While you can use socket.io for this there may also be a simpler way and that is to just use a post request / cron to check every X minutes if the server is reachable from 'Checking' server (that would just be the server that is doing the check)
So why not use socket.io? Well, without knowing how you node servers are setup, its hard to say if socket.io would be a good fit, this is simply because socket.io uses WSS to connect, so unless you are running it from the browser it will need additional configurations / modules setup on the server to actually use WSS (if you do go this route, you will need socket.io-client module on each system, this is important because this will allow you to connect to the socket.io server, also make sure the version of socket.io matches the socket.io-client build)
All in all, if I was building this out, I would probably just do a simple ping of each server and log it to a DB or what not but your requirements will really dictate the direction you go",0.0,0.14969423,0.022408364340662956
66,"Question\nI do not have administrative privileges' on my Windows 10 workstation. The IT department installed Python 2.7 as my request but I proceed a PIP upgrade without the ""--user"" setting, and now the already installed PIP got corrupted and I do not know how to recover it.
The corrupted PIP always return syntax error on lib\site-packages\pip_internal\cli\main.py"", line 60
sys.stderr.write(f""ERROR: {exc}"")
I can not run again the --upgrade or get-pip
I can write in the Python folder so I can change the main.py file.
Is there a way to manually recover the installation (without sudo)? I need to reinstall the Python?\nAnswer: It would be better to reinstall Python, yes.
It would be better to install a version of Python that was actually still supported, such as 3.6 or newer.",0.0,0.30673665,0.09408736974000931
67,"Question\nI have been working on a project which involves me to get the icon from the icon theme used on Linux so that I can use it with the Gtk Pixbuf like how Gnome-system-monitor displays the icon for all the process, the same thing I want to achieve. Any ideas about how to do this?
I am using python with Gtk on PopOS 20.10.\nAnswer: Gio.AppInfo in the Gtk library stack is a good point to start.
If you are looking for the approach that is used by the gnome-system-monitor then the prettytable.c file will the one you need to check.
There is one more approach, scanning the /usr/share/application/ directory and creating a file monitor for this directory. All icons of the application are that are in the menu can be found here.",0.20408164,0.28647602,0.006788833066821098
68,"Question\nThanks for your time.
Basically, what I'm trying to do is to set a object from a database list (.csv) and if i get an ValueError I would like to set that field value and keep adding data

ValueError: Field 'age' expected a number but got 'M'

I'm quite sure that's a doc for this, but I've been reading for some time and hasn't found.\nAnswer: How about filtering the data once you receive it? For example lets say age field expects an Integer, and before you save it you could check if the data is an Integer. But I also think the most efficient way is using try except.",0.20408164,0.22188729,0.00031704120920039713
69,"Question\nThanks for your time.
Basically, what I'm trying to do is to set a object from a database list (.csv) and if i get an ValueError I would like to set that field value and keep adding data

ValueError: Field 'age' expected a number but got 'M'

I'm quite sure that's a doc for this, but I've been reading for some time and hasn't found.\nAnswer: Using a simple 'try', 'except' block should work where you have a default value to use in 'except' before saving.",0.0,0.37850893,0.14326900243759155
70,"Question\nHow to import postgresql database (.sql) file from AmazonS3 to AWS RDS?
I am very new to AWS, and Postgresql.
I have created a database using PgAdmin4 and added my data to the database.
I have created a backup file of my database i.e..SQL file.
I have created a database instance on AWS RDS.
I have uploaded my database file and several documents s3 bucket.
I tried to integrate AWS S3 and RDS database using AWS Glue, but nothing is working for me. I am not able to figure out how to integrate S3 and RDS for importing and exporting datafrom S3 to RDS and vice versa.
Can you please tell me how can I set up RDS and S3?\nAnswer: What you can do is install a pure python library to interact with rds and run the commands via that library just like you would do with any normal python program. It is possible for you to add libraries like this to run in your glue job. In your case pg8000 would work like a charm",0.0,0.4827005,0.2329997718334198
71,"Question\nI am solving a stochastic differential equation and I have a function that contains an algorithm to solve it. So I have to call that function at each time step (it is similar to Runge Kutta's method but with a random variable), then I have to solve the equation many times (since the solution is random) to be able to make averages with all the solutions. That is why I want to know how to call this function in each iteration in the most efficient way possible.\nAnswer: The best way to implement a function on an iterable is to use the map function.
Since map is written in C and is highly optimized, its internal implied loop can be more efficient than a regular Python for loop.",0.10204082,0.10234922,9.511172294196513e-08
72,"Question\nI am solving a stochastic differential equation and I have a function that contains an algorithm to solve it. So I have to call that function at each time step (it is similar to Runge Kutta's method but with a random variable), then I have to solve the equation many times (since the solution is random) to be able to make averages with all the solutions. That is why I want to know how to call this function in each iteration in the most efficient way possible.\nAnswer: Some ways to optimize function calls:

if the function arguments and results are always the same, move the function call out of the loop
if some function arguments are repeated and the results for a given set of arguments are the same, use memoize or lru_cache

However, since you say that your application is a variation on Runge-Kutta, then neither of these is likely to work; you are going to have varying values of t and the modeled state vector, so you must call the function within the loop, and the values are constantly changing.
If your algorithm is slow, then it won't matter how efficient you make the function calls. Look at optimizing the function to make it run faster (or convert to Cython) - the actual call itself is not the bottleneck.
EDIT: I see that you are running this multiple times, to determine a range of values given the stochastic nature of this simulation. In that case, you should use multiprocessing to run multiple simulations on separate CPU cores - this will speed things up some.",0.30612245,0.30156446,2.077533645206131e-05
73,"Question\nI would like to send DirectInput keys to an inactive window without interfering with my actual mouse. I tried using PostMessage, SendInput and SendMessage but pywin32 uses virtual keycodes while ctypes does work with DirectInput. I have no idea how I can make it send in an inactive window.\nAnswer: Try using this, it manages to work for me send the keystrokes to the inactive window,
Use (but add error checking) hwndMain = win32gui.FindWindow(""notepad"", ""​prueba.txt: log keys"") hwndEdit = win32gui.FindWindowEx",-0.71428573,-0.53620195,0.031713832169771194
74,"Question\nI am using the ResNet18 pre-trained model which will be used for a simple binary image classification task. However, all the tutorials including PyTorch itself use nn.Linear(num_of_features, classes) for the final fully connected layer. What I fail to understand is where is the activation function for that module? Also what if I want to use sigmoid/softmax how do I go about that?
Thanks for your help in advance, I am kinda new to Pytorch\nAnswer: No you do not use activation in the last layer if your loss function is CrossEntropyLoss because pytorch CrossEntropyLoss loss combines nn.LogSoftmax() and nn.NLLLoss() in one single class.
They do they do that?
You actually need logits (output of sigmoid) for loss calculation so it is a correct design to not have it as part of forward pass. More over for predictions you don't need logits because argmax(linear(x)) == argmax(softmax(linear(x)) i.e softmax does not change the ordering but only change the magnitudes (squashing function which converts arbitrary value into [0,1] range, but preserves the partial ordering]
If you want to use activation functions to add some sort of non-linearity you normally do that by using a multi-layer NN and having the activation functions in the last but other layers.
Finally, if you are using other loss function like NLLLoss, PoissonNLLLoss, BCELoss then you have to calculates sigmoid yourself. Again on the same note if you are using BCEWithLogitsLoss you don't need to calculate sigmoid again because this loss combines a Sigmoid layer and the BCELoss in one single class.
check the pytorch docs to see how to use the loss.",0.40816328,0.22600114,0.03318304196000099
75,"Question\nNewbie here... 2 days into learning this.
In a learning management system, there is an element (a plus mark icon) to click which adds a form field upon each click.  The goal is to click the icon, which generates a new field, and then put text into the new field.  This field does NOT exist when the page loads... it's added dynamically based on the clicking of the icon.
When I try to use ""driver.find_element_by_*"" (have tried ID, Name and xpath), I get an error that it can't be found.  I'm assuming it's because it wasn't there when the page loaded.  Any way to resolve this?
By the way, I've been successful in scripting the login process and navigating through the site to get to this point. So, I have actually learned how to find other elements that are static.
Let me know if I need to provide more info or a better description.
Thanks,
Bill\nAnswer: Apparently I needed to have patience and let something catch up...
I added:
import time
and then:
time.sleep(3)
after the click on the icon to add the field.  It's working!",0.0,0.14196748,0.020154764875769615
76,"Question\nI create a Todo web application in Django and i deploy it on Heroku. I want to know how can i push the notification in my browser for upcoming task.Thanks in advance.\nAnswer: You should use websockets and async functionality of Django to be able to push realtime notifications as they occur.
Basic http protocol does not give you such functionality.",0.81632656,0.17140257,0.4159269332885742
77,"Question\nIn my template, I want to show the join date of a user, so I am using {{ user.date_joined }} which shows the date and time (in local time zone - same as what is shown in the admin panel). To just show the date, I use {{ user.date_joined.date }}, but it seems to be converting the date and time to UTC before showing the date (I am in EST/EDT - I never remember which is which).
For example:
{{ user.date_joined }} ==> Feb. 18, 2021, 7 p.m.
{{ user.date_joined.date }} ==> Feb. 19, 2021
Is there a way for me to change this so that it shows the same date as the full date and time?\nAnswer: Found a solution/workaround for anyone else with a similar question.
Instead of using {{ user.date_joined.date }} like a traditional datetime object, I used {{ user.date_joined|date }}",0.40816328,0.28189826,0.015942854806780815
78,"Question\nI just tried to install package WoE using pip which works fine. Then in Jupyter Notebook when I try to run the command:
from WoE import WoE
I receive an error that there is no module named ""WoE""
I keep trying to figure out how to use sys.path.append to make this module work but I cannot figure it out. Any help or advice would be appreciated!\nAnswer: Try running command prompt as admin and then doing the command py -m pip install WoE. If that still doesn't work try restarting your computer, it could just be an issue with Jupyter not seeing the module yet. You can also do py -m pip show WoE and if that gives you a file location then that means it did install correctly.",0.0,0.15667921,0.024548375979065895
79,"Question\ni got this message when i wanted run a beysian personalized ranking by GPU in colab, How can i resolve this problem?
message is :
GPU training requires factor size to be a multiple of 32 - 1. Increasing factors from 100 to 127.\nAnswer: It could be that google colab is running out of ram
why?
because we are loading all data at once.or generating all data at once.
example :
google colab having 12 GB of ram. and it running out of ram.
So what i would suggest is:
we can process that data in chunks. if the total size of the data is 12 GB. than we can divide it into chunk(file) of 1 Gb.
12 GB data = 12 chunks(files) of 1 Gb
so now we have to load only 1 GB file into ram. which won't crash our notebook.",0.0,0.067638755,0.004575001075863838
80,"Question\ni got this message when i wanted run a beysian personalized ranking by GPU in colab, How can i resolve this problem?
message is :
GPU training requires factor size to be a multiple of 32 - 1. Increasing factors from 100 to 127.\nAnswer: On Colab a multitude of things could lead to crash. It's likely that you ran out of RAM or out of GPU memory.",0.0,0.3009786,0.09058811515569687
81,"Question\nI am relatively new to oop and had a quick question about the best way to approach coding a certain type of calculation. I'm curious if there's an established design pattern to approach this sort of problem.
Consider a chemical process flow where you convert materials (a,b) with attributes such as temperature, pressure, flow rate, etc. into a final product c. To get there, I need unit operations D,E,F... each with its own set of attributes (cost, size, etc.). I would only need information flow in one direction as closed loops will probably increase the complexity (if not, I would really appreciate insight into how closed loops would work).
a,b --> D --> E --> F --> c
Ultimately I would like to be able to do a system cost analysis, where I would sum up the cost attributes of D,E,F.
My current thought process to approach this is to define a ""materials"" object, then have D inherit materials, E inherit D... c inherit F then lastly a ""system"" object inherit c to analyze the system variables. Since I would like to be able to swap out D,E,F for say G,H,I, there also needs to be code for conditional inheritance where D must be able to accept inputs a,b (based on defined attributes) and E be able to inherit D for the same reason. One of the things I'm unsure of is how object c would be able to understand how to sum up attributes of all the inherited objects (probably based on some consistent naming convention of objects/attributes?).
Sorry for the somewhat lengthy question - if you are aware of AspenPlus, I'm looking to replicate a smaller scale version of this (ie no solvers) in Python. Thank you for reading through this!\nAnswer: I would argue that in your case functional programming is actually more suited than OOP since what it boils down to is a set of operations process on ""blank"" materials that results in a new material, well actually the same with different properties.
If I was restrained to OOP I would create different classes :

MaterialType which is basically a string or enum (of a, b & c)
ExternalProperties for temperature/pressure, etc.
Material which contains the Material_Type and various properties/functions aimed at transforming the material type so for instance it could contain a transform function with an unbounded list of ExternalProperties
Laboratory",0.0,0.2535457,0.06428541988134384
82,"Question\nSuppose I have a list of 100 numbers. I can find the mean by summing and dividing by the number of elements. But how can I find two values, one that gravitates towards the left of the list (assuming the list is ordered) and one towards the right, so that the list is equally divided into three blocks?
Sorting the array and taking the 33th and the 66th elements doesn't work because I could have all 1's before the 33th position and bigger values after, so the 33th position would be too early in the array. Those two'means' depend on the values of the array and not solely on the indices.
I'm sure what I'm trying to do has a proper naming but I can't really remember it now.\nAnswer: You could try numpy.quantile for example np.quantile(your_list, [0.33, 0.66]) I think should do the trick",0.0,0.09777081,0.009559131227433681
83,"Question\nFor example: I want to know wether 5 and 500 have a 1:100 ratio, I also want to know how I can see if they roughly have the same ratio or not, how do I do this??\nAnswer: If you need to know whether a/b and c/d are roughly the same ratio, then (in Python 3 only) you can do math.abs(a/b - c/d) < margin. The smaller the positive number margin is, the more close the ratios have to be for the expression to return True. margin = 1/100 would be within a percentage point.",0.10204082,0.21417844,0.012574846856296062
84,"Question\ni need to create random numbers in python, i have been using the random library, but is this library really random or is it just pseudo random?  and if it is pseudo random how can I get real random numbers in python?\nAnswer: All computer generated random numbers are pseudo-random.  If you want a more ""randomized"" version, you can use the secrets module instead of the random module.",1.0,0.39542067,0.36551615595817566
85,"Question\nI'm having a hard time connecting my facial recognition system (realtime) to the database.
I am using python language here. Try to imagine, when the system is doing REAL-TIME face detection and recognition, it will certainly form frame by frame during the process (looping logic), and I want if that face is recognized then the system will write 'known face' in the database. But this is the problem, because what if the upload to the database is done repeatedly because the same frame is continuously formed?
the question is, how do you make the system only upload 1 data to the database and if the other frames have the same image, the system doesn't need to upload data to the database?\nAnswer: you dont show any code, but to do what you're asking you want to have a flag that detects when a face is found and sets the variable. Then clear the variable once the flag leaves the frame. to account for false positives you can wait 4-5 frames before clear the flags and see if the face is still in the frame (i.e someone turns their head and the tracking looses the face)",0.0,0.13328385,0.017764585092663765
86,"Question\nI am using django-email-verification for sending verification link to email of the user but it tales time to send email, i want to send mail with celery for the same to speed it up, please guide me how can i add celery configs?\nAnswer: Celery isn't going to make this run any faster. What Celery will do for you is make the task asynchronous.",0.0,0.13601303,0.018499543890357018
87,"Question\nIn python, after I set some filter with warnings.filterwarning (or some package I import does), how can I access the list of active filters? I tried sys.warnoptions but it always gives me an empty list.\nAnswer: I found it looking at the source code. It's warnings.filters.",0.40816328,0.043080747,0.133285254240036
88,"Question\ndo I convert it a txt file? how do I inject the new line in between the other lines? I'm trying to inject a wallet address to a simple mining batch file without needing to physically open it prior.
pretty much the last step to automating my mining rigs for full self sufficiency.
if anyone has any way of doing this, please describe in full detail or show an example, as I am self taught and in way over my head for a project that's exceeding expectations before release lol\nAnswer: I would read in the entire file with f.readlines() so you get a list of strings (where each string represents a line in the file), write some logic that determines where the new string should go in between, and then re-write that to a file after.",0.0,0.07594794,0.005768089555203915
89,"Question\nI made an API for my AI model but I would like to not have any down time when I update the model. I search a way to load in background and once it's loaded I switch the old model with the new. I tried passing values between sub process but doesn't work well. Do you have any idea how can I do that?\nAnswer: You can place the serialized model in a raw storage, like an S3 bucket if you're on AWS. In S3's case, you can use bucket versioning which might prove helpful. Then setup some sort of trigger. You can definitely get creative here, and I've thought about this a lot. In practice, the best options I've tried are:

Set up an endpoint that when called will go open the new model at whatever location you store it at. Set up a webhook on the storage/S3 bucket that will send a quick automated call to the given endpoint and auto-load that new item
Same thing as #1, but instead you just manually load it. In both cases you'll really want some security on that endpoint or anyone that finds your site can just absolutely abuse your stack.
Set a timer at startup that calls a given function nightly, internally running within the application itself. The function is invoked and then goes and reloads.

Could be other ideas I'm not smart enough (yet!) to use, just trying to start some dialogue.",0.0,0.13710332,0.018797319382429123
90,"Question\nI was able to convert a.py file to and exe file,
however when I try to send it via Gmail, it detects as a virus.
Also, when trying to transfer the file on a USB flash drive, the computer says it's a virus.
Any ideas on how to fix this?\nAnswer: Apart from getting your exe signed (not really a viable option unless you're working on a big and important project) or writing the program in a natively compiled programming language like C, no, there is no way to avoid the detection since the Py2Exe converter you're using embeds the Python interpreter and all needed dependencies into the binary, which is a technique often used by viruses.
EDIT FOR:
I didn't actually get the fact that Gmail is the thing blocking the exe, not your AV. Well, as said by other comments, Gmail blocks certain files by default. Try adding the exe to a zip or rar archive and send that instead of the plain.exe.",0.0,0.25876606,0.06695987284183502
91,"Question\nI was recently trying to solve a data science test. Part of the test was to get the number of observations in a dataset for which the variable X is less than the 4th 5-quantile of this variable X.
I don't realy understand what they meant by the 4th 5-quantile! I tried using pandas df.quantile function but I wasn't able to figure out how to use it in my case\nAnswer: 4th 5-quantile translates value = data.quantile(4/5)",0.0,0.24653059,0.06077733263373375
92,"Question\nI have two applications that access the same DB. One application inserts data into a table. The other sits in a loop and waits for the data to be available. If I add a new connection and close the connection before I run the SELECT query I find the data in the table without issues. I am trying to reduce the number of connections. I tried to leave the connection open then just loop through and send the query. When I do this, I do not get any of the updated data that was inserted into the table since the original connection was made. I get I can just re-connect and close, but this is a lot of overhead if I am connecting and closing every second or 2. Any ideas how to get data that was added to a DB from an external source with a SELECT query without having to connect and close every time in a loop?\nAnswer: Do you commit your insert?
normally the best way is you close your connection, and it is not generating very overhead if you open a connection for the select query.",0.0,0.097344816,0.009476013481616974
93,"Question\nLet's say I have two complex images Z_1 and Z_2. I want to make a relative-phase map of the second image with respect to the first. This means:
Z_2_relative = Z_2 * np.exp(-1j * np.angle(Z_1))
This creates a new complex valued matrix where the complex-phase should now be given by
np.angle(Z_2_relative) == np.angle(Z_2) - np.angle(Z_1)
But according to python these two are not equal. I bet it has something to do with the np.angle function.. but I cant pinpoint it, or know how to fix it...
PS: Sorry, cant make a reproducible piece of code atm. Can do it later today\nAnswer: Bah.. stupid question. Sorry for anyone that read it. If you do module 2pi, then everything is the same",0.0,0.09492239,0.009010260924696922
94,"Question\nI'm attempting to thread a function call in my Python catastr^H^H^H^H^H^Hreation, and I've read up on how to use the threading.Thread() call.  My function takes a simple string argument, so theoretically it should be as easy as:
thread = threading.Thread(target = my_func, args = (string_var, ))
bearing in mind that the args() needs to be a tuple.  Got it.  However, it appears as though I'm still doing something wrong because I continually get the barffage from Python:
TypeError: my_func() takes 1 positional argument but 2 were given
I'm a bit stumped here.  Any guidance?
Thanks!\nAnswer: Seems the issue is that because it's a method (thanks gribvirus74 for the idea) and I'm attempting to thread it, it won't inherit the self.  And that appears to be the issue.  I moved the function outside of the class and called it with the Thread().  Works fine now.",0.0,0.26525688,0.07036121189594269
95,"Question\nI'm sorry for the title of my question if it doesn't let clear my problem.
I'm trying to get information from an image of a document using tesseract, but it doesn't work well on pictures (on print screens of text it works very well). I want to ask if somebody know a technique that can help me. I think that letting the image black and white, where the information I want is in black would help a lot, but I don't know how to do that.
I will be glad if somebody knows how to help me. (:\nAnswer: Using opencv might help to preprocess the image before passing it to tesseract.
I usually follow these steps

Convert the image to grayscale
If the texts in the image are small, resize the image using cv2.resize()
Blur the image (GaussianBlur or MedianBlur)
Apply threshhold to make the text prominent (cv2.threshold)
Use tesseract config to instruct tesseract to look for specific characters.
For example If the image contains only alphanumeric upper case english text then passing
config='-c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"" would help.",0.0,0.14957136,0.02237159200012684
96,"Question\nIn VS Code, many extensions, such as Tab Nine and Lint, rely on specific Python packages to function. On the other hand, the code I develop may need a different set of packages. Because there is the potential for package conflict and because we want the environment that we develop code to mimic the production environment,  it is convenient to have the dev environment/extensions use one Anaconda Environment and the code I develop to use a different Anaconda Environment. But I am not sure how to configure this. Can someone help?\nAnswer: Down in the bottom left corner of VS code you can manually select the python environment depending on which codebase you are working with. The selection can be saved in the settings.json file so you don't have to manually reselect each time.",0.0,0.3556738,0.1265038400888443
97,"Question\nSo I am trying to build a login dialog for my application using qt/python.
I got confused by QT's view/model architecture. QT provides models and view for tables, lists etc., but how do I go about implementing a login dialog using this architecture?
I use a ui file that I created with QtDesigner.
Should I create a user model that interfaces with the DB and retrieves user data, handling the login process, and return this result to the view? (view and controller combined, as per QT's terminology)
I would like to use the same architecture throughout the application, but I got confused with this one. Do I even need a model for this?\nAnswer: Models are for binding data to view. You can create list model with user/password fields and use QWidgetMapper to bind it to two QLineEdits but there's no much use in it since there's no much data and no complicated interactions.",0.0,0.27338088,0.07473710179328918
98,"Question\nSo im trying to simply just trying to make a directory with kivy and i simply cant get it to work. I a useing the os.mkdir() function.
I have tried to just simply os.mkdir(""a""), but i couldnt find any directory named a (after doing a full phone search, with the phone plugged in to my pc).
I have alos tried os.getcwd() as in os.mkdir(os.getcwd()+""a""), but to no avail.
To put it simply i am lost, can't find anything about it online either... So if you know how i would greatly appreciate to be enlightend on the subject, thx in advance.
And i am importing os, also tried to run os.mkdir before importing kivy.\nAnswer: I have tried to just simply io.mkdir(""a"")

There is no io.mkdir - do you mean os.mkdir?
The directory will be created inside the current directory of the script, which is a folder inside the app's private directory as defined by Android.
That means you should see your new directory in e.g. os.listdir(""."").",0.0,0.4690978,0.22005273401737213
99,"Question\nI want to deploy a flask app with http.server (ngnix not installed by admin). I want any user who logs into the cluster to access it. Is it possible?\nAnswer: HTTP server interfaces are visible to all users that are connected to a machine that has direct network access to the machine your server is running on.
If you need them to access the interface just provide the ip address and port where the server is running and the will be able to access it as users of the Flask app you are running. Just make sure you allow the users to access the needed resources.",0.40816328,0.2769516,0.017216501757502556
0,"Question\nFor establishing many-to-many relationships, django admin has a nice widget called 'filter_horizontal'.
The problem is - it only displays primary key of the related table.
How do I add other fields from the related table?
For example, if I have a many-to-many relationship in my Order model with User model, in 'Order' django admin I can only see User's primary key(id). How do I add their name into the widget?\nAnswer: Turns out it is changed in  str method for the User model",0.0,0.09872341,0.009746312163770199
1,"Question\nHey everyone I installed Python3.9 on my Mac using homebrew package manager but now I do not know how to install packages to it for use
Can anyone please tell me, thanks!\nAnswer: You should first do some research on the Python virtual environment, but the final answer to your question is to use pip install for installing Python packages. Be aware that there are other options out there, but pip is the most prevalent.",0.0,0.17261416,0.02979564666748047
2,"Question\ni need to halign text in kivymd button to the left and i found solutions for default kivy like text_size: self.size and halign='left', is button still a label in kivymd like in kivy? If no then how do you align text inside it?\nAnswer: Try the following:
text_align:'center'",0.0,0.15862095,0.02516060695052147
3,"Question\nI am using Windows subsystem for Linux WSL with the Ubuntu App (Ubuntu 20.04 LTS). I have installed Anaconda (Anaconda3-2020.11-Linux-x86_64) on my Windows 10 Education 1909. I have Jupyter notebook, and can run this in the Firefox on my computer and it seams to be working properly. However when I try to install packages such as:
Ubuntu console: pip install scrapy
Then the Jupyter notebook can not find it.
Jupyter notebook: import scrapy
I am currently working in the base environment, but I believe that Jupyter is actually running python from a different source (I also have Anaconda on my Windows).
I confirmed this by running:
import sys and sys.version both in the WSL and in the Jupyter notebook.
Jupyter notebook returns: '3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16) \n[GCC 7.3.0]'
WSL returns: '3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]'
confirming that the ""wrong python is used"".
I am hesitant to delete my Windows Anaconda since I have my precious environments all set up there and are using them constantly.
The spessific package that forces me to linux can be found at ""http://www.nupack.org/downloads"" but requires registration for downloads.
I do not have Anaconda or python in my Windows environment variables.
I would be happy If I either would know where to install my packages (as long as they are in Linux), or if someone knows how to force Jupyter to use the Anaconda from WSL.\nAnswer: Thanks to Panagiotis Kanavos I found out that I had both Anaconda3 and Miniconda3 installed and that the WSL command line used the miniconda3 version while Jupiter Notebook used Anaconda3.
There is probably a way of specifying which version to use but for me I simply deleted Miniconda and it now works.",0.0,0.013220966,0.00017479393864050508
4,"Question\nAn analytic task has been given to me to solve it by python and return back the result to the technical staff. I was asked to prepare the result in a jupyter notebook and such that the resulting code would be fully runnable and documented.
Honestly, I just started using jupyter notebook and generally found it pretty useful and convenient in generating reports integrated with codes and figures. But I had to go into some level of difficulty when I wanted to use specific packages like graphviz and dtreeviz, which was beyond doing a simple pip install xxx.
So, how should I make sure that my code is runnable when I do not know what packages are available at the destination Jupyter notebook of the next guy who wants to run it or when they want to run it using a Jupiter Lab? especially regarding these particular packages!\nAnswer: One solution for you problem would be to use docker to develop and deploy your project.
You can define all your dependencies, create your project and build a docker image with them. With this image, you can be sure that anyone who is using it, will have the same infrastructure like yours.
It shouldn't take you a lot of time to learn docker and it will help you in the future.",0.40816328,0.1904847,0.04738396406173706
5,Question\nI am creating an ERP web application using Django. How can i connect multiple apps inside of a project with one database. I am using the PostgreSQL database and also how can i centralized the database for all modules of ERP. How can i perform operations in other module and see if user is authenticated or not\nAnswer: Your apps use only the database(s) set up in your settings.py file.,0.0,0.24754804,0.061280034482479095
6,"Question\nI am looking for a method to get information of a ""trend"" regarding some hashtag/key word on Twitter. Let`s say I want to measure how often the hashtag/key word ""Python"" is tweeted in time. For instance, today, ""Python"" is tweeted on average every 1 minute but yesterday it was tweeted on average every 2 minutes.
I have tried various options but I am always bouncing off the twitter API limitations, i.e. if I try to download all tweets for a hashtag during the last (for example) day, only a certain franction of the tweets is downloaded (via tweepy.cursor).
Do you have any ideas / script examples of achieving similar results? Libraries or guides to recommend? I did not find any help searching on the internet. Thank you.\nAnswer: Try a library called:
GetOldTweets or GetOldTweets3
Twitter Search, and by extension its API, are not meant to be an exhaustive source of tweets. The Twitter Streaming API places a limit of just one week on how far back tweets can be extracted from that match the input parameters. So in order to extract all historical tweets relevant to a set of search parameters for analysis, the Twitter Official API needs to be bypassed and custom libraries that mimic the Twitter Search Engine need to be used.",0.0,0.14187938,0.02012975886464119
7,"Question\nI want to know how to get the most recent message sent in a channel of a discord server with discord webhooks in python? I have not tried anything yet.\nAnswer: Webhooks are only meant for sending messages, not reading messages in a channel. The only way to get the last message from the channel is if you have a bot user in the server that can read message history in that channel.",0.0,0.22921687,0.0525403767824173
8,"Question\nI am looking for a way to print out the raw http query string when I use the request library in python3. It's for troubleshooting purposes. Anyone has an idea how to do this. I tried to use prepared requests, but it is not what I'm looking for.Any suggestion?
thanks\nAnswer: import requests
response = requests.get('https://api.github.com')
You can use:
response.text or response.content or response.raw",0.0,0.044584394,0.001987768104299903
9,"Question\nI wanted to import the albumentations package to run a deep learning task, but it has conflicts and failed when I tried to install it in the current environment, so I used conda create --name to create a new one, and in the new environment the albumentations package is installed successfully, but I can not find it in the python interpreter setting, and the project keeps showing ""No module named 'albumentations' "", so, how to fix this problem?\nAnswer: I just tried create a new Base environment in the Setting-->Python Interpreter--> add --> Virtualenv Environment, and in the new Base environment, the newly created interpreter will appear below the Existing environment option.",0.0,0.11706936,0.013705235905945301
10,"Question\nI have a problem parsing DAG with error:
Broken DAG: [/usr/local/airflow/dags/test.py] No module named 'airflow.providers'
I added apache-airflow-providers-databricks to requirements.txt, and see from the log that:
Successfully installed apache-airflow-2.0.1 apache-airflow-providers-databricks-1.0.1 apache-airflow-providers-ftp-1.0.1 apache-airflow-providers-http-1.1.1 apache-airflow-providers-imap-1.0.1 apache-airflow-providers-sqlite-1.0.2 apispec-3.3.2 attrs-20.3.0 cattrs-1.3.0 clickclick-20.10.2 commonmark-0.9.1 connexion-2.7.0 flask-appbuilder-3.1.1 flask-caching-1.10.0 gunicorn-19.10.0 importlib-resources-1.5.0 inflection-0.5.1 isodate-0.6.0 marshmallow-3.10.0 marshmallow-oneofschema-2.1.0 openapi-schema-validator-0.1.4 openapi-spec-validator-0.3.0 pendulum-2.1.2 python-daemon-2.3.0 rich-9.2.0 sqlalchemy-jsonfield-1.0.0 swagger-ui-bundle-0.0.8 tenacity-6.2.0 termcolor-1.1.0 werkzeug-1.0.1
But the scheduler seems to be stuck:
The scheduler does not appear to be running. Last heartbeat was received 19 hours ago.
How can I restart it?\nAnswer: well after remove all deps in the requirements, the worker in mwaa run normally, now can try test the bad deps",0.0,-0.22878438,0.05234229192137718
11,"Question\nI am working on Google Colaboratory, and I have to implement OCL (Object Constraint Language), I searched a lot, but I didn't find how to implement it. Can someone give me an idea please?\nAnswer: It is surely possible for you to implement OCL, duplicating the efforts of one of the existing Open Source implementations such as Eclipse OCL or USE. There is an official OMG specification that will define what you need to do, however it has many deficiencies that will require research to solve and design around. I would be surprised if you can implement a 'full' implementation of OCL from scratch with plausible accuracy in less than a person year.
I suspect that you have mis-stated what you want to do or have misunderstood what someone has instructed you to do.",0.0,0.22818995,0.052070651203393936
12,"Question\nI want to know how I can send messages in discord, without creating a bot.
Like I want the program to send messages through my own account. Most of the results I got when I searched this up is to create a bot. But I would like to know if there's a way to do it without creating the bot. Thanks :)\nAnswer: To access discord with a bot trough your own account, you can't use a discord bot. What you could do, is to automate ""your input"" in discord. Imagine a google sheet for example and now recording your input to copy the first line, delete it afterwards, then paste it in discord and send the message. now you could repeat this for every line in the file.
(You can find such program using google)
BUT this solution restricts you to your input. Any events discord provides like on_member_join for example aren't useable for this approach. It's more a user bot than a discord bot",0.0,0.2572233,0.06616383045911789
13,"Question\nHow do I download QtDesigner for PyQt6? If there's no QtDesigner for PyQt6, I can also use QtDesigner of PyQt5, but how do I convert this.ui file to.py file which uses PyQt6 library instead of PyQt5?\nAnswer: You can install QtCreator or use command line pyuic6 -o main.py main.ui",0.40816328,0.046617627,0.1307152658700943
14,"Question\nwith tokenize I know you can split text into individual words, but I am confused on how to add characters to indicate the beginning and end of sentences after tokenizing. In my case I want to put ^ to indicate the beginning of the sentence and $ to indicate the end of the sentence. I am asking because I am trying to implement bigram probability models and this is for a school assignment, which is why this is a reinvent the wheel problem.\nAnswer: tokenize is a part of python distribution, and intended to parse python source code. Is this actually a good tool for your problem? Have you tried nltk?",0.0,-0.006367266,4.0542079659644514e-05
15,"Question\nI'm working on Flutter project in Android Studio platform and I faced a problem with how to write and run python API code inside my Flutter project without letting it as a backend code in another platform? since when I run my Flutter project that connected with python API code in another platform as a backend using post method, it's worked with the emulator but it does not work with my physical android device.
So is there any recommend solution for either the first problem or the second.
Thanks.\nAnswer: No it's not possible to write python code Inside Flutter code
But you can write your api in different framework like Django,mongodb and use it in your Flutter app",0.0,0.029621422,0.0008774286834523082
16,"Question\nI am currently starting a kind of larger project in python and I am unsure about how to best structure it. Or to put it in different terms, how to build it in the most ""pythonic"" way. Let me try to explain the main functionality:
It is supposed to be a tool or toolset by which to extract data from different sources, at the moment mainly SQL-databases, in the future maybe also data from files stored on some network locations. It will probably consist of three main parts:

A data model which will hold all the data extracted from files / SQL. This will be some combination of classes / instances thereof. No big deal here

One or more scripts, which will control everything (Should the data be displayed? Outputted in another file? Which data exactly needs to be fetched? etc) Also pretty straightforward

And some module/class (or multiple modules) which will handle the data extraction of data. This is where I struggle mainly


So for the actual questions:

Should I place the classes of the data model and the ""extractor"" into one folder/package and access them from outside the package via my ""control script""? Or should I place everything together?

How should I build the ""extractor""? I already tried three different approaches for a SqlReader module/class: I tried making it just a simple module, not a class, but I didn't really find a clean way on how and where to initialize it. (Sql-connection needs to be set up) I tried making it a class and creating one instance, but then I need to pass around this instance into the different classes of the data model, because each needs to be able to extract data. And I tried making it a static class (defining
everything as a@classmethod) but again, I didn't like setting it up and it also kind of felt wrong.

Should the main script ""know"" about the extractor-module? Or should it just interact with the data model itself? If not, again the question, where, when and how to initialize the SqlReader

And last but not least, how do I make sure, I close the SQL-connection whenever my script ends? Meaning, even if it ends through an error. I am using cx_oracle by the way


I am happy about any hints / suggestions / answers etc. :)\nAnswer: For this project you will need",0.0,0.12296152,0.015119535848498344
17,Question\nCreated a Flash Restful API with various end points for my website. Some endpoints need the users username and password to get user specific data. I’m currently sending these as parameters in the API call but I’m assuming this isn’t secure so how does one do this safely?\nAnswer: you can make a seperate api route that acts as a login and returns a sessionID/token on a successful login that can be used for authenticating to those endpoints you mentioned.,0.0,0.28647506,0.08206795901060104
18,"Question\nI have created an instance on GCP to run some machine learning model for an app I am working on for a little project. I want to be able to call one of the methods in one of the files from my app and I thought a Cloud Function would be suitable.
To make this question simpler, let's just imagine I have a file in my instance called hello.py and a method in this file called foo(sentence). And foo(sentence) simply returns the sentence parameter.
So how do I call this method foo in python.py and capture the output?
Thanks\nAnswer: At Google Cloud (And Google also), ""all is API"". Thus, if you need to access from a product to another one, you need to do this through API calls.
If your Cloud Functions needs to invoke a script hosted on a Compute Engine VM instance, you need to expose an API on this Compute Engine instance. A simple flask server is enough, and expose it only on the private IP is also enough. But you can't directly access from your Cloud Functions code to the Compute Engine instance code.
You can also deploy a Cloud Functions (or a Cloud Run if you need more system packages/libraries) with the model loaded in it, and like this perform all the computation on the same product.",0.81632656,0.2244891,0.3502715826034546
19,"Question\nI'm looking to keep my mouse events in sync with my objects after resizing the screen.
I'm told I need to create a data structure to keep track of:

Resizing events

New coordinates to match the resize


How can I accomplish this using simple algebraic equations and integrate it into a resize event for accurate updating?\nAnswer: Do it the other way around create a virtual game map, scale to the size of the window when drawing the scene and scale to the size of the virtual map when receiving an event.",0.40816328,0.106609344,0.09093477576971054
20,"Question\nAfter a click, a mini banner (or a container drawer) opens, which I should click on it, but I can't interact with it.
I was trying the ""driver.switchTo"" command but since there is no Iframe, I don't know how to do it.
the body is this:
  <a class=""option""> Go </a>\nAnswer: I've had issues with this in the past, and largely I couldn't figure out what to do either.  If you can just leave your computer alone during the process you can always try to use pyautogui to kind of cheat your way through it.",0.40816328,0.14366066,0.06996162980794907
21,"Question\nI am working on a Python Turtle Snake Program, and I want to make my Turtle longer.
By that, I mean that I increase the 1 Cube to 2 Cubes, then 3, etcetera, for my Snake Game.
Can you please inform how I could do that with turtle?
Thanks.\nAnswer: I once made a game like that. You can just create more turtles and use them as the body. Make the first turtle as the head of the snake. Then whenever it eats food,run a function that will create another turtle as a body component and put it in the previous position of head component. Repeat this everytime the snake eats a food.
I am not so good in explaining,so sorry for any inconvenience : )",0.0,0.16917765,0.02862107753753662
22,"Question\nI am very new to python and pygame and I'm currently making a very basic experiment with it. So far, I know how to draw an image on the screen, move it around, add a background, stuff like that. But what I have always been wondering is, how do I switch between screens in a game and how do I keep the code tidy when doing so?
Every time I try to add a new thing to my already unstable code, it takes only a few lines to break it and it becomes a confusing mess.
If for example, I want a start screen that shows the title and a ""press any key to continue"" kinda thing, how do I do it?\nAnswer: A quick fix:

Make the entire screen white and then draw the second screen onto the first one.
Then when you need the other screen, just refill the screen with black and then continue.
This can be achieved by putting the screens in their own separate functions.

Let me know if this helps out",0.0,-0.14437455,0.02084401063621044
23,"Question\nI have been playing around with django channels + angular. I have created an app that simply sends notifications to front end with a counter 1,2,3,4. It works fine, except if I open the page in multiple tabs. I am also not able to disconnect from the websocket, I can use unsubscribe but it does not really  close the connection but that's more kind of a angular question. Anyways how can I make my socket multithread, So if I make multiple requests from the same computer but from different tabs it will work and 2 different instances of the consumer will be created therefore, If I load 2 pages in the same computer the counter should be different independently increasing counter. Do I need redis for that?\nAnswer: my url router was missing.as_asgi()
this worked:
URLRouter([path('wscrawpredict', CrawPredictConsume.as_asgi(),name=""wscraw"")])",0.0,0.05709973,0.0032603791914880276
24,"Question\nSo the default of anaconda is python 3.8, but you can invoke python2 by running python2 a_py_script.py. The issue comes from the fact that you'll need to import things (say biopython), and you can't import them as any conda install -c conda-forge biopython or pip install biopython will be understood to automatically slot it into python3.8 packages exclusively.
I ask this because I have some python2.7 scripts that demand packages outside the default install scope and ideally I'd like to do this without having to create a new python=2.7 env and track down everything I need.
I've tried pip2.7 install biopython and python2.7 -m pip install biopython to no avail. Could it be that I technically don't have python 2.7 even though I'm able to invoke it from command line via python2 because python3 just naturally has some special limited backwards compatibility to run my python2 scripts? (I did notice that conda list includes only 3.8 and no mention of 2.7)
I've tried cloning my env but I don't know how to do it in such a way that swapts just the version of python. conda create --name py27test --clone base python=2.7 says too many arguments. I'd like to know if this is even advisible as my base environment I would presume is entirely built off of v3.8 so swapping out the python versions will just be bad time hence why this seems impossible?\nAnswer: You can't mix Python versions in a conda environment. You can call Python executables from outside your environment, but that's unadvisable for anything requiring dependencies.  If you must use Python 2.7 and need dependencies installed, that needs to be done in a contained environment, one that does not mix Python 3 packages into it.
If you care about using your Python 2.7 scripts long-term, you should consider migrating them now; using unsupported software is only going to get harder over time.",0.0,0.35726446,0.12763789296150208
25,"Question\nHi so I have been working on a notebook the last few days and showed it to my advisor yesterday and we walked through it together. I tried to start working on the project this morning and cannot find the file that I was working on. What is strange is that the directory that I was working in says it was last modified yesterday but when I look through the directory the file I am looking for cannot be found. I know that you are probably thinking ""this ding deleted the file on accident"" and although I really dont know how that could have happened, that is one suspicion of mine, but when looking at https://stackoverflow.com/questions/38819322/how-to-recover-deleted-ipython-notebooks they mention that it should go to trash for my version of jupyter notebook upon deletion.
I am asking if there is any way to possibly get the file back? or anywhere I can look for the file? I have looked in my trash can but it is not there.
MacOS Big Sur 11.2.3
Jupyter NoteBook 6.1.5
Conda Version: 4.9.2
Conda-build version: 3.20.5
Python: 3.8.5.final.0\nAnswer: I do not know why this was the case but here is:
I could not find the file of interest. Did all methods from before in above link, once I looked in the icloud Desktop in my finder it suddenly appeared in the normal desktop directory. Idk why, but if this happens to you, check the icloud directory corresponding to the directory you are in and it may appear in the corresponding normal directory after. Lesson learned: do some version control.",0.0,0.14022362,0.019662663340568542
26,"Question\nI'm making a simple project where I will have a downloadable scraper on an HTML website. The scraper is made in Python and is converted to a.exe file for downloading purposes. Inside the python code, however, I included a Google app password to an email account, because the scraper sends an email and I need the server to login with an available Google account. Whilst.exe files are hard to get source code for, I've seen that there are ways to do so, and I'm wondering, how could I make it so that anyone who has downloaded the scraper.exe file cannot see the email login details that I will be using to send them an email when the scraper needs to? If possible, maybe even block them from accessing any of the.exe source code or bytecode altogether? I'm using the Python libraries bs4 and requests.
Additionally, this is off-topic, however, as it is my first time developing a downloadable file, even whilst converting the Python file to a.exe file, my antivirus picked it up as a suspicious file. This is like a 50 line web scraper and obviously doesn't have any malicious code within it. How can I make the code be less suspicious to antivirus programs?\nAnswer: Firstly, why is it even sending them an email? Since they'll be running the.exe, it can pop up a window and offer to save the file. If an email must be sent, it can be from the user's gmail rather than yours.

Secondly, using your gmail account in this way may be against the terms of service. You could get your account suspended, and it may technically be a felony in the US. Consult a lawyer if this is a concern.

To your question, there's basically no way to obfuscate the password that will be more than a mild annoyance to anyone with the least interest. At the end of the day, (a) the script runs under the control of the user, potentially in a VM or a container, potentially with network communications captured; and (b) at some point it has to decrypt and send the password. Decoding and following either the script, or the network communications that it makes will be relatively straightforward for anyone who wants to put in quite modest effort.",0.0,0.36955023,0.1365673691034317
27,"Question\nI'm making a simple project where I will have a downloadable scraper on an HTML website. The scraper is made in Python and is converted to a.exe file for downloading purposes. Inside the python code, however, I included a Google app password to an email account, because the scraper sends an email and I need the server to login with an available Google account. Whilst.exe files are hard to get source code for, I've seen that there are ways to do so, and I'm wondering, how could I make it so that anyone who has downloaded the scraper.exe file cannot see the email login details that I will be using to send them an email when the scraper needs to? If possible, maybe even block them from accessing any of the.exe source code or bytecode altogether? I'm using the Python libraries bs4 and requests.
Additionally, this is off-topic, however, as it is my first time developing a downloadable file, even whilst converting the Python file to a.exe file, my antivirus picked it up as a suspicious file. This is like a 50 line web scraper and obviously doesn't have any malicious code within it. How can I make the code be less suspicious to antivirus programs?\nAnswer: Sadly even today,there is no perfect solution to this problem.

The ideal usecase is to provide this secret_password from web application,but in your case seems unlikelly since you are building a rather small desktop app.
The best and easiest way is to create a function providing this secret_password in a separate file,and compile this file with Cython,thing that will obcufate your script(and your secret_password) at a very good extend.Will this protect you from lets say Anonymous or a state security agency?No.Here comes the reasonable thinking about how secret and important really your password is and from who you mainly can be harmed.
Finally before compiling you can'salt' your script or further obscufate it with bcrypt or other libaries.

As for your second question antiviruses and specifically windows don't like programms running without installers and unsigned.
You can use inno setup to create a real life program installer.
If you want to deal with UAC or other issues related to unsigned programms you can sign your programm(will cost money).",0.20408164,0.37926507,0.030689233914017677
28,"Question\nHow do I modify a np array based on the current value and the index? When I just want to modify certain values, I use e.g. arr[arr>target_value]=0.5 but how do I only modify the values of arr > target_value where also the index is greater than a certain value?\nAnswer: For that very specific example you would just use indexing I believe
eg arr[100:][arr[100:] > target_value]=0.5
in general it could be conceptually easier to do these two things separately. First figure out which indices you want, then check whether they satisfy whatever condition you want.",0.0,0.27599192,0.07617153972387314
29,"Question\nPrevious version of code wrote fine with Python 2.7 to AWS MySQL Version 8 with the following:


""""""INSERT INTO test_data(test_instance_testid,
meas_time,
data_type_name,
value,
corner_case,
xmit,
string_value)
VALUES('15063', '2021-03-19 20:36:00', 'DL_chamber_temp', '23.4',
'None', 'None', 'None')""""""

But now, porting to Python 3.7 to the same server I get this:

pymysql.err.InternalError: (1366, ""Incorrect integer value: 'None' for column 'xmit' at row 1"")

This makes sense since it is a str value 'None' and not Python type None (although it used to work).
It is legal to fill these columns as NULL values--that is their default in the test_data table.
If I change the code and set the values to Python None, I get a different error which I don't understand at all:

""""""INSERT INTO test_data(test_instance_testid,
meas_time,
data_type_name,
value,
corner_case,
xmit,
string_value)
VALUES('15063', '2021-03-19 20:36:00', 'DL_chamber_temp', '23.4',
None, None, None)""""""


pymysql.err.InternalError: (1054, ""Unknown column 'None' in 'field list'"")

I really appreciate any help or suggestions.
Thanks, Mike
Thanks for the help! Yes, NULL does work, but I'm stuck on how to handle value types on the fly within my method. Depending on the call I need to write a quoted value in one case and non-quoted NULL on others. For some reason my old code (illogically!) worked. I've tried everything I can think of without any luck.
Any thoughts on how to do this?\nAnswer: make default value as NULL at PhpMyAdmin",0.0,0.03450823,0.001190817798487842
30,"Question\nDoes anyone have any good suggestions for where I can store my custom Python modules on Google Cloud Platform?
I have a bunch of modules that I would like to access from the different GCP services I am using (App Engine, Compute Engine, Cloud Functions etc), without having to copy the Python files and upload to the service's Python environment each time.
I was thinking GCS could be an option but then I am not sure how I would then get the module into, say Cloud Functions or App Engine?
Any ideas?\nAnswer: The code will eventually need to be written to your service's local storage. Python does not access code remotely during execution unless you write your code to do so (download the module and then execute). Package your code as modules and publish to PyPI and then add them as dependencies. When you deploy a service, your modules will be downloaded.",0.40816328,0.21743095,0.03637882322072983
31,"Question\nCan you help with a definitive answer for MAC and PC.
I have come across similar questions to this quite a lot on stackoverflow where a user will be using an editor such as IDLE or ATOM and they will get the module not found error, then they will go to terminal or command line prompt and try to pip install the module, and it will either install the module or say requirement already satisfied.
On returning to the editor and running the script it will still give the module not found error.
I have seen the same question, but often asked about for specific modules.
This error started happening a lot for me when I was required to install anaconda for a course and was required to use other editors, but had previously been using a download of Python and working with IDLE.
I often go between a MAC and a PC, where I have got the same issue, where I have an instance of anaconda installed and another instance of python installed.
It is very confusing how to untangle the different paths where some have modules and other don't.
Could someone give some advice on how to rectify this?
I wonder if there is a solution where I could do a pip install that would globally update all versions of python with a module?\nAnswer: This could happen if you have multiple versions of Python installed on your computer. For example you have a package installed on Python 3.5 but you run your script on Python 3.8. On cmd or Powershell you could try something like py -3.8 -m pip list or py -3.5 -m pip list to check which libraries you have installed on each version of Python. Then if the library is missing from the version that you used on your script you can install it specifically for this version using something like py -3.5 -m pip install library_name.",1.0,0.35024214,0.4221852719783783
32,Question\nis it possible to call a web service from cooja? May be I can read from border-router then call web service (via python script for example). I can ping border-router but I dont know how to read from node or write to node in cooja.I am new to contiki-ng and cooja. thanks in advance\nAnswer: You can try the websense example at the folder ~/contiki/examples/ipv6/sky-websense,0.0,-0.25772133,0.0664202868938446
33,"Question\nFor all those who are not familiar with ren'py: It's basically python with some modifications. Since the project is coded in python, which can easily be edited by anyone it is not a good idea to include a license validation in the python files.
An executable starts the game, so I thought about wrapping it with a license validation in an.exe (but I honestly don't know how I can take an executable, put some code around and have one executable including the actual one). Maybe there is another way, which is safer than the one I named, suggestions?\nAnswer: You don't. Ren'Py only features basic encryption to prevent players from accidentally deleting/modifying files.
As security, game encryption isn't a fight worth picking. You have to decrypt the files to run them and that will always be a weak point to exploit. Anything you put on top is just delaying whoever wants in. You can write your game in binary and it will do exactly squat to someone who really wants to take it apart.
Ren'Py is designed to be mod friendly. Nothing you do will stop someone from dropping a rpyc file into the game directory and hooking into the game. Even if you modify the engine to only read specific files, you won't stop someone who can just insert the functionality back in. All you're really doing is making it more difficult to preserve the game after you're dead.
Nintendo can't stop people from extracting assets from their games. You don't stand a chance. You should hope to be so lucky as to have people interested enough in your game to want to mess around with the assets and code.
If you're talking about a license players need or some sort of login mechanism, you need to implement an online server to validate the credentials they input. There is no secure front-end way to validate credentials.",1.0,0.4740715,0.27660077810287476
34,"Question\nI wanna install three packages in anaconda, but I face the following error.
 Error: Could not install packages due to an OSError: [Errno 2]  No such file or directory: 'c:\\programdata\\anaconda3\\lib\\site-packages\\numpy-1.20.1.dist-info\\METADATA'.
I have changed HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystem@LongPathsEnabled to 1, but it does not still work. There are some similar problems with pip and some solutions are provided but I could not apply them. Is there any difference between the pip\\METADA error and numpy\\METADATA error?
I have basic knowledge and I would be thankful if someone can explain how to fix it with details. For example, some people suggested installing in another directory, but I do not know how to do this.\nAnswer: You could remove the folder of ""numpy-1.20.1.dist-info"" in error information firstly, and try install again.",0.0,0.14126527,0.01995587721467018
35,"Question\nIn my admin.py, I have a lot of ModelAdmin classes and I want them to all override a function, has_module_permission. I want to base the module permission off of the user's permissions for the model corresponding to the ModelAdmin, and to do that I need to know the model name in order to run request.user.has_perm('app.add_<model>') or something similar.
I think an easy way of doing this is to create a class that has this function which is somehow able to get the model name, and then have each of the ModelAdmin classes inherit from that. The problem is that I don't know how to access the model's name from ModelAdmin. What's the best way of doing this?\nAnswer: If you just need the the model's name from ModelAdmin you can simply do the following: self.model.__name__",0.0,0.47985345,0.23025932908058167
36,"Question\nI'm lost with Python. After troubleshooting with a friend through Discord for literally months I am about to give up. This is my last attempt at trying to get Python fixed on my Windows 10 laptop.
I'm using a Lenovo Legion 5, bought the laptop in November 2020. I've not been able to get anything related to Python to run in the CMD window. I can run Python no problem, but nothing I have installed through pip has ever worked. I can use virtualenvs, but only through PyCharm for example. Python has never really worked through the command line.
Yes I tried reopening the CMD window, rebooted the system many times, ran the CMD as administrator, installed the path variables for both Python and esptool.py but nothing seems to help.
I honestly don't know where to start because none of the 250+ websites I've visited to so far that suggested a fix for any kind of issue I've been experiencing with Python has been working. I can run Python fine by the way, just none of the things installed through pip will work.
Let's start with a use-case:
I'm trying to run esptool.py so installed it with pip install esptool. This install worked fine, and I can confirm it is installed with pip show -f esptool.
However, when running esptool.py version it told me:
'esptool.py' is not recognized as an internal or external command, operable program or batch file.
So I added the local folder from the previous step to the %PATH% variables, after running esptool.py version it gave me a popup asking me with what kind of program I should open this, I didn't select to open with this kind of program from now on. This makes it so that I do not get an error, what now happens is that another window quickly opens and then exits without an error code. So I have no clue what's happening.
What should happen is that it should tell me which version is installed in the CMD window.
There have been a few other things going on with my Windows 10 install, for one, the username that I used during the installation wasn't used to create the user directory. Windows 10 somehow instead chose a name that was related to the first 5 characters of my email address, which is totally strange as I haven't used that string in the installation of Windows 10 at all.",0.0,0.10785097,0.011631831526756287
37,"Question\nI'm a student. For my Distributed Systems project, I'm expected to create a gRPC project. I'm creating the project in Eclipse.
Two service implementation are to be coded in Java and the other is to be done in another coding language.
I've tried searching for help online but the results I'm getting are related to gRPC and how gRPC works, not about the coding or using other coding.
Ideally, I would like to use Python as the other language and to create it in Eclipse if possible. Does anyone have any information, documentation or examples I could look at, so I could can reference it?
I am able to see online searches for both Java and Python, but I'm not sure how to use both in one project.
Thank you.\nAnswer: So I got a lot of feedback for lecturers and classmates.
The server doesn't care what programming language is used.
So Java, Python, Node.js etc, could all be sent to the server.
A generalised simplistic idea of how I was able to understand is: Python converts its code to binary and sends it to the server. Same with Java and Node.js.
I don't know why, but I was digging myself deeper trying to figure out what code (i.e. the binary) that needed to be the communication between the server and code. I was trying to encapsulate Python into Java and vice versa.
Why did I think this? Your guess is as good as mine.",0.0,0.21069491,0.04439234361052513
38,"Question\nI was trying to apply a deep learning algorithm(CNN) in python but after separating training-testing data and transforming time series to image step my Colab Notebook crashed and restarted itself again.
It gives an error like ""Your session crashed after using all RAM"" and when I checked app.log I saw something about tcmalloc: large alloc. I didn't find anything to fix this crashed.
Do you have any idea how to prevent this warning and fixed this situation?\nAnswer: Your session ran out of all available RAM. You can purchase Colab Pro to get extra RAM or you can use a Higher RAM machine and use the Neural Network there",1.0,-0.09185153,1.1921396255493164
39,"Question\nI'm contributing to a project that is using sqlalchemy. This project has a model.py file where they define all their classes, for example Foobar(BASE). Now, I've created another module mymodel.py where I need to extend some of those classes. So for example, in mymodule.py I have Foobar(model.Foobar)   which I use to extend the parent class with new properties and functions. The problem is that when using either of those classes, I get this error from sqlalchemy: sqlalchemy.exc.InvalidRequestError: Multiple classes found for path ""Foobar"" in the registry of this declarative base. Please use a fully module-qualified path..
My question then is, how can I fix this error without renaming my classes? Since they are defined in a different module, how do I use a ""fully module-qualified path""?\nAnswer: As stated by SQLAlchemy there a two classes found named Foobar. One is model.Foobar and the second one is mymodel.Foobar. You need to use the fully module-qualified path which is mymodel.Foobar to reference the new class.",0.0,0.31752503,0.10082214325666428
40,"Question\nHello I know that the key to analyzing data and working with artificial intelligence is to use the gpu and not the cpu. The problem is that I don't know how to use it with Python in the visual studio code, I use Ubuntu, I already have nvidia installed\nAnswer: You have to use with the libraries that are designed to work with the GPUs.
You can use Numba to compile Python code directly to binary with CUDA/ROC support, but I don't really know how limiting it is.
Another way is to call APIs that are designed for parallel computing such as OpenCL, there is PyOpenCL binding for this.
A bit limiting, but sometimes valid way - OpenGL/DirectX compute shaders, they are extremely easy, but not so fast if you need to transfer data back and forth in small batches.",0.40816328,0.36258328,0.0020775364246219397
41,"Question\nI would like to know how to convert annotations in YOLO format (e.g., center_X, center_y, width, height = 0.069824, 0.123535, 0.104492, 0.120117) to x1, y1, x2, y2 coordinates?\nAnswer: Given that the upper-left corner of the image is [0,0]: For the upper-left corner you have to do [x,y] = [center_X, center_Y] - 1/2 * [width, height]. For the bottom-right corner [x,y] = [center_X, center_Y] + 1/2 * [width, height].",0.20408164,0.20085609,1.0404172826383729e-05
42,"Question\nIt's probably a dumb question but i cant find the answer anywhere. So, I have made a simple site with Flask and it have a database in SQL(SQLite3). I'have never uploaded a site before and I don't know how to get the data after deployed. Plz help.\nAnswer: If your code already works locally, make sure it has a relative path like './data/sqllitedb.db'.
Put it in a folder that isn't available to access from your website.
When you deploy to your website, it should use the same relative path.
Sqlite is great because it is just a local file and as long as you use relative paths to your main site, you should be able to access it",0.0,0.1582228,0.025034451857209206
43,"Question\nI am using Python to automate some installations in my everyday workflow.
The installation requires mounting a.dmg file to the system in order to start and complete the installation. Everything works fine until I try to eject/unmount the attached volume, it gives an error that the volume is used by Python and cannot be ejected/unmounted. The installation process is already completed by the time the unmount is executed, so in theory, files should no longer be in use.
Force unmount helps with the unmount process, but for some reason, it interferes with subsequent subprocess.Popen command that starts the installed application and the app crashes at startup. The crash doesn't occur if the volume is not unmounted, which is a sign the issue is caused by the unmount process.
I would like to try to unmount the volume without forcing the process, but I don't know how to unlock the files being used by Python for the installation. Is there a way to force python to unlock those files?
Thanks in advance.\nAnswer: The issue was related to the fact the current working directory was set to a folder inside the mounted volume. Switching the CWD to HOME before unmounting process and subprocess execution fixes the issue.",0.0,0.19849896,0.039401840418577194
44,"Question\nI have a problem: we are using a package that is not maintained for a while now. So we forked it in order to maintain it ourselves. The package already exists lets say it is named package_a. Most of the code and the __init__ are in the package_a/ folder.
Now we want to make our own package that will include our maintained code and we want to name is package_b. So far so good but the problems is that package_b wants to have the code and the __init__ in package_b/ folder and github changes the contributions for all files when a folder is renamed. And I would like that credit for contributions stays where it is due, the 10k+ lines of code didn't just appear in my local repo out of thin air. Any suggestions how we can have package named package_b but keep the code in the original folder package_a/?
I am thinking along the lines of trying with some clever way of importing package_a into package_b or something along the line but I hope for a definite answer.\nAnswer: Instead of copying the code or trying to import A into B, extract the common code into a 3rd package which both A and B import. Or perhaps a subclass. This doesn't solve your contribution problem, but it does avoid making a big maintenance hassle by copying and pasting 10,000 lines of code.

Git doesn't record copies and renames, but it can recognize when they happen. To give Git the best chance of recognizing a copy, do only the copy in its own commit. Make no changes to the content. Then in a second commit make any necessary changes to the copied code.
In normal Git you can nudge git log and git blame to honor copies and renames with -C. Git doesn't do this by default because it's more expensive.
Github will do what Github will do.
Regardless of who Github says who wrote what line their contributions will still be in the project history. That's how it goes. You make your contribution and then others put their own work on top of it. This is normal. Their contributions remain in the history.
""History sheer"" is also normal, that's when a change touches many lines but is otherwise insignificant. For example, if you were to restyle the code that would cause a history sheer. git blame will say that was the last commit to touch the code. git blame -w mitigates",0.81632656,0.40447283,0.1696234941482544
45,"Question\nSorry for my stupid question, but i dont understand how to add all permissions in my voice channel for one user. I have this await channel.set_permissions(member, some_permission), but i dont know what permission i need to use\nAnswer: You should just make a role with those permissions and then give that role to the user. for example, await member.add_roles(var) var being the role and member being the member you want to add the role too.",0.0,0.16481495,0.027163967490196228
46,"Question\nI trained a chatbot in a file on my computer. I have everything else for my website up and running but I cannot get the python file into the site. Do you have any advice for how I can make an html messenger that uses user inputs and converses with the AI chatbot that way? Thank you!\nAnswer: One easy way to do this is by building a server, where you can have your chatbot code. You can use Flask. The instant the HTML asks a question, you send an API call with the required message, which when received by the API runs that message through the chatbot and returns the response in the output. This can continuously go on in the chat.",0.40816328,0.16074544,0.061215586960315704
47,"Question\nBackground
I have a google sheet, who's data I need to process on my local system. The nature of processing is very tedious and long so I wrote a script for it.
Problem
I need to access the google sheet through the python script to process it further. Online it is mentioned that to read the private google sheet directly, I need to create a GCP project and within that project, I need to create a service account. After that I should download the credentials and share the google sheet with that service account email.
Two problems here are :

Downloading the credentials -- insecure and my organization prohibits it.
Sharing of google sheet with service account email. organization also prohibits sharing sheet with outside organization emails.

What I found as a solution
I came across a solution of impersonating a service account but I could not find anything as to how can I do that. (Would appreciate any insights on that). All the other solutions suggested to download credentials which is a big NO.
For the sharing of sheets thing, I guess we can use drive API, but same problems are with that.
I tried using gcloud auth login and gcloud auth application-default login but was getting errors

Request had insufficient authentication scopes."". Details: ""Insufficient Permission: Request had insufficient authentication scopes.

using ['https://www.googleapis.com/auth/spreadsheets',  https://www.googleapis.com/auth/drive'] as scopes
What I need? (Summary)
How to access google sheets API (or download the Sheet from google drive) without downloading any sort of credentials.json.\nAnswer: problem
Your only option to access private user data is to be authorized as a user who can access the file. Either logging in using Oauth2 or using a service account

Downloading the credentials -- insecure and my organization prohibits it.

In order to use a google api you must first register your application on google developer console and download the client credentials then a user must authorize the application using Oauth2, which would mean both downloading the credentials.json file from google developer console and you getting user token credentials from the authorized user.

Sharing of google sheet with service account email. organization also prohibits sharing sheet with outside organization emails.

In order to use a service account you would again need to first register your application on google developer console and download the client credentials for the service",0.40816328,0.53598607,0.016338665038347244
48,"Question\nI'm working on a NLP classification problem over a large database of emails (~1 million). I need to use spacy to parse texts and I'm using the nlp.pipe() method as nlp.pipe(emails,n_process=CPU_CORES, batch_size=20) to loop over the dataset.
The code works but I'm facing a (maybe not so)weird behavior:
the processes are being created but they are all in SLEEP state but one, casually some of them go in RUN state for a few seconds and then back to sleep. So I find myself with one single process using one core at 100% but of course the script not using all the CPU cores.
It's like the processes don't get ""fed"" input data from pipe.
Does anybody know how to properly use spacy nlp pipe or anyway how to avoid this situation? no way to use nlp.pipe with the GPU?
Thank you very much!
Sandro
EDIT: I still have no solution but i've noticed that if I set the batch_size=divmod(len(emails),CPU_CORES), the processes all starts running at 100% CPU and after a few seconds they all switch to sleeping state but one. It really looks like some element in spacy pipe gets locked while waiting for something to end.... any idea??
EDIT2: Setting batch_size=divmod(len(emails),CPU_CORES) while processing a large dataset leads inevitably to a spacy memory error:

MemoryError: Unable to allocate array with shape (1232821, 288) and data type float32

*thing that is maybe not so weird as my machine has 10GB of RAM and (1232821×288×32)bits / 8 = 1.4GB multiplied by 6 (CPU_CORES) leads to a 8.5GB of RAM needed. Therefore I guess that, having other stuff in memory already, it turns out to be plausible. *\nAnswer: Ok, I think I found an improvement but honestly the behavior it's not really clear to me. Now the sleeping processes are way less, with most of them stable running and a few sleeping or switching between the two states.
What I've done was to clean and speedup all the code inside the for loop and set the nlp.pipe args like this:
for e in nlp.pipe(",0.0,-0.35110188,0.12327252328395844
49,"Question\nI'm working on a NLP classification problem over a large database of emails (~1 million). I need to use spacy to parse texts and I'm using the nlp.pipe() method as nlp.pipe(emails,n_process=CPU_CORES, batch_size=20) to loop over the dataset.
The code works but I'm facing a (maybe not so)weird behavior:
the processes are being created but they are all in SLEEP state but one, casually some of them go in RUN state for a few seconds and then back to sleep. So I find myself with one single process using one core at 100% but of course the script not using all the CPU cores.
It's like the processes don't get ""fed"" input data from pipe.
Does anybody know how to properly use spacy nlp pipe or anyway how to avoid this situation? no way to use nlp.pipe with the GPU?
Thank you very much!
Sandro
EDIT: I still have no solution but i've noticed that if I set the batch_size=divmod(len(emails),CPU_CORES), the processes all starts running at 100% CPU and after a few seconds they all switch to sleeping state but one. It really looks like some element in spacy pipe gets locked while waiting for something to end.... any idea??
EDIT2: Setting batch_size=divmod(len(emails),CPU_CORES) while processing a large dataset leads inevitably to a spacy memory error:

MemoryError: Unable to allocate array with shape (1232821, 288) and data type float32

*thing that is maybe not so weird as my machine has 10GB of RAM and (1232821×288×32)bits / 8 = 1.4GB multiplied by 6 (CPU_CORES) leads to a 8.5GB of RAM needed. Therefore I guess that, having other stuff in memory already, it turns out to be plausible. *\nAnswer: I've found that using n_process=n works well for some models, like en_core_web_lg, but fails with others, such as en_core_web_trf.
For whatever reason, en_core_web_trf seems to use all cores with just a batch_size specified, whereas en_core_web_lg uses just one unless n_",0.20408164,-0.84392995,1.0983282327651978
50,"Question\nGiven an arbitrary array with real numbers as entries, I want to return an array with the same shape, whose entries are the average over the closest neighbours of the original array.
What I mean by this in the case of a given array of dimension 2, is that if the array has shape (n,m) with entries a_{i,j}, then on the entry (i,j) the value of the new array should be:
average(i,j)=1/4 (a_{i+1,j} + a_{i-1,j} + a_{i,j+1} + a_{i,j-1}),
where the first index is taken mod n and the second mod m.
I would like to create a function whose argument is an arbitrary array, and returns an array of the same shape and entries the averages over the entries on the closest neighbours of the given array (for a d-dimensional array there are 2d closest neighbours, obtained by summing +1 and -1 on each index)
I know how to do this for a fixed dimension d (just generalising the above equation), using d nested for loops, but I don't know how to do it when the dimensions are not fixed.\nAnswer: Scipy has a scipy.ndimage.convolve function, which can do exactly this. it takes the array, and a matrix of values to multiply the neighbors with. It should work for any number of dimensions.
However if you are trying to write this function manually, I'd suggest trying an iterative or recursive approach, where each iteration or layer of recursion handles a dimension. In a 3D case, you would first handle the first dimension, and have the one sixth of the value of the neighbors in that dimension. The next iteration would do the same for dimension 2, etc.
The factor to multiply each neighbor with is 1/(2n), since each entry has 2 neighbors in each dimension. This should scale up to any arbitrary number of dimensions.",0.0,0.23651302,0.05593840777873993
51,"Question\nI am confused as to how to get into script mode in Python's standard IDLE. I surprisingly cannot find how to get into it on Google.\nAnswer: I believe you mean the simple editor that ships with Python, ""IDLE""?
If you have an IDLE console window open already, go to file->New File, or press ctrl+N to get a window where you can write and save a python file like a text editor.
When you've written a script in that editor you can save it as you'd expect from File->Save and run it from Run->Run Module, or press F5 as a quick shortcut. It should then output any text to the IDLE console window.
Also see the comments directly beneath your question for some resources :) Happy scripting!",0.0,0.21942997,0.048149511218070984
52,"Question\nimport yfinance as yf
terminal: ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behavior is the source of the following dependency conflicts.
yahoo 0.1 requires requests==1.1.0, but you have requests 2.25.1 which is incompatible.
how can I address this issue
Regards! :)\nAnswer: The problem is that yfinance requires a newer version of requests than yahoo. For me it said the same thing as you got when I tried.
Importing yfinance worked for me.
I didn't specify anything for requests and just installed the above. Generally, you can specify installation version using =x.x at the end, i.e. pip install requests==1.1.0.",0.0,0.53941536,0.2909689247608185
53,"Question\nimport yfinance as yf
terminal: ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behavior is the source of the following dependency conflicts.
yahoo 0.1 requires requests==1.1.0, but you have requests 2.25.1 which is incompatible.
how can I address this issue
Regards! :)\nAnswer: It sounds like you have both yahoo and yfinance installed in the same environment/setup. I suggest creating a virtual environment with virtualenv so you can have the appropriate requests for each libraries.
Alternatively you can submit a PR for yahoo to use a more recent version of requests",0.0,0.3673687,0.13495975732803345
54,"Question\nI am trying to code a function that will accept a list and then return the length of the list times the largest integer (not float) in the list, if the list does not contain an integer, it needs to return an empty string.
I had been trying to use the max function but I cannot work out how to only call for the largest integer not float, or is there a way to sort the list first to only look at integers?\nAnswer: you might be able to use an if statement to check if the number is an interger",0.0,-0.037062585,0.0013736352557316422
55,"Question\nRobot Framework supports two ways to interact with the tests and modify the test structure:

the visitor internal API
the listener API

I would like to prepend, to specific tests, a given keyword, or even more precisely to wrap the test in a given keyword, if that test has a specific tag. Which of the two approaches should I choose, and how I should use the API of the Body of Robot Framework to wrap it in a Wait Until Keyword Succeeds?\nAnswer: I think the Listener API might be the way to go but why not use the Test and Suite Setup Keywords to be able to pull off what you are trying to do. However, in general, i.e. not just in Python or Robot Framework, it is a bad idea to have one test depend on the execution of another test. Ideally, each test should be able to run on its own, whether first or last or anywhere in the sequence so perhaps there might be a need to refactor your tests and/or Keywords to be able to achieve what you are attempting.",0.20408164,0.39495224,0.036431584507226944
56,"Question\nI have created project and even i have client_secret.js file, when i try to run the program first time it asks to login, when I tried to login through gmail (which I have used to create project) it says

""Google hasn’t verified this app""

Message, so for that i'm looking how to get out from this step.\nAnswer: This is occuring because the application you created is not verified by google so you are not able to login in that you need to verify it by google suite and then you are able to login I also get that problem when i tried to create an browser for myself",-0.35714287,-0.09234524,0.07011778652667999
57,"Question\nSo, i entered code below to round a column to the nearest integer, though it still shows up with decimals.
Eg. I want to round 62.040 to just 62 though once the code is entered it shows 62.0 in the output
data['Final'] = np.ceil(data['Total'].round())\nAnswer: This should work pretty ok data['Final'] = round(data['Total'])
This converts the decimal number to it nearest integer and the returned number doesn't have a decimal point
Note: I'm assuming that data is just dictionary that contains float numbers, since no more information was provided",0.27210885,-0.13836491,0.16848871111869812
58,"Question\nI'm new with dramatiq and I do not find a way how to start dramatiq in detach mode like Celery?
I try to start with flags --d --detach, but nothing works.
Please inform me how to start dramatic in detach mode when I start dramatiq app:broker\nAnswer: I found it's already work on detach mode, but if you wish to read the logs you need to have your terminal in open view",0.0,-0.12642008,0.015982037410140038
59,"Question\nI'm working in python and I have an image that is represented by the shape (4,386,258), I would like to represent it with the shape (3,386,258). Is there a way to do that? I don't know how to combine the information in order to get the right map\nAnswer: Assuming your image is 386 pixels by 258 pixels and you have 4 channels, it means you want to convert the colour representation to one using only 3 channels, probably RGB. You haven't specified what libraries you used to read-in the image or where it is from, but most would have methods to handle this for you, so you don't have to write the conversion manually. They should also have methods of finding what the current colour representation is.",0.0,0.23950982,0.05736495554447174
60,"Question\nSo i was trying to make a voice classifier, the challenge was to predict if the person talking is me or is other person, i already build a simple code that converts.wav file into an array, so all my datasets of audio files of me talking will be converted into arrays, the thing is that I am a beginner and i dont know how to only use one variable for predicting(my voice), is like saying true or false, how can i make a neural network(CNN) that works like that?
Or should i use another type of machine learning? And not deep learning? I would like to use deep learning for this project.\nAnswer: Because it seems that you are relatively new to the topic, I would propose that you start with an easier machine learning model for this simple classification task. Maybe for the first model a Logistic Regression could be enough.
So, you say you collected some samples of your own voice (your positve class), but did you also collect some negative samples from other people?
After this, I would suggest to chunk the converted samples into equal siced sequences and label them (your voice or another?).
Now with this as basis, you can test some easier models like the Logistic Regression or the KNN. You then might move forward to some simple Feed-Forward-Neural-Networks and then go on to some advanced models like CNN's or RNN's.",0.0,0.13328993,0.017766205593943596
61,"Question\nIf I send out a request to a user for them to reply to a yes or no question, how can I make it such that after a certain amount of time, the question expires and is no longer accepting replies?
Background
I'm currently working on a discord.py bot, and I'm getting to the part where I start scanning the messages for user input. How it works is that somebody can challenge someone to a game of tic tac toe, and then the challenged person has 10 seconds to reply with yes or no. How can I make it such that after 10 seconds, the challenge expires?
Examples
One example I thought of is the bot Dank Memer. If you challenge someone to a tic tac toe game on there, then the challenged person has to respond to you within a time limit, or else the challenge expires. It also works with commands like pls trivia, where after 10 seconds it stops accepting answers to the question.
I'm pretty sure that people have already asked this question, but I have looked over and over on Google and I can't find a solution. It's probably because of my wording, I think.
It's also hard for me to do a minimal-reproducible example, as that would require you guys to have to create a discord account and token for the bot, which is not at all efficient.\nAnswer: You can use a await client.wait_for(check=check). Check is a function you must define that needs to return either true or false. Within check, you can add various conditions and actions if you need to. I made a command that does a math equation with it, if you'd like to see an example.",0.27210885,0.19329524,0.0062115853652358055
62,"Question\nIs there a way to display a list of the widgets or the items inside a list view? the items were also added by buttons, then next thing that i want to do is to display a list of the added items in another screen? how would i do this?\nAnswer: You can provide an id to your ListView and later on access it in your code where you want it. After accessing ListView you can just call lstvw.children (assuming you accessed it as lstvw). It will provide you list of all the children.",0.0,0.23355108,0.054546110332012177
63,"Question\nI would think it's obvious that it should uninstall all packages when removing an environment, since how would they be accessed otherwise, but I haven't seen documentation saying so, so I'm checking here if all packages need to be removed first.\nAnswer: The conda environment will be deleted. Sometimes some packages stay behind, although they are not bound to any environment. You can delete these under
<your anaconda folder> -> envs -> <the env you deleted>.",0.20408164,0.36186737,0.02489633671939373
64,"Question\nI am planning to use a same user model for two different users with some additional fields in one model. Is it possible to do this in Django? If it is possible, how can I do it?\nAnswer: You have two options :
Firstly
You create a new Model (call it ExtendedUser) and you put the extra fields in that model.
You then set your ExtendedUser model to inherit from your MainUser model. This will create two tables with a one to one link between them.
The advantage of this is that your ExtendedUser model will inherit any methods defined in your original MainUser model, and you only need add methods to the ExtendedUser model if you absolutely have to. You can also write functions which can deal with both the MainUser and ExtendedUser models easily, since they are all instances of the MainUser model. One of the benefits of using inhertiance is if you write a query on the ExtendedUser model fetching all fields, it well fetch the MainUser fields too.
A Second (worse) option :
Have a single extended model with a flag on it which says whether to use the extra fields. This can be made to work, but will make your queries more complex, and if you need the extra fields to be indexed, the indexes are likely to be a bit less efficient.
Note : having a second table with a relationship between the two is no bad thing in SQL - that is how data is normally managed. Having special optional fields for some types of users isn't great as mentioned.",0.0,0.35080552,0.12306451052427292
65,"Question\nI've tried conda install -c anaconda boost and I get the error fatal error C1083: Cannot open include file: 'boost/python.hpp': No such file or directory. I've added the boost download to the system and user paths, I've followed the direction on how to install boost python on Windows. What am I missing? What else can I do? I get the same error regardless of what I try.\nAnswer: My answer comes quite late, but I hope it will be of use for other people. In fact the arborescence has changed in an update
for instance in my case with Anaconda

in the 1.67 version, python.hpp is in \anaconda3\envs\myenv\Library\include\boost
in newer version (tested with 1.73),  python.hpp is in \anaconda3\envs\myenv\Library\include\boost\python which prevents it from encountering it

You can either modify the path referencing these files or install the version that works",0.0,0.5248035,0.2754187285900116
66,"Question\nI'm using Pygame.midi and you can select from 127 instruments.
I want to know how I can change/add midi instruments to this list.
Is there a list of pygame.midi instruments anywhere and if not what libraries is pygame using to actually do midi?
I've seen something called PortMidi and I'm wondering if this is what Pygame is using.
I just want to know how I can access pygame.midi and add instruments.\nAnswer: Pygame is not providing the instruments.  It's just sending MIDI commands to your system's built-in synthesizer, usually part of your sound card.  MIDI allows for 127 instruments at a time.  There is a standard called ""General MIDI"" that tries to define a baseline of 127 instruments, and that's probably what your system is using.
You use ""sysex"" (system exclusive) messages to tell the synthesizer to change the instrument-to-channel assignments.  pygame supports that with the write_sys_ex command, but you have to know the internals of your synthesizer to know which commands to send.",0.40816328,0.24478358,0.02669292688369751
67,"Question\nI'm just picking up some async Python and trying to write a simple TCP port scanner using the Asyncio module.
I can open a full-fledged TCP connection with 3-way handshakes via asyncio.open_connection. However, I want to create an SYN-ACK half-open connection—similar to what nmap uses—using asyncio. I was rummaging through the streams API but couldn't find anything. Is there a high-level method to do this? If not, how do I do this?\nAnswer: asyncio doesn't give you such control on TCP/IP stack layers and even hides some complex tasks such as callbacks, low-level protocols, transports.
You can do it using a raw socket.
Modules that can be useful

python-nmap
scapy",0.40816328,-0.008335084,0.1734708845615387
68,"Question\nI'm wondering how to install two different versions of a module for python. I host some scripts on a server and some particular ones require older version of the module because some things just doesn't work in the newer version and vice versa, some things doesn't work on the older version.
I know i can use pkg_resources and require a certain version, but if i try to install and use another version it just replaces the other one.\nAnswer: Easiest solution would be to use virtual environments for one or both of your scripts. This is however not recommended in the long run if you want to scale and manage. You should switch up the script using the older version or move to something like a docker based solution.",0.81632656,0.14519048,0.45042362809181213
69,"Question\nI'm new to Python and Pandas, and i'm struggling to create a frequency distribution table form my df.
My dataframe is something like this:




Balances
Weight




10
7


11
15


12
30


13
20


10
15


13
20




edit: The balance numbers are its respective ID
I need the frequency of each balance used (in this example, balance 10 would be 2 and so on) the min, max and mean of the measurements results.
I was to use df.groupby(['balances']) but how can i use the results form using df.groupby to creat a new table? Is that the way?\nAnswer: df.groupby(['balances']).count() should solve what you're looking for",0.20408164,-0.0013976097,0.04222172126173973
70,"Question\nI have documents in different MongoDB databases referencing each other (mongoengine's LazyRefereneceField), so each time I need to get the field's value, I need to connect and disconnect from the field's relevant database, which I find very inefficient.
I've read about connection pooling, but I can't find a solution on how to implement it using MongoEngine. How can I create a connection pool and reuse connections from it every time I need to the value for a LazyReferenceField?\nAnswer: MongoEngine is managing the connection globally (i.e once connected, it auto-magically re-use that connection), usually you call connect just once, when the application/script starts and then you are good to go, and don't need to interfere with the connection.
LazyReferenceField is not different from any other field (ReferenceField, StringField, etc) in that context. The only difference is that it's not doing the de-referencing immediatly but only when you explicitly request it with the.fetch method",0.0,-0.037835598,0.0014315324369817972
71,"Question\nI salute all brethren here. Please I was tasked by my superior in a pharmacy outlet to generate barcode taking into consideration drug's country of origin ID or code, supplier code, product code and product classification as in pain killer etc. All these must be serialize into a readable barcode. So it means this must link to a database. Please brethren, how do I go about this in python kivy?\nAnswer: Too opened question I think?
You encapsulate:

Install kivy
Write your program
Deploy it

I suppose you want to focus on how to program it and you will manage to learn how to install and deploy it.
I suppose that you want to use the phone's camera to analyse the barcode.
I have an application that need to take photos and analyze the text on it to write it in a form. I didn't manage to make it work personnaly. Try to know if the kivy cam is ""stable enough"" to allowed saving/loading/real_time_analyze on Android.
If you manage to make your camera work, you will need to focus on the analyse of the barcode. I think that you can reuse working library and use it on your application (or on a AWS lambda as a micro-service).",0.0,0.2732327,0.07465610653162003
72,"Question\nI'm trying to find the correlation between categorical and numerical columns in my dataset using Python, can anyone help?
Here is the data that I have.
Thank you in advance.




Light_Sensor_Reading
Light_Sensor_Status




231
Dim


231
Dim


231
Dim


231
Dim


231
Dim


231
Dim


231
Dim


231
Dim


231
Dim


232
Dim


950
Very_bright


988
Very_bright


987
Very_bright


986
Very_bright


986
Very_bright


986
Very_bright


986
Very_bright


986
Very_bright


986
Very_bright


986
Very_bright


986
Very_bright


986
Very_bright


986
Very_bright


985
Very_bright


985
Very_bright\nAnswer: it doesn't mean anything to calculate the correlation between two variables if they are not quantitative.
I think what you want to do is to study the link between them.
The purpose is to explain the first variable with the other one through a model.
You can use the logistic regression.
But you will only know if there is a link between these two variables with a T-Test or a wilcoxon test depending on the normality of the data",0.10204082,-0.030507863,0.017569154500961304
73,"Question\nWhen I run time.mktime(datetime.strptime(""2012-03-09"", ""%Y-%m-%d"").timetuple())
I get the value 1331251200.0.
Now I want to know how can I invert this? So if I pass in 1331251200.0, how do I get as output 2012-03-09? I know strftime is the inverse of strptime, but I don't see how to invert the whole thing because of the timetable function.\nAnswer: So the way to do this is by time.strftime(""%Y-%m-%d"", time.localtime(time.mktime(datetime.strptime(""2012-03-09"", ""%Y-%m-%d"").timetuple()))).
I didn't know strftime would turn the timetable to a string before I asked.",0.0,0.123910666,0.0153538528829813
74,"Question\nI am unfamiliar with the instaboy.py python package. I am wondering if there are any security issues with this package like possibly getting information leaked. I am wondering how does the API work if there are a lot of people using this package. Wouldn't you need your own personal Instagram API token? I am confused by the whole concept and if anyone could explain even just a little bit it will be much appreciated.\nAnswer: Bots are now easily detected by Instagram. Your account could be banned for 3 days, 7 days, 30 days or definitively if Instagram detects too many attempts
Usually bots simulate a browser via Sellenium and then create a ""browse like a human"" bot to create likes, follow, unfollow, etc.",0.0,0.09221268,0.008503178134560585
75,"Question\nI would like to programatically delete image placeholders which are full (PlaceholderPicture objects in the python-pptx API)
and replace them with empty ones (PicturePlaceholder), the goal being to ""refresh"" pictures instead of always having to fill out an empty presentation.
I think I know how to delete a shape, but creating a new PicturePlaceholder proves harder than expected.
When I try to create one I am asked to provide a sp and a parent and I cannot find in the documentation what these parameters refer to.
Does my approach make sense, and if so is it possible to create new placeholders programmatically? Could anyone explain what are the parameters I should pass to the constructor?
Thanks in advance.\nAnswer: python-pptx has no API support for this, which means you would need to directly manipulate the underlying XML of the slide to accomplish it (and be outside the python-pptx ""safety net"").
sp is the shape element (generally an <p:sp> element, hence the name) and parent would be a reference to the slide.
As a general approach, I would create an empty placeholder and compare its XML to that of a ""filled"" placeholder which is the same in every respect. Then the job is to make one into the other.
Note that a picture placeholder has a relationship to the ""image part"" (e.g. PNG file) stored elsewhere in the PPTX package (.pptx file). This will need to be dealt with and not left dangling.
Another possibility is just to swap the image and leave everything else in place, which on paper at least would be less disruptive.
Because none of these options has API support, pursuing them involves taking on responsibility for a lot of details that python-pptx takes care of for you and learning a fair amount about the internals. If that sounds like something you'd rather avoid then you'll want to find another way to accomplish the broader outcome you have in mind.",0.0,0.3762253,0.14154547452926636
76,"Question\nI use the below statement, extracting cell content as legend name (I have 3 set of values to plot on y-axis, thus need 3 legend parameters)
plt.legend(df.iat[0, 0], df.iat[0, 1], df.iat[0, 2])
the compiled error says that 'legend only accepts two non-keyword arguments'
I print out the df.iat[0, 0], df.iat[0, 0], df.iat[0, 0], all these 3 are the cell content I expect.
Anyone know how I can fix it?\nAnswer: You have to put the values in a list:
plt.legend([df.iat[0, 0], df.iat[0, 1], df.iat[0, 2]])",0.0,0.3334645,0.11119857430458069
77,"Question\nIs a very simple page, that have some buttons redirecting to external links. That external links have to change arbitrarely when certain people decides it.
I was wondering about making a login screen in other page and depending which user logs in, set the link writting it in a txt file that would be recovered by the main page. Don't need security so i just would make the user go to the other page and login and that would change the link in a txt file hosted in the same directory that the index.html is.
The problem is that i cannot make that idea (writing a.txt in the project folder) with JavaScript (or i think so), and i don't think that using a database worth it (neither want to because i read that would have to use node.js or something like that and i really don't know those tecnologies, only some python and sql server -that is in fact not supported by my hosting service-), cause are just 3 people associated to 1 link each. I can easily make that task with python but don't know how to call it from JS and neither if Ajax request would be useful for this case.
I hope someone can help me!
edit - the changes have to apply for the page itself not in the user session, so everyone who enters see the page with the changes
edit 2 - absolutely all the info (users and links) are always the same, doesnt change so i can put it in variables, are just 4 or 5 so i can easily use an array, the only thing that i would have to register is which one to pick (since it depends on who decides it).\nAnswer: IT seems you want to use some sort of client side storage across different html pages - for that, you can use cookies, localSotrage or sessionStorage.",0.0,0.09383914,0.008805784396827221
78,"Question\nToday I faced a problem regarding pip3 in Ubuntu. Ubuntu comes with python 3.8 but I wanted to use latest versions of python, like 3.9 or maybe 3.10. So I installed it using 'ppa:deadsnakes' repository and also installed pip. But the problem is I want to use pip in python 3.9 instead of version 3.8. So I changed the default python version to 3.9 and everything crashed.
So reverted to python 3.8. Whenever I install some package it gets installed using python 3.8.
Help me, how can I use python 3.9 pip and install packages in python 3.9 without changing the default version.
Any help is appreciated.
--> Thing I want is that when I want to install python package using
pip3 install <package_name> it must install in python3.9 and not in python3.8\nAnswer: You don't need to install pip separately
You should be able to refer to it as such
python3.9 -m pip install",0.6122449,0.25354046,0.128668874502182
79,"Question\nToday I faced a problem regarding pip3 in Ubuntu. Ubuntu comes with python 3.8 but I wanted to use latest versions of python, like 3.9 or maybe 3.10. So I installed it using 'ppa:deadsnakes' repository and also installed pip. But the problem is I want to use pip in python 3.9 instead of version 3.8. So I changed the default python version to 3.9 and everything crashed.
So reverted to python 3.8. Whenever I install some package it gets installed using python 3.8.
Help me, how can I use python 3.9 pip and install packages in python 3.9 without changing the default version.
Any help is appreciated.
--> Thing I want is that when I want to install python package using
pip3 install <package_name> it must install in python3.9 and not in python3.8\nAnswer: you can use pip for python3.9 by pip3.9 install <package-name>",0.0,0.32073814,0.10287295281887054
80,"Question\nToday I faced a problem regarding pip3 in Ubuntu. Ubuntu comes with python 3.8 but I wanted to use latest versions of python, like 3.9 or maybe 3.10. So I installed it using 'ppa:deadsnakes' repository and also installed pip. But the problem is I want to use pip in python 3.9 instead of version 3.8. So I changed the default python version to 3.9 and everything crashed.
So reverted to python 3.8. Whenever I install some package it gets installed using python 3.8.
Help me, how can I use python 3.9 pip and install packages in python 3.9 without changing the default version.
Any help is appreciated.
--> Thing I want is that when I want to install python package using
pip3 install <package_name> it must install in python3.9 and not in python3.8\nAnswer: Hello everyone I fixed my issue.
The problem is we cannot override default python version in Ubuntu as so many things depend on it.
So I just made an alias as : alias pip3='python3.9 -m pip' and alias for python : alias python3='/usr/bin/python3.9'
If anyone face this issue please do what I specify and you will be good to go.
Now all my packages are being installed in python3.9.",0.30612245,0.15108174,0.024037621915340424
81,"Question\nI made a sample app that just says hello world with kivy and I am trying to put it on an android tablet I bought.
I used a virtual machine (Virtual box) and use bulldozer to loaded to the tablet. However when I run it the terminal just prints in a loop
#waiting for the application to start
any idea on what could be the problem, or how can I start the application\nAnswer: Make sure you have enabled developer mode on the phone, the phone must be plugged in to the VM (In VirtualBox: Devices -> USB -> {phone}) and that the screen is unlocked.
On the phone you should see a ""USB debugging connected"" notification when it's plugged in.
If the phone is properly connected, you'll see it in Android Studio in a number of places, including the LogCat and right at the top of the screen in a little dropdown to the left of the run button.
Lastly, you can run the app manually and then connect the debugger if you want. Just run the app on the phone, then click the little icon in the top right with the green bug with an arrow over it.",0.0,0.29368937,0.08625344932079315
82,"Question\nI made a sample app that just says hello world with kivy and I am trying to put it on an android tablet I bought.
I used a virtual machine (Virtual box) and use bulldozer to loaded to the tablet. However when I run it the terminal just prints in a loop
#waiting for the application to start
any idea on what could be the problem, or how can I start the application\nAnswer: I had trouble with this too. But if it runs that far then it should've successfully compiled the.apk. It should be in the same folder that you ran the buildozer command. You can even run an HTTP server with buildozer serve and access it via localhost:THE_PORT_NUMBER_HERE. Then you can just transfer or download the.apk to your device. And then install and run it after accepting the correct permissions.",0.0,0.2552088,0.0651315301656723
83,"Question\nI am developing a code using python and Django for web scraping. I have provided the required url and already fetched the data.
But there is a tab on the url saying ""show more results"".
How can I make my Django project to click on that tab of ""show more results"" on the url and the fetch the new results now.
I have used Beautiful Soup library for web scraping/\nAnswer: BeautifulSoup is an HTML parser and nothing more, if you want to automate clicking and so on you have to use other tools such as selenium.
Another idea might be getting the URL of ""Show more results"" and scrape the stuff that it loads. I assume it is loading the thing dynamically so look at your Network Tab (Ctrl+Shift+i then Network) and see which URL it is loading you may be able to scrape it.",0.0,0.138237,0.019109468907117844
84,"Question\nI am trying to scrape information off of a website (on Chrome) where I need to click on each subpage to get the information I need. After about 7, I get blocked by the website. I think if I was able to switch IPs either each time or once I get blocked, that would work.
I am using Selenium to open the site and navigate to the subpages. I have tried using a try-catch block so far and a while loop but I am getting errors I do not know how to address.
Does anyone have an alternative approach or previous success doing this?\nAnswer: You can use rotating proxies to change your IP per request or with a time interval but If you don't want to use any proxy you can restart your router to get a new IP address from your ISP but If you have a static IP from your ISP even through you restar your router your IP will stay the same.",0.0,0.11901373,0.014164267107844353
85,"Question\nI'm doing a segmentation task. Now I successfully segmented the area of interest and find the contour of the area. How can I calculate the min/max axis length of the contour? The axis does not have to be orthogonal.
What I already got are:
coordinates of points on the contour.
centroid of the contour.
What I already tried:
I have found a fitting-ellipse of the contour. However, fitting-ellipse can only find the orthogonal axis which might not be the minimum or maximum length across the centroid.\nAnswer: Since you already have the contour, you might want to do this:
for alpha = 0 to 45 degrees, rotate the contour by alpha, draw a line over the centroid of your contour that is parallel to X-axis, find out the intersection points of this line and the contour, that is one ""axis"" at current angle alpha. continue rotating the contour and find more of such ""axis"", find the longest one as your max axis, shortest one as your min axis.",0.0,0.08924472,0.007964620366692543
86,"Question\nI'm working on a Python application where the desired functionality is that the webcam is used to take in a live video feed and based on whether a condition is true, an image is clicked and uploaded to a database.
The database I am using is MongoDB. As far as I can understand, uploading images straight-up to a database is not the correct method. So, what I wanted to do is the following:

an image is clicked from the webcam
the image is uploaded to an S3 bucket (from the same Python script, so using boto3 perhaps)
a URL of the uploaded image is retrieved (this seems to be the tricky part)
and then this URL along with some other details is uploaded to the database. (this is the easy part)

My ideal workflow would be a way to take that image and upload it to an S3 bucket, retrieve the URL and then upload this URL to the database all in one.py script.
My question is: how do I upload an image to an S3 bucket and then retrieve its public URL all through boto3 in a Python script?
I also welcome any suggestions for a better approach/strategy for storing images into MongoDB. I saw on some pages that GridFS could be a good method but that it is not recommended for the image uploads happening frequently (and really that using AWS is the more preferable way).\nAnswer: You don't need to'retrieve' the public url, you get to specify the bucket and name of the s3 object when you upload it, so you already have the information you need to know what the public url will be once uploaded, its not like s3 assigns a new unique name to your object once uploaded.",0.20408164,0.045547843,0.02513296529650688
87,"Question\nsince i couldn't find the best way to deal with my issue i came here to ask..
I'm a beginner with Python but i have to handle a large dataset.
However, i don't know what's the best way to handle the ""Memory Error"" problem.
I already have a 64 bits 3.7.3 Python version.
I saw that we can use TensorFlow or specify chunks in the pandas instruction or use the library Dask but i don't know which one is the best to fit with my problem and as a beginner it's not very clear.
I have a huge dataset (over 100M  observations) i don't think reducing the dataset would decrease a lot the memory.
What i want to do is to test multiple ML algorithms with a train and test samples. I don't know how to deal with the problem.
Thanks!\nAnswer: This question is high level, so I'll provide some broad approaches for reducing memory usage in Dask:

Use a columnar file format like Parquet so you can leverage column pruning
Use column dtypes that require less memory int8 instead of int64
Strategically persist in memory, where appropriate
Use a cluster that's sized well for your data (running an analysis on 2GB of data requires different amounts of memory than 2TB)
split data into multiple files so it's easier to process in parallel

Your data has 100 million rows, which isn't that big (unless it had thousands of columns).  Big data typically has billions or trillions of rows.
Feel free to add questions that are more specific and I can provide more specific advice.  You can provide the specs of your machine / cluster, the memory requirements of the DataFrame (via ddf.memory_usage(deep=True)) and the actual code you're trying to run.",0.0,0.29496247,0.08700285851955414
88,"Question\nI am tinkering with redis and mysql to see how caching can improve performance. Accesing data from Cache is/should be faster than accessing it from database.
I calculated the time required for both the case in my program and found out that accesing from cache was much slower than accesing from the database. I was/am wondering what might be the cause(s).
Some points to consider:

I am using Azure Redis Cache.
The main application is on VM instance.
I hosted MYSQL server on another VM instance.
The table is very small with 200-300 records.
There is no error in the time calculation logic.

EDIT:
Load time for cache=about 1.2s
Load time for mysql= about 15ms
Turns out my application and MySQL server were in a same region while the redis cache was in a different region across the globe causing much higher latency.
But I would still want someone to explain why the fetch time for sql was much more smaller.\nAnswer: If the table of 200-300 rows is fully cached in MySQL's ""buffer_pool"", then it won't take much time to fetch all of them and send them back to the client.  15ms is reasonable (though it depends on too many things to be more specific).
If you are fetching 1 row, and you have an index (esp, the PRIMARY KEY) to locate that one row, I would expect it to be even faster than 15ms.
I'm summarizing a 40K-row table; it is taking under 2ms.  But note: client and server are on the same machine.  15ms could represent the client and server being a few hundred miles apart.
How long does a simple SELECT 1 take?  That will give you a clue of the latency, below which you cannot go without changing the physical location of machines.",0.0,0.20627129,0.04254784435033798
89,"Question\nCurrently, I have two python bots running on VDS, both of them are using selenium and running headless chrome to get dynamically generated content. While there was only one script, there was no problem, but now, it appears that the two scripts fight for the chrome process (or driver?) and only get it once the other one is done.
Have to mention, that in both scripts, Webdriver is instantiated and closed within a function, that itself is ran inside a Process of multiprocessing python module.
Running in virtual environment didn't do anything, each script has their own file of chrome driver in their respective directories, and by using ps -a I found that there are two different processes of chromedriver running and closing, so I am positive that scripts aren't using the same chrome. 
Sometimes, the error says ""session not started"" and sometimes ""window already closed"".

My question is - how do I properly configure everything, so that the scripts don't interfere with each other?\nAnswer: For anyone having the same problem - double-triple-quadriple-check that the function, that you're passing in the Process, is the one instantiating Webdriver. I can't believe this problem is fixed just like that.",0.0,0.29543293,0.08728061616420746
90,"Question\nI'm new to LDA and I want to calculate the topic similarity between words. Can I get the topic distribution of a word? If so, how can I do this in gensim.ldamodel?\nAnswer: Gensim's LDA mallet wrapper has a load_word_topics() function (I would assume this is true for its python LDA implementation as well).  It returns a matrix that is words X topics.  From that, you can get a vector of frequencies each word in each topic.  That would be the topic-word distribution for a given word.",0.0,0.32265925,0.10410899668931961
91,"Question\nI have a template where I'm showing with a 'for' my elements, and those elements have a Edit button, so what I want to do, is when the user clicks edit, a pop up appears with the form and the information of that item.
So I don't know how to do that:(\nAnswer: so if you are using Javascript in your template then you can get the row index of clicked button and then get information from there and bind it to your popup.",0.0,0.049075723,0.0024084264878183603
92,"Question\nI would like to know how to get the first message in a channel.
But more preferably could you point me to the part of the documentation that explains this, please?\nAnswer: TextChannel has a history method.
If you type await channel.history() you can get all messages in that text channel. But this starts from last message. To start from first message, use oldest_first=True in method. Also you can limit the amount of message with a keyword argument limit.",0.81632656,0.3331877,0.2334231585264206
93,"Question\nI work on one project in that code in python script.py and with help of pyinstaller build exe file but issue is that I want to add publisher name in exe file. FREE
exe with publisher name
I browse on google but I do not get a solution.

i don't have a code sign certificate.
I don't have a signtool
i don't know how to connect a self sign certificate with an exe file.

Please guide me.\nAnswer: There is no way to do this for free.  If you want to sign your executables, you will need to buy a code-signing certificate and use signtool to sign them.  Code signing certificates cost about $300 per year.  signtool is part of the Windows SDK; you can get it in the free ""community"" editions of Visual Studio.",0.0,0.22645664,0.0512826107442379
94,"Question\nPlease can you provide an example of how this can used in a python notebook environment with docplex. I see examples with java on ATSP problem. The point is I do not know how to create these cuts upfront. Given a LP root node, I can generate the cut. So, ""add_user_cut_constraint(cut_ct, name=None)"" should in a way take in as input the root node. How do I retrieve that in a generic way?\nAnswer: Look at this code in my contribs repository:
https://github.com/PhilippeCouronne/docplex_contribs/blob/master/docplex_contribs/src/cut_callback.py
It is not a notebook, but you'll get the idea on how to interface callbacks with Docplex.",0.0,0.31744266,0.10076984018087387
95,"Question\nOOBPermutedVarDeltaError states that

For any variable, the measure is the increase in prediction error if the values of that variable are permuted across the observations. This measure is computed for every tree, then averaged over the entire ensemble and divided by the standard deviation over the entire ensemble

To find its equivalent would I need to calculate the increase in prediction error across observations and then average it across the entire ensemble? Being fairly new to ML, I am not sure how I would implement it even. Any help would be most appreciated.\nAnswer: No, there arent any. Python and Matlab, does not scale 1:1",0.0,-0.014240682,0.00020279703312553465
96,"Question\nrecently I made a little program that generates a country with a bit of info, like the country name, size and most importantly cities.
It generates a random number of cities and towns and villages based on the country's size, this is all text based so that is why I would give it a nice flare with some graphics.
Now, I have been researching a bit and it seems that my best option would be Perlin noise or some form of noise, so my question is, how would I go about doing this? Where do I start? Are there other ways of accomplishing this?
So, in short, how do I take the randomly generated text cities and towns and villages in my program and randomly generate them on some form of map? I hope this all makes sense. Also, my program is written in python.\nAnswer: There is lots of problem in what you saying. What platform are you using? Arcade? PyGame? etc. I will write the arcade code for you so you can use. To use arcade, type pip install arcade in Windows, or pip3 install arcade on Linux.
Edit:
Please can you post your code so I can implement?",0.40816328,-0.04656732,0.20677991211414337
97,"Question\nI would like to generate some data (position of the snake, available moves, distance from the food...) to create a neural network model so that it can be trained on the data to play the snake game. However, I don't know how to do that. My current ideas are:

Play manually (by myself) the game for many iterations and store the data (drawback: I should play the game a lot of times).
Make the snake do some random movements track and track their outcomes.
Play the snake with depth-fist search or similar algorithms many times and store the data.

Can you suggest to me some other method or should I choose from one of those? Which one in that case?
P.S. I don't know if it is the right place to ask such a question. However, I don't know whom/where to ask such a question hence, I am here.\nAnswer: If using a neural network, start simple. Think inputs and outputs and keep them simple too.
Inputs:

How many squares to the left of the head are free
How many squares to the right of the head are free
How many squares forward of the head are free
Relative position of next food left/right
Relative position of next food forward/back
Length of snake

keep inputs normalized to the minimum and maximum possible values to keep inputs in range -1.0 to 1.0
Outputs:

Turn Left
Turn Right
Straight Ahead

(choose the output with highest activation)
Next problem is training. Typical application might be use a genetic algorithm for all the weights of the above neural network. This randomizes and tests new versions of the neural network every life. After X attempts, create a new evolution and repeat to improve. This is pretty much doing random moves automatically (your second choice)
Next problem is fitness for training. How do you know which neural network is better? Well you could simply use the length of the snake as a fitness factor - the longer the snake, the better and more 'fit'",0.40816328,0.10235077,0.09352128952741623
98,"Question\nIs there a way to add a comment to an order's timeline via the REST API?
If so, what's the scope access? And how to do this.
Thanks\nAnswer: You cannot add a comment to the timeline. You can see your App's interactions with an order on the timeline, exposed by Shopify, but you cannot inject stuff yourself. If you want to decorate an order with comments, you would add your comment to the order notes. That works fine, but as you can tell, it is not timestamped by Shopify, so it lacks an ""official"" standing... nonetheless. Just use Notes.",0.81632656,0.30073488,0.265834778547287
99,"Question\nSo in my table there are a number and a timestamp row, there are multiple numbers per day
its like:




number
timestamp




3
20.02.2021 16:05:00


7
20.02.2021 16:10:00


20
20.02.2021 16:15:00


5
21.02.2021 16:00:00




now i want the average of the numbers of the day of 20.02.2021 but i don't know how i should do that with SQLAlchemy
any suggestions?\nAnswer: Not sure if this will fully answer your question, but to get just the date portion of a timestamp you can recast the timestamp as date by:
select your_timestamp_column::date",0.0,0.15130442,0.02289302833378315
0,"Question\nI'm trying to make a small data science tool (kinda like a mini version of WEKA). Now, I have these datasets that have large amounts of features (70-100+), and they're mostly categorical. I'm using Python sklearn for the Machine Learning logic and I need to convert these categories into numeric values according to the sklearn errors I've gotten.
Given this, One Hot Encoding isn't an option because it will enlarge the dimensionality too much.
I've researched other ways that may work like frequency encoding, label encoding, etc. But I'm not really sure what to choose in my case.
Also, would anyone know how WEKA actually handles these? I inputted my datasets in WEKA and they worked fine, they gave me good results!
Any help would be greatly appreciated. Thanks!\nAnswer: That depends on the algorithm: Some handle categorical attributes natively, like J48 (Weka's C4.5 implementation), which performs multi-way splits on categorical attributes. Others have to convert the data, like SMO (support vector machine), which binarizes nominal attributes and increases the number of attributes to learn from.",0.0,0.20640886,0.04260461777448654
1,"Question\nI have a programming project in mind, but have only scratched the surface of my programming experience. I'm just about done with my first Python class, so that's about how much experience I have.
Now my question, is it possible to read and recognize an image (business card/playing card/ image with text) and produce an operation based on the information read? The image would be at a distance, and I would be using a camera or xbox kinect.\nAnswer: yeah it's possible, you can train machine learning models.",0.0,0.05398214,0.0029140713158994913
2,"Question\nI'm trying to create something and I don't know if it's possible or ""clean"" :

From python, call function A of my C++ code to compute something complicated
The C++ code returns just the pointer to the python
Do other things in python...
From python, call function B of my C++ code, it takes the pointer and other things as arguments.

I really don't need to use my complicated C++ class in my Python algorithm, that's why I just want to save the pointer in python.
Anyone has any advice on how to do that?
Edit : In the end I wrapped the c++ class in python, thank you everyone.\nAnswer: A pointer is just data that can be marshaled and sent to anything. It is however a very bad idea because when doing that, you have to assure that that pointer remains valid as long as the python part has the pointer. There is no possibility to check whether the pointer is still valid, so dereferencing a pointer that you receive from an external party could crash your program.
A better idea in a lot of situations is to send a key to a table. When that key is sent back, it can be used to get the needed information from that table and it can be handled when the table doesn't have the key anymore. It is easiest to use std::map for the table. Of course, you could store the pointer in a container and check for that, but a string or number is easier to debug.",0.20408164,0.19048727,0.0001848070096457377
3,"Question\nI'm trying to create something and I don't know if it's possible or ""clean"" :

From python, call function A of my C++ code to compute something complicated
The C++ code returns just the pointer to the python
Do other things in python...
From python, call function B of my C++ code, it takes the pointer and other things as arguments.

I really don't need to use my complicated C++ class in my Python algorithm, that's why I just want to save the pointer in python.
Anyone has any advice on how to do that?
Edit : In the end I wrapped the c++ class in python, thank you everyone.\nAnswer: It would be better to create a class in C++ and store that pointer in the class itself as private. Then create function calls to access those pointers.
Once the class is implemented generate the.so file of your lib and import it in python. This way you can simply use your C++ code in python and also will not have to save the pointer.",0.0,0.11220628,0.012590249069035053
4,"Question\nI can't figure out how to write regex that matches these:

everyone hi
hi everyone
hi

But not this:

everyone hi everyone

The regex (?:everyone )?hi(?: everyone)? will match the latter as well (which is not what I want). How to make such a regex? Or is it just not possible? I couldn't do enough research because I couldn't express the problem in correct words. Sorry if I posted a duplicate\nAnswer: You could explicitly make a regex for each case (the first will capture two), utilizing beginning and end of line tokens
(^hi( everyone)?$)
(^everyone hi$)",0.40816328,0.5536597,0.021169202402234077
5,"Question\ncan anyone tell me how to fix this?
ModuleNotFoundError: No module named'sklearn.linear_model._logistic'
File ""c:\users\chintan\appdata\local\programs\python\python37\lib\site-packages\streamlit\script_runner.py"", line 332, in _run_script
exec(code, module.dict)
File ""C:\Users\chintan\Desktop\streamlit\Final_year_project\App.py"", line 329, in 
main()
File ""C:\Users\chintan\Desktop\streamlit\Final_year_project\App.py"", line 264, in main
loaded_model = load_model(""logistic_regression_hepB_model.pkl"")
File ""C:\Users\chintan\Desktop\streamlit\Final_year_project\App.py"", line 96, in load_model
loaded_model = joblib.load(open(os.path.join(model_file),""rb""))
File ""c:\users\chintan\appdata\local\programs\python\python37\lib\site-packages\sklearn\externals\joblib\numpy_pickle.py"", line 588, in load
obj = _unpickle(fobj)
File ""c:\users\chintan\appdata\local\programs\python\python37\lib\site-packages\sklearn\externals\joblib\numpy_pickle.py"", line 526, in _unpickle
obj = unpickler.load()
File ""c:\users\chintan\appdata\local\programs\python\python37\lib\pickle.py"", line 1088, in load
dispatchkey[0]
File ""c:\users\chintan\appdata\local\programs\python\python37\lib\pickle.py"", line 1376, in load_global
klass = self.find_class(module, name)
File ""c:\users\chintan\appdata\local\programs\python\python37\lib\pickle.py"", line 1426, in find_class
import(module, level=0)\nAnswer",0.0,0.17707312,0.031354889273643494
6,"Question\nI am working on a project that just scrapes data from 3 devices (2xserial and 1xssh). I have this part implemented no problem.
I am now heading towards the second part where I need be be able to send the data I need using protobuf to the clients computer where they will receive and display on their own client.
The customer has provided examples of their GRPC servers, and it's written in C#.
Currently, for security reasons, our system uses RedHat 8.3 and I am using a SSH Protocol Library called Paramiko for the SSH part. Paramiko is a Python library. Also the machine I am extracting data from only runs on Linux.

Here are my main questions, and I apologize if I got nowhere.
1.) The developer from the client side provided us with a VM that has a simulator and examples written in C# since their side was written in C#. He says that it's best to use the C# because all examples can be almost re-used as it's all written, etc. While I know it's possible to use C# in Linux these days, I've still have no experience doing so I don't know how complicated/tedious this can get.
2.) I write code in C# and wrap all the python code, which is also something I've never done, but I would be doing this all in RedHat.
3.) I keep it in python because sending protobuf messages works across languages as long as it is sent properly. Also from the client side, I'm not sure if they will need to make adjustments if receiving protobuf messages written in Python(I don't think this is the case because it's just serialized messages, yea?).
Any advice would be appreciated. I am looking to seek more knowledge outside my realm.
Cheers,
Z\nAnswer: If you're happy in Python, I would use option 3. The key thing is to either obtain their.proto schema, or if they've used code-first C# for their server: reverse-engineer the schema (or use tools that generate the schema from code). If you only have C# and don't know how to infer a.proto from that, I can probably help.
That said: if you want to learn some new bits, option 1 (using C# in your system) is also very viable.
IMO option 2 is the worst of all",0.0,0.33951455,0.11527013033628464
7,"Question\nPython: how to get unique ID and remove duplicates from column 1 (ID), and column 3 (Description), Then get the median for column 2




ID
Value
Description




123456
116
xx


123456
117
xx


123456
113
xx


123456
109
xz


123456
108
xz


123456
98
xz


121214
115
abc


121214
110
abc


121214
103
abc


121214
117
abz


121214
120
abz


121214
125
abz


151416
114
zxc


151416
135
zxc


151416
127
zxc


151416
145
zxm


151416
125
zxm


151416
121
zxm




Procced table should look like:




ID
xx
xz
abc
abz
zxc
zxm




123456
110
151
0
0
0
0


121214
0
0
132
113
0
0


151416
0
0
0
0
124
115\nAnswer: Well you have e.g. 6 'ID' with value '123456'. If you only want unique 'ID', you need to remove 5 'ID' rows, by doing this you will not have duplicate 'Description' values anymore. The question is, do you want unique ID or unique Description values (or unique combination of both)?",0.0,-0.12550038,0.015750344842672348
8,"Question\nI'm running a jupyter notebook remotely on a server by

connecting to the server: ssh server:address
initialize the jypter notebook ipython notebook --no-browser --port=7000
using another terminal window, stabilish remove connection to the notebook ssh -N -f -L localhost:6001:localhost:7000 server:address 
finally I access it throw localhost:6001 in my browser.

The thing is: I'd like to keep the notebook running when I turn my computer off. Any ideas on how can I do it?\nAnswer: You could create a crontab on your remote server with your command ipython notebook --no-browser --port=7000 to manage the execution of Jupiter notebook, that could be the way go.",0.0,0.1600005,0.025600161403417587
9,"Question\nI'm not sure what to search for this.
I am writing a piece of code using python fire to create a command line interface.
python test.py function argument
is there a way to make the shell interpret the following like the command above:
test function argument
Similar to how I can just call jupyter lab and it will open a notebook etc.
I have a feeling this is more to do with setting up my bashrc or similar instead of something I can do in Python.\nAnswer: You're correct that it has to do with adding to your.bashrc. You want to set an alias.

Make sure your code has an appropriate shebang line at the top, ex. #!/usr/bin/python3
Add the following to.bashrc, ex. alias test=python3 /path/to/test.py

From there, you can use sys.argv in your code to handle arguments within the program.",0.0,0.21149957,0.04473206773400307
10,"Question\nI'm not sure what to search for this.
I am writing a piece of code using python fire to create a command line interface.
python test.py function argument
is there a way to make the shell interpret the following like the command above:
test function argument
Similar to how I can just call jupyter lab and it will open a notebook etc.
I have a feeling this is more to do with setting up my bashrc or similar instead of something I can do in Python.\nAnswer: Add the hashbang (at the start of the file, in case you don't know) 
#!/usr/bin/env python 
or
#!/usr/bin/env python3
Replace the 3 with whatever version you have installed and want that file to run
Save the file to an already existing PATH or add the file location to PATH
Now you can hopefully run it by typing test.py function argument
Rename test.py to test 
Now you should be able to run it as test function argument
Also make sure your file is set as executable",0.0,0.15341586,0.023536425083875656
11,"Question\nWe know that when generating a class in Python2, if there is a metaclass attribute, it will be generated by the metaclass referenced by the attribute. Bug when the class is not generated, it should have no attribute, how does it control the generation of the class?\nAnswer: Because the body of the class definition statement is evaluated before the creation of the class object.
So indeed, the class doesn't have that attribute, the class doesn't even exist yet. But the class definition statement has been evaluated.",0.6122449,0.4210971,0.03653748333454132
12,"Question\nCan you advise me on the analogs of the socket library on Python? The task is this, I need to write a very simple script with which I could execute remote commands in cmd windows. I know how this can be implemented using the socket library, but I would like to know if there are any other libraries for such a case.\nAnswer: Sockets is a low level mechanism by which two systems can communicate each other. Your OS provides this mechanism, there's no analogs.
Next examples come from the application layer and they work with sockets in their lower communication layers: a socket open by your http server, usually 80 or 443 or a websocket open by your browser to communicate with your server. Or the DNS query that your browser executes when tries to resolve a domain name, also works with sockets between your PC and the DNS server.",0.0,0.25305474,0.06403669714927673
13,"Question\nI'm currently dealing with a material science dataset having various information.
In particular, I have a column 'Structure' with several pymatgen.core.Structure objects.
I would like to save/store this dataset as.csv file or something similar but the problem is that after having done that and reopening, the pymatgen structures lose their type becoming just formatted strings and I cannot get back to their initial pymatgen.core.Structure data type.
Any hints on how to that? I'm searching on pymatgen documentation but haven't been lucky for now..
Thanks in advance!\nAnswer: pymatgen.core.structure object can be stored with only some sort of fixed format, for example, cif, vasp, xyz... so maybe you, first, need to store your structure information to cif or vasp. and open it and preprocess to make it ""csv"" form with python command.(hint : using python string-related command).",0.40816328,0.20271313,0.04220976307988167
14,"Question\nI want to make a bot with discord.py that can stream videos from mp4 into a voice channel. Is it possible? and if it is possible how would I be able to do it (and sorry if this is a stupid question i am a beginner)\nAnswer: At the moment it isn't possible, you can only stream audio.",0.0,0.00078713894,6.19587694927759e-07
15,"Question\nmy goal is to make a bot that is able to show youtube videos through screen sharing or camera. Does anyone know how to do it?
I tried to find out how to do it but I have not managed to find something similar on internet, even on stack overflow.\nAnswer: It happens that zoom will share a video in the way that you want.",0.0,0.05125475,0.0026270493399351835
16,"Question\nLet's say i need to have the javascript in a seperate file, then how can i add a {{variable}} for example in the javascript file without having to use a <script> tag in the HTML?
Is that even possible with jinja?\nAnswer: You are not limited to generating HTML files with Jinja. You can generate a JS file that uses a {{ variable }}.",0.0,0.27857685,0.07760506123304367
17,"Question\nI have a Django server running in an elastic beanstalk environment. I would like to have it render HTML templates pulled from a separate AWS S3 Bucket.
I am using the Django-storages library, which lets me use static and media files from the bucket, but I can't figure out how to get it to render templates.
The reasoning for doing it like this is that once my site is running, I would like to be able to add these HTML templates without having to redeploy the entire site.
Thank you\nAnswer: To my best knowledge, Django-storages is responsible for managing static assets and media files, it doesn't mount the S3 bucket to the file system, what you might be looking for is something like S3Fuse which will mount the bucket on the File System, which will allow you to update the template and have them sync. This might not be the best solution because even if you got the sync to work, Django might not pick those changes and serve the templates from memory.
I believe what you're looking for is a Continuous Delivery pipeline, that way you won't be worried about hosting.
Good Question though.",0.0,0.27101666,0.07345002889633179
18,"Question\nI have two USB to CAN devices (can0 and can1), they both are connected to a Linux machine which has socketcan installed in it. I have read the basics of CANopen protocol, i have not seen any example that can establish communication between two CANopen devices using Python CANopen library.
I read in the documentation that each devices must have a.eds file, so I took a sample.eds file from the Python CANopen library from christiansandberg github and trying to establish communication and make them talk to each other using PDO's, but I could not able to do that.
We have a battery and wanted to communicate with it, the battery works on can-open protocol and they have provided the.eds file for the battery. I guess a usb2can device with the CANopen Python library can do the work. But I just don't know how to establish communication between the usb2can device and the battery. It would be helpful with any insights in framing the packets.\nAnswer: This is what you need to do:

Get the necessary tools for CAN bus development. This means some manner of CAN listener in addition to your own application. It also means cables + terminating resistors. The easiest is to use DB9 dsub connectors. An oscilloscope is also highly recommended.
Read the documentation about the device to figure out how to set node id and baudrate, or at least which default settings it uses.
Find out which Device Profile the device uses, if any. The most common one is CiA 401 ""generic I/O module"". In which case the default settings will be node id 1, baudrate 125kbps.
Your application will need to act as NMT Master - network managing master - on this bus. That is, the node responsible for keeping track of all other nodes.
If the device is CANopen compliant and you've established which baudrate and node id it uses, you'll get a ""NMT bootup"" message at power up. Likely from node 1 unless you've changed the node id of the device.
You'll need to send a ""NMT start remote node"" message to the device to bring it from pre-operational to operational.
Depending on what Device Profile the device uses, it may now respond with sending out all its enabled PDO data once, typically with everything set to zero.
Now check the documentation of the device to find out which data that resides in",0.40816328,0.31341648,0.008976955898106098
19,"Question\nI have a number of free functions in a couple of Python modules and I need to create a UML Class Diagram to represent my entire program.
Can I represent a free functions in a Class Diagram somehow or do I need to create a Utility Class so I can represent them in my Class Diagram?\nAnswer: You will need to have some class in order to represent a ""free function"". You are quite free in how to do that. What I usually do is to create a stereotyped class. And it would be ok to use «utility» for that. Anything else would work, but of course you need to document that in your domain.
Usually a stereotype is bound to a profile. But most tools allow to use freely defined stereotypes. Though that is not 100% UML compliant it is quite a common practice.",0.0,0.24858189,0.06179295480251312
20,"Question\nI have a number of free functions in a couple of Python modules and I need to create a UML Class Diagram to represent my entire program.
Can I represent a free functions in a Class Diagram somehow or do I need to create a Utility Class so I can represent them in my Class Diagram?\nAnswer: Even though UML was conceived in a time, when object orientation was hyped, it doesn't mean that it cannot be used for functions. What many don't realize is, that Behavior in the UML is a Class. Therefore, any Behavior can be shown in a class diagram. Just put the metaclass in guillemets above the name, e.g. «activity». If you plan to describe the function with an activity diagram, that makes perfect sense. However, if you plan to describe it in (pseudo) code or in natural language, you can use «function behavior» which is defined as a behavior without side effects. Or, if it can have side effects, just use «opaque behavior».",0.0,0.30193114,0.09116241335868835
21,"Question\nI am developing a virtual stock market application on django and came upon the following problem. In any stock market application, there is an option of limit buy, stop loss and target sell. This essentially means to buy a share if it ever touches a price which is lower than the current price, to sell a share if it touches a very low price and to sell a share if it touches a high price respectively. For this, the server needs to constantly monitor the live data coming in from the API of a particular stock and perform the action if it happens. However, during this time no one may be making any requests on the site so how do I get django to monitor the prices of the stocks every 5 seconds or so to check if the order needs to be executed or not?\nAnswer: The common approaches here would be to create a Django management command that performs the checks, you can then use a cronjob on your server to schedule this every minute.
Alternatively, you can use an asynchronous worker, which can also be used to schedule repetitive tasks frequently. The most commonly used solution is Celery but it does have a bit of a learning curve (which tends to manifest itself in reliability) that some other solutions such as Dramatiq seem to be trying to improve upon. Celery is probably the easiest to find instructional information for though.",0.0,0.3683111,0.1356530785560608
22,"Question\nI'm wondering how does it work with python package repositories for CentOS (and also other distributions) as I can't find any article about that. Where do python packages/version come from?
My question comes from fact that I want to install python package Quart, and it offers only 2-years old package version 0.6.15 on both CentOS 7 and 8, while on Ubuntu it offers latest 0.14.1.\nAnswer: The Quart 0.6.* releases are the last ones to support Python 3.6. If you install Python 3.7+ you can then install the latest Quart versions.",0.40816328,0.3032195,0.011013197712600231
23,"Question\nI was reading about the Django UserPassesTestMixinmixin, and I came across the term CBV Mixin. What is this, and how is it useful? Is a CBV Mixin a general type of mixins, and are there CBV mixins in any other framework apart from Django?\nAnswer: CBV is just a shortcut from Class-based views, which is a generic term for any view in Django that is defined as a class in your code, especially one inheriting from django.views.View.
So the CBV Mixin is just any mixin that can be used in a Class-based view.",0.0,0.34446168,0.11865384876728058
24,"Question\nI have a React Native app where I want to send an image to my Flask backend to do some image processing (annotations) then return this new image back to React Native to display it.
I spent a whole day trying to figure this out but was unsuccessful. Does anyone have any ideas on how to achieve this?
I plan on using Firebase's storage system to store these images so I wouldn't mind using that either if that makes things easier.
What I've tried so far is sending the image uri to Flask and read the image file and was able to do the image processing however I couldn't figure out how to send the new image back to React Native...\nAnswer: How are you sending the image to Flask right now?
Typically you could implement an async function on RN that awaits a response.
In plain English:
A function that uploads an image to the back end and awaits for the image to be processed.
Expects in return a URL of the image on the back-end (or Firestore).",0.0,0.26563698,0.07056300342082977
25,"Question\nSo, here is the problem I ran into, I am trying to build a very small-scale MVP app that I will be releasing soon. I have been able to figure out everything from deploying the flask application with Dokku (I'll upgrade to something better later) and have been able to get most things working on the app including S3 uploading, stripe integration, etc. Here is the one thing I am stuck on, how do I generate SSL certs on the fly for customers and then link everything back to the Python app? Here are my thoughts:
I can use a simple script and connect to the Letsencrypt API to generate and request certs once domains are pointed to my server(s). The problem I am running into is that once the domain is pointed, how do I know? Dokku doesn't connect all incoming requests to my container and therefore Flask wouldn't be able to detect it unless I manually connect it with the dokku domains:add command?
Is there a better way to go about this? I know of SSL for SaaS by Cloudflare but it seems to only be for their Enterprise customers and I need a robust solution like this that I don't mind building out but just need a few pointers (unless there is already a solution that is out there for free - no need to reinvent the wheel, eh?). Another thing, in the future I do plan to have my database running separately and load balancers pointing to multiple different instances of my app (won't be a major issue as the DB is still central, but just worried about the IP portion of it). To recap though:
Client Domain (example.io) -> dns1.example.com -> Lets Encrypt SSL Cert -> Dokku Container -> My App
Please let me know if I need to re-explain anything, thank you!\nAnswer: Your solutions is a wildcard certificate, or use app prefixing.
So I'm not sure why you need a cert per customer, but let's say you are going to do
customer1.myapp.com -> routes to customer1 backend. For whatever reason.
Let's Encrypt lets you register *.myapp.com and therefore you can use subdomains for each customer.
The alternative is a customer prefix.
Say your app URL looks like www.myapp.com/api/v1/somecomand
you could use www.myapp.com/api/v",-0.35714287,0.06399965,0.17736102640628815
26,"Question\nI have a dataset with thousands of rows. Each row is a person, that I need to insert into 4 clusters. I know that have many possibles to do that and to find the best clusters, but in this case, I know the characteristics of each cluster. Generally, with ML, the clusters are find with IA.
For example, imagine that I have 4 columns to look: money_spending, salary, segment, days_to_buy. Also, I have:
Cluster 1 -> money_spending: 350-700
salary: 700-1000
segment: farmacy
days_to_buy: 12
Cluster 2 -> money_spending: 500-950
salary: 1000-1300
segment: construction material
days_to_buy: 18
Cluster 3 -> money_spending: 900-1400
salary: 1200-2000
segment: supermarket
days_to_buy: 20
Cluster 4 -> money_spending: 250-600
salary: 550-1000
segment: farmacy
days_to_buy: 30
What's the best way to apply this to my dataset? I would use k-nearest, but I don't know how to use my cluster information.
Can someone help me?
Plus: If I have more columns or more clusters the solution keeps works?
Edit: My original dataset only have the columns. The clusters are knowing, but are not in dataset. The job is exactly apply this cluster information to dataset. I don't have any idea how to do that.\nAnswer: You can try the following approach:

Run K means and find the best number of k using the Elbow method or Silhouette graph.

Use the cluster labels as a class.
e.g. if 4 is the optimal number of the cluster then class=0,1,2,3 ( which will be the cluster labels)

Merge the class with the original dataset and treat it as a supervised learning problem

Try running any classification model after the train test split.

See the classification report to check model performance.


PS

Try using data with normalization too as many clustering algorithms are sensitive to outliers.

Please see if the class is somewhat equally distributed like  1000,800,1150,890 and not 1500,80,150,..etc as",0.0,0.023103297,0.000533762329723686
27,"Question\nI try to use Coverage-Gutters. But the documentation is quite complicated and I kinda don't understand on how can it be used. I already installed the extension but I tried a number of things and it seems to be not working. Is there anyone know how to use it?\nAnswer: The gutters are based on coverage output file, for example the coverage.xml file.
Configure a coverage plugin, run your tests with the coverage plugin and let Coverage-gutters watch.
An example for a Django:
$ coverage python manage.py test
$ coverage xml
A generated coverage.xml has now been written in your project's root and your VSCode Coverage-Gutters should now be visible.",0.40816328,0.48248369,0.005523522850126028
28,"Question\nso I have more of a general question that I can't wrap my head around, and I haven't seen explicitely explained within the docs. So let's take a random two events from my simulation (what they exactly are shouldn't matter in the scope of this question)
10.1622 Customer02: Do something
13.6176 Customer08: Do somethiing
The first column is the internal time these events took place. Can I ask someone to explain what is the interpretation of these numbers? Are these simply meant to be real world seconds, meaning that 3.5~~  real world seconds passed between the first event and the second in the simulation and that the first event took place 10 real world seconds into the simulation?
What is the practice if I want times in my simulation (like interval between customers arriving, the time it takes to serve a customer) etc. to be expressed in real-world time? Let's say I have a variable ""intervalbetweencustomers"" which is set at ""10.0"" at the moment. If I want it to have the value of a real-world minute, how do i do that?\nAnswer: The ""tick"" of the simpy clock can be in any unit you want (seconds, minutes, hours, ect)
Tick are not integers so you can have half a tick.
Just pick a unit and convert everything to that unit when you need a time related parameter for a simpy function like env.timeout
simpy does not have a time units as a parameter so any conversion you will need to do yourself.  There are python libraries if you need to convert dates, or the difference between two dates, to a number",0.40816328,0.29029992,0.013891770504415035
29,"Question\nTLDR:
Is there a way to create a hex value between 0x20 and 0x7E with 5 volts? Is there a cheap component on the market or circuit logic that can achieve this?
I'm not sure what the proper terminology for this is, but here's what I'm trying to do:
I have a bluetooth module connected to my pico via UART0 TX and UART0 RX. The use for this is a bit long to explain, but essentially, I want the bluetooth module to work without my pico attached to it. I have a device that outputs a signal, the pico reads the signal, then it tells the bluetooth module to transmit to the receiver. However, since the data to transmit isn't actually important, it makes sense to cut out the pico and simply have the bluetooth module read the signal directly then transmit.
I have the device that outputs exactly when I want, but it outputs the equivalent of 00 in hex. My computer is connected via bluetooth and can read it just fine. However, the pico, reading the input through RX, can't. I've found no way for micropython using UART to read 00 - UART.any() and UART.read() want a character, and 00 only corresponds to NULL.
So essentially, I need some way to transmit a hex value between 0x20 and 0x7E without using the raspberry pi pico. Is there some kind of component that is able to do this? In practice, the bluetooth module will be connected to 5V power with up to 5 amps.
Any idea on how to get the Pico to read 00 in hex through the RX pin is welcomed too. The purpose of this is to not need multiple Picos, since the receiver and the transmitter will be a good distance from each other.\nAnswer: I found the issue. The pico actually can accept 0 through the UART RX pin. The issue was me having a wire misplaced. My computer saw the 0 input which made me think the pico couldn't handle it, but in fact it was never receiving it. Thanks for the help Kotzjan. Would have been interesting to fake a value into the port though!",0.40816328,0.2432977,0.027180660516023636
30,"Question\nI want to know the difference between a pyspark dataframe and a createOrReplaceTempView
I know that with pyspark dataframe I have to use python, and with the createOrReplaceTempView  it is SQL, but in terms of memory, using the cluster, parallelizing, both are the same?
For example if I use.toPandas() I will put all the data in memory, does something similar happends with createOrReplaceTempView? or is still distributed?
Also I would like to know, if I use CREATE OR REPLACE TEMP VIEW tablename how do I pass this table to a pyspark dataframe
Regards\nAnswer: I want to know the difference between a pyspark dataframe and a createOrReplaceTempView


I know that with pyspark dataframe I have to use python, and with the createOrReplaceTempView it is SQL, but in terms of memory, using the cluster, parallelizing, both are the same?

They're both should be similar

For example if I use.toPandas() I will put all the data in memory, does something similar happends with createOrReplaceTempView? or is still distributed?

.toPandas() collect all data and return to driver's memory, createOrReplaceTempView is lazy",0.0,0.24634111,0.06068394333124161
31,Question\nI followed the official mediapipe page but without any result so can someone help to install mediapipe in raspberry pi 4 in windows it is easy to install it and use it but in arm device like raspberry pi i did not find any resources.\nAnswer: if you use python3 you can try sudo pip3 install mediapipe-rpi4,0.10204082,0.11470127,0.00016028703248593956
32,Question\nI followed the official mediapipe page but without any result so can someone help to install mediapipe in raspberry pi 4 in windows it is easy to install it and use it but in arm device like raspberry pi i did not find any resources.\nAnswer: I ran the command sudo pip3 install media pipe-rpi4. This worked. When I try to import the module in python I get  ModuleNotFoundError: No module named ‘mediapipe.python._framework_bindings’,0.0,0.15724272,0.02472527138888836
33,"Question\nI am trying to run my code in Solaris environment with Python 3.7.4 [GCC 7.3.0] on sunos5. When importing necessary libraries from scipy import stats I face this issue. Does anybody know how can I fix this?
Thank you in advance.\nAnswer: This is a library linking issue. Try the following, as it may need re-installing, or updated:
pip install pyhull
If that doesn't work, you may have to recompile python, and any additional libraries or utilities (coreutils, binutils, glibc, even gcc).
If you have the Oracle C compiler, you can try that by using cc, or gcc if that is easier. The Oracle C compiler would be specific to your Solaris installation and may work better.
If you do not want to do that, you might want to try a packaged version of python for your version of SunOS / Solaris and then install pyhull or qhull using pip.",0.0,0.26197356,0.0686301440000534
34,"Question\nhow to check whether the user is logged in or not from admin panel
what are the changes i need to make in models.py and admin.py to achieve this
from django.contrib.auth.models import User\nAnswer: You can't really. Django Contrib Admin doesn't track this information. The most you can get is if User has is_staff or is_superuser to check if they could potentially visit django.contrib.admin interface.",0.0,0.10595238,0.011225907132029533
35,"Question\nlack of indentation color, occurs problem where the indentation is missing. is there anyone who can help me to configure IDLE to show color (light\dim color) in each indentation.\nAnswer: 'Show color in each indentation?""
In my Python 3.7 IDLE, I managed to distinguish the multiple levels of indentation in iterations (by default, the IDLE would show orange color for starting code text). It seems that we only able to adjust the indentation space (default is 4 spaces for python) and the color of entire coding text.
We can adjust the properties in Python shell --> Options --> Configure IDLE.",0.0,0.36637247,0.13422878086566925
36,"Question\nI have created a chatbot using nltk, keras and tkinter. And i have also created a website using python and flask. how can i intergrate both of them. i.e i can i make my chatbot run after the website gets open(run)
when i am importing the chatgui.py(this is chat bot file) and executing it in my main.py(this is the python file file that is building the website using the flask framework), only any one of them is running, not the both.
Plz suggest me some idea, how can i make both of them run.\nAnswer: When a client visits your site, the client computer will not execute python code. Only the server side will execute the python code, so the tkinter part of your app is not needed. The user's gui is rendered with html/javascript in their browser.
There are a lot of ways to go about it, but I think the most common approach would be to scrap the GUI part written with tkinter, and instead re-create a javascript based GUI that will be served by flask. Have chats instead pass between the client and server with javascripts fetch api.",0.0,0.14741099,0.021730000153183937
37,"Question\nI'm trying to figure a way to know how to differentiate between methods and property/attribute, and how to use them correctly without getting mixed up (when to use parenthesis when not to)
From what I understand, (correct me if I am wrong):

Methods are functions that takes in an input (exp a value) and returns an output;  *exp.sum()


Property/attribute tells you additional information or describe the characteristics  *exp.index.shape.columns

My question is,.info() and.describe() are somewhat similar to.shape and.index which give you the description and info but why is it a method and not an attribute/property?\nAnswer: The rough rule of thumb I would offer is to use an attribute for data that can be used as stored, and a function for data that needs to have something done to it before it's returned.
In your example,.shape just returns the tuple as it is stored by the system. It's a straightforward attribute.
By comparison,.info() and.describe() both apply filtering/formatting etc to the data before it is returned. You can also optionally pass them parameters to specify how the data is filtered/formatted before it is returned to you.
There are other reasons why attributes might be accessed through functions, such as using getters and setters to access protected attributes, but in your present examples it's because the functions don't just return the data, they do something to the data first.",0.0,0.36376357,0.13232393562793732
38,"Question\nI want to mimic the function that my keyboard's rgb software does using a piece of python code so I need to find what info the software sends to the keyboard.\nAnswer: you can use the Event Viewer if you are using Windows 10.
Start Windows Event Viewer through the graphical user interface. Use the below steps to open Event Viewer on Windows:

Open Event Viewer by clicking the Start button.
Click Control Panel.
Click System and Security.
Click Administrative Tools.
Click Event Viewer.",0.0,0.38447535,0.14782129228115082
39,"Question\nI'm making a cyberbullying detection discord bot in python, but sadly there are some people who may find their way around conventional English and spell a bad word in a different manner, like the n-word with 3 g's or the f word without the c. There are just too many variants of bad words some people may use. How can I make python find them all?
I've tried pyenchant but it doesn't do what I want it to do. If I put suggest(""racist slur""), ""sucker"" is in the array. I can't seem to find anything that works.
Will I have to consider every possibility separately and add all the possibilities into a single dictionary? (I hope not.)\nAnswer: You could try looping through the string that you are moderating and putting it into an array.
For example, if you wanted to blacklist ""foo""
x=[[""f"",""o"",""o""],["" ""], [""f"",""o"",""o"",""o""]]
then count the letters in each word to count how many of each letter is in each word:
y = [[""f"":""1"", ""o"":""2""], ["" "":""1""], [""f"":""1"", ""o"":""3""]]
then see that y[2] is very similar to y[0] (the banned word).
While this method is not perfect, it is a start.
Another thing to look in to is using a neural language interpreter that detects if a word is being used in a derogatory way. A while back, Google designed one of these.
The other answer is just that no bot is perfect.
You might just have to put these common misspellings in the blacklist.
However, the automatic approach would be awesome if you got it working with 100% accuracy.",0.0,0.21769363,0.047390516847372055
40,"Question\nI'm making a cyberbullying detection discord bot in python, but sadly there are some people who may find their way around conventional English and spell a bad word in a different manner, like the n-word with 3 g's or the f word without the c. There are just too many variants of bad words some people may use. How can I make python find them all?
I've tried pyenchant but it doesn't do what I want it to do. If I put suggest(""racist slur""), ""sucker"" is in the array. I can't seem to find anything that works.
Will I have to consider every possibility separately and add all the possibilities into a single dictionary? (I hope not.)\nAnswer: Unfortunately, spell checking (for different languages) alone is still an open problem that people do research on, so there is no perfect solution for this, let alone for the case when the user intentionally tries to insert some ""errors"".
Fortunately, there is a conceptually limited number of ways people can intentionally change the input word in order to obtain a new word that resembles the initial one enough to be understood by other people. For example, bad actors could try to:

duplicate some letters multiple times
add some separators (e.g. ""-"", ""."") between characters
delete some characters (e.g. the f word without ""c"")
reverse the word
potentially others

My suggestion is to initially keep it simple, if you don't want to delve into machine learning. As a possible approach you could try to:

manually create a set of lower-case bad words with their duplicated letters removed (e.g. ""killer"" -> ""kiler"").

manually/automatically add to this set variants of these words with one or multiple letters missing that can still be easily understood (e.g. ""kiler"" +-> ""kilr"").

extract the words in the message (e.g. by message_str.split())

for each word and its reversed version:
a. remove possible separators (e.g. ""-"", ""."")
b. convert it to lower case and remove consecutive, duplicate letters
c. check if this new form of the word is present in the set, if so, censor it or the entire message


This solution lacks the protection against words with characters separated by one or multiple white spaces / newlines (",0.0,0.26457542,0.07000015676021576
41,"Question\nI have a vector of floats V with values from 0 to 1. I want to create a histogram with some window say A==0.01. And check how close is the resulting histogram to uniform distribution getting one value from zero to one where 0 is correlating perfectly and 1 meaning not correlating at all. For me correlation here first of all means histogram shape.
How one would do such a thing in python with numpy?\nAnswer: You can create the histogram with np.histogram. Then, you can generate the uniform histogram from the average of the previously retrieved histogram with np.mean. Then you can use a statistical test like the Pearson coefficient to do that with scipy.stats.pearsonr.",0.0,0.09612572,0.009240154176950455
42,"Question\nI am new in python and I want to make writing bot with PyAutoGui. It works good but it cant write character like this ""č"", ""š"", ""ř"", ""ž"".How to write this special symbols?\nAnswer: Ok, after some time I finally know how to do this. I just use another plugin ""keyboard""",0.0,-0.031784892,0.0010102794039994478
43,"Question\nWe have an existing python application (let's call it control app) that does operation data logging as well as smaller controlling tasks on a machine. We want to extend this application with a web interface, which is based on flask (let's call it web app). Both parts, the control app as well as the web app, are already present, however, the setup feels somehow fishy. In the process of rethinking the setup, I'm somehow undecided on how to structure those two parts.
At the moment, the control app gathers machine data and stores it in a postgres database. Based on several machine states, additional operations are performed that provide new input for the PLCs that control the machine.
The web app currently polls the database to react to machine states to e.g. update visualisation data, change some (state representing) images and such things.
The web app polling the database is the part that somehow smells. So my idea was to unify both apps into one to have the web app tightly coupled to the control app to be able to react on machine state changes instead of polling the database for those state changes.
Based on that idea, I'm wondering how to add a flask app to an existing python app. When I'm not mistaken, the flask app consumes the application's main thread, which would break to already existing logic. Thus I would need to have one of the two parts running on another thread. Thinking about this problem, I'm further wondering whether this merging is a good idea at all.
So, the questions are: Is it a good idea to merge both applications? If yes, how to merge them without breaking one of them? If not, how else should I try to get rid of the database polling (how to synchronize and also move some data from the web app to the control app)?\nAnswer: It's not a good idea to merge them per se -- problems in one part will affect the other, and this sort of tight coupling is a bad idea both because you can't run the two parts of the program on separate machines and because if one crashes, so does the other one. It's better to have them communicating over some sort of protocol.
If I were designing this, I would probably do the same thing as you did, except that instead of using an SQL database for this, I would use something like Redis which stores its data in memory. Redis allows you to subscribe to events rather than poll for updates,",1.0,0.27575868,0.5245254635810852
44,"Question\nSo I was wondering there is a way to directly change the x and y coordinate of a canvas object after you created it. I know.move function but it only changes the coordinate but not directly setting it.\nAnswer: You can use.moveto(item_id, x, y) to move the canvas item to absolute position (x, y).",0.20408164,0.28420633,0.006419965997338295
45,"Question\nI am writing a trading bot in python for Binance platform. I have selected 300 cryptos. Binance has a websocket API for each  pair. I am able to fetch price data for one pair. I need to parallely fetch prices for 300 cryptos and do some calculations. The data is pushed every 100ms. Each pair has a different url. So I guess I need to open 300 websocket connections in parallel.
All of this should be done under 100 ms and store the data in a single list. I haven't used multiprocessing, multithreading, asyncio etc so I have no I idea how to do this in python.\nAnswer: I would try with:

Use a *.yaml with all urls as config file
Have an empty and global list in the main thread where you will store the newest value (list) or more (dict) sent by each socket url
Open 1 socket per url in an own Thread with threading.thread.
Check if you have to ping from time to time so your client isnt disconnected.",0.0,0.07537937,0.005682049784809351
46,"Question\nI am creating a bot for my server but someone is removing the reaction on my server, so I want to detect that who is removing the reaction. But I have no idea how to do that or even if it is possible or not.
Your help is highly appreciated\nAnswer: Unfortunately, The API does not give the information on who removed the reaction.",0.0,0.19849259,0.03939930722117424
47,"Question\nI currently have a socketio webserver using WSGI and I host it with gunicorn. I also have set up a javascript client which uses a web browser. I've managed to get these two to be able to communicate.
I'm working on creating an information service which takes events which happen in a game from a log and then parses it through a seperate python script to create a readable GUI. But that bit doesn't really matter.
Once I have my string of text from my seperate python program, how do I send it to all clients connected to my webserver? The program isnt part of the socketIO server so, as far as I'm aware, can't use emit().
My idea was to create a seperate python client which connected to the socketio server and do it that way. I've illustrated the flow below:

socketIO python Client -- DATA --> socketIO server
socketIO server -- DATA --> ALL socketIO clients.

I'm struggling to be able to work out a way to perform this? Could anyone help or suggest a more efficient way? Let me know if my explanation is unclear.\nAnswer: Thanks Miguel, I hadn't seen that in the documentation. Despite reading it over and over, I was stuck for a while. My stupid mind didn't come across the thought that I'd need to install Redis-server...
All working now!
Lesson learned: check requirements for my program...",0.0,-0.05201149,0.0027051949873566628
48,"Question\nI have an app using twilio.
I have made the twilio number public and setup call forwarding from that number to my personal number to show alerts both on the app and my phone.
The issue is that I am unable to identify the caller on my phone as it shows the twilio number instead of caller number on my phone.
Is there a possible way to show the caller's number on my phone when forwarding the call.\nAnswer: This is possible. Just don’t set the callerId attribute of the Dial verb and it will pass the callerId of the inbound call leg.",0.40816328,0.20100784,0.042913373559713364
49,"Question\nI coded my own virtual assistant in python. I know how to start apps through my voice via os.system or os.startfile, but I don't know how to close the current window?\nAnswer: os.system('taskkill /F /IM processname')
e.g:  os.system('taskkill /F /IM chrome.exe')",0.0,0.31271935,0.09779338538646698
50,"Question\nCan anyone please suggest me how to implement this specific use case?
Every morning a python job from on premise server

clears the S3 folder contents and then
extracts files on premise from db and then
uploads the extracted files to the above S3 folder.

Sometimes, the python script which is scheduled to run through windows task scheduler job is just starting and finishing in seconds without doing any work.
In order to send an alert notification when this happens, I am thinking of writing a lambda that is scheduled to run after like 5 minutes to see if the folder contents is deleted or not in the last few minutes to an SNS topic. Is this doable? Here the lambda trigger is not an S3 event but a scheduled event that can able to read S3 delete action.\nAnswer: Sure, you could do that.
An easier method might be to add a step at the end of the Windows task that basically says ""The job completed successfully"". It could upload this file to S3.
Then, the scheduled AWS Lambda function could simply check the LastModified date of that file. If it is older than one hour (or whatever), then send an alert via Amazon SNS.",0.13605443,0.28415465,0.021933676674962044
51,"Question\nCan anyone please suggest me how to implement this specific use case?
Every morning a python job from on premise server

clears the S3 folder contents and then
extracts files on premise from db and then
uploads the extracted files to the above S3 folder.

Sometimes, the python script which is scheduled to run through windows task scheduler job is just starting and finishing in seconds without doing any work.
In order to send an alert notification when this happens, I am thinking of writing a lambda that is scheduled to run after like 5 minutes to see if the folder contents is deleted or not in the last few minutes to an SNS topic. Is this doable? Here the lambda trigger is not an S3 event but a scheduled event that can able to read S3 delete action.\nAnswer: On AWS:
You can set up an event notifier on the s3 bucket supporting event type s3:ObjectRemoved:, s3:ObjectCreated:, where the event notification can be on SNS Topic
[https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-event-notifications.html]",0.13605443,0.32320058,0.03502368554472923
52,"Question\nWe want to implement attribute prediction and image search model in single application.
Step1. Upload of image,will give attribute details.
example,If we upload dog image, then attribute details will display like color, breed.
step2. On click of any attribute will show similar matching result.
example on click of attribute  like breed it will display matching breed  dog images from image data.
Please suggest,how we can achieve that\nAnswer: With step 1, I recommend you should use a multi-label images classification. It will help you get attribute of input image like what object in in the image, what color is it,...
With step 2, you can query the attribute in your database or you can use a recommendation system for more advance.",0.40816328,0.04031831,0.13530991971492767
53,"Question\nI have a string that I would like to check for comparison.
df['ACCOUNTMANAGER'][0] this value has some special character that I cannot compare using string comparison. I tried to compare using bytes but it failed. I would like to check how the data is stored there for comparison. Is there a way to do this?\nAnswer: I figured it out. It was being stored as utf-8 encoded format and now comparison works. I used for comparison byte b'\x8a' in if statement.",0.0,0.06344855,0.004025718197226524
54,"Question\nI'm new to python (and to programming in general). I have no experience with GitLab, but I've been given a directory in GitLab that should have all the scripts I need to install all the python modules I need, but I'm not sure how to do that. I can download the directory as tar.gz or tar.bz2 or tar, but I'm not familiar with these types of files and not sure which I need or what to do with it? Any help would be greatly appreciated.\nAnswer: If you have git installed, you can clone the project to your computer with the command:
git clone <url>
You can get the URL from GitLab, there are for HTTP or ssh.
After it, if the project has a requirenment.txt file, you can install all requirements with: pip install -r requirements.txt.  Else, you will need to install all libraries with pip or conda.",0.0,0.30592334,0.09358908981084824
55,"Question\nI made a wrapper for aws cloudwatch in python with boto3.
And I am doing the unit test with moto, everything go smoothly in my local PC.
After I pushed my code to Azure Repo to trigger the pipeline, I always get the error message.
I want to know how is the compatibility of Azure pipeline to run the'moto' library.\nAnswer: After testing for few days I found the problem finally.
I have to scale my searched day to contain my logs timestamp
It means my timestamp of metrics is datetime.today(), and I need to set the Metric Data Queries start date and end date parameter to datetime.today() -(+) timedelta(1) to scale the query date, I don't really know why it work without the scaling in local PC.",0.0,0.10020578,0.010041197761893272
56,"Question\nI try to establish a website-connection(web-login) via python. The login appears to need 3 keys (next to password and username of course). 2 of them are handed over via get and the third one is a csrf-key. The csrf-key is not contained in the html body of the current page nor is it in the link. (I checked this explicitly by using strg+f)
What other common ways are there to generate the csrf-key on the fly? (I explicitly checked by inspecting the post request that the csrf is included in the request, but I don't understand how it gets there)\nAnswer: The csrf key must be somewhere in the webpage you are trying to access.
The csrf key is not generated by the user, instead, it is a unique secret value generated by the server-side application and transmitted to the client.",0.0,0.123138905,0.015163189731538296
57,"Question\nI'm trying to make a 3D game in Ursina. I managed to have a working FirstPersonController, but it is using WASD. I'm french, and thus have an AZERTY keyboard, which isn't really convenient... Do you know how I could change my controls to move the FirstPersonController?
I'd also like to be able to use ""special"" buttons of my mouse. For exemple, the ""forward"" and ""backward"" ones, to sprint and jump!\nAnswer: For changing keys you need to scroll up and when you see""ursina.prefabs.first_person_controller import FirstPersonController"" then
you do ctrl + click on ""first_person_controller"". Next, you scroll down until you get to the line 44-45 and you change.If it doesn't work use visual studio code.",0.0,0.22516721,0.05070027336478233
58,"Question\nI'm trying to make a 3D game in Ursina. I managed to have a working FirstPersonController, but it is using WASD. I'm french, and thus have an AZERTY keyboard, which isn't really convenient... Do you know how I could change my controls to move the FirstPersonController?
I'd also like to be able to use ""special"" buttons of my mouse. For exemple, the ""forward"" and ""backward"" ones, to sprint and jump!\nAnswer: You can either copy the code, which you're probably going to do if you need custom movement anyway, or you can rebind the keys with input_handler.rebind(to_key, from_key)",0.27210885,0.3141924,0.0017710261745378375
59,"Question\nI have tried searching everywhere but to no avail. Can anyone guide me as to how to pass data from typescript to python? The data is such that I will use it to do webscraping using beautifulSoup over in python. As I'm not putting any implementations here, any general guidance will do :)\nAnswer: Have you tried creating an api on python?
I guess you can pass the data from typescript to python using Http in the form of json or just save the file using typescript and read it using python and edit it as you want.",0.0,0.09879708,0.009760863147675991
60,"Question\nI'm trying to create a magic Square function out of 2d list: [[8, 1, 6], [3, 5, 7], [4, 9, 2]], where It checks if all the columns adds up to 15 from a 3x3 square. I'm stuck on how to write function checks for the sum of columns if anyone can help out that will be highly appreciative. Without using numpy\nAnswer: My advice would be compare the two answers thus far and pick the performance and readability answer  and write a test for the method ensuring people do not break it.",0.0,0.110827446,0.012282722629606724
61,"Question\nIs there anyway to install a lightweight version of TensorFlow for prediction only?
I built a Docker image with TensorFlow 2.5 and python 3.8 slim. The image size is 1.8Gb but I need it to be 1Gb maximum.
As I will only use this Docker image for prediction, I guess there must be a way to reduce TensorFlow and install only the needed modules.
If you know if TF lite or TF serving can help me to reach this out, please tell me how.\nAnswer: I think the only way yo use TF in a lightweight mode is using TFlite or TFLite Micro. For this to work yo need to convert your frozen inference graph to a.tflite file.",0.0,0.2497592,0.062379658222198486
62,"Question\nI need to make a python3.7 installation for CentOS8, so I can install that via rpm/yum rather than building from source on the target machine (need to avoid installing gcc and other build deps there).
Is that a reasonable possibility? I'm comfortable building python from source, but I don't know how to package up the resulting install in a portable way (portable to other machines running the same OS). RPM would be ideal but I'd be happy with a tgz and a known set of yum runtime dependencies.
Note that there is no official CentOS8 python 3.7, only 3.6 and 3.8. I specifically need 3.7.
Googling for ""build python distribution"" or ""build python RPM"" just shows how to build python modules for distribution, not python itself.
(I know miniconda/miniforge is an alternative way to get this done, but I'd prefer to do the build myself.)\nAnswer: I didn't figure out how to build an RPM, but it turns out it's good enough just to build with --prefix=/opt/python37 and then tar that up. On the target machine, untar and add the shared lib to ld.so.conf and it works fine.",0.0,0.20396113,0.041600145399570465
63,"Question\nI am trying to introduce dynamic workflows into my landscape that involves multiple steps of different model inference where the output from one model gets fed into another model.Currently we have few Celery workers spread across hosts to manage the inference chain. As the complexity increase, we are attempting to build workflows on the fly. For that purpose, I got a dynamic DAG setup with Celeryexecutor working. Now, is there a way I can retain the current Celery setup and route airflow driven tasks to the same workers? I do understand that the setup in these workers should have access to the DAG folders and environment same as the airflow server. I want to know how the celery worker need to be started in these servers so that airflow can route the same tasks that used to be done by the manual workflow from a python application. If I start the workers using command ""airflow celery worker"", I cannot access my application tasks. If I start celery the way it is currently ie ""celery -A proj"", airflow has nothing to do with it. Looking for ideas to make it work.\nAnswer: Thanks @DejanLekic. I got it working (though the DAG task scheduling latency was too much that I dropped the approach). If someone is looking to see how this was accomplished, here are few things I did to get it working.

Change the airflow.cfg to change the executor,queue and result back-end settings (Obvious)
If we have to use Celery worker spawned outside the airflow umbrella, change the celery_app_name setting to celery.execute instead of airflow.executors.celery_execute and change the Executor to ""LocalExecutor"". I have not tested this, but it may even be possible to avoid switching to celery executor by registering airflow's Task in the project's celery App.
Each task will now call send_task(), the AsynResult object returned is then stored in either Xcom(implicitly or explicitly) or in Redis(implicitly push to the queue) and the child task will then gather the Asyncresult ( it will be an implicit call to get the value from Xcom or Redis) and then call.get() to obtain the result from the previous step.

Note: It is not necessary to split the send_task() and.get() between two tasks of the DAG. By splitting them between parent and child",0.0,0.3299522,0.10886845737695694
64,Question\nI began my project without creating a virtual environment. Now I am wondering how big of a mistake was that. pip install Pillow not executing no matter what I do to include an image in my classs Model. Can or should I migrate my files to a new virtual environment. Or should I start coding from scratch.\nAnswer: You don't have to start coding from the scratch. You can create a new virtual env using python -m venv <name-of-the-env> and install your dependencies.,1.0,0.14068383,0.7384243011474609
65,"Question\nI am creating an app that allows me to edit users on the frontend. Before doing this I wish to have all the users listed in a page. They are categorized by their group. Users that are in a group show up how they are supposed to. I want to also display users that are not in a group.
This is how I fetch the users in a specific group:
groupuser = User.objects.filter(groups__name='mygroup').
How do I display a user without any group in my template?
I am new to Django and don't know much about this.\nAnswer: You can use the field lookup __isnull.
User.objects.filter(groups__isnull=True) gives you all the users that do not belong to any group.",1.0,0.38344193,0.3801438510417938
66,"Question\nI've got Pycharm installed on my chromebook by enabling linux apps. I've started to learn Python using a tutorial by 'Programming by Mosh'. In one of the projects that he does in the tutorial, he adds an.xlsx file to a project in Pycharm. Mosh (he uses a Mac) did this by right clicking on project and then clicking 'Reveal in Finder' and then pasting the file onto the window that opens. Could you explain how I can do this on my chromebook, because I can't seem to find the 'Reveal in Finder' option.\nAnswer: Both the operating systems are quite different. If you want to create an Excel file(.xlsx is used by Microsoft Excel), you can use Office Online and then download it to your project directory in Chromebook.
I would actually prefer you to use Google sheets instead if you're using Chrome OS.",0.0,0.24482143,0.05993753299117088
67,"Question\nI've got Pycharm installed on my chromebook by enabling linux apps. I've started to learn Python using a tutorial by 'Programming by Mosh'. In one of the projects that he does in the tutorial, he adds an.xlsx file to a project in Pycharm. Mosh (he uses a Mac) did this by right clicking on project and then clicking 'Reveal in Finder' and then pasting the file onto the window that opens. Could you explain how I can do this on my chromebook, because I can't seem to find the 'Reveal in Finder' option.\nAnswer: Go to the dropdown menu of  'Project' at the top and then click 'Project files'. Then just paste your file into it.",0.0,0.26617694,0.07085016369819641
68,"Question\nI would like to know how to do the average, standard deviation and mode statistics of an NxN matrix.
Can anybody suggest how to do this in python?\nAnswer: As pointed out in a comment,  this is easily found by a search.
The scipy library offers functions for all of these, either for the whole array or for specified axes. scipy.stats.describe will give the mean and variance all at once, and standard deviation is easily computed from variance. For mode use scipy.stats.mode
Alternatively for mean and standard deviation, there are numpy.mean and numpy.std respectively.",0.0,0.43824053,0.19205476343631744
69,"Question\nI am coding with the latest version of Visual Studio Code with Python. For some reason I get those red underlines on my code, where there would be a ""problem"". The annoying part of this is that I get it even while still typing the code. Could someone please tell how to remove it?\nAnswer: I think you may also be able to press Ctrl(or command) + Shift+P then type disable error squiggles then disable it like that. This works for c/c++ so I am not sure if it will work on python! This also disables it showing all errors",0.0,-0.09886414,0.009774117730557919
70,"Question\nBefore running buildozer I put a java file in ga/chatme/web/WebHandler.java.
Then, I set the android.add_src to ga/chatme/web but, I cant seem to import it with autoclass('ga.chatme.web.WebHandler')\nAnswer: I fixed it by putting everything in an src folder then pointing android.src to it",0.0,0.11756569,0.013821692205965519
71,"Question\nCan the xlrd module change the file propeties? Such as author,title,subject,etc..
I want to change a.xls file's properties and don't know how to do that.\nAnswer: As far as i know that's not possible with xlrd. The rd part of the name means ""read-only"". You would need to use the xlwt library for writing or one of the newer options like XlsxWriter.",0.40816328,0.22607887,0.03315473347902298
72,"Question\nI've recently started learning how to use Django and I was wondering whether or not you could create a button in HTML that can directly call a python function without changing the URL. I was told this was possible with JS but I wanted to know if you could do that in Python.
Thanks for your time reading this!\nAnswer: Depending on your application you may want to look into Django forms. Otherwise JS (ajax) is the way to go. More details could always help.",0.0,0.116648674,0.013606913387775421
73,"Question\nI'm trying to figure out how to implement a workflow so that a sensor task wait for external dag to complete, for only wait for a certain number of days. It's a daily job so I'd like a sensor job to wait for example 3 days, and on the forth day send out an email, and either waiting or do some other task.
Could someone please help to shed some light on how to achieve this? Also how do we communicate the ""days counter"" from one day to another? Many thanks for your help.\nAnswer: You can use a ExternalTaskSensor with the following configurations:

timeout = 3 * 24 * 60 * 60 - 3 days in seconds, after that time sensor will fail
poke_interval = 12 * 60 * 60 - 12h between sensor checks, you can adjust  it to let say a check every hour. It will reduce number of times when you check the external dag state
mode = ""reschedule"" - in this way the sensor will not occupy worker slot for 3 days, it will be scheduled, executed and if condition is not met it will be rescheduled to be executed in next poke_interval seconds. It's a good practice to use this mode for long running tasks.

Additionally you can build your waiting DAG as wait_task >> [success_task, fail_task] where

wait_task is your sensor
success_task has trigger rule all_success and is followed when   the sensors succeeds
fail_task with all_failed trigger rule and handles scenario when sensor finally return false or timeouts",1.0,0.14114475,0.7376323342323303
74,"Question\nI have a script that downloads a CSV from Outlook, does some things in Excel, and then sends an email out to people.
When I run it manually with Outlook open, it runs perfectly. But when I try to run it with Task Scheduler, I get this error
""com_error(-2146959355, 'Server execution failed', None, None)""
If I close Outlook and run it using Task Scheduler, it works.
My issue is that I want to run this during the workday so I don't have to remember to send it. During the workday, I always have Outlook open.
Does anyone know how to get it so that the scheduled task can run during the day with Outlook open? Any help would be appreciated!\nAnswer: Is it possible that there is a mismatch in security contexts? Try running the script or Outlook (or both) ""as administrator.""",0.0,0.13899755,0.019320320338010788
75,"Question\nWhat I'm trying to figure out is how to go back a position in a string.
Say I have a word and I'm checking every letter, but once I get to a ""Y""
I need to check if the character before was a vowel or not. (I'm a beginner in this language so I'm trying to practice some stuff I did in C which is the language I'm studying at college).
I'm using a For loop to check the letters in the word but I don't know if there's any way to go back in the index, I know in C for example strings are treated like arrays, so I would have a For loop and once I get to a ""Y"", that would be my word[i] (i being the index of the position I'm currently at) so what I would normally do is check if word[i-1] in ""AEIOUaeiou"" (i-1 being the position before the one I'm currently at). Now I don't know how that can be done in python and it would be awesome if someone could give me a hand :(\nAnswer: in Python, strings are iterable, so you can get the [i-1] element of a string",0.0,0.13812816,0.019079389050602913
76,"Question\nI have a dataset of postal codes for each store and nearby postal codes for each. It looks like the following:




PostalCode
nearPC
Travel Time




L2L 3J9
[N1K 0A1', 'N1K 0A2', 'N1K 0A3', 'N1K 0A4', '...
[nan,nan,9,5,nan...]




I know I can explode the data but that would result in tons more rows ~40M. Another preprocessing step I can perform is to remove the values in each list where the travel time is not available. However, then I would need to remove it from the nearPC list.
Is there a way to incorporate networkx to create this graph? I've tried using
G = nx.from_pandas_edgelist(df,'near_PC','PostalCode',['TravelTime'])
but I don't think it allows lists as the source or targets.
TypeError: unhashable type: 'list'
Is there a way around this? If not how can I remove the same indices of a list per row based on a conditional in an efficient way?\nAnswer: You've identified your problem, although you may not realize it.  You have a graph with 40M edges, but you appropriately avoid the table explosion.  You do have to code that explosion in some form, because your graph needs all 40M edges.
For what little trouble it might save you, I suggest that you write a simple generator expression for the edges: take one node from PostalCode, iterating through the nearPC list for the other node.  Let Python and NetworkX worry about the in-line expansion.
You switch the nx build method you call, depending on the format you generate.  You do slow down the processing somewhat, but the explosion details get hidden in the language syntax.  Also, if there is any built-in parallelization between that generator and the nx method, you'll get that advantage implicitly.",0.0,0.22620445,0.05116845667362213
77,"Question\nI am building a website for myself and family in Django and React. Every time I ask them to check my process and get their opinions they get a webpage not found error or this site can't be reached. How do I make is so that anyone on my local network can check my page when its running. This is the default in flask, I don't understand why django runs like this? (safety maybe?) I can't find any good information on how to add an allow all to the ALLOWED_HOSTS section in the settings of the main app.
I also know that you can add IP address, But I'm a noob and I don't follow how to get the ip address of all my families devices without alot of work?? Does anyone have a good solution for this??\nAnswer: It is very simple

just go to your command line and type ipconfig if its windows and ifconfig if it is linux
Note your ip address on your local network
Write that address in allowed hosts in settings.py
Run command python manage.py runserver :
Take any device connect to your local network and in its browser window type servers ip:port number
For eg 192.168.25.1:8080",0.10204082,0.23193175,0.016871651634573936
78,"Question\nI want to find out when an alert happens so I can automatically accept. I've placed self.driver.switch_to.alert.accept() in various places in the code but I always get a selenium.common.exceptions.NoAlertPresentException. When I don't place it anywhere I get an selenium.common.exceptions.UnexpectedAlertPresentException. When I use expected_conditions I get a selenium.common.exceptions.TimeoutException. I don't know what to do at this point. Can anyone help?
Python/Django Backend. Using Selenium (Firefox)\nAnswer: I don't think there's a method for that, but if you know it will happen, make a while loop with try-catch block until the alert happens",0.0,0.49687445,0.24688422679901123
79,"Question\nI was finally able to get python 3.9.0 installed(was using 3.4.2) on my raspberry pi, however I cannot use pygame now stating ""ModuleNotFoundError: no module named 'pygame'. I know where pygame is located, but I have no idea how to get python 3.9.0 to see where it is to read it.\nAnswer: You have to re-install all of your packages when you upgrade to a new major version of Python.  You'll need to pip3 install xxxx again.",0.40816328,0.2599792,0.021958524361252785
80,"Question\nI have been trying to import dlib, cv2, and os into my python project. I have successfully downloaded cv2 and os on the base(root) environment on conda, but i couldn't do it with dlib, so i created a virtual environment to download it, which worked. Now what i know the answer to is, when i wanna import all three at the same time, how can i access both the environments? I am currently using the base environment and therefore was able to import cv2 and os, but when it try it with dlib, i keep getting the error message ""no module named 'delib'.\nAnswer: Since you have used a virtual environment to install dlib, it can only be accessed inside that specific environment and not anywhere else.
You can install both cv2 and os in the above-mentioned virtual environment and use it for your further works instead of using the base environment.",0.0,0.10112417,0.010226096957921982
81,"Question\nI have a flask app in which it has a button to download an HTML report. The download button when hit creates an xlsx file. It worked fine until that app was running on my local server because the python code found my Downloads directory no matter what os it is using python's os module.
Now the app is deployed on a remote server, in this case, how do you let the system know the client's download directory path where this xlsx file can then be created? Or any other pointers for a better solution if I'm missing something?\nAnswer: If i understand correctly - you want to specify a directory to which the file should be downloaded on users computer when he/she hits download button.
This is not possible, and is handled fully by the browser.
Browser processes the request/stream of special type and then creates the output file in the  location specified by the user in browser settings. The os library which you mentioned  relates to your server machine not client, so any os opertaions that you provide in  your code will be executed on your server (like creating a file). So that's why it worked on your local machine - which was server and client at once.
Why it's dissalowed?
Imagine a ""virus file"" being uploaded to your C:\Windows\System32. Web applications could be granted control over your machine with a simple button download. The huge security issue doesnt allow for client's machine access from web application",0.20408164,0.23922247,0.0012348777381703258
82,"Question\nI have a  list = [D1, D2, D3,...]  where each DX is a 2d Array of simmilar dimensions (let's say 4x5). I now want to average the DX. So my result 2d Array should for example be:
 result[0,0] = (D1[0,0] + D2[0,0] +...) / len(list)
 result[0,1] = (D1[0,1] + D2[0,1] +...) / len(list)
etc.
Is there a neat function for this? I somehow can't find the correct terms for googeling this..\nAnswer: You should try to use numpy.mean(). Probably you can create the same list with numpy, and then calculate the mean for axis=None, and you can get the average of the whole list of matrices.",0.0,0.07200754,0.0051850853487849236
83,"Question\nMay I know how to merge 2 excel into 1 like this in python.
I've tried Pandas to merge it by ""name"" and keep the sequence ""index"", but no luck. as there are more than 1 location. so, the result should have 2or more location in row.
Many thanks




index
name
price




1
apple
2


2
orange
3


3
grape
7


4
banana
1


5
kiwi
2.5


6
lemon
1







index
name
location




1
apple
US


2
apple
UK


3
banana
Columbia


4
banana
Costa Rica


5
kiwi
Italy


6
lemon
US







index
name
price
location_1
location_2




1
apple
2
US
UK


2
orange
3
N/A
N/A


3
grape
7
N/A
N/A


4
banana
1
Columbia
Costa Rica


5
kiwi
2.5
Italy



6
lemon
1
US\nAnswer: you can try pd.concat to combine them.",0.0,0.051015317,0.002602562541142106
84,"Question\ni can t use #%% anymore as a jupyternotebook cell. Its not marked as one or visuallized in any way. I tried reinstalling but didn t work.
I recently had a weird message which said something like ""save code browsing activated"" and casually just clicked away.
Any ideas how to get it back to normal?
I programm with #%% 99% of the time so i am kind of lost right now :D
regards martin\nAnswer: Check that you are in the right Python virtual environment, inside your VS Code. If you are in a virtual environment where you do not have the ipython packages installed, that would explain it.",0.0,0.036278665,0.0013161415699869394
85,"Question\nIn a python defaultdict object (like obj1), I can call obj1['hello','there'] and get item. but when the input list is variable (for example: input_list), how I can call obj1[input_list]?
when I call obj1[input_list], python raise this error:
TypeError: unhashable type: 'list'
when use obj1[*input_list], python returns:
SyntaxError: invalid syntax
So what is the correct way to put list as variable in defaultdict?\nAnswer: The error TypeError: unhashable type: 'list' states that list is not hashable, but a dict always needs a hashable key!
If you test my_normal_dict[2,3] you can see that it actually treats these two numbers as the tuple (2,3) because the error is KeyError: (2, 3), so you need to input a hashable iterable like a tuple.
For example, my_dict[tuple(my_list)] should work, as long as all the elements of the tuple itself are hashable!
Note though: If your list is large, it may be relevant that this needs to copy all elements into a tuple.",0.40816328,0.33600056,0.005207457579672337
86,"Question\nI have a select tag, and what my logic is that after the user selects an option, we would send that response and search through the db to find similar records. The problem is how to access that data.
After the user has selected an option, I would take them to other page which is just a GET request page. So how to access the select tag data(which is a POST request data) on a GET request page.\nAnswer: You can send selected data to server with POST and save data in SESSION
and Redirected to Next Page",0.0,0.029993773,0.0008996264077723026
87,"Question\nI'm working on price drop notification application and I'm thinking of using Scrapy for that but, I am not sure how to use it for that, do i need to check the product price regularly after specific interval or is there any other way of doing that.\nAnswer: Scrapy is a good way of scraping the data, but then you still need to figure out how to store the old value, and repeatedly run the scraper to check against it, and send a notification.
For simpler sites, without much page navigation, only using requests and BeautifulSoup is usually easier, I'd only add Scrapy into that mix to handle crawling through multi-page navigation.

Zyte (made by creators of Scrapy, formerly ScrapingHub) have a PaaS offering for doing automatic hourly scrapes, and within that you can call external services for notifications.",0.40816328,0.2794664,0.016562888398766518
88,"Question\nI've been trying to save the state_dict of a Pytorch model with 
torch.save(agent.qnetwork_local.state_dict(), filename) where  filename = datetime.now().strftime('%d-%m-%y-%H:%M_dqnweights.pth')
type(filename) returns str which shouldn't be a problem with torch.save() and it should output a non-empty file. Instead I get an empty file with just the date and time and nothing after that. Putting the date and in the middle of the filename results in an empty file with everything after the date and time cut off.
torch.save(agent.qnetwork_local.state_dict(), 'checkpoint1.pth') and any time I hardcode the string works and gives me the expected non-empty file.
What is going on and how do I fix this?
I am running this code in a Python v3.6.8 virtualenv with Pytorch v1.8.1+cpu on Windows 10.\nAnswer: The colon was the problem in filename = datetime.now().strftime('%d-%m-%y-%H:%m_dqnweights.pth') since it was running on windows.
Changing it to filename = datetime.now().strftime('%d-%m-%y-%H_%M_dqnweights.pth') works as expected.",0.0,0.3908618,0.1527729481458664
89,"Question\nI know that TextBlob ignore the words that it doesn’t know, and it will consider words and phrases that it can assign polarity to and averages to get the final score.
Are there any other problems and defects that I don't know about?
Also, I would like to know how it is possible to fix them.
Considering that we can use TextBlob both with a dictionary and through machine learning, I think a solution could be to use a larger dictionary and improve the train set.
Are my intuitions right?\nAnswer: Most of the Challenges in NLP sentiment analysis tasks are semantic ones like Irony and sarcasm ambiguity in th text,Multipolarity...
Thay why TextBlob may not yield the best resulat depending on your text and if it contains multiples languges, you can add new models or languages through extensions.",0.0,0.09043956,0.008179313503205776
90,"Question\nCan we insert elements in the Queue if maxsize = 0?
what Does this means:- 'A max size of zero ‘0’ means an infinite queue'?
If we will put maxsize= 0 how can it have  infinite queue??\nAnswer: When the documentation says ""the queue size is infinite"", it doesn't mean that infinite memory has been allocated for the queue, nor that you somehow could add an infinite number of elements to it. This is of course not possible, as there are always finite memory constraints.
When you provide 0 to the constructor, the Queue implementation does not itself pose any limit on how many items you could add to the queue. It will dynamically allocate more memory as needed while items are being added to it. It will never ""block"" an insertion, as it would do in the case of a non-zero limit. The only constraint is available memory.",0.0,0.34342384,0.11793993413448334
91,"Question\nI am trying to use kivy as the graphical system and yet i dont know how to put inside a button and a label at the same time where label shows text and button closes popup also there is one import ant thing here it must be a function that can have editable title and contents and buttob(text) and can be launched like: self.popup(text1, text1, text3) Does anyone know how do it?\nAnswer: Kivy popup contents: What do you want? Nobody can answer this!!! Give a precise question (here are some propositions for your next SO post).
I am trying to use kivy as the graphical system: How to use kivy app as a graphical system?
and yet i dont know how to put inside a button and a label at the same time where label shows text and button closes popup

Kivy: how to add a widget (Label/Button) to an app?
Kivy: how to change Label text?
Kivy: how to bind a function on a button click?
Kivy: How to close a popup?

also there is one import ant thing here:.
it must be a function that can have editable title and contents and buttob(text):

I think you should check out what theses terms are: POO, Inheritance, attribute, method.",0.0,0.34995872,0.12247110158205032
92,"Question\nI need to make an algorithm in which the person types a login and password and he has three attempts, I did a ""while x <= 3"" (ox has a value of 0) and then an if inside the while with the login and password condition. But how do I stop while asking for login and password 3 times even hitting the login and password on the first or second attempt?\nAnswer: What about the break keyword. When you want to stop the loop just use the break",0.0,0.33557594,0.11261121183633804
93,"Question\nI'm using a NodeJS server to catch a video stream through a WebRTC PeerConnection and I need to send it to a python script.
I use NodeJS mainly because it's easy to use WebRTC in it and the package 'wrtc' supports RTCVideoSink and python's aiortc doesn't.
I was thinking of using a named pipe with ffmpeg to stream the video stream but 3 questions arose :

Should I use python instead of NodeJS and completely avoid the stream through a named pipe part? (This means there is a way to extract individual frames from a MediaStreamTrack in python)

If I stick with the ""NodeJS - Python"" approach, how do I send the stream from one script to the other? Named pipe? Unix domain sockets? And with FFMpeg?

Finally, for performance purpose I think that sending a stream and not each individual frames is better and simpler but is this true?


Thanks all!\nAnswer: Finally, I found that the MediaStreamTrack API of Python's aiortc has recv().
It's a Coroutine that returns the next frame. So I will just port my NodeJS script to python using this coroutine to replace RTCVideoSink. No piping or whatsoever!",0.0,0.21212366,0.04499644786119461
94,"Question\nI've deinstalled and reinstalled a bunch of Python Versions and edited my system variables in order to clean up the quagmire of pip not being able to install packages to the right locations. Long story short: Sublime now can't find numpy (or any 3rd party module), because it somehow figured that the python 2.3 version that comes with ChemDraw is the one I really want to use, not the 3.9 version I had used previously.
I've tried a bunch of tutorials online (such as resetting everything to factory settings) however nothing seems to unlink the python build system from that python 2.3 version that came with ChemDraw. Even deinstalling everything and installing Sublime 4 keeps that association. Like...how?
So here's my problem: My build system for python is linked to the wrong python version and I don't know how to link it to the python 3.9 that's located in AppData/Programs.
How can I associate the python.exe of python 3.9 in AppData with my python Build System instead of the current python 2.3, which is located ProgramFiles?\nAnswer: Ok, so the problem was that there was a system variable called PYTHONPATH, which I don't remember setting. It seems that Chemdraw, if installed with ChemScript, installs it's own python installation, which is 2.X.
That python install seems to set its own system variable called PYTHONPATH, which Sublime seems to prioritize for it's default python build system.
Delete whatever incorrect path is set in PYTHONPATH and paste the following into it instead:
C:...\Python\Python39\Scripts;C:...\Python\Python39;C:...\Python\Launcher;",0.0,0.31656262,0.10021189600229263
95,"Question\nI have a 1 dimensional array. I want to get a certain percentile(say,5%) of the fitting of this data(Monte Carlo method is best, guassian KDE method is also OK) as fast as possible. Because this function is used millions of times.
My way is using the scipy gaussian_kde.
My question is:

Any other ways to get higher speed of gaussian_kde?
Main cost is kde = gaussian_kde(x, bw_method=0.02) #about 220us\nAnswer: You seems to rewrite a well-optimized function of Numpy called np.quantile. Calling np.quantile(x, 0.05) provide the exact/accurate 5% quantile and is 4 times faster than your optimized implementation! Note that if you need to call this functions a lot of time on different independent arrays, then you can speed up the computation even more thanks to Numba parallelism.",0.0,0.19324166,0.037342336028814316
96,"Question\nIn timeseries analysis I want to set date as an index but one date has 500 entries 50 for each item and 10 for each store. In such case how can I make date as an index.\nAnswer: How are you! First off, why do you need de time as index, maybe you can reconsider that idea?
Anyway using datetime should work and the chances to get a duplicate record is minimal",0.0,0.12571597,0.015804504975676537
97,"Question\nI'm trying to see if a function returns an integer who's value should be either a 1 or 0.
0 == (1 or 0)
0 is equal to 1 or 0, this sounds like it should be true, but it's not.
Why? And how to do what I'm looking to do correctly?\nAnswer: 1 or 0 evaluates to 1, and since 0 is not equal to 1, the expression is false.
I suspect what you are trying to do is something like 0 == 0 or 1 == 0",0.5442177,0.2408976,0.09200309216976166
98,"Question\nI'm trying to see if a function returns an integer who's value should be either a 1 or 0.
0 == (1 or 0)
0 is equal to 1 or 0, this sounds like it should be true, but it's not.
Why? And how to do what I'm looking to do correctly?\nAnswer: You are using the brackets. Which means you are forcing precedence on it. so python will first evaluate the expression 1 or 0  which is 1.  and then it will evaluate the next part,  0 == 1   which is false.",0.0,0.2702062,0.07301139831542969
99,"Question\nI want to remove FC layer of VGG16 and add UNet layers. I don’t know how I can fine tune VGG16.\nAnswer: I tried to fit model on my data using suggested changes. Features are now extracted and saved in dataframe and as a csv file. When I tried to give it to classifier it gives error on training on these features. Error was got 6 columns instead of 8 or got 9 columns instead of 8. The problem was in values in csv file. The command used in training classifier was on this line
X_train = genfromtxt(X_train_file_path, delimiter=',')
ValueError: Some errors were detected!
Line #2 (got 7 columns instead of 8)
Secondly, Can you please guide me in adding encoder decoder blocks of unet.",0.0,0.11632353,0.013531163334846497
0,"Question\nI am currently working on a program with Kivy and Python that solves quadratic equations. Because the user has to enter his quadratic function or the values ​​of the variables, I wanted to ask how exactly this works with Kivy and Python.
In Python it works with a = float(input(print(""Your a: ""))).
But how do I save an input as a variable in the form ""float"" so that I can calculate with it?
Thank you!\nAnswer: I don't know exactly what you are asking for. To save an input as a float in a variable, just do a = float(input(""Your a: "")), and then to calculate with that you can do x = a + 89, or print(a**2). Hope I solved your question.",0.0,0.08577192,0.007356822025030851
1,"Question\nI want to edit the aged payable xlsx report in Accounting->Report->Aged payable odoo14. However, I am not getting the report template in the odoo sh staging branch editor console. Where can I find the model and the template in the file system?\nAnswer: These type of reports are one of the technical implementations in Odoo. These reports are implemented via sql queries based in functions.
You can find their implementation in enterprise > account_reports > models.
In models folder, multiple files have respective report implementations like partner ledger, aged receivable and payable.",0.40816328,0.23578346,0.029714802280068398
2,"Question\nI'm new to python and was trying to create a python bot, I wanted a optimized way to modify and access my bot configs per server. I had 2 ideas on how/when to fetch configs from the database for optimization.

this is what you would normally do - just fetch data variables(fetch a variable at a time) for each command, this would keep the bot simple and minimize unused recources.

In this one, whenever the user uses a command for the first time, it fetches the entire config table and stores it in a loaded dict from which you can access the config from. you can also update the config in the dict and every 30m-1hr it will log the values in the table and empty the dict. The benefit of this one is less sql calls but potentially less scalability because of unused objects in the dict.


Can someone help me decide which one is better, i dont know normally how you would make discord bots or the convention.\nAnswer: Your second approach is called caching the data. You're basically creating a cached database in your application (the dictionary) and save a bunch of usually necessary data to access them quickly. It is what every (almost every) major service (like Steam) does in order to minimize the main database calls.
I think this is the better practice however it has its drawbacks.
First, from time to time, you have to compare the cached data with what you have in the original database because your bot will not have a single user and while the cached data is available to one user, another user might alter the data in the original database.
Second, it is harder to implement than the first approach. You need to determine which data to store, which data to update rapidly and also you need to implement an alarm system for the users to update their cache whenever the main data is altered in the database.
If I were you and I just wanted to mess around with bots, I would go with just fetching the data each time from the database. It's easier and it is good enough for most applications.",0.0,0.26091653,0.0680774375796318
3,"Question\nI have abc.pth file which is a feature file. I want to determine the shape/dimension of this file through google colab. how to load it and determine its shape?\nAnswer: path=""/path/to/abc.pth""
import torch a=torch.load(path) a.size()

and it will output the tensor

torch.Size([1, 2048, 19, 29])",0.0,0.27197564,0.0739707499742508
4,"Question\ndef browse_base(self):
option=QFileDialog.Options()
file=QFileDialog.getOpenFileName(widget,""Open Single File"",""CC"",""All Files(*)"",options=option)
self.base_addr.lineEdit.setText(file[0])

I have a problem here: 'QLineEdit' object has no attribute 'LineEdit'
And i don`t know what i need to do\nAnswer: It looks like you're assuming that self.base_addr has a lineEdit attribute that is a QLineEdit, but it seems that self.base_addr itself is already a QLineEdit (which has no lineEdit attribute), so that using just self.base_addr.setText(file[0]) should probably work.
However, this may just be the tip of the iceberg, but it is very hard to tell from the code in your question.",0.40816328,0.25902605,0.02224191278219223
5,"Question\nIs there a way to know which codes on github are in python 3? So far, I haven't any mentionned.
Tks\nAnswer: Very simply:

Yes, you can find Python3 code.
No, you cannot do so effectively.

Code on GitHub is not identified by language and version -- obviously, since you would have found that in your investigations before coding.  Yes, you can generally determine the language of a code file, but only with detailed examination -- you would almost need to pass the file to a Python compiler and reject any with syntax errors.  This is not an effective process.
You can reduce the search somewhat by gleaning *.py files and then look for frequent, 3-specific features, such as all print commands using parentheses (coding style in Python 2, mandated in Python 3).  This merely reduces the problem; it does not give you a good request mechanism.",0.0,0.39545226,0.1563824862241745
6,"Question\nI have a compatibility issue when running :
pip install numpy==1.19.4
pip install tensorflow=2.5.0
pip install gensim==4.0.1
On Ubuntu 18.04, with Python 3.9.5 (installs made inside docker container).
I get the following exception when trying to import gensim:
ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject. 
Not sure how i can make this work, I tried downgrading several libraries but still not achieved to make it work in Ubuntu.
Edit : it works on Python 3.8.10\nAnswer: It is the numpy version, downgrade numpy to 1.19.2 or lower and see.",0.20408164,0.22785616,0.0005652277613990009
7,"Question\nI have a compatibility issue when running :
pip install numpy==1.19.4
pip install tensorflow=2.5.0
pip install gensim==4.0.1
On Ubuntu 18.04, with Python 3.9.5 (installs made inside docker container).
I get the following exception when trying to import gensim:
ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject. 
Not sure how i can make this work, I tried downgrading several libraries but still not achieved to make it work in Ubuntu.
Edit : it works on Python 3.8.10\nAnswer: Tensorflow is currently not compatible with numpy 1.20.
Also, many changes happened from gensim 3.X to 4.X, which may lead to some problems.
Try installing the following setup:

numpy 1.19.2
gensim 3.8.3
tensorflow 2.3.0",0.20408164,0.2415402,0.001403143280185759
8,"Question\nI want to run python script on google cloud using android studio

ex: I have an android application which contain button and google cloud VM instance which has a python script.
I want when click button, the script is run and output send to storage.
how I can do that?\nAnswer: I suppose you're running a linux-based distribution as your operating system. I think that you could do that by sending a command to the VM via ssh, and by using pipes in linux you could direct the output to a specific file.
Let's say you have your python script named script_one.py
With this command python3 script_one.py > output.txt you basically run the script and the output of it is stored in the file output.txt in the same directory where the python script is, now of course you could use absolute or relative path to redirect your output to another place.
Depending on the language you're developing your application in, this can be implemented in different ways.",-0.71428573,0.2765565,0.9817683100700378
9,"Question\nI have a Python script that creates a new user and configures it, I want this to be ran anytime a user SSHs into the server but the username isn't a valid one, how could I do this?\nAnswer: That is an incredibly bad idea. How would they learn what password you assigned? Consider how easy it would be to write a denial of service attack to log in as millions of unknown users. Hackers do that EVERY DAY to any public-facing server. Much better idea to have a web site where people register for a new username.",0.0,0.13843787,0.0191650427877903
10,"Question\nAssuming there are 2 tables in one page. table 1 shows the list of all items and table 2 shows the list of all deleted items. whenever i click delete button the corresponding row from table 1 is moved to table 2.
I am not sure how to do all these in one page!\nAnswer: Once, you click delete, you send a request to the server, once you get the response, you run an animation where you remove row and put on another table, this can be done by giving an id to your tr and then remove id and adding to the top of the 2nd table.",0.0,0.16520101,0.027291372418403625
11,"Question\nI want to get feature value of an object with YOLOv5. I'm guessing there is a hint in ""detect.py"" in opensource.
How can I get feature value of the object used for inference?Please tell me how to resolve.\nAnswer: The variable 'det' inside the def run in detect.py(line 181), you can know the xyxy value, the confidence score, and the number of class name of the object.
Since 'det' is a tensor data type, you will need to converting 'det'.
If you want to get only the number of class name of the object, you can easily get it by converting cls in detect.py(line 205) like 'int(cls)'.",0.40816328,0.29582685,0.012619473040103912
12,"Question\nBasically I am writing a script to reset a django webapp completely. In this script, I want to reset the database, and there is a command to do it from django extensions. Unfortunately, I haven't been able to run it programatically. It works fine when I run it via command line, but it just won't execute when I try programatically.
I have tried using os.system and subprocess.
I have also tried using management.call_command('reset_db'), but it keeps saying that there isn't a command called reset_db. I have checked to make sure the django_extensions is in my installed apps, so I have no idea why that isn't working.
Does anyone know how I could fix this? Thank you!
Also I am using python3, the most recent version of django I believe, and it is a MYSQL server that I am trying to delete.\nAnswer: I can't know without seeing your way of invocation directly, but my guess is the script's not running in the virtualenv. Here are some debug notes:
./manage.py --help | grep reset_db: Does this output anything?
./manage.py shell_plus
Then try:

from django.core.management import call_command
call_command('reset_db', '--help')

Anything then?
Also within./manage.py shell_plus, try import django_extensions
Outside of the shell, try this: pip show django, pip django-extensions.
If it doesn't show those (e.g. WARNING: Package(s) not found: django-extension) and you think they're already installed, try this:
which python, which pip. Are you using venv, virtualenv, virtualenvwrapper, pipenvorpoetry`?
Try env | grep VIRT, do you see a VIRTUAL_ENV? If not you may need to make one.
When you run the script, you need to have your environmental variables set so you hook in to your site packages. In poetry we can do poetry run./manage.py ourscript or poetry run./ourscript.py without needing to be sourced. But we can also easily drop into virtualenv via poetry shell.
If you created an environment like virtualenv -ppython3.8.venv,",0.0,0.6366788,0.4053599238395691
13,"Question\nThere is an exe of the compiled python file 3.9 version.
exe is 100% packed by pyinstaller
Decompiled exe, moved 1 line from abc.pyc to main.pyc Launched uncompyle6, gave an error
AttributeError: module 'xdis.opcodes.opcode_39' does not have attribute 'END_FINALLY'
Tried decompyle3 the same error. Who knows how to fix it?\nAnswer: Support for Python 3.9.5 is added in xdis 5.0.10; it is not available in 5.0.9.",0.40816328,0.49200213,0.007028952706605196
14,"Question\nI am extremely new to coding. I have been completing a Udemy course and everything has been going smoothly until I installed Visual Studio (as per the course instruction), however, I think I change the python path during the installation/setup of Visual Studio.
I don't really know how this works, but is it the case that I changed the path (from the one during the python installation process) to a different path associated with Visual Studio? I remember clicking/accepting something to do with PATH on Visual Studio, and now when following the course structure and trying to run the code from Visual Studio on Git Bash, I am hit with the:
""Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.""
I have troubleshooted to 'environment variables', but I am not grasping how to perform the edit so that the python path is back to how it was.
I have two paths it seems:

C:\Users********\AppData\Local\Programs\Python\Python39\Scripts\

and

C:\Users********\AppData\Local\Programs\Python\Python39\

I don't quite understand how the'move up' and'move down' functions work.
Could somebody please put this n00b out of their misery. Please XD\nAnswer: these two paths are coreect you dont have to edit these. when you install python these two paths qre added to your envirnoment variables path",0.0,0.27476886,0.07549792528152466
15,"Question\nI would like to ask if it's possible to run a set of tests written in python (pytest)
on a running NodeJS application running in Docker?
What I want to achieve:
1.setup github action to run and build the 'test Docker container' on pull_request (done)
2.run pytest as soon as the node container starts (pending)
3.run another github workflow based on the test results of pytest (there is also a question how to achieve it,I saw somewhere that maybe cypress can help)
Please let me know if I should provide Dockerfile if it's necessary
thanks in advance\nAnswer: Solved by using ENTRYPOINT in Dockerfile, where I put my bash script which run npm start & pytest -c xxx",0.0,0.277548,0.07703288644552231
16,"Question\nWhen doing recommendation system (collaborative filtering) for retail business, there are no actual rating (like 1-10 satisfactory with the item).
Therefore, I use no. of times each user purchase a certain item as implicit rating. However, in doing so, there could create bias between slow-moving products (such as Television) and fast-moving products (such as Chips, Snacks) since the no. of times customers would buy the slow-moving products are much less than fast-moving ones.
My questions are :

Is there anything better than no. of times a customer buys the certain items or visit counts to use as rating?
How do you add weight to help lessen the bias between slow-moving and fast-moving items.

Thank you for your answer in advanced!\nAnswer: Attempting to answer your specific questions:

Yes, but it depends on what other information you have about the items. I have tried to answer making some reasonable assumptions.

I can presume that you have the cost of the item. You can normalise by the cost of the item to ensure some reduction (not complete removal) of bias. You can do a direct normalisation (weighted_rating = implicit_rating * cost per item). Here, the cost_per_item serve  the purpose of weights to reduce bias. Alternatively, you can experiment with groups of prices by binning or clustering the prices across all items to form groups of products, thereby giving you a  group_mean_price. This can then be used as weights.


If you know something else about the item (such as perishable/ consumable/ bulky) you can have more representative clusters. Thereafter, you can assign implicit_rating to each item. Now, you can normalise each implicit_rating by using an aggregate statistics for the group rating (say group_mean_implicit_rating). So, weighted_rating = implicit_rating/group_mean_implicit_rating",0.40816328,0.6250433,0.04703693091869354
17,"Question\nI have made a Flask API for a spacy ner code and deployed it on Docker. In the code I have used python's logging to return the outputs to a file, info.log.
The question is, how to access the log file in the container after running it.\nAnswer: Since I had to look for a long time, I picked up bits of answers from different places and am compiling it here for anyone who is stuck.
After running the container, go to the terminal and post the following commands.
(I used pycharm and the terminal started inside the directory where my code and dockerfile were stored)
docker ps
(this shows the containers running currently)
docker exec -it 'container-name' bash
(now you have entered the container)
ls -lsa
(this will show all the files in the container, including the log file)
cat info.log
Now, you can see the log file contents on the terminal.",0.0,0.36363792,0.13223254680633545
18,"Question\nHello I have a selenium script in python which extract data with login on webpage. It take around 50 sec to execute and I want to deploy that script as an api. But API is getting timeout.
There we can also do one thing that we save that data in some google sheet using that script.
Please anyone can suggest how can i do this or any relevant content?\nAnswer: Could you provide us a screenshot of API timeout or logs? Showing Python code with requests will be also helpful (sorry for answering instead of commenting because I don't have enough reputation points)",0.0,0.086539745,0.0074891275726258755
19,"Question\nI have a small application built using python and tkinter, and I want  that application to run on startup.
Here I have few questions

My question is how do I actually set the app to run at startup.
My application needs to login, but I don't understand how I can connect, application with database without placing database credentials in user's downloaded application.\nAnswer: There are 2 points here:

how to set-up an application to run at start-up?
If what you call startup if the machine startup before any user logs in, then the answer will depend on the OS. You will have rc scripts on Unix (and Unix-like) systems, or services on Windows.

how can an unattented application connect to the database?
A common way is to have the application run under a dedicated login (user name). Then you configure the database to accept passwordless connections from that specific login on that specific machine (ideally the same one). Of course that login should have the less possible privileges...",0.0,0.11196762,0.012536748312413692
20,"Question\nI'm using PyCharm and also installing dependencies with pip I dont know how to make PyCharm to get this new dependencies that I install and are in the folder {project_base}/env/lib/python3.9
If I go there I can see the libraries installed, but PyCharm is not able to seen it, and allow me to use it in the code.
I've seen in the preference the Python Interpreter section, but I need to use pip, since I'm documenting the package to install with pip for other users.
Any idea how to point my PyCharm to my env/lib/ folder?
Regards\nAnswer: When you choose Python Interpreter, PyCharm will automatically pick up libraries available on that Python installation.
Make sure PyCharm uses the Python Interpreter that has its libraries installed in env/lib/python3.9 and then you should see available libraries on the interpreter configuration screen.
If you install libraries with pip e.g. env/bin/python -m pip install mylib they will become automatically available to PyCharm, because PyCharm and pip will use the same Python installation to manage these libraries.",0.40816328,0.32205546,0.007414556574076414
21,"Question\nCreated Pythonshell with simple script, like just requests.get(<elasticsearch_endpoint>). Elasticsearch cluster is in VPC.
I tried using self-referencing groups, endpoints but nothing worked. Also custom connection with JDBC fails Could not find S3 endpoint or NAT gateway for subnetId (but it exists).
I see that for Spark jobs ESConnector is available but can not find any working way to make it with Pythonshell jobs. Is there any way to allow such connection?\nAnswer: Solved, I was missing route to NAT gateway in private subnet.",0.0,0.4417261,0.19512194395065308
22,"Question\nI ssh'd into a linux server to run Airflow. I have made the scheduler(airflow scheduler -D) and database initialized (airflow db init). However, even when trying to create the simplest of DAGs using python (I also tried using Airflow's predefined example py scripts), Airflow does not list the DAG when running the airflow dags list command.
I'm sure the syntax of my py code is correct because the DAG showed up on a windows instance but my setup for airflow within Linux is somehow not correct? Also used python3 script.py to execute.\nAnswer: Basically the dags folder's permission's weren't allowing anything to be written into it. I just sudo'd every command or chmod the folder. Also to ensure Airflow was correctly run, I suggest using a YAML file with docker compose to streamline Airflow setup.",0.40816328,0.14243793,0.07060995697975159
23,"Question\nI have a single pdf with multiple copies of the same document merged into one. I want to digitally sign each and every copy, meaning the pdf must have multiple digital signatures. I'm using endesive library in Python to digitally sign the PDF. The signature is showing as valid when I sign the document once but when I'm writing the same signature multiple times, it shows that the signature is invalid (when opening the document). Is it right to digitally sign a document multiple times and if yes, how to achieve it using Python's endesive library?\nAnswer: There is a strong difference between digital and manual signature: with a manual signature, you add a signature on a paper sheet, and you can/must separately sign each page.
With a digital signature, you sign a whole document seen as a sequence of bytes. AFAIK, there is no way to have different signatures on different parts of the document.
What is possible is to assemble the parts into a large document and then sign the final document. That final signature will be an evidence that the signer attests that the document is valid at the time they signed it, and that it has not be tampered since that point.
A common usage for multiple signature is to prove that many human beings all agree with the content of a document. In administrative processes, an employee prepares a document, signs it to marks that they are responsible for the content, and a manager signs again to mark that the document has been controlled.",0.0,0.4041288,0.16332007944583893
24,"Question\nI have a single pdf with multiple copies of the same document merged into one. I want to digitally sign each and every copy, meaning the pdf must have multiple digital signatures. I'm using endesive library in Python to digitally sign the PDF. The signature is showing as valid when I sign the document once but when I'm writing the same signature multiple times, it shows that the signature is invalid (when opening the document). Is it right to digitally sign a document multiple times and if yes, how to achieve it using Python's endesive library?\nAnswer: In order to make multiple signatures, you must incrementally save the pdf, each time you add a signature.
If you use PyMuPDF to build the document, you must save as
saveIncr()
PDF only: saves the document incrementally. This is a convenience abbreviation for doc.save(doc.name, incremental=True, encryption=PDF_ENCRYPT_KEEP).
write(garbage=0, clean=False, deflate=False, deflate_images=False, deflate_fonts=False, ascii=False, expand=0, pretty=False, encryption=PDF_ENCRYPT_NONE, permissions=- 1, owner_pw=None, user_pw=None)
(Changed in v1.18.3)
PDF only: Writes the current content of the document to a bytes object instead of to a file. Obviously, you should be wary about memory requirements. The meanings of the parameters exactly equal those in save(). Chapter Collection of Recipes contains an example for using this method as a pre-processor to pdfrw.",0.0,0.39762706,0.15810728073120117
25,"Question\nI'm trying to be able to deploy a project that I made with pyqt5 using pyqtdeploy, I read the whole documentation, which sucks (sorry for the language), I even read the 1.3.2 version, and I still don't know how to make it work. I installed all the packages (using pip and the downloadable files for the demo) and run the setup, but I don't see the executable pyqtdeploy mentinned in the documentation, and the build-demo.py does not work.
I'm on Windows by the way,\nAnswer: This is not an answer but can sort of one since it ""solve"" the problem
OK I took a look at the whole code and to add to the fact that there are not the executable in it (the one mentioned in the documentation), all the imports are incorrect, and we have to either rename all the files or modify the import, I think that I'm just going to change and stick with pyinstaller, even if we can only do.exe.
However if someone have figured out how to make it work, I would like to know",0.0,-0.279099,0.07789624482393265
26,"Question\nI am creating a discord bot and I use a JSON file to store everyone's data, the problem is that when I have to update my game and if I upload the code to Heroku it will rewrite everyone's data there is two ways I can fix this,
Get the JSON file from Heroku
or Rewrite the code in Heroku with the new code (best option)
If you know how to do either of these commands please let me know down below! :)\nAnswer: Either way is to use heroku postgres database which will help you to store data...",0.0,-0.062111855,0.003857882460579276
27,"Question\nI am creating a discord bot and I use a JSON file to store everyone's data, the problem is that when I have to update my game and if I upload the code to Heroku it will rewrite everyone's data there is two ways I can fix this,
Get the JSON file from Heroku
or Rewrite the code in Heroku with the new code (best option)
If you know how to do either of these commands please let me know down below! :)\nAnswer: I am using repl.it to host my bot now and it works so far, freecodecamp made a great video explaining how to host a discord bot on youtube",0.0,-0.35481635,0.12589463591575623
28,"Question\nI am new to game development. I am trying to start high and make a 3D RPG. I know the road is not gonna be easy. That is why i decided to use Ursina and python to make my game.
However i wanna add a cutscene showing a Backstory. I have the video in mp4 format but i cannot seem to know how to play it inside the game with Ursina.
Anyhelp will be much appreciated.
(Side question : do you think Ursina is good for a beginner in 3D gaming? If i want to publish my game on my website, isn't it better for me to learn javascript? I read about Unity but it is too big to download for a little side project)\nAnswer: Well, I don't think there is a way to do that. the closest thing you can do to that is having a folder filled with all the frames of your video in.png or.jpg files, then adding a quad to the world and changing the texture of it to the next frame every fraction of a second depending on the framerate. this, however would make your computer l a g. trust me, I've tried it. it would probably be better to have a separate window with some sort of module that plays.mp4 files for playing the file.
In other words, there is no feasible way to do that.",0.0,0.13955027,0.019474277272820473
29,"Question\nI am new to game development. I am trying to start high and make a 3D RPG. I know the road is not gonna be easy. That is why i decided to use Ursina and python to make my game.
However i wanna add a cutscene showing a Backstory. I have the video in mp4 format but i cannot seem to know how to play it inside the game with Ursina.
Anyhelp will be much appreciated.
(Side question : do you think Ursina is good for a beginner in 3D gaming? If i want to publish my game on my website, isn't it better for me to learn javascript? I read about Unity but it is too big to download for a little side project)\nAnswer: From Entity Basics in the documentation:
e4 = Entity(model='cube', texture='movie_name.mp4') # set video texture",0.0,0.15818554,0.025022665038704872
30,"Question\nI need to fetch swipe up count which show in the insights of Instagram.As Facebook is not providing swipe up count through their graph API so how can I get that data.
Scraping won't work as I already did and I want to fetch those data in python or javascript
Thanks in advance for help\nAnswer: for now facebook is not providing this data in graph-api and it is only provided in influences in insights so for now its not possible for now to fetch but you can get by web scraping
Facebook can provide this data in next version of graph-api",0.81632656,0.23825252,0.3341695964336395
31,"Question\nI am learning how to use PyQt5 and there are quite a few points that elude me.
I have started implementing QThreads to replace the threads I have been using in my UI as I realised that mixing threads and QThreads could possibly lead to issues later and have started using pyqtSignal simultaneously
So far I have seen that the pyqtSygnal needs to be implemented on a class level to be able to work ( putting it in a class constructor does not work )
In the architecture I use currently, I have a pyqtSignal that is instantiated in the main thread and is then used by all of the child threads. This is due to having one class that is responsible for my logs of all the program.
I am unsure if this is a good implementation or not.
Here are my questions :

are pyqtSignals thread safe?I know that they use a queue system to be thread safe but is the emit() method itself thread safe? My understanding of them is a bit limited
Do I need to protect my pyqtSignals with locks?
Can I have multiple different signals emitting to the same slot without having any issues?\nAnswer: The signals are one of the few elements of the QObjects that are thread-safe so it is not necessary to protect the data. So you can have different types of signals connected to the same slot.
Since the signals are thread-safe then Qt recommended to communicate QObjects that live in different threads.",0.40816328,0.3254041,0.006849080324172974
32,"Question\nI have found how to get the offset and lag of a specific topic and consumer group, but how to get the ip address of consumers in the consumer group?
Java or Python SDK are both OK.\nAnswer: ok...Finally I found describe_consumer_groups in KafkaAdminClient",0.0,0.2363081,0.05584151670336723
33,Question\nI'm trying to make a custom 'Share' button with telebot. Is there any option to handle InlineKeyboardButton with switch_inline_query parameter is set? I want to know in which chat/user the message was sent.\nAnswer: I made'share' button by deep link with unique'start' parameter value.,0.0,0.17136323,0.029365358874201775
34,"Question\nI am making a music player with flutter so, I wanna to ask how to access all the files of a particular extension ( mp3, wav ) with it's details in Flutter or if not in Flutter then Python to play those music files with details like song name, author name\nAnswer: If u are using flutter with native app, then you can use path_provider and the dart File to get all you want in Android. However, ios is more like a sandbox, so it is hard to get what u want.",0.20408164,0.1687277,0.0012499013682827353
35,"Question\nI'm doing some load testing with locust and I cant seem to figure out how to change the Hatch Rate to a slower ramp up rate.  1 locust per second is still too fast so is there a way to change this to something that would resemble 1 locust every 20 seconds?
I've tried using gevent.sleep(19) within the on_start method and setting the hatch rate to 1 locust per second in the UI but this only hatches each locust 1 second apart and then each hatched locust sleeps for 19 seconds (they are still 1 seconds apart).
Is there a way of forcing each locust hatched at run time to wait 20 seconds before the next locust executes? (e.g. the first locust hatches and runs the on_start method, the next locust waits 19 seconds and then runs the on_start method, the next locust waits 19 seconds more and then runs the on_start method.)\nAnswer: The hatch rate/ramp up parameter accepts float values. Use a hatch rate of 0.05 to spawn one user every 20 seconds.",1.0,0.30934858,0.4769993722438812
36,"Question\nI currently have the IVP solver working with an event, which looks like this:
sol = solve_ivp(fun3, (0, tend), V, method='LSODA', events= event, args=(r_s,), t_eval=t) 
However I want the solver to check two event, not one. So I want something like this:
sol = solve_ivp(fun3, (0, tend), V, method='LSODA', events= (event, event2), args=(r_s,), t_eval=t) 
However that doesn't work.
Does anybody know how to check for multiple events?\nAnswer: It works! PyCharm showed a warning, but it actually works",0.0,0.27786022,0.07720630615949631
37,"Question\nI have used idle before, but never set it up. my problem is actually getting a py file to work with. I don't actually know how to make one and it isn't an option when using save as on a text file. (only text file and all files(?) are put as options) I've attempted to open the py files already in the python folder but when selecting edit with idle I get prompted to pick what to open the file with and then if I click python nothing happens.
what am I missing?\nAnswer: If you select File => New and then File => Save as, the default will be to make it a.py file if you add no other extension.",0.20408164,0.042397976,0.026141606271266937
38,"Question\nI have the following problem:
I have to run some test/diagnostic Python script on a Windows system. Due to explicit requirement, the system has no default system-wide Python instance, but there are two different Python instances installed, used locally by applications running on the system. However, both these instances lack some basic modules my script uses (like logging, urllib, configparser etc.).
I want to run %PYTHONPATH%\python.exe myscript.py where %PYTHONPATH% points to one of the installed Python instances, but install the required additional modules ""somewhere"" outside %PYTHONPATH% (preferrably, in the same directory where my script is installed) so that my script can use them.
As my script is a test tool, it should not modify the OS or installed software, so the Python installation under %PYTHONPATH% should not be changed in any way.
It is also expected that the installation can be fully automated, ie. the best way to install would be just have the modules in the same.zip file with my script which is unpacked onto the target path.
It is also important that the system has no Internet access, so I have to download required files on another machine and copy them to the target system.
Can you guide me how to do it?\nAnswer: I found an answer myself - it is quite simple:

obtain the zip file containing standard modules from the appropriate Python version distribution (in my case it was the file python38.zip, it is inside the main zip file downloadable from Python site)
Unpack the contents of this file to c:\mydir\Python38\site-packages, where c:\mydir is the directory containing my script
set the environment variable PYTHONUSERBASE=c:\mydir before running my script

Now I can run the script and it finds all ""missing"" standard modules in c:\mydir\Python38\site-packages.",0.20408164,0.22264522,0.00034460664028301835
39,"Question\nDoes python allocate memory in a continuous fashion in memory while implementing list or use a dynamic allocation?
If continuous, how does it append new elements? It should cause problems in implementing a large list.
How does this actually work?\nAnswer: I don't know how it is implemented behind the hood, but as it is accessible by index it doesn't look like a linked list. To me, it looks like a vector of pointers/references, but I could be totally wrong.",0.0,0.13331342,0.01777246780693531
40,"Question\nI suspect it may be rather kid question – but anyway.
How to open another Telegram chat or group or channel using pyTelegramBotAPI? I want to forward the user (not message, the user himself) to another channel if he clicks certain button.
I saw content type migrate_to_chat_id in Message class declaration. Should I use it? If so, how to get an id of channel I need? It won't send any message to my bot.
I would better use ""t.me/..."" url.\nAnswer: Partly solved.
Speaking about the buttons, it is indeed easy. You just use named parameter url= in InlineKeyboardButton() method.
For other cases. You need to open another channel(s) from function depending on several conditions for instance. Still don't know. Import requests and make GET request? I suspect that something for it should already be in pyTelegramBotAPI, but searching in lib files wasn't successful.",0.0,0.28114438,0.07904215902090073
41,"Question\nThe program is a standard flask program, and it does some cleanup as part of the initialization. In the cleanup() procedure, using os.remove(""abc.txt""), I noticed that the file is removed, but not reclaimed by the OS.
I use both ""python website.py"" and ""gunicorn website:app"" to run the application and both have the same problem, in Linux environment. In MacOS, I can't reproduce it.
After the file is os.remove, it is no longer listed in ""ls"" command, but when I run
lsof | grep deleted
I can still see this file being listed as deleted but opened by the python application.
Because this file is already ""os.remove"", it is not listed in ls command, and du will not calculate this file.
But if this file is big enough, df command will show the space of this file is still being occupied, not being reclaimed. Because this file is still ""being open by the flask application"", as the lsof program claimed.
As soon as I stop the flask application from running, the lsof will not have this file, and the space is reclaimed.
Usually when the file is too small, or when the application stops or restarts frequently, you won't notice the space is being occupied. But this is not very reasonable for keeping the space. I would expect the website running for years.
When searching internet for ""open but deleted files"", most suggestions are ""find the application and kill it"". Is there a way to keep the flask application running without restarting it? My application doesn't actually ""open"" this file, but simply os.remove it.
Suggestion on how to delete file and re-claim the space immediately?\nAnswer: The Flask application either needs the large file to continue running, or does not release unneeded resources.
If the app needs the large file, that's it. Otherwise, the app is buggy and in need to be corrected.
In both cases, the ""being open"" status of the large file (that, at least on Linux, leads to the file still being present in the mass memory system) cannot be controlled  by your script.",0.0,0.36579764,0.13380791246891022
42,"Question\nI have made a tester file to check my other files are implementing correctly, however I can only get an output by clicking run on the file  (top right green arrow). When I type 'python Test.py' in the terminal I get :

Fatal Python error: initsite: Failed to import the site module
ModuleNotFoundError: No module named'site'
Current thread 0x0000000102c0ce00 (most recent call first):

I assume this is an environment issue for my terminal? I am in the correct folder (one above Test.py).
Any ideas how I can change this?
Thanks!\nAnswer: It looks like the python interpreter you are taking when you run python Test.py in the terminal is different from the one when you click the Run Python File in Terminal(green triangle) button.
Could you type this command in the VSCode terminal(CMD: where python; Powershell: get-command python) to check which python you are using when you take python Test.py command to run your python file? And compare it with another one.",0.40816328,0.19896108,0.043765559792518616
43,"Question\nI am forced to ask this question
My mentor has given me a task to extract data from files with pure python, there were some txt file which were easy but there is a file with xlsx extension and I can't find any where if it is possible to extract the data from it with pure python (I have been searching for more than 3 weeks now).
Please if it is not possible tell me so that I can show this to her with confidence because my mentor keeps insisting that it is possible and I should do it with pure python but she refuses to give me any clues and tips.
And If it is possible tell me how to do it or where to read more about it.\nAnswer: The short answer is no, the long answer is, you can unpack the.xls file and iterate through the resulting.xml ""by hand"".",0.27210885,0.0699327,0.04087519645690918
44,"Question\nI am forced to ask this question
My mentor has given me a task to extract data from files with pure python, there were some txt file which were easy but there is a file with xlsx extension and I can't find any where if it is possible to extract the data from it with pure python (I have been searching for more than 3 weeks now).
Please if it is not possible tell me so that I can show this to her with confidence because my mentor keeps insisting that it is possible and I should do it with pure python but she refuses to give me any clues and tips.
And If it is possible tell me how to do it or where to read more about it.\nAnswer: Previous answers regarding unpacking/unzipping the XLSX file is the correct starting point. Thereafter you'll need to know how the extracted files work together. It's rather convoluted.
The best thing to do is be specific about exactly what data you want to extract then I'm sure you'll get some sample code that shows how you achieve your objective",0.13605443,-0.312859,0.20152325928211212
45,"Question\nWhen we train an XGB model using AWS built-in models
e.g. (container = sagemaker.image_uris.retrieve(""xgboost"", region, ""1.2-1"")),
Based on my understanding, The training job requires numerical vectors for the train and validation.
Meaning that if you have a dataset with categorical values and strings, you need to convert them into a vector. the model only deals with float numbers,
(Outside Sagemaker, I can use TFIDF to vectorize my features and construct a DMatrix), but this approach doesn't seem to be supported by Sagemaker.

Does anyone know how this data transformation is done in Sagemaker?
Is this a bad idea to use BlazyngText unsupervised learning to generate the vectors?
Should we have a preprocessing step and in that step we use TFIDF?\nAnswer: Ok, after pulling my hair for a while, we built a solution as below:
On the preprocessing step, we used TFIDF to vectorize categorical values and then stored the data into S3.
On the training step, we read that training input and fed it into the XGB estimator.",0.0,0.18112421,0.03280597925186157
46,"Question\nI am trying to deploy a model which uses the pillow (PIL) library on ubuntu (Ubuntu 20.04.2 LTS). The model was built on windows, and i have since discovered that PIL returns different arrays when reading the same image between Ubuntu and Windows.
This seems to be because of the jpeglib version (9 not 8). Does anyone know how to change this version so i can replicate the same results on a linux mahcine?
Thanks!\nAnswer: Solved by building Pillow from source with jpeglib-8 installed. This means jpegs loaded with the same decoder match.",0.40816328,0.27209318,0.018515072762966156
47,"Question\nI'm running PyCharm on Manjaro(latest version of both) and upon starting PyCharm, a prompt appears that says I should ""Please consider switching to the bundled Java runtime that is better suited for your IDE(your current Java runetime is 11.0.11+9 by Oracle Corp. at /usr/lib/jvm/java-11-openjdk""
I've tried choosing different runtimes using Ctrl+Shift+A but none of them stop this prompt from appearing. Is this something I should worry about or should I brush it off?
New to both Python and Manjaro so not really sure how to proceed. Unable to find anything pertaining to this issue on this distro.
Edit: I downloaded the most recent version(11.0.12) and adding it as a custom runtime, but for some reason it shows that I'm still using the aforementioned version...\nAnswer: TL;DR: Run $ PYCHARM_JDK=/path/to/runtime/ pycharm
If you installed PyCharm through the package manager (e.g. pacman), the launcher will be installed to someplace like /usr/bin/pycharm. This is basically a shell script that sets the JDK for PyCharm before calling the main executable. When the environment variable PYCHARM_JDK is not set, it defaults to a system Java 11 JDK.
Changing the boot runtime within PyCharm doesn't change this script. You'll need to set PYCHARM_JDK from the command line, in your.bashrc, or by editing /usr/bin/pycharm. Use Help -> About to confirm that the expected runtime was selected.
As for which runtime to use, jbr_jcef-11_0_11-linux-x64-b1341.60 works nicely on my machine. The JetBrains support forum will have to explain what the differences are.",0.0,0.34760487,0.12082914263010025
48,"Question\nI am new to python, and I've attempting to code different things while following different tutorials. I know that packages are important, but how do I make sure that I don't install anything malicious?\nAnswer: Just download the pip library from official pages and don't worry until you interact with downloaded files directly or indirectly; nothing will be harmful if it's pip file.",-0.71428573,0.21513045,0.8638144135475159
49,"Question\nSo I would like to know if its possible to make a code the
makes a cooldown
per role
like premium has no cooldown
and bronze has a 10 second
cooldown if so how can I do it?
for commands\nAnswer: Why with commands? Wouldn't it be much easier to add and remove the roles with a command and control the cooldown via the server settings?
With a simple level-system this would be done very quickly.
(You could store the number of messages in a simple.json file and after a certain amount of messages add/remove the corresponding role.)",0.0,0.22601807,0.05108416825532913
50,"Question\nI am trying to get my head around OOP thinking when say I have your general example of a person class and then a class for family members
family members share the same address but not all persons have the same address.
how do you make it so you can create families where members share the same address without having to enter the same address on each person, but that different families have different addresses?
I have seen examples where you define subclasses that inherit from the main class person but you have to hardcode each family as a class. Any views as to how to make this work in OOP? I might be looking at the problem in the wrong way...\nAnswer: One way to do it is to have 2 classes: Family and Person.
The Family class will have two main attribute:

a collection of Person (as array, list, etc)
address

The Person class will need to have a reference to the Family object in which it belonged into. So its attributes might be like:

name
date of birth
...
reference to Family Object

Generally, to represent hiearchy with OOP, you can do something called composition, which is to have some objects as the attribute of another object.",0.13605443,0.18519175,0.002414476592093706
51,"Question\nI have been adding python types to my function parameters and returns for some time now, but I haven't figured out how to make use of these beyond the popup hints when I hover over a parameter.
Is there a way to get red underlining or some other report feedback showing me locations where the type of a parameter being passed does not match the parameter's typing definition?\nAnswer: As far as I know VSC doesn't support python type hinting. If you really need type hinting maybe try downloading the Community Edition of Pycharm
It has type checking and hinting features and is completely free.",-0.35714287,0.2670948,0.3896726667881012
52,"Question\nWhen I use Jupyter Notebook inside the browser or inside Google Colab I can comment multiple lines of code by selecting them and pressing crtl + #. This also sometimes worked in Visual Code and not only for Python but also for C++ (of course then each line began with a //). However, I have no idea what enabled this behavior and how to reproduce it, it was very convenient. If you have any idea how to enable this, I would very much appreciate your help.\nAnswer: In VS Code you can use Ctrl+/ for comment multiple lines and Alt+Shift+A for block comment.",0.0,0.09020507,0.008136955089867115
53,"Question\nIt's my second day of Django and everything seems to be going petty well. Though the installation process is a bit longer, I still find it friendly to get started with. I am currently learning how to pass data to static file from views.py. The problem I am having is that it is only showing the previous changes not the recent. I have hard-refresh but still not working. I don't know how to stop and rerun the server because I don't know how to combine both Ctr + BREAK.\nAnswer: Ctrl + break mean pressing ctrl and alphabet c key at same time it will break the server and if you want to rerun it run the command
python manage.py runserver
There may be issue that you won't Saving your work other then this try force reload CSS file by pressing ctrl+ f5 in browser tab in which Django project is running.It might be helpful.",0.0,0.15009016,0.02252705581486225
54,"Question\nI'm trying to change the outgoing smtp ip address, i succeeded to change ip address using source_address=(host,port)
example :         smtpserver = smtplib.SMTP(""smtp.gmail.com"", 587,source_address=('185.193.157.60',12323)
But i can't find how to add username and password of the proxy ( if the proxy requires username and password )
I tried : smtpserver = smtplib.SMTP(""smtp.gmail.com"", 587,source_address=('185.193.157.60',12323, 'username', 'password')
But it didn't work\nAnswer: From the docs...
SMTP.login(user, password, *, initial_response_ok=True)",0.0,0.116300225,0.01352574210613966
55,"Question\nThe question is very simple but don't know how to implement it in practice. I would like to train a tensorflow LSTM model with the dataset, which is incredible large (50 millions records). I am able to load the data file to a local machine but the machine crash during the pre-processing stage due to limited memory. I have tried to del un-used files and garbage collection to free the memory but it does not help.
Is there any way, I can train a tensorflow model separately for example, the model will be train 5 times, each time only use 10 million records and then delete 10 million records after training to free the memory ram. The same procedure will be repeated for 5 times to train a tensorflow model.
Thanks\nAnswer: There are some ways to avoid these problems:
1- You can use google colab and high-RAM in runtime or any other Rent a VM in the cloud.
2- The three basic software techniques for handling too much data: compression, chunking, and indexing.",0.81632656,0.17780447,0.407710462808609
56,"Question\nI'm creating a script that downloads and shifts pdfs into different specific directories based on a search. I have code that generates the folder and subfolders recursively, I simply need to be able to download the pdfs into that file. I'm wondering how I can dynamically change the download location before I download each file in Selenium without having to start a new driver session. I could use os commands to move the files, but their names are a convoluted mess so having them go directly into the specified folder is preferable. Thank you!\nAnswer: try this
it will
pipq10
'''''()'''''
()qlqst
(0)piyQ",-0.71428573,0.22264192,0.8778334856033325
57,"Question\nNeed some help to solve this puzzle folks:
When creating a child class in Python and defining the init method, I would like to import all the attributes of the super/parent class except certain positional parameters and
certain parameters (which are not defined inside parent classes' init method's list of positional or keyworded/non-keworded parameters but) defined within/inside the parent class init method with a default value.
Is there a way to prevent/avoid certain/specific parent class attributes to be imported in the child class upon initiation? I am aware that we can override methods in the child class which do not mimic parent class behaviors, but I am not aware how to do the same with attributes, so I was thinking if I could avoid them completely!
I will give an example of why I need a certain child class to mimic everything from its parent except certain attribute.
Consider a parent class ""Car"". The child class will be ""ElectricCar"". The parent class has an attribute defined called ""liters_gasoline"" with certain integer as  its default value.
Now, I would like to inherit everything in the ElectricCar sub-class except that ""liters_gasoline"" parameter, because ElectricCars don't use fuel/gasoline. How to prevent this ""liters_gasoline"" parameter from being inherited in the child class? I don't want this parameter in child class!
How to do this?\nAnswer: If something inherits from a parent class it should have all the attributes of the parent class. For example ""liters_gasoline"" should only be in the class of a gasoline car not an electric car.",0.20408164,0.18695354,0.00029337164596654475
58,"Question\nI have a list, a = [0,1,2,3,4,5].
I'm trying to reverse the slice a[1:4], with a[1:4].reverse(). However, the value of a does not change. Why does this happen?
Note: I don't want to know how to make this work - rather, why this happens.\nAnswer: a[1:4] creates a new list. You called reverse on this new list, not a",0.1632653,0.502978,0.11540474742650986
59,"Question\nI have a list, a = [0,1,2,3,4,5].
I'm trying to reverse the slice a[1:4], with a[1:4].reverse(). However, the value of a does not change. Why does this happen?
Note: I don't want to know how to make this work - rather, why this happens.\nAnswer: Here's what a[1:4].reverse() actually does:

create a sublist (slice) of a, so a[1:4] == [1, 2, 3]
reverse that list in-place, so it's now [3, 2, 1]
throw that list away because you didn't ask it to do anything with it",0.1632653,0.19540328,0.0010328494245186448
60,"Question\nI was curious if you made an application in python how do you ship it? If you made it in a virtual enviroment can you just give that out for download? Do you need some kind of install process?
(In my case it would be a webapp made with flask intended to run on a linux server)\nAnswer: In your case I'd heavily suggest you build a docker container and archive the code via git. In my experience, just doing pip freeze > requirements.txt does not suffice when the intended audience is not python literate. Especially issues with venv and conda setups have cost me way to many hours of my life.",0.0,0.1067065,0.01138627715408802
61,"Question\nI'm working on a Django project and i want to be able to disable or enable email registration verification in admin panel, with a radio button, if it's set to ON, when user registers to site need to verify email, but if it's set to OFF user account activate after registration and don't need to activate account with email verification\nAnswer: Hope you are having a good time.
I would like the purpose behind allowing email verification after registration because it's sole purpose is to validate authentic users and not allow unidentified login attempt to pass.
Because there have been so many hacking attempts just because of insecure security steps on login panel itself these days, I think it is better if we retain the email verification and not switch its permission.",0.0,0.039220214,0.0015382252167910337
62,"Question\nI have a list including some names, as example:
data_set_names=['matrix_1','matrix_2','matrix_3'].
Inside a loop, I want to use each of these names for storing the output of some computations, which are as NumPy array. I highly appreciate if someone could tell me how to do that. As I looked it up online, exec() function can be used to convert a string to a variable name, but in my case, it is not useful.\nAnswer: You can use the dictionary of str/numpyArray",0.20408164,0.30772662,0.01074228249490261
63,"Question\nI am using Python 3.9 on Windows 10 which I downloaded directly from the Microsoft Store.
I tried running a script in PowerShell: Bash *.sh
This script is supposed to tell my computer to execute a.py script which uses scipy.io and many other modules.
Then I received this error:

ModuleNotFoundError: No module named'scipy'

My strategy was to make sure pip was up to date, then use it to install the desired packages, then run some commands to see if the packages were installed.
I ran this command to update pip:
python3 -m pip install --upgrade pip
I ran this command to get some modules:
python -m pip install --user numpy scipy matplotlib ipython jupyter pandas sympy nose
I also tried this command just in case:
pip install scipy
and got the result:

Requirement already satisfied...

I ran the command pip list to make sure scipy was in the list (and it was there).
Then I ran the command python and my prompt changed to "">>>"" and entered import scipy and did not receive any errors.
I am very confused as to how I have scipy installed yet have my script tell me it isn't there. Please help!\nAnswer: I had the same issue. You might have multiple versions of python and you only installed scipy on the one you are not using
OR
you are using an IDE which has the option to use packages you install that are not by default in python. Pycharm has that. When you make a new project, it has a tick option saying ""Inherit global site-packages"" which means it will use additional packages you have installed any.",0.0,0.29739308,0.08844264596700668
64,"Question\nHi I was wondering if i could add buttons in my terminal using python as I am making a project and decided not to use tkinter and other GUI modules. Instead I wanted to do it in the python terminal and for your information I am using a MacBook.
I think that you can use the cursers module or something like so to make buttons just that I don't know how to do so :(
Thanks for any help provided! ;)\nAnswer: in Python we can not create a button in the terminal and there is no way at all and only using graphical environments this is done which is not in the terminal
You can do two things: a mouse and keyboard control with a click function, and if you do a Google search for this function, it will bring you
Or put a text yourself, for example print (1- hello)
And then define an input, such as go input
And then define an if and say if go == 1: print (hello)
There are two ways you can
I hope this article is useful for you and you have answered your question",0.40816328,-0.19674164,0.3659099340438843
65,"Question\nI am in the Microsoft Visual Studio environment, most screenshots of this environment show an activity bar in the left sidebar. However, I cannot get this to appear in my display, I have sought to access this activity bar in view but cannot see any reasonable options.
Any thoughts?\nAnswer: My experience is from Visual Studio Code and not Visual Studio 2019 but I suspect they are most likely similar..
Press F1 and type ""View: Toggle Activity Bar Visibility"" and click on it when it shows up.  That should make your activity bar visible again.",0.0,0.3023895,0.09143941104412079
66,"Question\nI am in the Microsoft Visual Studio environment, most screenshots of this environment show an activity bar in the left sidebar. However, I cannot get this to appear in my display, I have sought to access this activity bar in view but cannot see any reasonable options.
Any thoughts?\nAnswer: On Microsoft Visual Studio 2019 if you check on the ""Debug"" menu, you can find other windows that are helpful when the system is running.
I'm not sure which window you are looking for, but there's a ""Diagnostic Tools"" that could be the one.",0.0,0.28458858,0.08099065721035004
67,"Question\nI am in the Microsoft Visual Studio environment, most screenshots of this environment show an activity bar in the left sidebar. However, I cannot get this to appear in my display, I have sought to access this activity bar in view but cannot see any reasonable options.
Any thoughts?\nAnswer: Follow the steps:

Open the Command Palette ( Ctrl + Shift + P ) and type Keyboard Shortcuts.
Click on Toggle Activity Bar Visibility, and choose your shortcut using the
keyboard!
It's done!",0.0,0.23541057,0.05541813746094704
68,"Question\nI have 20000 RGB images. I set batch_Size = 1 (due to GPU capacity). So now does it mean the model weights are changing with one-by-one pictures or it depends on the steps_per_epoch?
How should I set the steps_per_epoch and epochs for using all of 20000 images to be involved in training in different epochs?\nAnswer: Yes, the weights are updated after each batch.
The steps_per_epoch should be the number of datapoints (20000 in your case) divided by the batch size. Therefore steps_per_epoch will also be 20000 if the batch size is 1.",0.0,0.49089324,0.24097618460655212
69,"Question\nHello guys I have an existing Django project which has certain features and also has user data. For certain features, users use the API served through Django (also the authentication), but since there is a need for new feature which needs to be implemented through FastAPI,  I need to have the same users authenticate or (better to say) to be recognized by FastAPI (as the same user in Django) to save or retrieve an action corresponding to user in the db (through FastAPI).
How to achieve that? How do I store the user data, like user_id and username for each user safely? How to properly design the database table?
Please do let me know, how to start.
Thank you.\nAnswer: Assuming you've implemented your own AbstractUser sub-class named DannyUser, you then have a couple of choices:

Embed all of the information needed into a new field (a JSONField subclass for example) -- not really recommended as this leaks random information into a housekeeping table and is ill-advised IMO.

Design a new Model for each authentication service and tie it via a foreign key & reverse relationship to your DannyUser instances. Then you could use something like my_danny_instance.fastapi_account.auth_token where needed. This will also allow you to encrypt specific fields or use a secret management solution later on.


Simply normalize your data as far as it makes sense to.",0.20408164,0.30196542,0.009581233374774456
70,"Question\nThe ""COPY CONTENT FROM [available translation languages]"" button is not showing in the admin page editing interface.
The package was installed properly, following official documentation.
What can cause the issue and how to fix it?\nAnswer: The issue was in app register order. wagtail_modeltranslation app should be registered on top of user apps",0.40816328,0.008666635,0.15959757566452026
71,"Question\nI can write to a file using yaml.dump but how do I write to a string?\nAnswer: I have found the solution. Its an overloaded function. If you don't pass the file parameter, it will output to a string.",0.0,0.28813905,0.08302410691976547
72,"Question\nI know how to do it in db shell, but I need to do it in the code in which file and where should I place print(len(connection)). Maybe I need to do something else?\nAnswer: Try to place it in your views, but be sure your querysets are evaluated.
But I would recommend you to use django-silk, it's much more powerful kit for your goal.",0.0,0.2612114,0.06823139637708664
73,"Question\nI am fairly new to using sockets, and this will probably have a simple answer that I am overlooking, but since an hour of agonizing has not yielded results so... what the heck.
How do I receive for.sendall() in the python socket module? By this I mean how do I receive data from a socket with out a buffer? is there a simple solution for this like some sort of conn.recvall() function or do I have it write out logic to do this? If I do have to write logic for it, then how should I do it? Should I just keep using.recv() with some arbitrary buffint or do I have to split the inputs into segments before sending? Which is more efficient, or better? Is there a smarter way to go about it?
Thanks\nAnswer: send and sendall will chop your buffer into pieces for sending over the network.  It's important to remember that TCP is a streaming protocol, not a packet protocol.  If you send 1,024 bytes, it might be received by the other end as 1,024 bytes, or as one of 256 and one of 768, or one of 1,000 and one of 24.  The receiver need to know when the transmission is complete.  Sometimes it's fixed buffer, sometimes you'll send a byte count first, sometimes you use a special termination character, sometimes you wait for a timeout.  The receiver just needs to keep calling.recv until he knows its done.
Some of the higher level Python packages (like twisted (which I recommend)) can handle that for you.",0.0,0.2864954,0.08207960426807404
74,"Question\ni have cookie get from website.  how do i log that cookie using requests?
cookie=""sb=Vma2X7D6JF_aBy6ESWdwm-OL; datr=Vma2X2YjSxJ-JzCD368WGfmL; locale=vi_VN; wd=1366x657; c_user=100029745455196;""
how login with requests?\nAnswer: No, you can't login with cookie, you need login with credentials: username & password.
If you have logged in with credentials, you MAY use that cookie to bypass login step.",0.0,0.14194894,0.02014950104057789
75,"Question\nI am using paramiko library to connect with a specialized environment. Its based on linux but when we SSH in it provide its own shell. We can write help to get list of all commands that are supported in that session.
I am using paramiko with python2.7 to provide a CLI client (it automates few things) that connect with the host and let us run the supported commands. Now I would like to provide tab-completion in the client CLI. I am not sure how this can be done. I am thinking there would be some support or some specialize character that can be send to get back response but I am not sure how it can be accomplished.
I am hoping to avoid sending help command, parse the list of commands supported, and then provide a local tab-completion based on list of command. I want a more generic and dynamic solution.
Any or all ideas are welcome.\nAnswer: You can try simulating the partial input and the Tab key press and parsing the results, undoing the simulated input afterwards. But that is not a good idea. You will have to end up re-implementing terminal emulation, what is an insane task. Without a full terminal implementation, you can never be sure that you never get an output that you won't be able to parse.
The shell is a black box with input and output. It should only be used as such. You should never try to ""understand"" its output.
Using the help command is a way more reliable solution.",0.40816328,0.20566952,0.041003722697496414
76,"Question\nI have an SAS file that is roughly 112 million rows. I do not actually have access to SAS software, so I need to get this data into, preferably, a pandas DataFrame or something very similar in the python family. I just don't know how to do this efficiently. ie, just doing df = pd.read_sas(filename.sas7bdat) takes a few hours. I can do chunk sizes but that doesn't really solve the underlying problem. Is there any faster way to get this into pandas, or do I just have to eat the multi-hour wait? Additionally, even when I have read in the file, I can barely do anything with it because iterating over the df takes forever as well. It usually just ends up crashing the Jupyter kernel. Thanks in advance for any advice in this regard!\nAnswer: Maybe you don't need your entire file to work on it so you can take 10%. You can also change your variable types to reduce its memory.

if you want to store a df and re use it instead of re importing the entire file each time you want to work on it you can save it as a pickle file (.pkl) and re open it by using pandas.read_pickle",0.0,0.009528279,9.078810398932546e-05
77,"Question\nI have a question regarding transfer learning. Let suppose there is a neural network model that takes an input of shape (250,7). I want to initialise the model with the weights of this pre-trained model and then train it on my dataset to update the weights according to my dataset. But my dataset is of shape (251,8). Is there way to initialise the weights using pre-trained model considering my input shape is different? If so how can I do that? Insights will be appreciated.\nAnswer: You could try adding another layer before the transfer learning model. Just like the last layer, this will update its weights on your dataset and should work fine.",0.40816328,0.08547491,0.10412778705358505
78,"Question\nI have a string variable containing the HTML of an email.
The email contains Google drive attachments(links)
I extract the attachment ID of the same and the Google drive attachments are always in the following format:
'https://drive.google.com/file/d/123456789/view?/usp=drive_web'
123456789 being the file ID which I am trying to extract
When there is only one attachment, I extract the ID using the below code:
html_string.split('<a href ""https://drive.google.com/file/d/')[-1].split('/')[0]
However, I do not know how to extrapolate this where there are multiple file IDs in the HTML string.
I am looking to find a list of all the strings between
this substring - '<a href ""https://drive.google.com/file/d/'
and the next immideate '/'
example:
a small sample HTML file looking like so:
<a href ='https://drive.google.com/file/d/123456789/view?/usp=drive_web' >attachment_1</a>random HTML text with multiple '/'<a href ='https://drive.google.com/file/d/987654321/view?/usp=drive_web' >attachment_2</a>
Need to extract the following list :
['123456789','987654321']
Using a code that would work for any number of attachments\nAnswer: Once you fix your html_string.split call, you'll find it is returning a list whose elements (except for the first) each start with one of the numbers you want.",0.0,0.12382579,0.015332825481891632
79,"Question\nHell all, so let try to describe my problem in the best light possible. I have experience with Python but have never used FME. I would like to delete a full row of data based on criteria in a particular column.
I have an idea of how I would write this in Python but being new to FME not quiet sure where I start here with the transformations.. For example: the Column will have an A, B, or C. If the column for any row has a ""C"" I want to to delete this full row. While it seems simple enough Python I'm not quite sure where to start with FME. Any help or helpful tips to point me in the right direction that would be great.\nAnswer: You need a ""tester"" transformer, add you're conditions in the transformer. You'll have 2 output ports, the first one (passed) is the data matching the conditions, the second (failed) not matching, that it.
(google translate from french)",0.0,0.22927168,0.05256550386548042
80,"Question\nI am working with time series datasets where I have two different cases. One where my sequences are of same size and the other one where the sequences are of different lengths. When I have same length sequences I can merge all the datasets and then fit the model once.
But for different length sequences, I was wondering how differently should the keras model.fit will behave

if the models are fitted with each different length sequences one by one with  batch size=length of sequence
if the models are fitted once with all the sequences merged together having a fixed batch size

And based on the given scenario what should be the correct or better course of action?\nAnswer: In first scenario, the weights will be optimized first with first dataset and then will be changed(updated) for the second dataset and so on. In second scenario, you are simultaneously asking the model to learn patterns from all the datasets. This means the weights will be adjusted according to all the datasets all at once. I will prefer the second approach because NN have the tendency to forget/break when trained on new datasets. They are more likely to focus on the data they have seen recently.",0.0,0.20644677,0.04262026771903038
81,"Question\nOS: windows 10
I install python3.9, pip install many 3rd party packages and use them well on both Pycharm and CMD terminal;
Later, I install MSYS2 and then I tpye 'python' on the terminal of MSYS2 and get into python terminal; it seems well until now. When I import sth, getting the warning ""No module named 'xxx'"" which can be imported well on Pycharm or CMD terminal.
so I guess the path of 3rd party packages not be included for MSYS2, how to resovle it?\nAnswer: Here is the standard way to use Python 3 and pip in MSYS2:

Select the ""MSYS2 MinGW 64-bit"" shortcut or run mingw64.exe to start a MinGW 64-bit shell.  (32-bit should work too.)
Run pacman -S $MINGW_PACKAGE_PREFIX-{python3,python3-pip} to install both Python 3 and pip.
Run pip install PKGNAME to install a package you need.
Run python path/to/script.py to run your script.

I tested these instructions just now.  I was able to run pip install pyserial and then I was able to run a script that started with import serial.",0.40816328,0.23506004,0.029964732006192207
82,"Question\nI have a UAV with a LiDAR onboard flying and scanning on 3D space. I have the UAV's GPS position with good precision, and I wanted to know how to build a 3D map using the LiDAR's pointCloud. Our goal is for the UAV to avoid obstacles in front of it, and it would be very helpful in visualizing the operation remotely.
I have ardupilot's GPS and orientation data through mavlink and publish it on ROS for my application, as well as the LiDAR's scan as a PointCloud2. Can I somehow set a GPS static origin and build a map around it, using something like octomap_server?
Any tips on what to look for would be greatly appreciated!
Thank you kindly.\nAnswer: The octomap_server will assume the origin is (0,0) and try to build a map around that. As your question implies, if you're using the lidar for object detection and avoidance, you should not be operating in the GPS frame. If you're trying to use the lidar and with octomap_server for a long term data reference, this work should be done in the map frame; here you shouldn't have any issues with setting default points. If you would like to just use it for a short term reference and continuous object avoidance, it should be done in the odom frame; again, default position shouldn't be a problem here.",0.0,0.22906303,0.05246987193822861
83,"Question\nI am trying to write a scalable app using python and docker and mongodb
The app just runs a web search and returns a list of web pages
I then store this list in a mongodb collection using dictionary
I also have created a simple docker file that exposes the python entry point
I now would like to understand how to make a docker compose file so that I can scale the app, e.g. “my 10 million users” want to make more searches and return more values (at the moment I am limiting the search to 10 results).
Please can you advise?\nAnswer: I don't think a providing a Compose file will necessarily help here. If you paginate the Mongo response, that'll help page load times
Otherwise, to distribute database load, you need to simply need to add more Mongo and Python containers. To handle web server load balancing, add a reverse proxy. You can refer to the replicas option in the Compose spec if you want to do that easily, otherwise, simply copy paste the same services
Worth mentioning that Elasticsearch is the more common tool for search results than Mongo",0.0,0.26333687,0.06934630870819092
84,"Question\nI'm using the direct runner of Apache Beam Python SDK to execute a simple pipeline similar to the word count example. Since I'm processing a large file, I want to display metrics during the execution. I know how to report the metrics, but I can't find any way to access the metrics during the run.
I found the metrics() function in the PipelineResult, but it seems I only get a PipelineResult object from the Pipeline.run() function, which is a blocking call. In the Java SDK I found a MetricsSink, which can be configured on PipelineOptions, but I did not find an equivalent in the Python SDK.
How can I access live metrics during pipeline execution?\nAnswer: The direct runner is generally used for testing, development, and small jobs, and Pipeline.run() was made blocking for simplicity. On other runners Pipeline.run() is asynchronous and the result can be used to monitor the pipeline progress during execution.
You could try running a local version of an OSS runner like Flink to get this behavior.",0.40816328,0.32289135,0.0072713010013103485
85,"Question\nWhat would be best way to solve following problem with Python?
I have real-time data stream coming to my object-oriented storage from user application (json files being stored into S3 storage in Amazon).
Upon receiving of each JSON file, I have to within certain time (1s in this instance) process data in the file and generate response that is send back to the user. This data is being processed by simple Python script.
My issue is, that the real-time data stream can at the same time generate even few hundreds JSON files from user applications that I need to run trough my Python script and I don't know how to approach this the best way.
I understand, that way to tackle this would be to use trigger based Lambdas that would execute job on the top of every file once uploaded from real-time stream in server-less environment, however this option is quite expensive compared to have single server instance running and somehow triggering jobs inside.
Any advice is appreciated. Thanks.\nAnswer: Serverless can actually be cheaper than using a server. It is much cheaper when there are periods of no activity because you don't need to pay for a server doing nothing.
The hardest part of your requirement is sending the response back to the user. If an object is uploaded to S3, there is no easy way to send back a response and it isn't even obvious who is the user that sent the file.
You could process the incoming file and then store a response back in a similarly-named object, and the client could then poll S3 for the response. That requires the upload to use a unique name that is somehow generated.
An alternative would be for the data to be sent to AWS API Gateway, which can trigger an AWS Lambda function and then directly return the response to the requester. No server required, automatic scaling.
If you wanted to use a server, then you'd need a way for the client to send a message to the server with a reference to the JSON object in S3 (or with the data itself). The server would need to be running a web server that can receive the request, perform the work and provide back the response.
Bottom line: Think about the data flow first, rather than the processing.",0.40816328,0.32556522,0.0068224393762648106
86,"Question\nI have used tkinter to make a complex GUI, with the mainwindow as a class, so I can put function defs anywhere, and the other perks of doing it that way. Currently the simpler subwindows are just defined as normal functions within this class, called like a normal function (and using Tk.Toplevel()).
My questions is - does it make sense to also write these subwindows as classes like the main window (or is it even possible?), or does this just end up making a whole mess? So far I can't figure out an elegant way to do it versus just making the subwindows traditional functions rather than classes... but I know in the long run the ""class"" way of doing it is better for the main window, so if the subwindows get more and more complex, should I somehow do the same approach?\nAnswer: Thanks all - it wasn't so much an opinion question, rather is it possible in the first place. Turns out it's quite simple and I was over-complicating it. I was trying to create instances of the sub-window class, which in turn was inheriting the entire main window class, trying to call it's own mainloop(), etc etc. Very messy.
For people in the same boat - code the subwindow as if it were a stand-alone main window, but use tk.Toplevel() rather than tk.Tk() to'start' it within init(), then to call (i.e. open) it you don't need to create an instance, just call it as though it were a function (it will call the class constructor and Toplevel() handles most of the rest).",0.0,0.42552066,0.1810678243637085
87,"Question\nWhich form name could be used to extract “view the audit log” data for an incident? for example，I can successfully extract related data using form ”HPD：help desk”，how to extract “audit log” data for an incident?\nAnswer: You can use ""HPD:HelpDesk_AuditLogSystem"" or ""HPD:Help Desk Audit Log"" form.
Regarding indexed field OOTB there is only 1 index configured- Original Request ID (ID:450).",0.0,0.42850935,0.18362027406692505
88,"Question\nI need to recognize written text in a table and parse it in json. I do it using python. I don't really understand how to extract photos of text from a table that is in pdf format. Because the usual table recognizer is not suitable, since written text is not recognized there. Accordingly, I need to somehow cut the cells from the table, how to do this?\nAnswer: PDF format has not 'table' and 'cell'.
Covert PDF into PNG format or other raster format and use OCR as say BlackCode.",0.0,0.16174263,0.026160677894949913
89,"Question\nHow can I best structure a program?
I have some classes in different.py files doing REST API calls to a service using a Bearer auth key.
Since I don't want to store the key in every class I would want to know how I should best approach this.
Where and how should I store the key? How should the classes using this key access it? What is industry best practice?\nAnswer: One good approach would be to make a Base class which loads the api key (and potentionally any other env variables) from a.env file and stores it as a class member. Then all classes which need the api key access would just inherit from Base class.
Don't forget to add.env file to.gitignore, so you don't share all the private information with other users.",0.0,0.32189047,0.1036134734749794
90,"Question\nOk, so I am trying to create a bot that displays information through an api, that uses https://[PROJECT_ID].firebaseio/.json?shallow=trye&download=myfilename.txt, but it doesnt show all the information in the firebase, for instance this request will bring up one set of info, but then I change.json to something like ""music.json"" and then it gives entirely new data seperate to the other-, does anyone know how I could get all json file names to make this process alot easier?\nAnswer: Downloading the JSON from a given path in the database should give all data under that path. I just tested this on a database of my own, and got the entire database.
If you're only seeing some of the data, it might be caused by how you process the resulting JSON file.",0.0,0.17059368,0.029102202504873276
91,"Question\nI'm trying to install the cxvpy module in Python 3.8:

pip install cxvpy

and get the following error

ERROR: Could not find a version that satisfies the requirement cxvpy (from versions: none) ERROR: No matching distribution found for cxvpy

how can i fix it?\nAnswer: I think you mean cvxpy (rather than cxvpy)? The Convex Optimization module? Once again, since I can't comment I'm posting this as an answer.",0.0,0.14746219,0.021745096892118454
92,"Question\nSo I've looked around and although there are a bunch of similarly phrased questions but I haven't found one that addresses my question. I don't really want to trawl through Stack Overflow, so here's to hoping this isn't a duplicate.
So I coded a Discord Embed that requires pinging to work. The text is displaying as a discord ping should look with the light blue background and such, but there is no ping and users simply get a new message notification instead of a ping. This is the case for role mentions as well as user mentions. For user mentions I used author.mention and for role mentions I used the ID. Does anyone know how I can change this ""setting?""
One possible workaround that I have thought up is that I could ping the needed parties and then instantly delete the ping right before sending the embed, but for my peace of mind I would prefer if the ping was the one which is displayed in the embed.\nAnswer: So as i know you can`t do a ""Ping"" in a Embed at least not what you call a ping. To ping People you have to do a ping in a normal message. You could do this before the embed and delete it or you could not delete it.",0.0,0.26861793,0.07215559482574463
93,"Question\nBasically I need to access to the execution date on a PythonVirtualenvOperator and as far as I know you can't pass the execution date as op_kwargs or provide_context=True. I read that by using pendulum one can achieve this but I haven't seen any useful docs about it. Does anyone knows how to achieve this or has an example that can illustrate this?\nAnswer: The provide_context is deprecated and no longer needed in Airflow 2. PythonVirtualenvOperator derives from PythonOperator and you can pass all the variables to it the same way as you would for PythonOperator.
You have the very same context dictionary passed to it (well, it's a serializable version of it, but datetime fields are serializable) and you can use it in the same way you would use them in PythonOperator.",0.0,0.44652903,0.19938817620277405
94,"Question\nI'm using 'virtualenv' to manage different environments. (ubuntu 18.04)
But when I workon an env with python3.7 and run the code(smpl-x from github), it says ""no module named '_bz2'"".
I tried these:

sudo apt-get install libbz2-dev
But it seems that bz2 is not installed in my virualenv, so it doesn't work.

some posts say that I need to rebuild python. But I don't know how to do this in my virtualenv.


Is there a way to solve this problem?
Any help would be greatly appreciated. Thank you very much.\nAnswer: Guys I have solved this problem.
I removed my global python3.7 and built it again from the source code. Then the problem is solved.
I'm a noob in linux so not very familiar with these operations.
Again very appreciate your attention. Thank you very much!!",0.0,0.33677378,0.11341658234596252
95,"Question\nI pulled a python image from hub images doing docker pull python:3.9-alpine.
Then i tried to launch the container from this image doing like docker run -p 8888:8888 --name test_container d4d6be1b90ec.
The container is never up. with docker ps i didn't find it.
Do you know why please?
Thank you,\nAnswer: Your container is not launched because there is no server(nginx,apache etc..) to point, there is only  python and the necessary dependencies.
In order to run that image you can try the following command:

docker run --name test_python -it [id_image]

And if you open another terminal  and use docker ps you will see that the container is up.",0.40816328,0.18792301,0.04850577563047409
96,"Question\nI'm extremely new to Elasticsearch and I can't seem to find an answer that will help me to get Python to detect if the data from the documents I have in an s3 bucket are already uploaded in elasticsearch. My goal is to have it see if the data from the s3 bucket is already in there, if it is then skip it, and move onto the next one until it finds a document that has data not uploaded yet. Can someone help me, please?\nAnswer: I think the easiest way would be to use DynamoDB to store that kind of information. So each file that you upload to ES, gets a record in DDB. Thus you can always verify if the file had been uploaded to ES, by checking for the presence/absence of records in DDB.",0.40816328,0.21449888,0.03750590234994888
97,"Question\nI have a script in python that uses cv-bridge. But I keep getting this error:
ERROR: Could not find a version that satisfies the requirement cv-bridge==1.15.0 (from -r requirements.txt (line 5)) (from versions: none)
ERROR: No matching distribution found for cv-bridge==1.15.0 (from -r requirements.txt (line 5))
To reproduce the error:
pip install -r requirements.txt
requirements.txt:
cv-bridge==1.15.0
Does anybody know how to solve it?\nAnswer: You have to actually install CV_Bridge and your script is failing because it cannot find a match for the exact version. As mentioned in the comment, this is almost certainly because of how old that version is. You should install the bridge via apt by doing sudo apt-get install ros-(DISTRO)-cv-bridge and sudo apt-get install ros-(DISTRO)-vision-opencv.",0.0,0.06928331,0.004800176713615656
98,"Question\nI am using locust to run load test. Specifically I am trying to use docker-compose and following the documentation at https://docs.locust.io/en/stable/running-locust-docker.html
I want to retrive test stats in CSV format per the directions in https://docs.locust.io/en/stable/retrieving-stats.html
Now when running this setup headless how can I get aggregated results in CSV format from all workers? The non headless version allows me to download the aggregated, results as a CSV, but am not sure of the headless version would work here.
Thanks!\nAnswer: You should only have to worry about running --headless --csv=example (as noted from the docs page you linked to) and such on the master. The workers don't need those as headless only applies to the master and they don't aggregate their own results. The CSVs generated by the master should contain all the results from all the workers. If you've tried this and you're not seeing all the data you're wanting, you may want to try adding --csv-full-history.
From the docs page:

The files will be named example_stats.csv, example_failures.csv and example_history.csv (when using --csv=example). The first two files will contain the stats and failures for the whole test run, with a row for every stats entry (URL endpoint) and an aggregated row. The example_history.csv will get new rows with the current (10 seconds sliding window) stats appended during the whole test run. By default only the Aggregate row is appended regularly to the history stats, but if Locust is started with the --csv-full-history flag, a row for each stats entry (and the Aggregate) is appended every time the stats are written (once every 2 seconds by default).",0.40816328,0.5290487,0.014613280072808266
99,"Question\nI'm  designing a UML class diagram for a Mastermind game and I'm having trouble figuring out how to make the attribute 'Name' unique and not changeable. I'm assuming that I do this by naming the 'Name' attribute private and show it in the class diagram as well. Any sort of help would be highly appreciated. Thanks\nAnswer: As per Python you can only indicate private attributes by adding an underscore in front of the name. Language-wise this is not a language construct but a convention. UML does not care since it's language-agnostic. Model it as private (by showing the - in front of the name). If you are generating code from that UML model the code generator might create the underscore automatically.
In any case, if you create a model with UML and indicate something to be private then the coder for the language must take care. For a closed implementation you just need to train your coders. For an open (library) implementation Python will let you stand in the rain with that.",0.20408164,0.39353722,0.03589341789484024
0,"Question\nI tried running PyInstaller to create a simple executable, but it generated errors which other posts here suggest its an enum vs enum34 issue. I am running Python 3.7 and for some odd reason my installation has only enum34 (1.1.6).
I am NOT an IT guy, but I am the sole programmer for a startup and am terrified of breaking my environment and not being able to fix it.
Is the proper procedure to simply install enum (I understand it may overwrite enum34), or uninstall enum34, then install enum?
I've also seen posts where folks had difficulties deleting enum34. Any hints on how to avoid those?\nAnswer: For the record:

enum on PyPI -- preexisting third-party enum library

enum34 on PyPI -- backport of the 3.4 stdlib enum

aenum on PyPI -- advanced enum library compatible with the stdlib version, and maintained by the same fellow (me)

enum in the stdlib -- this was added in version 3.4 and has been a part of Python ever since.


--
Sadly, I don't know how to solve your problem.",-0.35714287,0.27503204,0.3996451199054718
1,"Question\nI tried running PyInstaller to create a simple executable, but it generated errors which other posts here suggest its an enum vs enum34 issue. I am running Python 3.7 and for some odd reason my installation has only enum34 (1.1.6).
I am NOT an IT guy, but I am the sole programmer for a startup and am terrified of breaking my environment and not being able to fix it.
Is the proper procedure to simply install enum (I understand it may overwrite enum34), or uninstall enum34, then install enum?
I've also seen posts where folks had difficulties deleting enum34. Any hints on how to avoid those?\nAnswer: I ran into the same issue today. I just uninstalled enum34(using pip uninstall enum34) and then ran pyinstaller in the terminal and everything seemed to be working",0.0,0.31003422,0.09612121433019638
2,"Question\nI have thousands of files stored in MongoDB which I need to fetch and process.
Processing consists of a few steps which should be done sequentially. The whole process takes around ~2 mins per file from start to end.
My question is how to do that as fast as possible while being scalable in future? Should I do it in pure python or should I maybe use Airflow + Celery (or even Celery by itself)? Are there any other ways/suggestions I could give a try?
Any suggestion is appreciated.
Thanks in advance!\nAnswer: Celery alone is precisely made to do what you need - no need to reinvent the wheel.",0.81632656,0.15122646,0.442358136177063
3,"Question\nI am quite new to web application development (Django). However, I have a bit of experience in working with html, css, and  python.
I am developing an application where the user will upload documents as input and change the available parameters values and the application will generate a new document as a downloadable file. (Text editor: VS Code).
(To get an intuition, consider the online pdf resizing applications as an example).
I am building the application from scratch. The major issue is modifying the inputs and producing desired output.
My approach towards it is:

Create python scripts for the different major actions( As per our example case, create pdf_compression.py, merge_two_pdfs.py and so on).
Ask the user to upload the required documents. Store the docs uploaded (only temporarily) and use them for further operations (with assumption that default sqlite3 will be sufficient).
In the webpage, add a button or a link (e.g. ""Compress"", ""Merge"") and embed the python script created in step1 (paste the script or add a link to its file location). That will perform the specific task when the button is pressed and generate the output.
Download the generated document.

I have achieved till the step 2 in my task but am lost on how to achieve the ""3rd"" step. How should I proceed  ahead?
Any suggestions( with code examples) are highly appreciated. I am open to any different approach also.
Update
About the application
The application does not need to serve users globally like google. It will be hosted through a server locally( or though an institutions' local server) and users having access to the server will be accessing it.(in other words it does not need to be https://www... It only needs to be http://ip_address/url.)
Thanks in advance.\nAnswer: There's no need to embed your python scripts anywhere (or to use any javascript/AJAX, as some of the comments suggest).  The simplest way to do ""step 3"" is as follows:

When the ""Compress""/""Merge"" button (or link) is pressed, send a request to the server.  Typically you'd use the URL to indicate which action(s) to perform.

When the server receives the above request, generate the corresponding output file.  Any scripts that are installed on the server can",0.40816328,0.09212774,0.0998784601688385
4,"Question\npython.pythonPath setting is deprecated. What is an alternative way to retrieve currently selected Python path? Any VSCode environment variables? I want to specify python path in task.json without explicit full path.\nAnswer: You can open VS Code command selector with ctrl + shift + p and type ""Python: Select Interpreter"" to open a dropdown menu of all available interpreters. Doing this should create the appropriate entry to settings.json (which was python.pythonPath for me with version Python plugin version v2021.8.1159798656)
You can also change the interpreter from the bottom toolbar.",-0.35714287,0.22759426,0.3419175148010254
5,"Question\nI cant activate virtual env in vs code. I tried same code in the cmd console is work but not in the vs code terminal.
""D:\python\djangoapp\djangovenv\Scripts\activate.bat"" I write this code.
I am using windows 10 pro\nAnswer: yeah Its beacuse of terminal vs code was using powershell ı changed with cmd",0.0,0.13932794,0.019412275403738022
6,"Question\nI cant activate virtual env in vs code. I tried same code in the cmd console is work but not in the vs code terminal.
""D:\python\djangoapp\djangovenv\Scripts\activate.bat"" I write this code.
I am using windows 10 pro\nAnswer: You can also change your default terminal in VS Code, by pressing Ctrl+Shift+P and type Terminal: Select Default Profile and choose a terminal for your needs.
I was using PowerShell and I wanted to activate virtual environment, but it always gave me an error, so I switch from PowerShell to Command Prompt, and now it works.",0.0,0.0047047734,2.213489278801717e-05
7,"Question\nI am on Windows 10, running python 3.8.5 and have tried installing PyPDF2 using pip. I've uninstalled and reinstalled several times using these two commands:
""python -m pip install pypdf2""
""py -3 -m pip install pypdf2""
I did this through Visual Studio code being run as my user, as well as the command prompt run as an administrator. The install goes well but I get the error, ""Import 'PyPDF2' could not be resolved.""
I figure this is due to the file path to the installed package not being included in the PATH system environment variable. So I used ""python -m pip show pypdf2"" to find the install location:
""c:\users*username*\appdata\local\programs\python\python38\lib\site-packages""
So I added:
""c:\users*username*\appdata\local\programs\python\python38\lib\site-packages\PyPDF2"" to my PATH environment variable for both user and system, as well as the PYTHONPATH variable.
After such actions I restarted VS Code, which again is using the python 3.8.5 environment, and still the import could not be resolved.
What am I missing? Any assistance would be greatly appreciated.\nAnswer: I am using python 3.9.6. But when I typed like this in command prompt,
""pip install pypdf2""
it worked for me. Try like this sometimes it'll work.",-0.71428573,0.08064109,0.631908655166626
8,"Question\nIs there a getIamPolicy for google cloud sql service? If there is how to use it?\nAnswer: You don't have Cloud IAM role at the Cloud SQL instance level. You only have project level permission with access to all the Cloud SQL instance of the project. You can perform a getIamPolicy on the project to get all the policies and find which one give access to Cloud SQL
With Cloud SQL, you have users per instance, but there isn't getIamPolicy for this API",0.81632656,-0.034087837,0.7232046723365784
9,"Question\nI have a Vue, Django integrated project. I hosted the Vue project on Netlify and the Django project on Heroku. A python script (integrated into Heroku) is called on certain buttons which extract data and posts this to the Django API to be viewed on the frontend.
I have trained a font type for my pytesseract OCR script. However, when i run it on Heroku, it seems like i can only use the 'eng' (normal font) as 'language' for my pytesseract image_to_string function. If I have the.traineddata file of the font type that I want to use, how can I use this file within the pytesseract functions? I can call the individual file, but I need the right TESSDATA_PREFIX as well. Does someone know how to deal with this?
Thanks!\nAnswer: I had the same issue. It is olved by setting TESSDATA_PREFIX Config Var to a custom directory and inserting all.traineddata files to that directory.",0.0,0.22839475,0.05216415971517563
10,"Question\nI tried to use pyinstaller to compile a.py I wrote. I used a bunch of libraries including sklearn, nltk, gensim but did not use tensorflow at all. However, when I compiled the program, there were many warnings about tensorflow modules not found, such as:
WARNING: Hidden import ""tensorflow._api.v2.compat.v1.estimator"" not found!
Is it because other libraries somehow used tensorflow in their functions? Is there a way to find what the usage is? Thanks!\nAnswer: Gensim uses tensorflow.
You can add --hidden-import=""tensorflow._api.v2.compat.v1.estimator"" with your pyinstaller command and it will resolve this error.",0.0,0.37052828,0.13729120790958405
11,"Question\nI need to build a model that can expand multiple short sentences to multiple long sentences. I was thinking to use a pre-trained Transformer model to do this just like when we want to do a paragraph or text summarization except, in this case, I switched the output and input values. I tried this using t5-base, ran it on Google Colab, and using really minimum data like 10 rows of data, the idea was to see whether it works or not regardless of the output. But I always got errors like below:

RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 0;
11.17 GiB total capacity; 10.29 GiB already allocated; 237.81 MiB free; 10.49 GiB reserved in total by PyTorch)

I interpret this error as I did something wrong or my idea did not work. Is there anyone who can suggest how to do this?
Please advise\nAnswer: I reduce the batch size and it solves the problem.",0.0,0.1737262,0.03018079325556755
12,"Question\nI have two or three sets of Azure credentials for Work, Work Admin, and Personal.  This morning, I clicked the wrong login credential during an interactive login while doing some local development.  My local dev app now has an identity of me@company.com, when I need my identity to actually be me@admin.com.  Because I clicked the wrong identity, my application immediately starts getting obvious authorization errors.
My implementation is pretty naive right now, and I'm relying on the Python Azure SDK to realize when it needs to be logged in, and to perform that login without any explicit code on my end.  This has worked great so far, being able to do interactive login, while using the Azure-provided creds when deployed.
How can I get my local dev application to forget the identity that it has and prompt me to perform a new interactive login?
Things I've tried:

Turning the app off and back on again.  The credentials are cached somewhere, I gather, and rebooting the app is ineffective.
Scouring Azure docs.  I may not know the magic word, and as a consequence many search results have to do with authentication for users logging into my app, which isn't relevant.
az logout did not appear to change whatever cache my app is using for it's credential token.
Switching python virtual environments.  I thought perhaps the credential would be stored in a place specific to this instance of the azure-sdk library, but no dice.
Scouring the azure.identity python package.  I gather this package may be involved, but don't see how I can find and destroy the credential cache, or any out way to log out.
Deleting ~/.azure.  The python code continued to use the same credential it had prior.  ~/.azure must be for the az cli, not the SDK.\nAnswer: Found it!  The AzureML SDK appears to be storing auth credentials in ~/.azureml/auth/.
Deleting the ~/.azureml directory (which didn't seem to have anything else in it anyway) did the trick.",0.20408164,0.4057572,0.04067302867770195
13,"Question\nI'm currently trying to create a spider which crawls each result and takes some info from each of them. The only problem is that I don't know how to find the URL that I'm currently on (I need to retrieve that too).
Is there any way to do that?
I know how to do that using Selenium and Scrapy-Selenium, but I'm only using a simple CrawlSpider for this project.\nAnswer: You can use:
current_url = response.request.url",0.0,-0.10694605,0.011437457986176014
14,"Question\nI've set my TextInput's color to a diffrent one, even though - if you don't click on the TextInput you see a weird shadowed look on the TextInput. Blackish-grey at the top and white on the bottom. How do you get rid of this? I've tried foreground_color it doesn't work, even background_color - it doesn't work. How?\nAnswer: Just add background_normal: '' to the TextInput in your kv.",0.0,0.1297192,0.016827071085572243
15,"Question\nI have a result of my pre-grouping of grouping columns like this




Letters
Numbers




A
1


A
2


A
3


B
1


B
2


B
3


C
1


C
1


C
1




Now what I wanna do is, to define that C is a Significant factor for 1s as its 100% there but with A or B its only 33%. I wanna show that C significantly differs from A and B.
I think there should be a solution with pandas and data frames, but I'm not really sure how to find that value. Which of the pandas Methods can I use to find that value?
Expected Output:
C significantly differs from A and B in its result spectrum. This is proven by this factor of significancy (this factor is what i search)
Thanks for the help in advance :).\nAnswer: I found the answer by playing more with my data.
The answer is to build a new dataframe with 6 columns




A
B
C
1
2
3




1
-
-
1
-
-


1
-
-
-
1
-


1
-
-
-
-
1


-
1
-
1
-
-


-
1
-
-
1
-


-
1
-
-
-
1


-
-
1
1
-
-


-
-
1
1
-
-


-
-
1
1
-
-




That way I can use the corr function of pandas and see how significant my ""C"" actually really is.",0.0,0.09046143,0.008183270692825317
16,"Question\nI'm writing code in python to help me accomplish something in a Roblox game. I won't go into too much detail, but basically, the objective is to automatically click at certain points in the screen. To try to accomplish this, I am using pyautogui.click(). So for instance if the point I needed to click at was (300, 500), I'd do pyautogui.click(300, 500)
Outside of Roblox, pyautogui works just fine. However, whenever I have Roblox open and I use the commands, it doesn't work properly. So, let's say the mouse starts out at (0, 0), and I activate a click at (300, 500). When that happens, the mouse cursor doesn't move, and the click happens at (0, 0) where the mouse originally was. However, as soon as I move my actual mouse even slightly, the mouse cursor teleports back to (300, 500) where it was supposed to be.
This makes it impossible to do what I want because I want the program to click at certain spots without me having to move my actual, physical mouse. Does anyone know how I can use python to actually move the mouse properly?\nAnswer: you have to use pyautoit to make it work inside of roblox, i used autoit.mouse_click(""left"",x, y) to make the mouse go to an x and y position in your screen and click the left or right click.",0.0,0.35661733,0.1271759271621704
17,"Question\nI'm working on a mobile application where the backend is done by Dango but all the authentication and real-time database are in firebase
is there any way to integrate a Django admin with firebase instead of using firebase admin?
if not, can firebase admin be used with python to create an admin role and has the same permissions which Django gives to admin? and how to introduce the created role if possible to Django?\nAnswer: One of the many beauties of Django is the admin interface, which I use all the time. Django relies on certain types of databases - namely relational databases. With NoSQL databases, you lose Django admin. If you want to Firebase with Python, I would recommend Flask.",0.0,0.3049562,0.09299828112125397
18,"Question\nI use Visual Studio Code for all of my programming in Python, and rely heavily on the Interactive Window and Data Viewer. I usually work with large data sets, though this has only marginally affected run times in the past. For some reason recently, when clicking on a data frame in the Interactive Window, a Data Viewer tab will appear but not fully load the data frame. The blue line signaling that it is working just keeps scrolling across the top of the screen interminably. Everything else seems to work fine, and the programs are still able to run, however I can't really continue developing them.
Does anyone have any tips for how to make the Data Viewer work again? I have already uninstalled and re-installed Visual Studio Code, and have disabled extensions as well.\nAnswer: I have solved this by re-installing the Python extension. Thank you!",0.0,0.10909855,0.011902494356036186
19,"Question\nI want to have an image as a background to a Listbox, but I am am only able to find out how to change background color of one.\nAnswer: No, it is not possible to add an image as a background for the listbox.",0.0,0.17350167,0.03010283038020134
20,"Question\nI tried to import numpy but I received a ModuleNotFoundError: No module named 'numpy' error. Someone told me it could be because I didn't have numpy installed, but I already did.
upon installing numpy I got Requirement already satisfied: numpy in./opt/anaconda3/lib/python3.8/site-packages.
which python returns /Users/MacBook/opt/anaconda3/bin/python. I am new at this, but I'm guessing the reason I got that error was because the files aren't in the same place? If so, how do I move it to the right place?
side note: I have a similar issue with matplotlib and this is running on VS code if that helps. Also I use spyder and I don't get the numpy nor the matplotlib error over there, but the error seems to be on VS code\nAnswer: Try uninstalling numpy and re-installing. You also try re-install anaconda. I had this same issue and that fixed it.",0.20408164,0.2338394,0.0008855238556861877
21,"Question\nI have python file i want to share it to my friends, but they are saying they cant download the file.
Is there any way that they can run the python file without downloading that file? They cant download.exe files also.
I tried using online compiler for python but as my file has tkinter and many modules, compilers are not able to run my file.Can i run the code which i have mentioned in google colab or jupyter notebook?
Please suggest me how to do this. I just want to demonstrate the app which i created.\nAnswer: Ask to run '.py' extension in a Code Editor (eg. VSCode)
Or else just ask to run the '.exe' by downloading it. And then delete if not want to keep it.",-0.35714287,0.2286905,0.3432007431983948
22,"Question\nThe python file in question collects data from API and the server is down once in a while so it stops running. to solve this, I've written this batch file that would keep running the file even if it encounters an error, so in few minutes it would be running as usual.
:1
python3 datacollection.py
goto 1


Now, how do I achieve the same with bash file?
Note: I want to run the python file on AWS EC2 Ubuntu.\nAnswer: Just to add to @Willian Pursell answer. You could run screen command on your AWS server, then run while sleep 1; do python3 datacollection.py; done in your newly created session. Finally detach to the session using Ctrl + ad keys. With that, you will be able to exit your session. Exit the server, the command will keep running on the background. Cheers.",0.13605443,0.02218008,0.012967366725206375
23,"Question\nThe python file in question collects data from API and the server is down once in a while so it stops running. to solve this, I've written this batch file that would keep running the file even if it encounters an error, so in few minutes it would be running as usual.
:1
python3 datacollection.py
goto 1


Now, how do I achieve the same with bash file?
Note: I want to run the python file on AWS EC2 Ubuntu.\nAnswer: Use some kind of process manager e.g pm2 for this. It will be more stable than that.",0.13605443,0.06612408,0.00489025330170989
24,"Question\nI created a web application using django that utilizes a FedEx API so that a user can create shipments through my website. The response of this FedEx API call is in xml format that contains a lot of data such as; was the request a success or failure, error messages, error codes, tracking number (if successful) etc. I am using django background tasks to make these API calls and once a process finishes I am unable to retrieve the response from the API call.
I was thinking to maybe store the xml response in a file and feed it to the user but before I do that I wanted to find out if there is a better way.
I also would like to know the best practice for API responses in general, like does the user need to see the entire response and if so how can I display it to them in the best way possible.\nAnswer: I would recommend using Django/Celery/RabbitMQ for your background tasks pulling from the FedEx API. I would break each section into the smallest isolated separated steps  possible to make debugging the data flow easier (debugging different data cases can be really hard)

Create DB representation of API request that needs to be made to FedEx
Store the response of the API request that was made as S3 file or in DB blob (update previous API call as processed)
Parse the Response of the API request and store the normalized response data components in your DB (update previous response as processed)
Present normalized request response data to user if successful and necessary

Breaking it apart like this allows you to see exactly what went wrong at each step and why it went wrong; IE what if the initial request was invalid, what if the FedEx API returned an invalid resp, what if your parser didn't know how to parse a valid FedEx API response, etc.
Doing it this way allows for easier debugging and scalability long run.",0.0,0.22477227,0.0505225770175457
25,"Question\nI can install, uninstall and upgrade prettytable using pip [un]install [-U] prettytable.  But running the mbed-tools compile command keeps on reporting ""ModuleNotFoundError: No module named 'prettytable'""
I can see the package is added to my...\appdata\local\programs\python\python39\Lib\site-packages\ folder.  According to my environmental path the folder...\appdata\local\programs\python\python39\Scripts\ is added and many packages has en.exe file installed there, but prettytabe is missing.  Could this be the problem?  If so, how do I install it and ensure that it actually has en exe install too?
I'm running python 3.9.7 on pretty-much-the-latest Windows 10.\nAnswer: So it turned out I had Python 3.8.7 installed together with Python 3.9.7.  So uninstalling the older one solved the problem...",0.0,0.0032320023,1.044583859766135e-05
26,"Question\nThe first time I have installed Python on my machine, it was Spyder with Anaconda.
But, with this version, I wasn't able to install pyodbc.
So, I have install Visual Studio Code and everything works fine
But today, I have tried to update some libraries (like certify or scipy) but each time I use pip install, I update my Anaconda folder and not my WindowsApps folder.
So, when I use PIP, how to update the Windows folder and not Anaconda. And also how to remove Anaconda from my computer. In my Windows Settings, I have no app related to Anaconda (weird)\nAnswer: Check your path, and which pip executable is being executed.
If you run it with the full path to your install in WindowsApps, then it should detect and update that version.",0.20408164,0.15300006,0.0026093281339854
27,"Question\nIn a real system, some sensor data will be dumped into specific directory as csv files. Then some data pipeline will populate these data to some database. Another pipeline will send these data to predict service.
I only have training and validation csv files as of now. I'm planning to simulate the flow to send data to predict service following way:
DAG1 - Every 2 min, select some files randomly from a specific path and update the timestamp of those files. Later, I may chose to add a random delay after the start node.
DAG2 - FileSensor pokes every 3 min. If it finds subset of files with modified timestamp, it should pass those to subsequent stages to eventually run the predict service.
It looks to me if I use FileSensor as-is, I can't achieve it. I'll have to derive from FileSensor class (say, MyDirSensor), check the timestamp of all the files - select the ones which are modified after the last successful poke and pass those forward.
Is my understanding correct? If yes, for last successful poke timestamp, can I store in some variable of MyDirSensor? Can I push/pull this data to/from xcom? What will be task-id in that case? Also, how to pass these list of files to the next task?
Is there any better approach (like different Sensor etc.) for this simulation? I'm running the whole flow on standalone machine currently. My airflow version is 1.10.15.\nAnswer: I am not sure if current Airflow approach is best for this use case actually. In the current incarnation Airflow is really all about working on ""data intervals"" - so basically each ""dag run"" is connected to some ""data interval"" and it should be processing data for that data interval. Classic Batch processing.
If I understand your case is more like a streaming (not entirely) but close. You get some (subset of) data which arrived since the last time and you process that data. This is not what (again current) version of Airflow - not even 2.1 is supposed to handle - because there is a complex manipulation of ""state"" which is not ""data interval"" related (and Airflow currently excels in the ""data interval"" case).
You can indeed do some custom operators to handle that. I think there is no ready-to-reuse pattern in Airflow for what you want to achieve, but Air",0.40816328,-0.85123813,1.5860919952392578
28,"Question\nI wrote some script with tkinter as a GUI and pyinstaller to convert it to an exe.

However gmail doesn’t allow me to share the file
if I share the file with google drive then windows security gets triggered

What’s a good safe and easy way for people who don’t know how to install python to use the code I write in python.
Thank you\nAnswer: You could use Discord. I believe you can upload an exe to Discord.",0.40816328,0.28296453,0.01567472703754902
29,"Question\nI have created and pushed a docker image to Docker Hub. Am pulling the image on the other side on the client machines. However there are config files inside the image that are client site specific (change from site to site) - for example the addresses of the RTSP cameras per site. How would I edit these files on each client site? Do I need to manually vim each image on each client site manually or is there a simpler way?
Or is the solution to extract these config files entirely from the image, copy them separately to client site and somehow change the code to reach these files outside the image?
thanks\nAnswer: You better keep your image in DockerHub as a baseimage w/o any dynamic config in it (or simply ignore it).
On the client side, you need to create your local image from the baseimage from the DockerHub with replacing via COPY or by mounting it as Volume.
OR as @Klaus D. commented",0.0,-0.018766284,0.00035217340337112546
30,"Question\nI am stuck on a problem about Olympics athletes.
I need to find the number of different sports each country participates in from my data frame, I am unsure how to do this as the column 'country' obviously has duplicates as well as the column'sport' and I cant figure out how to group them and return a value for each country
Any help would be awesome :)\nAnswer: import pandas as pd
df = pd.DataFrame([{'Country': 'India', 'Sport': 'Badmintan'}, {'Country': 'China', 'Sport': 'Badmintan'},{'Country': 'India', 'Sport': 'Football'}])
print(df)
print(df.groupby(by=('Country')).count())
Answer:
Country
China        1
India        2",0.0,0.40691647,0.16558101773262024
31,"Question\nInvoke-Expression : At line:1 char: 522
...ntel Wireless Common;C:\WINDOWS\System32\OpenSSH;""C:\Mingw\bin;c:\MinG
token 'C:\Mingw\bin' in expression or statement.
C:\Users\admin anaconda3 shell\condabin Conda.psm1:107 char:9
Invoke-Expression -Command SactivateCommand;

CategoryInfo
: ParserError: (:) [Invoke-Expression), ParseExcep
tion
FullyQualifiedErrorid : Unexpected Token, Microsoft PowerShell.Commands. In
vokeExpressionCommand\nAnswer: The variables of the path are wrong for the

""C:\Mingw\bin;c:\MinG token 'C:\Mingw\bin'

have you noticed that there are double quote and single quote there. these signs caused problem. you have to correct these variables in you path.",0.0,-0.019321322,0.0003733134944923222
32,"Question\nI have a lot of.dst files. I have to convert all files into.jpg.  I tried with pyembroidery module in python, but I don't know how to save the conversion file. Can anyone please help me.\nAnswer: Import the Image module from PIL the os module.
Import the image to be converted using the Image.open() method.
Display the size of the image before the conversion using the os.path.getsize() method.
Convert the image using the Image.convert() method. Pass ""RGB"" as the parameter.
Export the image using the Image.save() method.
Display the size of the image after the conversion using the os.path.getsize() method.",0.0,0.39742976,0.15795041620731354
33,"Question\nCan I find a value from Z column if I have two known values from X and Y?




X;
Y;
Z




A;
1;
syfgusj


A;
2;
adaddsfd


B;
1;
adsghfjgftrds


B;
2m;
adergtw


C;
1;
adergtw


C;
2;
addfgftre




Values in Y column are string type.
I should find value in Z column by known values from columns X and Y.
I converted CSV to list of lists, but can`t even imagine how to make next step.\nAnswer: Considering values are unique, I would store the csv on a pandas dataframe, and then iterate over every row on the dataframe. If column 1 value and column 2 values are the ones you know, then return the z value from that exact row.
That should do it, tell me if u need further help!",0.0,-0.072604895,0.00527147063985467
34,"Question\nI have a large CSV file(300mb) with data about accidents based on pincodes/zipcodes. The file has basically header and comma separated values. Key fields are Month, Date, Year, Pincode, Count.
Count represents the accident count for that pincode, however each pincode can get several entries through the day say every few hours. So I want to be able to calculate the max accidents per pincode on a given date i.e I need to group by Month, Date, Year, Pincode and then sum over count after grouping?
I have an idea of how to do this if I loaded the large-ish file into a database or a cloud service such as GCP BigQuery but I want to be able to do this with Python/Pandas dataframes and then store the metrics I am calculating in a table. Is this approach possible with Pandas, if not then possibly PySpark is my last option but that involves the overhead of having to setup a Hadoop etc.
I am open to any other ideas as I am a PyNovice :)
Thank you\nAnswer: You can signup for Databricks Community Edition (for free), in which you can easily have a Spark-ready environment, also easy enough to upload your CSV file.",0.0,0.14242733,0.02028554305434227
35,"Question\nI used pyinstaller to convert py into exe, for opening.jpg files.
I would like to get the jpg path (not the executable) where my application is running from, so that it can open the jpg directly by double-clicking the jpg.
Could you advise how to achieve this in python?
Thanks!\nAnswer: Tried sys.argv[1] and it works, while os.getcwd() returned ""C:\WINDOWS\system32"". Not sure why the comment was removed but thanks!",0.40816328,0.35398966,0.002934780903160572
36,"Question\nI would like to reuse the same Spark application across multiple runs of a python script, that uses it's Spark session object. That is, I would like to have a Spark application running in the background, and access it's Spark session object from within my script. Does anybody know how to do that?\nAnswer: To the best of my knowledge, it is not possible. It is the security model of Spark to isolate each session to a distinct app.
What I have done in the past:

build a small REST server on top of Spark that listens to specific command. At boot time, the server creates the session and load the data, so that forthcoming transformations are fast.

cache data in Delta lake, you still have the boot time and data ingestion, but it’s much faster that accessing data from several sources and preparing the data.


If you describe a bit more your use-case, I may be able to help a little more.",0.0,0.2529003,0.0639585629105568
37,"Question\nI have installed Anaconda in Ubuntu and then installed the ViennaRNA in anaconda using the following command:
conda install -c bioconda viennarna
But when I run the code:
import RNA
It shows the following error:
ModuleNotFoundError: No module named 'RNA'
Does anyone know how to solve it?\nAnswer: I used python3.6 -c import RNA instead of python -c import RNA and it worked.",0.0,0.17326176,0.030019637197256088
38,"Question\nRecently, I've been interested in Data analysis.
So I researched about how to do machine-learning project and do it by myself.
I learned that scaling is important in handling features.
So I scaled every features while using Tree model like Decision Tree or LightGBM.
Then, the result when I scaled had worse result.
I searched on the Internet, but all I earned is that Tree and Ensemble algorithm are not sensitive to variance of the data. 
I also bought a book ""Hands-on Machine-learning"" by O'Relly But I couldn't get enough explanation.
Can I get more detailed explanation for this?\nAnswer: Though I don't know the exact notations and equations, the answer has to do with the Big O Notation for the algorithms.
Big O notation is a way of expressing the theoretical worse time for an algorithm to complete over extremely large data sets. For example, a simple loop that goes over every item in a one dimensional array of size n has a O(n) run time - which is to say that it will always run at the proportional time per size of the array no matter what.
Say you have a 2 dimensional array of X,Y coords and you are going to loop across every potential combination of x/y locations, where x is size n and y is size m, your Big O would be O(mn)
and so on. Big O is used to compare the relative speed of different algorithms in abstraction, so that you can try to determine which one is better to use.
If you grab O(n) over the different potential sizes of n, you end up with a straight 45 degree line on your graph.
As you get into more complex algorithms you can end up with O(n^2) or O(log n) or even more complex. -- generally though most algorithms fall into either O(n), O(n^(some exponent)), O(log n) or O(sqrt(n)) - there are obviously others but generally most fall into this with some form of co-efficient in front or after that modifies where they are on the graph. If you graph each one of those curves you'll see which ones are better for extremely large data sets very quickly
It would entirely depend on how well your algorithm is coded, but it might look something like this: (don't trust me on this math, i tried to start doing it and then just",0.0,0.28947094,0.08379342406988144
39,"Question\nRecently, I've been interested in Data analysis.
So I researched about how to do machine-learning project and do it by myself.
I learned that scaling is important in handling features.
So I scaled every features while using Tree model like Decision Tree or LightGBM.
Then, the result when I scaled had worse result.
I searched on the Internet, but all I earned is that Tree and Ensemble algorithm are not sensitive to variance of the data. 
I also bought a book ""Hands-on Machine-learning"" by O'Relly But I couldn't get enough explanation.
Can I get more detailed explanation for this?\nAnswer: Do not confuse trees and ensembles (which may be consist from models, that need to be scaled).
Trees do not need to scale features, because at each node, the entire set of observations is divided by the value of one of the features: relatively speaking, to the left everything is less than a certain value, and to the right - more. What difference then, what scale is chosen?",0.0,0.012354493,0.0001526334963273257
40,"Question\nI am at a total loss as to why this is impossible to find but I really just want to be able to groupby and then export to excel. Don't need counts, or sums, or anything else and can only find examples including these functions. Tried removing those functions and the whole code just breaks.
Anyways:
Have a set of monthly metrics - metric name, volumes, date, productivity, and fte need. Simple calcs got the data looking nice, good to go. Currently it is grouped in 1 month sections so all metrics from Jan are one after the other etc. Just want to change the grouping so first section is individual metrics from Jan to Dec and so on for each one.
Initial data I want to export to excel (returns not a dataframe error)
dfcon = pd.concat([PmDf,ReDf])
dfcon['Need'] = dfcon['Volumes'] / (dfcon['Productivity']*21*8*.80)
dfcon[['Date','Current Team','Metric','Productivity','Volumes','Need']]
dfg = dfcon.groupby(['Metric','Date'])
dfg.to_excel(r'S:\FilePATH\GroupBy.xlsx', sheet_name='pandas_group', index = 0)
The error I get here is: 'DataFrameGroupBy' object has no attribute 'to_excel' (I have tried a variety of conversions to dataframes and closest I can get is a correct grouping displaying counts only for each one, which I do not need in the slightest)
I have also tried:
dfcon.sort('Metric').to_excel(r'S:\FILEPATH\Grouped_Output.xlsx', sheet_name='FTE Need', index = 0)
this returns the error: AttributeError: 'DataFrame' object has no attribute'sort'
Any help you can give to get this to be able to be exported grouped in excel would be great. I am at my wits end here after over an hour of googling. I am also self taught so feel like I may be missing something very, very basic/simple so here I am!
Thank you for any help you can provide!
Ps: I know I can just sort after in excel but would rather learn how to make this work",0.81632656,-0.09853172,0.8369656801223755
41,"Question\nI have a numpy array with size (1000,6) and I fill part of it each time during my program. I need to find the first location of zero in this array. for this, I used np.where( array==0). but the output is a tuple of size 2 and each item of it is a numpy array and I do not how can I find the first index of occurring zero in this array. what should I do about this?\nAnswer: The first element of the tuple that you got should be the index you are looking.",0.20408164,0.12900579,0.005636383313685656
42,"Question\ni make a class in a py file in my django project. In this i need the logged in person's username. in views.py file, every function i pass request perameter, so that i could easily get the logged username. but in other py file i make a class, and there how can i get the logged user??? Thanks.
in class, i can not pass the request perametar. so i can not call requset.user. how can i get the logged user???\nAnswer: You may use sessions to get the logged in user as follows: request.session.get('_auth_user_id')
In your class you may pass the request as an argument.",0.0,0.32453567,0.10532339662313461
43,"Question\nI was wondering if there is some extension/framework/module to VScode/python that allows you to run code in blocks like on Google Colab, so you don't have to run time-consuming lines every time you change something not related in code. I'm pretty sure I saw something like that somewhere, but I don't know how it is named.\nAnswer: The extension is called jupyter by Microsoft. Install that and then Create the file with the extension.ipynb and you are good to go.",0.40816328,0.074999034,0.11099841445684433
44,"Question\nhow to get back 10 from df.iloc[10] where df = pd.DataFrame({'a':np.arange(1,12)})?
I tried df.index but it returns a weird np.array which doesn't contain anything close to 10.\nAnswer: The most simple solution if the index matches the row numbers is df.iloc[10].name which returns 10",0.0,-0.08120453,0.0065941764041781425
45,"Question\nFrom what I understand
python -m zipapp myapp
creates a.pyz file of the directory named myapp. How do i create a.pyz of the current folder in which my working directory is set.
I tried python -m zipapp. which doesnt work.
What would be the exact argument for packaging of the working directory?
Same doubt for the following:
If I do pip install -r requirements.txt --target <directory>, how do i mention current working directory in <directory>\nAnswer: Thinking  about the same later, I have reached the conclusion that being able to make the zip file of the directory the terminal is set to doesn't make much sense due to which I don't think there's an solution to this question. Hence I believe zipapp can only take place for a given folder in the directory and not the directory the terminal is set in.",0.0,0.2267313,0.05140708386898041
46,"Question\nI have a large number of fits files that I'm unable to open becuase of a missing SIMPLE keyword. When I try to open them using astropy's fits.open() it gives the following error:
OSError: No SIMPLE card found, this file does not appear to be a valid FITS file
I tried adding the ignore_missing_simple=True option to fits.open(), but this still gives an error:
OSError: Empty or corrupt FITS file
I'm not sure what to do here. My first thought was to edit the fits header to add a SIMPLE keyword, but if I can't open the file I don't know how to deal with this. I'm wondering if theres more going on than just the missing SIMPLE keyword given the second error.
If it matters, I've downloaded the fits files, I did not generate them myself.\nAnswer: We traced the problem to astropy.io.fits version 4.3
Files generated with version 4.2 can be read with 4.0 or 4.2 but this error pops up with 4.3.1. You could try again after downgrading astropy.io.fits.",0.20408164,0.23702413,0.0010852075647562742
47,"Question\nI have 2 variables in a pandas dataframe which are being used in a calculation (Var1 / Var2) and the values consist of both floating point values and missing values (which I chose to coerce to 0). In my end calculation I am receiving 'inf' values and NA values. The NA values are expected but how do I derive a useable number instead of the 'inf' values?
some 'inf' values are appearing when VAR1 = float and Var2 = 0, others appear when both VAR1 and VAR2 are floats.
My initial approach was to round the floats to 2 significant figures before the calculation but I still received the inf values.\nAnswer: You may be getting inf because you are dividing by zero. For example, if var1 = 5 and var2 = 0, then you are computing 5 / 0.
In pure Python this returns a ZeroDivisionError, but in lots of data libraries they avoid throwing this error because it would crash your code. Instead, they output inf, or ""infinity"".
When var1 and var2 are both floats, it may be that var2 is extremely small. This would result in var1 / var2 being extremely large. At a certain point, Python doesn't let numbers get any larger and simply represents them as inf.
Rounding wouldn't help, because if var2 = 0, then it would round to 0, and if var2 is very small, it would also round to 0. As discussed earlier, dividing by zero causes the inf.",0.40816328,0.44061017,0.001052800682373345
48,"Question\nI have multiple pipelines with scripts defined on my azure devops.
I would like to be able to run them from my local machine using python.
Is this possible?
If yes, how to achieve it?
Regards,
Maciej\nAnswer: You can't run them in this way that you will take YAML code and put it python or actually any language and run it locally. You need to build agent to run your pipeline. So you can create a pool, install agent on you local machine, change your pipeline to use this pool.",0.0,0.1894334,0.03588500991463661
49,"Question\nDoes anyone know how can I forward an album (which contains multiple medias in it) with telegram api since forwardMessage only gets a message_id, I know this is possible because my friend uses some package in python and he can do that but I couldn't find anything on official telegram docs\nAnswer: There is currently not method in the bot API which let's you do this easily. You'll have to catch all the messages in the media group and resend the media as album via sendMediaGroup.",0.0,0.1876806,0.035224009305238724
50,"Question\nWhen I want a user in Python3, from terminal
sudo su
python3
>>> import getpass
>>>getpass.getuser()
'root'
how can I get the normal username?\nAnswer: If you are running the script interactively, you should be using sudo to run it and os.getlogin() will give the correct user.",0.20408164,0.25016093,0.002123301150277257
51,"Question\nI have to install GRPC python from source as the target machine does not have internet connection. The target machine has python 3.7 and pip3 installed. Can anyone share the process how to do it. Thanks in advance\nAnswer: Here's how I solved the problem. The main issue with using the downloaded gRPC package is that the cython compiler is platform dependent. The cython compiler is found in the grpc/_cython and it looks something like this ""cygrpc.cp37-win_amd64.pyd"". Here cp37 is the python version, win is the os or the platform name and amd64 is the architecture.
What I did in order to solve this issue, is I had to download the corresponding cython file for 32bit Linux platform - cygrpc.cpython-37m-i386-linux-gnu.so. I then created 2 separate grpc packages - one for Linux and one for Windows. This can be extended to as many platforms and architectures you want.
After this using pf = platform.system() to determine the os and architecture and called the respective grpc package.
This comprehensively solved the issue for me.",0.0,0.07592106,0.005764007102698088
52,"Question\nWorkflow:

Website:

doesn't have an API
requires login
clicking on a button to download a file

Is a Javascript button




save file to a download location

Question:

Is there a way to do this through python?\nAnswer: I suggest you to use Selenium to mimic the browser environment.

Try to login by giving your credentials using selenium.
find the button tag and use the html-tag-id to click on it.
Try to find the download location of the file / try to download it directly using the button",0.20408164,0.43463784,0.05315616354346275
53,"Question\nI have a python project written with PySide2 and now I want to migrate to PySide6. I used Qt Linguist to translate UI and created.ts files with help of this command:
pylupdate5 utility from PyQt5 package (but it worked fine for my project with PySide2). Now I plan to get rid of PySide2 and PyQt5 packages.
So, I need to replace pylupdate5 with something from PySide6 package. I assumed lupdate should to the job but it seems it works only with C++ code. It gives me errors like Unterminated C++ character or Unbalanced opening parenthesis in C++ code. An with lupdate -help I don't see how I may switch it to python mode (similar to uic -g python).
Does anyone know how to create.ts files for Qt Linguist from python source files?\nAnswer: lupdate of Qt 6.1 does not support Python but in Qt 6.2 that problem no longer exists so you have 2 options:

Install Qt 6.2 and use its lupdate.
Wait for PySide6 6.2.0 to be released (at this moment it is only available for windows) and use lupdate.",1.0,0.29584342,0.4958364963531494
54,"Question\nHi dear ladies and guys,
so i've been struggling today to find out how to make flower use the redis backend to get the historical tasks. I've read that Flower has the --persistent flag but this creates its own file.
Why does it need this file? Why doesn't it just pull the records from redis?
I don't get it. ( I have RabbitMQ as broker and Redis as backend configured in the Celery() constructor)\nAnswer: The short answer is that flower won't know which task results to look for in the backend.  Since redis databases can be shared with other processes, flower can't guarantee that a key that looks a certain way will contain a celery result that it ""should"" be monitoring.  The persistent flag lets flower keep track of the task results it ""should"" be monitoring by saving a copy of any tasks that it sees going through the broker queue and, thus, keep track of relevant results.",0.0,0.14948648,0.02234620787203312
55,"Question\nI have recently downloaded some models for blender but when I unzipped it the file was in.py format I want to add them to my project but I cant I tried ""open in"" method but no results..... can anyone tell me how to fix it\nAnswer: You can simply change its format from.py to.blend
you can do this with help of notepad.
open.py file in notepad and save as.blend format.",0.0,0.05428487,0.00294684711843729
56,"Question\nI have recently downloaded some models for blender but when I unzipped it the file was in.py format I want to add them to my project but I cant I tried ""open in"" method but no results..... can anyone tell me how to fix it\nAnswer: Please specify what the content of the.py file is.
A.py file is a python script, and you can't just ""convert"" it in a blender file. They are two different things.
It might be the case that the.py file is just a text file (saved as a.py file) that contains the coordinates of a mesh. In this case, just change.py in.obj and import it in blender.",0.0,0.16389567,0.02686178870499134
57,"Question\nI'm trying to fix leaks in a ctypes-based Python binding over a C library. I'm confused as to the behavior of memory management of strings and other pointers when working with ctypes.
There are, to my knowledge, 3 cases I need to solve, basically:
A) Creating memory in C and returning a c_char_p as the result of the function to the Python.
B) Creating a string in the Python, and passing it as a char const* parameter of a C function, and not freeing it in the C: this is the only one I've solved confidently: I can just do my_str.encode(""utf-8"") into a c_char_p, and the Python Alloc/GC handles both the allocation and deallocation.
C) Creating memory in Python, and storing it in the C (for example as a char* field within a struct), to be deallocated later (with a function from the C library).
My questions:
A)

is the c_char_p thus created a second pointer? If so, what happens to the pointer allocated by the C?
how do we free the pointer returned from the C?
is there a difference of behavior between how c_void_p and c_char_p are handled as restype by the GC?

C)

What is the correct way to ""give ownership"" of a memory pointer allocated in Python to the C?.encode() clearly makes a Python object that's GC'ed.
is a string created by create_string_buffer also Python-GC'ed?\nAnswer: This is an adaptation of the exchange I had on the Python discord server with someone who answered the questions:

is the c_char_p thus created a second pointer? If so, what happens to the pointer allocated by the C?

Yes, it is. The c_char_p is a PyObject* pointer that lives on the heap, and it stores the original C char* pointer inside it.

how do we free the pointer returned from the C?

Pass the c_char_p to free() using ctypes

is there a difference of behavior between how c_void_p and c_char_p are handled as restype by the GC?

No, in both cases, when the c_void_p or c_char_p",0.0,0.62453175,0.3900398910045624
58,"Question\nI have a set of commands, each from Juniper & Aruba switches, that I would like to automatically convert. Is there a way to use a string of Juniper commands and have them output a string of Aruba commands? If so, how would I approach this using Python? Would I use ""if else"", python dictionary commands, or some other syntax?
For example:
I have just come up with this script:
def Juniper(sets): print ('host-name', set1) print ('console idle-timeout 600\n' 'console idle-timeout serial-usb 600\n' 'aruba-central disable\n 'igmp lookup-mode ip\n' 'console idle-timeout serial-usb 600\n') print (""logging (system1) oobm"") #I AM TRYING TO ADD THE INPUT OF system1 IN BETWEEN LIKE THIS ^ AS SHOWN ABOVE
set1= input('Enter hostname with quotations:\n') system1 = input('Enter system log IP address:') Juniper(set1)
Please let me know how to add an input in between two strings or words\nAnswer: I would approach simply using show command with display json on juniper side and than use a jinja2 template to build your Aruba config using which dynamic data you want from juniper output and yes you can do all in python from extracting data from device till the convertion. Extracting data from device you can use scrapli nornir and than use the result as data for render with jinja template.",0.0,0.06430149,0.004134681541472673
59,"Question\nI want to plot graphs that share variables and datasets from other CoLab files, I would like to know how I could access those variables.\nAnswer: You could create a new folder 'VARIABLES' where the variables are saved, read, and re-written (i.e. updated) as txt or csv files. Otherwise, defining a variable in one Colab Notebook will only be accessible within that Colab Notebook and not between Colab Notebooks.",0.40816328,0.20535886,0.04112963005900383
60,"Question\nNewbie in Power BI/Power query and python.  I hope to ask this question succinctly.
I have a ""primary"" query in PBI but need to change the values of one column (categories) based on the values in the (description) column.  I feel there is a better solution than a new conditional if/else column, or ReplaceReplacer.text in M Code.
An I idea I had was to create a list or query of all values in (description) that need to have their category changed, and somehow use python to iterate through the (description) list and when it finds a value in (description), it knows to drop the new value into category.
I've googled extensively but can't find that kind of ""loop"" that I can drop a python script into Power Query/Power BI.
What direction should I be heading in, or am I asking the right questions?  I'd appreciate any advice!
John\nAnswer: You are having a rather simple ETL task at hand that clearly doesn't justify incorporating another language like Python/Pandas.
Given the limited information you are sharing I would imagine to use a separate mapping table for your categories and then merge that one with your original table. And eventually you only keep the columns you are interested in.
E.g. this mapping or translation table has 2 columns: OLD and NEW. Then you merge this mapping table with your data table such that OLD equals your Description column (the GUI will help you with that) and then expand the newly generated column. Finally rename the columns you want to keep and remove all the rest. This is way more efficient than 100 replacements.",0.0,0.28017747,0.07849941402673721
61,"Question\nI'm using pip 21.2.4 and python 3.9.7
I'm pretty sure I have pip installed since if I run pip --version in the terminal it gives me pip 21.2.4 from C:\Users\rohaan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pip (python 3.9)
If I run pip install pygame in the terminal I get Requirement already satisfied: pygame in c:\users\rohaan\appdata\local\packages\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\localcache\local-packages\python39\site-packages (2.0.1) 
Despite the requirement being satisfied when I import pygame I get  ModuleNotFoundError: No module named 'pygame'
Does anybody know how to fix this?\nAnswer: This means you have multiple versions of python installed on your device, one has the module and the other tries to run the file, go to the control panel and check it out.",0.0,0.09574008,0.009166162461042404
62,"Question\nI have two Python environments in my Anaconda ""base"" and ""image_analytics"".
Every time I close and start Anaconda Prompt, I can see ""base"" environment activated by default. I have to write ""conda activate image_analytics"" every time. I want image_analytics environment to get activated every time by default I restart Anaconda Prompt. Any solution? I am a beginner. Any help is appreciated.\nAnswer: It actually quite simple, because the anaconda prompt is just a shortcut to cmd with an added call to activate the base env, which can be modified to start other envs

Locate the shortcut for anaconda prompt, either by going to Start Menu->Right Clik Anaconda Prompt->Open File Location or by navigating to the equivalent on your system of C:\Users\<Username>\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Anaconda3 (64-bit)

Open the properties of the shortcut: Right Click->Properties

Modify the Target field. Original for me that activates the base env: %windir%\System32\cmd.exe ""/K"" C:\Users\<Username>\miniconda3\Scripts\activate.bat C:\Users\<Username>\miniconda3, which I can modify to e.g. start the py37 env: %windir%\System32\cmd.exe ""/K"" C:\Users\<Username>\miniconda3\Scripts\activate.bat C:\Users\<Username>\miniconda3\envs\py37. Note that I simply modified the last path in that line to point to an environment instead of base.",0.81632656,0.3360169,0.2306973785161972
63,"Question\nI opened a series of files to edit them and my run button is greyed out. I can't figure out exactly why, but I think it has something to do with the ""add configuration"" prompt nest to the run button. I don't know how to add a configuration and don't want to mess up my editor. I have the newest version of the community version of PyCharm.\nAnswer: Add Configuration is greyed out because maybe you haven't added python path to the project try adding it and you should be clear before posting questions maybe some code and pictures will help. Try this and let me know if this have solved your problem.",0.0,0.05704248,0.003253844566643238
64,"Question\nI'm trying to find a library (any language, but preferably C# or Python) which will let me open an XLSX file, iterate through the chart objects, and find data about the chart - ideally including the data backing the chart.
The Pandas Python package, or ExcelDataReader NuGet package have useful functionality for opening the file and reading a grid of numbers, as well as ways to add charts, but I don't find any way to read the charts.
Curious to hear from anyone who has ideas/solutions.\nAnswer: Hey I have a good solution for C#. In C# you can use OLEDB, this allows you to connect a C# code to a excel or access database (so long the database is in the C# code files). You don't need to get any addons for this is you have C# on Visual Studio.",0.40816328,0.17497337,0.05437753349542618
65,"Question\nI use python confluent-kafka 1.5.0 with schema registry for consuming avro message from kafka.
I am only consumer, without access to admin producer or broker or something else.
I know topics name and from message fields i  get subject and namespace also in schema registry client, sr client get schema using schema_id from message.
According my goal i need method get schema(only id or subject name) BEFORE i start consuming. That is mean, i want to get schema by topic name manually
Please, read attentively, i know how get schema manually after i get schema_id from message.\nAnswer: The only way to get the schema ""before consuming"" is to not consume and instead interact with the registry directly",0.0,0.15794694,0.024947237223386765
66,"Question\nI want to update the selected python interpreter inside vs code from a script.
I tried:

.vscode/settings.json in the workspace allows setting python.defaultInterpreterPath but it works only once. Even if select ""default interpreter"" in the UI (e.g. python 3.8), close vs code, change the setting to 3.9, open vscode again, vs code remembers 3.8.
$config_path/User/workspaceStorage/.../state.vscdb also contains a key ms-python.python which got data for the selected interpreter autoSelectedWorkspacePythonInterpreter - but that does not even change when changing the interpreter in vscode (from the name of the setting that kind of makes sense)

Where does vs code even store the information which interpreter is selected by hand?
Any pointers welcome how to change the used python interpreter (preferable from a script running outsideof vs code, but I would also write a vs code extention if that is easier :))\nAnswer: $config_path/User/globalStorage/.../state.vscdb  --> ms-python.python.",0.40816328,0.71206075,0.09235367178916931
67,"Question\nI've seen mention of sigkill occurring for others but I think my use case is slightly different.
I'm using the managed airflow service through gcp cloud composer, running airflow 2. I have 3 worker nodes all set to the default instance creation settings.
The environment runs dags fairly smoothly for the most part (api calls, moving files from on prem) however it would seem as though its having a terribly hard time executing a couple of slightly larger jobs.
One of these jobs uses a samba connector to incrementally backfill missing data and store on gcs. The other is a salesforce api connector.
These jobs run locally with absolutely no issue so I'm wondering why I'm encountering these issues. There should be plenty memory to run these tasks as a cluster, although scaling up my cluster for just 2 jobs doesn't seem like it's particularly efficient.
I have tried both dag and task timeouts. I've tried increasing the connection timeout on the samba client.
So could someone please share some insight into how I can get airflow to execute these tasks without killing the session - even if it does take longer.
Happy to add more detail if required but I don't have the available data in front of me currently to share.\nAnswer: Frustratingly, increasing resources meant the jobs could run. I don't know why the resources weren't enough as they really should've been. But optimisation for fully managed solutions isn't overly straight forward other than adding cost.",0.0,0.13038993,0.01700153388082981
68,"Question\nI followed the polls tutorial for Django and am beginning my own hobby project today. When creating an app, the following really confused me -
Your apps can live anywhere on your Python path. In this tutorial, we’ll create our poll app in the same directory as your manage.py file so that it can be imported as its own top-level module, rather than a submodule of mysite.
So my question is - What is the difference between a top-level module vs a submodule for my project?
And how do I decide which one to choose?
Any help will be greatly appreciated as I am a noob coder working on my first hobby project lol.\nAnswer: There's no real other difference than matters of taste, organization, and naming.
For what it's worth, with my 10+ years of Django, I still generally like to just have the app packages parallel to the project package (i.e. what that passage suggests).
(It's occasionally also handy to make the project package an app, for some very core models such as custom users, etc.)
There are cases when it does make sense to have packages of interrelated apps, but those are generally for reusable apps such as allauth etc.",0.81632656,0.25513285,0.31493836641311646
69,"Question\nWhat columns do I have to consider while implementing K Means? I have 91 columns after pre processing. And also to how many columns do I have to apply K Means clustering? Is it all of them or only a few to be considered?\nAnswer: It's actually about trial and error. There is no straight way to say which columns are going to help you the most until you try and figure it by yourself.
but you can use dimensionality reduction algorithms like PCA to project data to a lower dimension without much data loss. It's a common approach and also helps with the speed of your clustering algorithm.",0.0,0.10875857,0.011828426271677017
70,"Question\nI just noticed an odd behavior in Dymola 2022, and I wonder if and how can I take care of it from the Python interface.
Odd behavior, because I expect Dymola to consistently set the working directory at.\Documents\Dymola at startup, unless told otherwise.
However, when I instantiate Dymola from its Python interface, at startup the working directory is set according to the setting Tools > Options > Settings > Save startup directory as follows:

Do not save - then when I instantiate Dymola, the current directory is set as the directory where the python environment is open. Possibly interesting here:

the directory where the python environment is open is not the directory where the function is located.

Example: in VSCode open folder >.\project, the function is in.\project\functions\dymIO\instantiateDymola.py, then the Dymola working directory at startup is set at.\project




Save this directory >.\Documents\Dymola - then the current directory is set reliably at.\Documents\Dymola


One solution/fix would be to set the working directory after Dymola is instantiated, I am aware of it.
My question is rather to get an understanding of what might be going on with this behavior, and if there is a possibility to take care of it right at startup rather than afterwards\nAnswer: I don't know exactly what is happening with Python at the moment, but I can answer what happens inside Dymola:

When you start a program from Start-Menu the current directory is as default set to the installation directory of the program (in this case Dymola).
Not important: When Dymola is started it first ensures that.\Documents\Dymola exists.
Not important: If the environment variable DYMOLAWORKDIRECTORY is not set it is set to.\Documents\Dymola
If the current directory is the same as the installation directory of Dymola then it sets current directory to.\Documents\Dymola - otherwise it keeps the current directory. This allows you to make a short-cut ""Start Dymola Here!""
And then the settings where you might have changed 'Startup Directory' is processed.

But I don't know why the Python-interface changes current directory to.\project, and if",0.81632656,-0.014608562,0.6904531717300415
71,"Question\nI opened command prompt (I'm on Windows) and I typed:
'''none
pip3 install discord
'''
Then it said it installed discord, and I was ready to go! (I already had Python 3.9.7 installed)
Then when I opened VSCode up and typed:   import discord   I got this error message:

""discord"" is not accessedPylance
Import ""discord"" could not be resolvedPylancereportMissingImports

What does this mean, and how can I fix it? I was really looking forward to coding the bot, but don't know how, now that this is messed up.\nAnswer: check in the bottom left corner of the VS Code window for which version of Python is it using. This issue usually occurs for me when I’m working in a virtual environment but VS Code is pointing to my global Python installation.",0.0,0.1583792,0.02508397027850151
72,"Question\nI opened command prompt (I'm on Windows) and I typed:
'''none
pip3 install discord
'''
Then it said it installed discord, and I was ready to go! (I already had Python 3.9.7 installed)
Then when I opened VSCode up and typed:   import discord   I got this error message:

""discord"" is not accessedPylance
Import ""discord"" could not be resolvedPylancereportMissingImports

What does this mean, and how can I fix it? I was really looking forward to coding the bot, but don't know how, now that this is messed up.\nAnswer: You can change the Python Interpreter in VSCODE to solve this issue.


Open settings in VSCODE by pressing Ctrl + Shift + P. Make sure your.py file is open while doing this step.


Search there Python: Select Interpreter and choose the right version.

Reload VSCODE and see if it works!",0.0,0.30230147,0.09138617664575577
73,"Question\nI opened command prompt (I'm on Windows) and I typed:
'''none
pip3 install discord
'''
Then it said it installed discord, and I was ready to go! (I already had Python 3.9.7 installed)
Then when I opened VSCode up and typed:   import discord   I got this error message:

""discord"" is not accessedPylance
Import ""discord"" could not be resolvedPylancereportMissingImports

What does this mean, and how can I fix it? I was really looking forward to coding the bot, but don't know how, now that this is messed up.\nAnswer: Open an integrated Terminal in VS Code,

run python --version, it should be python3.9.7 which is selected as python interpreter and shown in status bar.

run pip show discord to check if its location is \..\python3.9.3\lib\site-packages. If not, reinstall it.",0.0,0.37001097,0.13690811395645142
74,"Question\nThe Tkinter app works good after deployment with Pyinstaller,
but it opens 2 windows:.exe and app (tkinter container).
Any ideas? how to fix it?\nAnswer: I'm not sure I understand completely but you may want to try saving the file as.pyw vs.py before making it an executable. you may be seeing the console running.",0.0,0.02909875,0.0008467371808364987
75,"Question\nIs it possible to have someone not have python installed, double click on my installer and through that get an executable, the python, and the needed libraries?
For that I would probably also want to know, if I can type anything but python path/file.py to run the python script. Because for that you would need to add the python installation to path. And is there a way to bypass this? To type something else instead of python like manually type the location where the executable is or so?
My boss asked me this today. Don’t think that’s a common task to do with python but if I know how to do that with python, I for one can do it with python and I will also easier understand how one normally writes installers and programs with guis and such. I do like python though.
I guess this is not an easy question. I really am interested though. If you can only answer the second question, I would also be very grateful because then i can figure out the rest I think.\nAnswer: You can use pyinstaller to create an executable. pip install pyinstaller in cmd.
Then cd into the directory where your file is, and type pyinstaller --onefile filename.py",0.40816328,0.29916695,0.011880200356245041
76,"Question\nI built a docker container with anaconda and other packages. in the container, I used echo ""export PATH=""/root/anaconda3/bin:$PATH"""" and ~/.bashrc and /bin/bash -c ""source ~/.bashrc"""", conda command worked fine, and the python version was correct.
however, when committed and pushed the container to the docker hub, and then pulled it elsewhere, it gave me ""bash: conda: command not found"" when i tried to use conda command.
Would anyone tell me how to solve this problem? Any advice or suggestion will be appreciated.\nAnswer: Find the result of this problem myself.
I build and commit this docker in one machine with root permission，and the Anaconda3 was installed in /root/anaconda3/bin. I then pulled and run this docker in another machine without root permission, in which the installation path can't be accessed. I thus got the ""conda command not found"" error.
To solve this problem, it seems that I have to ture to someone with root permission for help. Thank @David Maze anyway.",0.0,0.2081204,0.04331410303711891
77,"Question\nI'm developing a service in Python which has a package as a dependency. I've developed that dependency myself.
I have a private PyPI server that contains artifacts(a wheel file) of every commit in the master branch(for both packages).
I'd like to create a CI which will use pip to install the dependency from the private PyPI server(because it's faster and easier than installing from the source code itself every time).
Example for the project's dependencies:

packages A requires package B
(which are all in the private repository and not public packages).

My problem is that if I'm trying to execute a CI on the A repository right after the B repository had a merge to master,
the private PyPI server won't have the latest version of B yet(Today there's a Jenkins job that is activated after every merge to master and uploads the B wheel to the private PyPI server,
but it takes a few minutes until the package is being built). The result is that package A might contain an old version of B instead of using the latest version of B.
Any suggestion on how to improve this process?
Installing from a private PyPI, in comparison to installing directly from Gitlab decreases the time it takes to install dramatically.
BTW, today I am using Gitlab's enterprise edition and Jenkins.
Thanks\nAnswer: You just need to check if package B is up-to-date before you start the installation of A. So (in your Jenkins job) just put a build step to download everything for B, and after that everything for A.",0.0,0.21474099,0.046113692224025726
78,"Question\nI am building a Streamlit dashboard that will have to read data from a DataFrame. The problem is that I have a local csv and a local Excel file form which I am reading data using pd.read_csv().
However, when I share my dashboard url with others, the data will fail to load because they won't have the file locally.
How can I read the contents of a csv and Excel file and turn them into a ""hardcoded"" pandas DataFrame?
I guess my question is: how should I store and read my data without having local csv and Excel files?
Edit: sorry for no code or MRE, but I literallyu have no idea how to do this. If I had a piece of code, it would simply be a pandas dataframe with sample data in it.\nAnswer: In R I would use dput() function to show me the code necessary to create a data frame.
For Python I know that print(df.to_dict()) would do something similar to be a ""hardcoded"" Pandas DF.
So I would do the following:
1: print your df. df.to_dict()
2: copy and paste the necessary code to create the data frame inside your streamlit app. Something similar to this: {'a': {0: 1, 1: 2}, 'b': {0: 3, 1: 3}}
3: ""load"" the data frames by creating them everytime the application is run. df = pd.DataFrame.from_dict({'a': {0: 1, 1: 2}, 'b': {0: 3, 1: 3}})
PS: note that this solution is not scalable neither would work if your data keeps changing from time to time. If that's the case, you would need to keep printing and pasting your new df to your code every time.",0.40816328,0.16898024,0.057208526879549026
79,"Question\nI just need some advice about what database should I use, and how should I store my data.
Namely I need to store big chunk of data per user, I was thinking about storing everything in JSON data, but I thought that I could ask you first.
So I am using Django, and for now MySql, I need to store like 1000-2000 table rows per user, with columns like First Name, Last Name, Contact info, and also relate it somehow to the user that created that list. Also I need this to be able to efficiently get data from database.
Is there any way of storing this big data per user?
Thank you!\nAnswer: I know pandas is a library that works very well for storing data. So maybe look into that and see what file formats are well documented with it.",0.0,0.14143807,0.020004726946353912
80,"Question\nI am working on a regression problem and my dataset is very imbalanced. My features are age, sex, weight, medication dose, some lab results and I am trying to predict one column of continuous values.
In my dataset some individuals are represented by more samples than others. For example 30 lines of data from one individual, 10 from a second individual and 1 from a third and so on. I do not know how to select the training set so that the model is not biased towards specific subjects.
I divided the training and testing set in a way that there is no data from the same individuals in both sets but still training a model with a training set that is not balanced regarding the amount of data from each individual would bias the model.\nAnswer: I would suggest to duplicate samples. So that, for example, every individual will have 30 rows of data.
As an alternative, you can also adjust the weights. So that an individual with 30 samples will have weight 1, an individual with 10 samples will have weight 3, and an individual with 30 samples will have weight 30 [it's an equivalent to duplicating, but doesn't increases the training set]",0.81632656,0.32732916,0.23911845684051514
81,"Question\nI have code using selenium that successfully exports an xls file from my desired website. Once the xls file is downloaded I am unsure how to use selenium to open the file. Once this data gets opened and downloaded I would like it to automatically be flowing into Power BI.
Any thoughts on how to achieve this?\nAnswer: You don't need Selenium. It's plain vanilla to directly access a web based Excel file with Power BI. This is what Power BI is made for.
However, if you already have an Excel file locally, I'd suggest you upload it to Onedrive and import it into Power BI from there. And once your report is in the Online service you can directly refresh it via schedule or trigger.
Importing the Excel file from disk is not the recommended way of doing things unless it's a one-off.",0.0,0.13812786,0.019079307094216347
82,"Question\nI have a python project and I need to use its venv in the terminal offered by PyCharm. However, some dependencies show as not installed but they are present in the venv folder. I think the project is using another venv in the Terminal. How can I check which venv is being used in the terminal and how can I change it to be the one in the project's folder?\nAnswer: If you execute any program using 'run' button in PyCharm, it will give you the python it is using, in your case, it should be venv. Now from Terminal use that command instead on just python/python3.
For example, your are running a program using run button, you would see something like
/Users/soumyabratakole/.local/share/virtualenvs/pythonProject-8Ymrt2w6/bin/python /Users/soumyabratakole/PycharmProjects/pythonProject/main.py
Now you can use /Users/soumyabratakole/.local/share/virtualenvs/pythonProject-8Ymrt2w6/bin/python instead of just python from your terminal to use your venv.",0.0,0.3252175,0.1057664155960083
83,"Question\nIs there a kepler library i can install in my django project to use kepler.gl in my frontend, like in leaflet-django. If not, how do i use Kepler.gl maps as frontend for django backend?\nAnswer: You can visit ""kepler.gl/demo"" and click in the left sidebar export to html. After that you can use this html for your endpoint (view template) in django.
The data can be passed trough the httpResponse in your views.py from the database to view into the script. If you like you can put the script code in seperate Javascript files and link them in your endpoint.",0.0,0.43107402,0.18582481145858765
84,"Question\nMy question isn't about a specific code issue, but rather about the best direction to take on a Natural Language Processing challenge.
I have a collection of several hundreds of Word and PDF files (from which I can then export the raw text) on the one hand, and a list of terms on the other. Terms can consist in one or more words. What I need to do is identify in which file(s) each term is used, applying stemming and lemmatization.
How would I best approach this? I know how to extract text, apply tokenization, lemmatization, etc., but I'm not sure how I could search for occurrences of terms in lemmatized form inside a corpus of documents.
Any hint would be most welcome.
Thanks!\nAnswer: I would suggest you create an inverted index of the documents, where you record the locations of each word in a list, with the word form as the index.
Then you create a secondary index, where you have as key the lemmatised forms, and as values a list of forms that belong to the lemma.
When you do a lookup of a lemmatised word, eg go, you go to the secondary index to retrieve the inflected forms, go, goes, going, went; next you go to the inverted index and get all the locations for each of the inflected forms.
For ways of implementing this in an efficient manner, look at Witten/Moffat/Bell, Managing Gigabytes, which is a fantastic book on this topic.
UPDATE: for multi-word units, you can work those out in the index. Looking for ""software developer"", look up ""software"" and ""developer"", and then merge the locations: everytime their location differs by 1, they are adjacent and you have found it. Discard all the ones where they are further apart.",0.40816328,0.2536868,0.023862987756729126
85,"Question\nIm trying to convert a pil image to a numpy array but i want the image to keep it's transperancy, how can i do that?
Iv'e tried using numpy.array()
But it doesnt keep the transperancy\nAnswer: I'm not quite sure what you mean by it doesn't keep its transparency. If you convert a PIL image with transparency using numpy.array() it should return a numpy array with shape width, height, 4 where the 4th channel will represent the alpha channel values. And after whatever processing you need to do, if you convert it back to a PIL image using Image.fromarray() and perhaps saving with Image.save() you should get back an image with the same transparency. Can't help you much more without seeing an actual snippet of the code and possibly the image.",0.0,0.17698354,0.03132317215204239
86,"Question\nI am using python request module to hit rest api. I have to use SSL for security measures.
I see that i can set
requests.get(url,verify=/path/ca/bundle/) 
However i am confused as to what needs to be passed as CA_BUNDLE?
I get the server certificate using
cert = ssl.get_server_certificate((server,port))
Can someone let me know, how i should use this certificate in my request? Should i convert the cert to X509/.pem/.der/.crt file?\nAnswer: Solved it. Apparently i needed to get the entire certificate chain and create a CA bundle out of it.",0.0,-0.013453066,0.00018098499276675284
87,"Question\nI am concepting an HID usb device which should be able to autoexecute a little Python/C++/(...) login program when connected to a computer, to allow the user to enter a password to access the content of the memory stick.
Any idea how to start with or what should I consider for this program?
Important:

OS will be Windows (probably 8+) and Linux (probably CentOS)
Computers will not have any program installed to interact with this usb device.
It is all about just and only the usb device.
Thanks in advance\nAnswer: User inserts USB device to PC
PC recognizes the USB device as a memory disk

The memory disk is empty (because password not entered) except
Only a file named enter-password.txt


User opens enter-password.txt and enters the password and saves
Real contents in the memory disk appear on the fly

It's compatible to all OS and no program is required in advance in PC.",0.0,0.32756883,0.10730133950710297
88,"Question\nI want to upload some files to sharepoint via office365 REST Python client.
On documentation on github, I found two examples:

one for larger files where this is executed:

 uploaded_file = target_folder.files.create_upload_session(local_path, size_chunk, print_upload_progress).execute_query()

one for small files :
target_file = target_folder.upload_file(name, file_content).execute_query().

In my case, I want to be able to upload files who are small and also files who are very large.
For testing, I wanted to see if the method for larger files works with smaller files too.
With a small file, while putting the size_chunk at 1Mb, the uploading was done, but the file uploaded was empty (0b), so I lost my content while uploading.
I wanted to know if there is someone who knows how can we do something more generic for whatever size of files. Also I don't understand what is the size chunk for larger files case. Do you know how one should choose it?
Thank you so much!\nAnswer: This problem is solved by installing office365-rest-python-client instead of office365-rest-client.",0.0,0.13479936,0.01817086711525917
89,"Question\nWhen one app is installed on Android, it will ask access for several kind pf permissions. I have thousands of apps install package and I try to find out what permissions each app requests. I want to finish this job on Windows. Do we have any tool or how to use some programming language like Python to do this?
Thanks a lot in advance!\nAnswer: Apk file is simple zip archive.
You can extract data from the archive and read AndroidManifest.xml entries.
Check lines like the <uses-permission android:name=""...""/>",0.40816328,0.44665492,0.0014816060429438949
90,"Question\nI am currently using Visual Studio Code as my IDE, and I have been trying to figure out how to make it in my scripts where if a certian key -such as ""l"" for our example- was pressed, the script would trigger a block of code that executed a function - such as the print(""Ayy you pressed a key! good job!"") function. I was wondering if anyone could tell me how to do that, or how to make the import keyboard work.\nAnswer: As answered in other questions, responding to a key press involves dealing with events.
Each key press triggers/ emits an event. There are couple of library that deals with events and in particular keyboard events.
modules keyboard and getKey are worth exploring. and to a straightforward answer, refer above answer.(just dont want to repeat, credit to him)",0.0,0.15196359,0.023092932999134064
91,"Question\nI am proficient in Bash and a beginner in Python (I have some experience with Flask and Requests).
I wrote a Bash script which asks for some input (four strings) and creates a configuration file based on that input. That's good for me, but I would like to convert it to a (no frills) web interface.  I know how to configure Apache, if necessary.
I know there are zillions of ways to do that.  I'd like some hints on how to tackle my problem, ideally using Bash or Python.  By the way, I've used Octave on CGI for some of this in the past, and I think it's excellent for math purposes, but I'd like to get ideas about some simpler, more generic avenues.\nAnswer: I would create a Django site for this. It can be setup really quickly. I would recommend you host it on PythonAnywhere. They have a free tier, and works really well. Django is similar to Flask, but I personally like Django. If you could be more specific on what your App needs to do, some sample code could be provided.",0.20408164,0.14176768,0.003883029567077756
92,"Question\nI want to use the ""newenv"" approach, but now I need to make changes to psynet (for example adding a log line or rpdb.set_trace()), where is this local copy? And can I simply change it or I need to reinstall the environment with reenv?\nAnswer: The simplest way to do this is certainly to edit the file in the venv, but as has been pointed out it's easy to lose track of that change and overwrite it. I do that anyway if it's a short term test.
However, my preferred approach when I want to modify a library like this is to clone its source code from Git, do the modifications in my cloned sandbox, and do a pip install -e. inside the module after having activated the venv. That will make this venv replace the previously installed version of the library with my sandbox version of it.",0.13605443,0.32778996,0.03676251694560051
93,"Question\ndf1 =





name
age
branch
subject
date of joining




1
Steve
27
Mechanical
Autocad
01-08-2021


2
Adam
32
Electrical
control sys
14-08-2021


3
Raj
24
Electrical
circuit
20-08-2021


4
Tim
25
Computers
clouding
21-08-2021




df2= [['name','branch']]
print(df2)





name
branch




1
Steve
Mechanical


2
Adam
Electrical


3
Raj
Electrical


4
Tim
Computers




Now I have two data frames,
I need only name and branch columns and remove the remaining columns, all these operations should apply to the original df1.  I don't want separately df2\nAnswer: Simply, Overwrite the df1 only
df1= df1[['name','branch']]
or
df2= df1[['name','branch']]
del df1
To delete df1 or df2.
del df1
or
del df2
Based on requirement",0.20408164,-0.106857,0.0966828465461731
94,"Question\nI am trying to take a piece of user input: 5+5 or 5*5 but my code receives it as a string. I want to convert it to an expression from which an answer can be deduced. However, I am not sure how to do this.\nAnswer: Simply use ""eval"", but unsafe.",-0.35714287,0.17687464,0.285174697637558
95,"Question\nI was wondering if there is any way to make decisions based on webhook responses in Mautic. To elaborate, I post a request via webhook and the corresponding API responds with an error (e.g. 411). I want to create a campaign that has a block depending on the response of that webhook if it receives 200 decision 1 is made and if it receives 411, another decision is made.
how can I implement this?\nAnswer: I don't think such feature exists by default, but there can be alternative ways to do this.

sending an api call from the system in question(like on success make a call do set some tag or field) same goes for error. However this is practical as long as you have control over that system.
Create a custom campaign Decision node which will listen to the responses of the webhook(in this case you will need to send the webhook using campaign only), Again the block here is that you will need to know how to code a custom campaign decision or will need to look for a developer who can do it.",0.0,0.11389625,0.01297235582023859
96,"Question\nI have an open net tab on my app and now I want to work by selenium to do several works.How can I define its driver?I know how to define a driver and open a webpage but I don't know this one. Can you help me?\nAnswer: finally,
you can take its html code by

driver.page_source",0.0,0.41975713,0.17619605362415314
97,"Question\nBasically user has to input their name like jOhn JhOnson
and i have to return it as John Jhonson
but if the user has 3 names like jonny jon jony
I have to return it like Jonny-Jon Jony
and i have no idea how\nAnswer: I would use a string tokenizer to count the number of names you have. Then you can easily manipulate each token and then append them altogether for your return value. I can't really help you beyond that without knowing what language you are using.",0.0,0.109687924,0.012031440623104572
98,"Question\nI am fairly new to the Python Flask world (or web applications in general), just started my first project on Friday using Jupyter Notebooks and am now trying to build a Flask web app from it - so please bear with me :)
My app creates pareto-efficient Portfolios based on the user-input.
On the index page I have a couple of input fields for user-input which I collect through a form. On calculate the app routes to the results page and in the process runs the data from the form through some calculations, then displays some tables with infos and plotly graphs.
Now I would like the user to be able to further manipulate the data but the data isn't accessible / persistently saved. - I've read something about using global variables or the session dict which are not recommended.
So I am asking myself two things:

What is the easiest way to implement the persistently save my objects & lists to manipulate the data between different routes
In case of having to use a database: What is the smartest logic to work with in that scenario to be able to receive the correct data in case of using a database (e. g. 50 people use the web app simultaneously - how do I make sure that user X retrieves his earlier data instead of the data from user y -- the web app does not require a login and I also don't plan on implementing one as I wanted this to be as quick and simple as possible)

Your help is much appreciated - I've probably written 5000 lines of code this weekend and maybe I just can't see the forest for the trees right now\nAnswer: You'll have to use a database, unless you want to store the data in-memory. However, in-memory data (python globals, for example) will be lost when the server performs a reboot - or crashes. This will be especially annoying during development, because all the data will be reset whenever you make a change in the code.
You could use an in-memory database like redis. This works very similar to using globals directly in the code, but it is a database and can be configured to save the data to the disk and survive crashes, etc...
Considering the sharing of data between users: I dont see a good way around having the users chose a login and associating the data with the user which is currently logged in. If its really just a for-fun project, you could use the User Agent and their",0.40816328,-0.008957505,0.17398974299430847
99,"Question\nim trying to retrieve the date and time of a message, and the date is correct, but the time is about 6 hours off. how can i fix this?
it is 5pm currently, but this line of code returns 23:00

msgDate = update.message.date

edit: it is returning the minutes properly, so its close, just not sure on what to do about the hours.\nAnswer: It seems as though function built into python-telegram-bot uses the UTC timezone, so yes, it does return the proper date and time, just converted into UTC",0.40816328,-0.50727654,0.8380300998687744
0,"Question\nWhile looking for how to allow list append() method to return the new list, I stumbled upon this solution:
list.append(val) or list
It's really nice and works for my use case, but I want to know how it does that.\nAnswer: list.append(val) returns None, which is Falsy, and so triggers or and returns the list. Nonetheless I consider this not pythonic, just make two separate lines without or",0.5442177,0.25253853,0.08507674187421799
1,"Question\nWhile looking for how to allow list append() method to return the new list, I stumbled upon this solution:
list.append(val) or list
It's really nice and works for my use case, but I want to know how it does that.\nAnswer: list.append(val) returns None, which is a falsey value, so the value of the entire expression is list. If list.append returned a truthy value, that would be the value of the or expression.",0.0,0.381683,0.14568190276622772
2,"Question\nI have a dataframe where the indexes are not numbers but strings (specifically, name of countries) and they are all unique. Given the name of a country, how do I find its row number (the 'number' value of the index)?
I tried df[df.index == 'country_name'].index but this doesn't work.\nAnswer: Why you don make the index to be created with numbers instead of text? Because your df can be sorted in many ways beyond the alphabetical, and you can lose the rows count.
With numbered index this wouldn't be a problem.",0.0,0.25984883,0.06752141565084457
3,"Question\nI am new to VScode and want to run Python in it for my college project. I have seen that in VScode all the programmes are executed in Windows PowerShell Terminal(By default). But the problem is that it also shows the file address which is being executed which I don't want. So, please could you give a suggestion which software should be used in the terminal which only executes code and doesn't show any file address. And How can I change it?\nAnswer: ctrl+shift+ is a Command Used to open terminal in VS code.. You can also Try Extension of python shell or powershell in VSCode...",0.0,-0.019084632,0.000364223204087466
4,"Question\nI have a Python program for customers to query price. Each time a customer can input some necessary information, the program will calculate and return the price to customer. Note: during the calculation process, the program also need to query a third party map service web API to get some information (such as google map API or other similar service).
I have a website developed using web development tools such Wix, Strikingly. It offers a capability to customize a web page by simply input a block of HTML codes. So, I want to study the possibility of using Django to convent my python program into HTML (incl. add some user interface such as text box and button), which can then be pasted into the website to form a unique webpage.
I am not sure if it is doable? Especially, the part of connecting third party map service API. Would Django be able to convert this part automatically to HTML as well? (how does it deal with API key and connection).\nAnswer: Python itself runs only on the console, and is meant to be the backend in site development, whereas HTML is meant only to be the frontend, so no calculation or data fetching. Wix is a frontend tool with some content management that offers customization but still in the frontends (html/css), and there's nothing more you could do with the content management other than using the built in table like feature. Trying to use the html generated by wix will be so much pain due to its css name optimization and making it quite unscalable.
 
If you don't wish to learn frontend building at all then you could look up other html generator tool for the frontend codes. From there, django itself is capable of building the entire website, using the html you generated as template, and passing the data you've computed into the templates. That's what Django is meant to do. In this case you would need to learn Django itself, which I would recommend if you intend to showcase your project as an interactive program rather just console logs.
Other alternatives include converting your python codes into javascript, which is capable of doing calculations and fetching from APIs, and you can include the javascript code directly in HTML with the  tag.",0.40816328,0.36083138,0.0022403087932616472
5,"Question\nThe error the application throws is:
ERROR:saml2.sigver:check_sig: 
ERROR:saml2.response:correctly_signed_response: Failed to verify signature
ERROR:saml2.entity:Signature Error: Failed to verify signature
ERROR:saml2.client_base:XML parse error: Failed to verify signature
And it seems to be a Windows problem. Does anyone know how should I implement this? The command used to verify the XML is:
C:\Windows\xmlsec1.exe --verify --enabled-reference-uris empty,same-doc --enabled-key-data raw-x509-cert --pubkey-cert-pem C:\Users\me\AppData\Local\Temp\tmp8wssc6_f.pem --id-attr:ID urn:oasis:names:tc:SAML:2.0:assertion:Assertion --node-id _579304c7-f1c4-5918-83ee-4b33c5df1e00 --output C:\Users\me\AppData\Local\Temp\tmpw9lbnowc.xml C:\Users\me\AppData\Local\Temp\tmpcg9l7jik.xml
And it returns b"""".
Thanks in advance.\nAnswer: Your question is a little vague.  It seems that you have sent an authenrequest and got a response and the application on your end is throwing the signature validation error.  If that is correct then you likely do not have the correct cacert from the identity provider defined in your application.
Questions about SAML and verifying XML signatures really need the original xml idealy in base64 so it is possible to try to check the signature.",0.0,0.14474839,0.020952096208930016
6,"Question\nThe error the application throws is:
ERROR:saml2.sigver:check_sig: 
ERROR:saml2.response:correctly_signed_response: Failed to verify signature
ERROR:saml2.entity:Signature Error: Failed to verify signature
ERROR:saml2.client_base:XML parse error: Failed to verify signature
And it seems to be a Windows problem. Does anyone know how should I implement this? The command used to verify the XML is:
C:\Windows\xmlsec1.exe --verify --enabled-reference-uris empty,same-doc --enabled-key-data raw-x509-cert --pubkey-cert-pem C:\Users\me\AppData\Local\Temp\tmp8wssc6_f.pem --id-attr:ID urn:oasis:names:tc:SAML:2.0:assertion:Assertion --node-id _579304c7-f1c4-5918-83ee-4b33c5df1e00 --output C:\Users\me\AppData\Local\Temp\tmpw9lbnowc.xml C:\Users\me\AppData\Local\Temp\tmpcg9l7jik.xml
And it returns b"""".
Thanks in advance.\nAnswer: For those who may face this problem in the future: Windows OS (still don't know certainly if the problem is caused due an OS particularity, I couldn't test it in other environments), pysaml2, and django-saml2-auth don't handle self signed certificates very well. I could solve the problem by just forking pysaml2/django-saml2-auth and passing downloaded cert-files from IdP (.pem) manually.",0.0,0.15593904,0.02431698516011238
7,"Question\nI have a time series in which i am trying to detect anomalies. The thing is that with those anomalies i want to have a range for which the data points should lie to avoid being the anomaly point. I am using the ML.Net algorithm to detect anomalies and I have done that part but how to get range?
If by some way I can get the range for the points in time series I can plot them and show that the points outside this range are anomalies.
I have tried to calculate the range using prediction interval calculation but that doesn't work for all the data points in the time series.
Like, assume I have 100 points, I take 100/4, i.e 25 as the sliding window to calculate the prediction interval for the next point, i.e 26th point but the problem then arises is that how to calculate the prediction interval for the first 25 points?\nAnswer: A method operating on a fixed-length sliding window generally needs that entire window to be filled, in order to make an output. In that case you must pad the input sequence in the beginning if you want to get predictions (and thus anomaly scores) for the first datapoints. It can be hard to make that padded data realistic, however, which can lead to poor predictions.
A nifty technique is to compute anomaly scores with two different models, one  going in the forward direction, the other in the reverse direction, to get scores everywhere. However now you must decide how to handle the ares where you have two sets of predictions - to use min/max/average anomaly score.
There are some models that can operate well on variable-length inputs, like sequence to sequence models made with Recurrent Neural Networks.",0.0,0.24663621,0.060829419642686844
8,"Question\nI have to parse and validate HIPAA 834 EDI file and generate 997 response with the success or error message
Sample 834 EDI File:
ISA00 00 30261401960      30261105741 2105250609*^005011891712750T*:~
GSBE161401960Facets202105250609171275X005010X220A1~
ST8340001*005010X220A1~
REF3800417558~
QTYDT958~
QTY**1381~
QTY**1381~
N1INHealthPLAN FI161105741~
INSY18030XNA**ACNN~
F0F951747732~
REF1L00417558~
REF170001~
REFDX0001~
DTP336D8*20040202~
PERIP**EMmvastola@wscschools.orgHP7169543565~
N3*130 Rosewood Dr.~
N4West SenecaNY*14224~
DMGD819810817MM~
HD024**HLTCPO1Y000*FAM~
INSY18030XNA**ACNN~
DTP303D8*20200701~
INSN01030XNA**NN~
REF0F951747732~
REF1L00417558~
REF170001~
REFDX0001~
NM1IL1TestmemberJessica***34962703984~
N3*130 Rosewood Dr.~
N4West SenecaNY*14224~
DMGD819820720*F~
HD024**HLTCPO1Y000*FAM~
DTP303D8*20200701~
DTP348D8*20200701~
INSN19030XNA**FN~
REF0F951747732~
REF1L00417558~
REF170001~ REFDX0001~
NM1IL1testySofia***34992599285~
N3*130 Rosewood Dr.~
N4West SenecaNY*14224~
DMGD820120524*F~
HD030**HLTCPO1Y000*FAM~
DTP",0.0,0.7633809,0.5827503800392151
9,"Question\nIs there a way to get the number of rows affected as a result of executing     session.bulk_insert_mappings() and session.bulk_update_mappings()?
I have seen that you can use ResultProxy.rowcount to get this number with connection.execute(), but how does it work with these bulk operations?\nAnswer: Unfortunately bulk_insert_mappings and bulk_update_mappings do not return the number of rows created/updated.
If your update is the same for all the objects (for example increasing some int field by 1) you could use this:
updatedCount = session.query(People).filter(People.name == ""Nadav"").upadte({People.age: People.age + 1)",0.0,0.36291254,0.13170550763607025
10,"Question\nas you can read in the title I have to build a reliable P2P data transfer Using UDP(I'm a student), so what I asking you guys is to not give me the code, actually, I hate copying and pasting so much, I feel pain in doing it, I'm asking for help to tell me what tool do I need, I'm familiar with Javascript and Java and Python, feel free to help me if you know any of these languages.
the reliability part will be achieved through checksum and ACKs(Acknowledgements), so I have to Implement them:
-I know how checksum can be calculated.
-ACKs can be Implemented in the way of Stop-and-wait protocol(I think it's the simplest one), if you know another protocol, that is okay.
I'm really lost, I don't know from where I begin if you have some code examples, please share a link that would be helpful, so I can build an idea from where do I start.
thanks.\nAnswer: Here are some reliable UDP examples in github like kcp, quic, utp. You can read them and get the essential from them.
As for how to do it, there are some little suggestions:

You need ACK to check if any packet/datagram lost, and then retransmit it
You need to use FEC
Congestion algorithm can be considered later.

When you try to build your own code, do a little goal in every step, don't build a complex protocol at the very begining.",0.0,0.12733841,0.016215071082115173
11,"Question\nI have created an.exe file by pyinstaller. And I want it to run on background even if I close it and I want to be able to access it taskbar's arrow at the bottom-right corner. For example; when I close Discord, it disappears but I can access it from the taskbar's arrow at the bottom-right corner. I want to exact same thing to happen with my app, how can I do it?\nAnswer: try this
pyinstaller -w --onefile ""yourfile""
you should be good",0.81632656,0.23881212,0.33352288603782654
12,"Question\nI have an idea to make a twitter bot that block annoying accounts:
my question is can I block accounts on behalf of other user?
I am using  tweepy
I already have tokens for my account\nAnswer: Yes, if you have access tokens for the other user. You can get these by using the sign-in with Twitter API, and have the other user go through that process to authorize your app.",0.40816328,0.119704664,0.08320837467908859
13,"Question\nSuppose you have a pandas.DataFrame like so:




Institution
Feat1
Feat2
Feat3
...




ID1
14.5
0
0.32
...


ID2
322.12
1
0.94
...


ID3
27.08
0
1.47
...




My question is simple: how would one select rows from this dataframe based on the maximum combined values from two or more columns. For example:

I want to select rows where the columns Feat1and Feat3 have their maximum value together, returning:





Institution
Feat1
Feat2
Feat3
...




ID2
322.12
1
0.94
...




I am certain a good old for loop can take care of the problem given a little time, but I believe there must be a Pandas function for that, hope someone point me in the right direction.\nAnswer: You can play arround with:
df.sum(axis=1)
df['row_sum'] = df.sum(axis=1)
or
df['sum'] = df['col1' ] + df['col3']
And then:
df.sort(['sum' ],ascending=[False or True])
df.sort_index()",0.0,-0.10100061,0.010201122611761093
14,"Question\nI need to increase the file size of a pdf without adding any new texts or images.  I can reduce the file size by compressing it. When I increase the file dimensions from A4 to A3, A2 etc using available online tools, it automatically gets compressed. What I want is to increase its size from 80 kB to 100 kB. Any idea how can it be done in Python or using any tool?\nAnswer: If you have Photoshop, Open the PDF in it and do ""save as"" and you will see a quality option then just increase it to the max...
If it doesn't work with PDFs then take a screenshot of the PDF and do the above suggestion again, it'll work",-0.35714287,0.21275324,0.3247815668582916
15,"Question\nI am trying to open discord and join a voice channel with a voice command. All I find is about bots, while I'm trying to do it with the user, me, in this case. Opening discord is not a problem, but I have no idea of how to do the voice channel thing, I'm still a begginer.\nAnswer: First of all, discord does not allow you execute client side commands directly or more specifically, pull users to voice channels. Making an user force-join a voice channel via a command would be a serious security exploit.
What you can do to make users join channels, is to let them join a waiting room of some sort, than pulling them to channels from there.
Now as far as i understand, you want to join a specific channel via a voice command yourself. In that case i would suggest not using discord API. I would implement a web scraper (in this case something like a web scraper would suffice since discord is basically running its website as an app, Press Ctrl+Shift+I and you will understand what i mean) to target a text containing the voice channel name i want to join. I would get that name from voice recognition. Then get that text position on screen and click it. You could use pyautogui for that purpose.
To be fair, this is not a beginner project at all, however with sufficient research and work you can make it.
Cheers",0.40816328,-0.22747537,0.4040364921092987
16,"Question\nI know when you import everything you can do thinks like nltk.bigrams(nltk.corpus.brown.words() for bigrams and nltk.trigrams(nltk.corpus.brown.words() for triagrams, but how do you do four grams?
I've seen other ways to do it, but they all do it with a ""string"" or a text they make up. How do you do it with the nltk.corpus.brown? Do you have to covert it to a string and if so, how?\nAnswer: To get n number of items you can use nltk.ngrams() with the number to get as the second argument.
In your example, to get four-grams, you can use nltk.ngrams(nltk.corpus.brown.words(), 4)",0.0,0.3610105,0.13032858073711395
17,"Question\nI had made this form in Django which takes some data and when a save button is clicked it saves it to the database. It stores the data that I give, but I noticed that every time I refreshed the page that contained the form it would automatically save an object with the Id as none and all values set to null. I wanted to know how to write a function that says the form only submits to the database when the save submit button is clicked and not when reloading the page and making a dummy object in the database.\nAnswer: I think the problem here is in the view, more specifically, how you're handling the request inside the view.
it seems that you're saving the object no matter what's the request method, that's why even refreshing the page will save a new object.",0.0,0.21655166,0.046894621104002
18,"Question\nI am gonna have an open source app in which it needs to send some data to an fastapi python api, how can i make it so that only the app can make requests to the api and not some random person abusing the api endpoint?\nAnswer: There are so many ways to do that. Even some of the techniques doesn't bother the API endpoint.

IP Restriction: You can restrict an IP from cloud provider which IP can call the API. Even you can have multiple IPs.
API KEY: You can provide an API KEY to the API client. When the request come along with the provided key then you work on it otherwise ignore.

The IP method is much better than doing on the application end.",0.40816328,0.28267694,0.015746822580695152
19,"Question\nI am able to login and receving SAMLResponse through HTTP-Redirect binding and also I can able to decrypt using privatekey and able to retrive claims.
My question is still do we need to verify saml response(ADFS)? if its how to do that
do I need to use IP(identity provider) public key? will it available in IP(Metadata)?
I have SAML response in the following request parameter
SAMLResponse = base64(deflate(data))
signature = hashvalue
sigAlg = sha256
how to validate?\nAnswer: Yes you need to also verify the digital signature of the SAML response. This is because the encryption is done using your public this is available to anyone that has access to your metadata and does not give any assurance that it was produces by your IdP.
To verify that you IdP is the one that produced the SAMLResponse you verify the digital signature of the SAMLResponse using the IdP public key. This is typically available in the IdP metadata.",0.0,0.23160982,0.053643111139535904
20,"Question\nMy question is to do with calling python from a command prompt in windows 10 after installing a new version (3.9 to 3.10).
When I type (into a cmd prompt): py i get Python 3.10.0.
When I type (into a cmd prompt): python i get Python 3.9.6.

So two questions:

Why do I get two different versions when typing python compared to py?
How can I ensure that they point to the same version or how can I select different versions?\nAnswer: This is because there are two versions of python on your computer. When you want to refer to a particular version of python just do:
py - version

For example, if you want to reference python 3.10 in cmd, do: py - 310
And when you want to reference to 3.9 do: py - 39

Make sure you have the correct spacing^",0.40816328,0.41767204,9.041649173013866e-05
21,"Question\ni have LogisticRegressionCv model it's.pkl file and import data as images but i don't know how to get it on flutter please help me If you know how or if I must to convert my model to other file formats.
please help me.
Thank you for your help.\nAnswer: as you've trained your model in python and stored it in pkl file. One method is in your flutter background, call python3 predict_yourmodel.py your_model_params and after the run, it will give your the model result.
Another way is implement a logisticRegressionCv in Flutter as it is a simple model, and easily be implemented. you can store all your params and l1 or l2 etc super-params in a txt instead of pkl file for readility.",0.0,0.16680932,0.02782534994184971
22,"Question\nI have written this to upload 10000 photos to Instagram, one each hour and whenever I run it I get these errors

INFO - Instabot version: 0.117.0 Started
INFO - Not yet logged in starting: PRE-LOGIN FLOW!
ERROR - Request returns 429 error!
WARNING - That means 'too many requests'. I'll go to sleep for 5
minutes.

this is my code am I doing anything wrong? Can someone please point it out and explain?

from instabot import Bot import time
bot = Bot()
image = 1
bot.login(username=""username"", password=""password"")
while image < 10000:
photo = str(image)
bot.upload_photo(f""{photo}.png"")
time.sleep(3600)
image += 1\nAnswer: You just need to go to api.py
This is a file in this InstaBot library
In case if you're using vscode editor then just ctrl+click on the last link shown in error logs in terminal of vscode
Then comment out the complete chunk of code starting from 559 to 585(complete if block)
Now you're good to go",0.20408164,0.016699076,0.03511222451925278
23,"Question\nI'm creating webapp and using DRF on the server. I want to start function on server, after event on frontend (for example - button clicked)
Example:

User is typing '2021' in input field on frontend and click the button,,generate""
The '2021' is transfering to function,generate_list_of_sundays(year)' on server
The function return list of all sundays in typed year
List is displayed to user on frontend

Of course this is simple example. I want to know how to get this type of communications between frontend and backend.\nAnswer: What you want is to expose a REST API using the Django REST Framework and have HTTP endpoints (URLs) mapped to functions on your backend (e.g. generate_list_of_sundays()).
Then, on the frontend, the button would submit an HTTP request to the given endpoint (i.e. could be POST, GET and so on).
The frontend can either use a form to submit the information or something like AJAX.
You could also avoid using the REST API approach and have Django views that extract data from the HTTP request and act upon it, but while it might seem easier, this would lead to a poor user experience.",0.0,0.2640097,0.06970112770795822
24,"Question\nThe book ""Learning Python"" by Mark Lutz states that in Python one can use various types of external modules, which include.py files,.zip archives, C/C++ compiled libraries and others. My question is, how does one usually handle installation of each type of module?
For example, I know that to use a.py module, I simply need to locate it with import. What about something like.dll or.a? Or for example, I found an interesting library on GitHub, which has no installation manual. How do I know which files to import?
Also, are there any ways of installing modules besides pip?\nAnswer: The answer depends on what you want to do.
You can use Ninja for example to use C++ modules and cython for C and there are various packages for almost any type of compiled code.
You can install packages via pip using the pypi package repository or by using cloned repositories that have a setup.py file inside.
Any other python based repo can be imported either by a custom build script (that they will provide) or by directly importing the relevant Python files. This will require you the dive into the code and check what are the relevant files.",0.20408164,0.2504918,0.002153902780264616
25,"Question\nI have two tables - ""Users"" and ""Projects"", i want to be able to show which users are assigned to which project. There may be multiple users assigned to the project.
I was thinking of creating a 'project_users_matrix' table where a new column would be created for each user and a new row created for each project, then the cells can just show a 1 or 0 depending on if the person is working on that project.
The 'cleaner' option would be to have columns 'user_1', 'user_2', 'user_3' in the project database but then there can't be an indeterminate number of users for a project.
Is there a better way to do this? It seems like there should be...\nAnswer: you need to create 2 more fields in project-table  1st for User_id and 2nd for Active/inactive in 1st field you need to store id of user who is working with that project and in 2nd field enter value 0/1 and provide button that if user is active on that table it shows 1. and once it done with his work.user can update it with 0.",0.0,0.11148226,0.012428294867277145
26,"Question\nI have two tables - ""Users"" and ""Projects"", i want to be able to show which users are assigned to which project. There may be multiple users assigned to the project.
I was thinking of creating a 'project_users_matrix' table where a new column would be created for each user and a new row created for each project, then the cells can just show a 1 or 0 depending on if the person is working on that project.
The 'cleaner' option would be to have columns 'user_1', 'user_2', 'user_3' in the project database but then there can't be an indeterminate number of users for a project.
Is there a better way to do this? It seems like there should be...\nAnswer: If users can participate in many projects, and projects can have many users then you have a many-to-many relationship and you need three tables: users, projects and an association table that contains user ids and projects ids only.  Each active user-project combination should have a row in the association table.
If users cannot participate in multiple projects simultaneously then you have either a one-to-many relationship between projects and users, or users and projects, which can be expressed by a foreign key column on the many side.",0.20408164,0.16677612,0.0013917017495259643
27,"Question\nI want to get the molecules from the SMILES using rdkit in python. The SMILES I used was downloaded from the drugbank.
However, when I using the function Chem.MolFromSmiles, some SMILES would report but some wouldn't:
Explicit valence for atom # 0 N, 4, is greater than permitted.
I found some explanation about this problem: it is because the SMILES generated a invalid molecule that doesn't exist in real world.  But I am not a chemistry student.... So anyone know how to fix this?\nAnswer: Your SMILES string would appear to have a neutral 4-co-ordinate nitrogen atom in it, which doesn't exist in real molecules.  4-co-ordinate nitrogen atoms have a positive charge, eg [N+] in a SMILES string.",0.81632656,0.20357537,0.3754640221595764
28,"Question\nI would like to hear that for the  Spyder 5.1.5 version is now it possible to recover an accidentally deleted files from a project. If it is, I would really appreciate how to do it because I am pretty worried about it. Thanks in advance.\nAnswer: I find a simple solution: keep your project or working directory in Google Drive, so a deleted file can be recovered from Google Drive's trash, even if the deleted file is not in Window's trash.",0.20408164,0.095333815,0.011826089583337307
29,"Question\nI would like to hear that for the  Spyder 5.1.5 version is now it possible to recover an accidentally deleted files from a project. If it is, I would really appreciate how to do it because I am pretty worried about it. Thanks in advance.\nAnswer: Not possible as far as I know.
If you're running on Mac, try to recover via TimeMachine, as it does snapshots every hour or maintain the working folder synchronized with a cloud service.",0.20408164,0.15491116,0.0024177359882742167
30,"Question\nin pandas the inplace parameter make modification on the reference but I know in python data are sent by value not by reference i want to know how this is implemented or how this work\nAnswer: Python’s argument passing model is neither “Pass by Value” nor “Pass by Reference” but it is “Pass by Object Reference”

When you pass a dictionary to a function and modify that dictionary inside the function, the changes will reflect on the dictionary everywhere.
However, here we are dealing with something even less ambiguous. When passing inplace=True to a method call on a pandas object (be it a Series or a DataFrame), we are simply saying: change the current object instead of getting me a new one. Method calls can modify variables of the instances on which they were called - this is independent of whether a language is ""call by value"" or ""call by reference"". The only case in which this would get tricky is if a language only had constants (think val) and no variables (think var) - think purely functional languages. Then, it's true - you can only return new objects and can't modify any old ones. In practice, though, even in purest of languages you can find ways to update records in-place.",0.40816328,0.4407617,0.0010626560542732477
31,"Question\nI have a question regarding Python and load balancing. Consider a virtual machine with 16 virtual cpu cores and single Python Rest-Service wrapped in Docker. I suppose it only is going to use one virtual core instead of the 16 when run.
Now consider 8 duplicates of the Python Rest-Service run in a docker compose parallel behind a load balancer (upstream). Do they use 8 of the 16 cores? And if so how do they know which one to use?
Also do the virtual cores make any difference, what about a real system with real 16 cores?
If anyone has any experience or knowledge I would be happy if you could share it.
Thank you!\nAnswer: Whether a REST service uses one or multiple cores depends on how you code it. You could, for instance, have a pool of python processes handling the requests.
Lets say that you have a python process. Since python grabs the Global Interpreter Lock (GIL) when running byte code, even if it has multiple threads, only one will run at a time. If you have multiple python processes, each one has its own unique GIL, so these processes will run in parallel using multiple cores.
How they run is up to the operating system, and it doesn't care if its running python or anything else. At any given time there are X threads available. The operating system scheduler keeps all cores running as much as possible. When the code on one core makes an operating system call, or when a certain time has elapsed, the OS may grab one of the pending threads to execute on that core. The old thread is now put back in the wait queue.
There are rules about which threads get the greatest precedence, etc... But this is all quite operating system dependent and so you have to be vague when talking the general sense.",0.0,0.26162392,0.06844707578420639
32,"Question\nCan someone help me with transforming the following table using a PYTHON function?
I need 2 new columns: A ""follower Type"" which will have entries as organic or paid and a ""Follower count"" which has the values corresponding to the type of follower.
Current Table -




org
organic follower
paid follower
start date
stop date




One
2
0
1634169600000
1634256000000


One
-1
0
1634256000000
1634342400000




Desired Table -




org
start date
stop date
Follower Type
Follower Count




One
1634169600000
1634256000000
Organic
2


One
1634169600000
1634256000000
Paid
0


One
1634256000000
1634342400000
Organic
-1


One
1634256000000
1634342400000
Paid
0




If anybody knows how to do this, please do let me know.
Thanks and Cheers!\nAnswer: Use reindex to change column order
'''
column_names = [""C"", ""A"", ""B""]
df = df.reindex(columns=column_names)
'''
Like below you can add columns to existing dataframe
df[newcolumn]=formula",0.0,-0.18389654,0.03381793946027756
33,"Question\nI am trying to connect to my BACnet device using BAC0. Yabe is able to detect the BACnet device. However when I try to connect to the device via BAC0.connect(network IP) followed by BAC0.device(device IP and other parameters) I get the error msg - IP address provided is invalid. Check if another software is using port 47808. When I run the command the Wireshark trace shows BACnet APDU protocol being used with appropriate Confirmed-REQ and Complex-ACK msg between the local-network-IP and device-IP, suggesting that the device was polled. However the Wireshark trace shows up after the command terminates with the error message. Could it be that the command terminates prematurely? If so, how to handle it?\nAnswer: Figured it out. The IP address was correct. But there was another software that starts automatically and runs in the background. It also uses BACNet. As a result the port 47808 was getting used by this software. Wireshark was capturing the communication to the device via this software, since the software has a discovery tool for BACnet devices. BAC0.connect works now.",0.40816328,0.16375422,0.05973578616976738
34,"Question\nFor the purpose of making a small piece of software for a friend to study I created a little GUI application with pysimplegui. Because he is not really tech-savy and still has a Win7, I tried to pack it as.exe ahead and then also created an installer and gave it via USB to him.
Unfortunately his Anti-Virus pops up and he gets sort of scared and didn't know what to do.
Figured out I could resolve it by signing the code with makecert but have to somehow get the key to be installed on his PC also.
Is there a way to implement that in the installer to spare him going through the technicalities?
I used the Auto-Py-To-Exe application to convert the python file btw and InnoSetup to create the Installer --- Before handing over, I did try it on a clean Windows7 VM and it worked after using the installer (thought the Registry wanted to get it installed properly).\nAnswer: Remove code from your program that may be harmful
Then virus detection software should ignore.",-0.71428573,0.077548265,0.6270011067390442
35,"Question\nThere are many answers on how to remove old Python versions and many on how to install new ones. Is there a way to install the latest Python version so that it replaces all old installations please?
I don't want to use a virtual environment, BTW.\nAnswer: remove all versions of python and by rightclicken on them end pressing uninstall, this will open the programs and features panel and just remove all versions of python you have there, then install the latest version via the python website. this process will not take very long.",0.0,0.11267197,0.01269497349858284
36,"Question\nI have created a knative service[gRPC server] in  aks cluster, I have exposed the service using istio gateway private static IP,
After using the command kubectl get ksvc I have got an address sample-app.test.10.153.157.156.sslio.io
When I try to use this address in the python client, it throws error saying failed to connect addresses, but if I try to hit the service using
curl sample-app.test.10.153.157.156.sslio.io I am able to hit the service, I don't know what i am missing here.. please suggest..\nAnswer: GRPC uses HTTP/2. You may need to explicitly name your port h2c. I'm assuming that you've tested the container locally without Knative in the path and have been able to make a grpc call in that case.",0.0,0.3666129,0.13440503180027008
37,"Question\nIm using python 3.9 and coverage 6.2
I would like to have a record of my most recent coverage but Im not sure if I should upload my.coverage file. Im guessing no since it sort of has info about my directory layout. So i would like to know how I should go about that, is it even standard to upload such a thing? If not, why not?
I also generated the htmlcov folder but I didnt upload it since it has a default gitignore for the entire folder.\nAnswer: Most people don't upload the.coverage or HTML report, because they don't need to track it over time.  But there's no harm in uploading them.  You mention directory layout as if it was a secret to protect, but isn't your layout already committed to GitHub?
If you want to commit the HTML report, you will need to remove the.gitignore file in the directory.",0.40816328,0.3395247,0.0047112563624978065
38,"Question\nI'm testing AI model with rabbit mq.
Becasue the process using AI model is heavy, i use rabbit mq to control the task order.
The problem is killing the callback function after consuming.
For example, there are three tasks in rabbit mq(A,B,C in order)

task A is consuming, callback function(include AI model) is running
task B,C is waiting untill basic_ack arrived
I want cancel task A,B and analysis C first

in this case,

how can i kill the process (task A),
delete the message B in rabbit MQ\nAnswer: I find the way but cannot sure the right way.
Use another queue to receive all message from rabbitmq
and create_task by using asyncio to cancel when i want
it works well only all funcs are asyncronose",0.0,-0.24671727,0.06086941435933113
39,"Question\nfor taking list input in the array I used

map(int,input().split())

but for using this input method I can't modify the array if I Iterated like a[I] then it shows

TypeError:'map' object is not subscriptable

Can you please tell me how can I resolve this problem?
I am a competitive programmer but I am new to python I  must have take input array by using  map(int,input().split())\nAnswer: Use list(map(int,input().split())) instead of map(int,input().split()) to convert your input to list, because map function returns an generator which can not be indexed.",0.20408164,0.16503954,0.0015242856461554766
40,"Question\nI had a Python code that print 33 line in terminal, so I want to jump back to line 11 and rewrite all text in that line, how can i do that?\nAnswer: You will have to use a package like curses to take control of the terminal.  That's what apps like vi use to do their drawing.  That lets you draw strings at specific row and column position.",0.20408164,0.15695906,0.0022205377463251352
41,"Question\nI had a Python code that print 33 line in terminal, so I want to jump back to line 11 and rewrite all text in that line, how can i do that?\nAnswer: This is why it's useful to have some familiarity with less.
You could use 11g to jump to the beginning of the line or 11G to jump to the end of the line.
Or from the command line, you could use less +11 filename.py or less +11 -N filename.py if you want to see the line numbers. You can also display a specific line of the file at a specific line of the terminal by using less +11 -j 10 filename.py.",0.0,0.34987164,0.12241016328334808
42,"Question\nif i wanted to train an lstm to predict the next date in a sequence of dates, how would i do that since lstm require a scaled value?
example of data:




date
next date




2012-05-12
2012-05-13


2012-05-13
2012-05-19


2012-05-19
2012-05-20


2012-05-20
2012-05-22


2012-05-22
2012-05-26


2012-05-26
2012-05-27


2012-05-27
2012-05-30


2012-05-30
2012-06-12


2012-06-12
2012-05-19


2012-06-19
2012-06-25\nAnswer: You could hand over the date split into three inputs: One would then be the year, the other the month, and the last the day. While normalizing your inputs definitely makes sense, however I would not entirely agree with your ""LSTM requires"".
Day and month are already limited to a range of values which can be scaled

day (1 - 31)
month (1 - 12)

For year you need to make an educated assumption based on your application. So that year can then also be transferred to a scaled value. Judging from your data, it might be that year is constant at 2012 and it is not needed to begin with.

year (2012 - 2013(?))

Note: Ask yourself whether you give the neural network enough system information to be able to predict the next date - meaning, is there already enough of a pattern in your data? Otherwise you might end up training a random predictor.",0.81632656,-0.0003274083,0.6669237017631531
43,"Question\nRecently i was struggling trying to take the pixel values of a 3D volume (np array) using specific space coordinate of a STL object.
The STL object is spatially overlapped with the 3D volume but the latter has no coordinate and so i don't know how to pick pixel values corresponding to the STL coordinates.
Any idea?\nAnswer: If the STL object is truly in the 3d volume's coordinate space, then you can simply STL's coordinate as an index to lookup the value from the 3d array.  This lookup does nearest neighbor interpolation of the 3d image.  For better looking results you'd want to do linear (or even cubic) interpolation of the nearby pixels.
In most 3d imaging tasks, those coordinate spaces do not align.  So there is a transform to go from world space to 3d volume space.  But if all you have is a 3d numpy array, then there is no transformation information.
Update:
To index into the 3d volume take the X, Y, Z coordinates of your point from the STL object and convert them into integer value I, J, K.  Then lookup in the numpy array using I,J,K as indices: np_array[K][J][I].  I think you have to reverse the order of the indices because of the array ordering numpy uses.
When you way 3d array and the STL align in python, how are you showing that?  The original DICOM or Nifti certainly have world coordinate transformations in the metadata.",0.0,0.27413893,0.07515215128660202
44,"Question\nWhereas assert in Python is ideal to verify wether certain function calls return an expected output for a given input, could it also be used to check for results printed on screen (i.e., in procedures that return no results but only have side efffects like printing stuff on screen)? The context of this question is how to write such automated tests that I would like to include in an automated grader tool.
Since expect the answer to this question to be: assert does not serve this purpose; what other trick could I use then to check the screen output produced by a procedure?\nAnswer: You could redirect the standard output to a file and then compare it's content for validity.",0.40816328,0.29780114,0.012179802171885967
45,"Question\nI have installed mypy via command prompt on Windows using this command line:
python -m pip install mypy, I'm using python 3.10.0, when I use pip list, it shows that  I have installed mypy 0.910 and mypy-extensions 0.4.3, but when I use it to check a program mypy hello.py, it said'mypy' is not recognized as an internal or external command, operable program or batch file.. Can anyone help me with this? I don't know how to fix this problem. Thanks for your help!\nAnswer: You can try these things

Restart your PC. Then try again.
Try pip install mypy==0.910. Try again.
Check whether the location of the pip environment is specified in the Environment variables (Control Panel > System > Advanced System Settings >Environment Variables). If it is not add it or specify the path manually.

example -: mypy C:\<your file location>\hello.py",0.40816328,0.07928985,0.10815773159265518
46,"Question\nEncountering an error when running this cell, does any one know how to fix it? Thank you.
cfos and autofluo images have been resampled align with the template/reference file in atlas. Is it necessary to debug this file?
File ""/homeanaconda3/envs/ClearMapStable/lib/python3.6/site-packages/tifffile/tifffile.py"", line 4696, in open
self._fh = open(self._file, self._mode)
FileNotFoundError: [Errno 2] No such file or directory: '/home/ClearMap2/Documentation/Example/Haloperidol/haloperidol/1268/debug_resampled.tif'\nAnswer: Thanks for all responding.
I've solved this problem. It simply need to turn on the debug mode at the beginning of the script. In my case, I turned on the change the code to 'ws.debug = True' in the Initialize workspace cell.",0.0,0.18296444,0.03347598761320114
47,"Question\nI just started learning Pyglet but hit a roadblock. I am using the i3 window manager, and when I open a window with Pyglet, it floats. My guess is that i3 treats it like a dialog and positions it in the center of my screen. I tried setting the style attribute of the Window object to to all possible values, but it had no effect. How can I set the type attribute of the window to DEFAULT so that the window is part of the i3 grid and not floating in the center?
I read on StackOverflow how to make a floating window with Tkinter and Cairo GTK(thinking that doing the opposite will give me a non-floating window), but I can't find a way to set the window type attribute in Pyglet the way it can be done with other libraries.\nAnswer: I figured it out by looking for the same issue with SDL. Apparently, the type hint is not needed if the window is specified as a resizable. By adding resizable=True to the window constructor arguments, the window is included as a part of the grid.
If the intended behavior is to have a floating window(ex. a game or dialog) set resizable=False. Else resizable=True.",0.0,0.39343518,0.1547912359237671
48,"Question\ncould you help me with followings.
paramiko 16.1.0 library is in [python default] /usr/local/lib/python2.7/site-packages
But I can not upgrade paramiko 16.1.0 library to current paramiko 2.8.0 library; so I had to download paramiko 2.8.0 library into my path /home/pylib
Please let me know how I can force python to use /home/pylib/paramiko [paramiko 2.8.0 library] in my python code
Note: for now I am stuck with Python 2.7;I can not update PYTHONPATH\nAnswer: It's good practice to use virtual environments to avoid conflicts like this.
What is a virtual environment?
A virtual environment is basically like creating a fresh installation of Python for only one project. This allows you to easily have two different versions of a library installed for different projects.
How do I use virtual environments?
Install the virtualenv package: pip2 install virtualenv
Go to the root of your project cd path/to/project/root
Create a virtualenv: virtualenv -p /usr/bin/python2 venv
Activate the environment:. venv/bin/activate
Install the package version you want: pip2 install paramiko==2.8.0
Run your program: python something.py
To exit the virtual environment use: deactivate
Next time you want to run your program make sure you activate the environment with. venv/bin/activate first. None of the other steps need to be repeated.",0.40816328,0.20995742,0.0392855629324913
49,"Question\nI'm working on a python project which requires me to divide a rectangular space in to triangles.

There can be no overlap or spaces between the triangles.
All of the rectangles area has to be filled.
Ideally the angles and the sizes of the triangles should differ from
each other in a random fashion.
The algorithm should return a list of all triangles as tuples of
their three corner coordinates.

I am requesting an algorithm to solve the above.
Things I have tried.
I have looked for packages and algorithms that might help me but haven't found any.
So far, I've thought about creating one random triangle at a time following the top border but get stuck when I reach the opposite vertical border. I don't how to continue from there while making sure that no space is left without a triangle.
I also thought about just drawing random lines from border to border, but I can't figure out how to find all the triangles created by their intersections as well as guaranteeing that all subareas are triangles.
Any help would be greatly appriciated!\nAnswer: If you think of drawing a diagonal across the rectangle, you have achieved your objective for 2 triangles, each being half of the rectangle area.
For multiple triangles, start at any one of the corners, draw a line from it to a random point on the opposite (not adjacent) side. Use that point to draw another line to a point on the original side (increasing the X position as you go).
The final line drawn must meet the opposite corner on the original side.
Save the coordinates of each triangle as 3 tuples. The first tuple of the first triangle, for example, may be {0,0}, depending on the point numbering nomenclature of your screen.",0.20408164,0.17216188,0.001018871203996241
50,"Question\nBasic question about python f-strings, but couldn't find out the answer: how to force sign display of a float or integer number? i.e. what f-string makes 3 displayed as +3?\nAnswer: use if statement if x > 0:.. """" else:.",-0.42857143,0.3558557,0.6153259873390198
51,"Question\nHow to achieve going to another page without refreshing after for example clicking a link to another URL. More examples, I have a list of books display and when I clicked on one of the book, it will redirect me to the selected book page. This is the example of the code link.
<a class=""link"" href=""{% url 'books:view_book_detail' obj.book.name obj.pk %}"">View book</a>
I know about the history.pushState() but how to actually use it? Can it be use with URL from Django. Are there any method else then stated?\nAnswer: I did this using many if statements in view.py
you need to use <button type=""button""> instead of <a>, and check if the POST request has that name to respond with different context, also you need to use if statements in template to update the frontend according to the response
IT IS A BAD PRACTICE, it is highly recommended to use JS framework to have a single page app",0.0,0.31182805,0.09723673015832901
52,"Question\nHow to achieve going to another page without refreshing after for example clicking a link to another URL. More examples, I have a list of books display and when I clicked on one of the book, it will redirect me to the selected book page. This is the example of the code link.
<a class=""link"" href=""{% url 'books:view_book_detail' obj.book.name obj.pk %}"">View book</a>
I know about the history.pushState() but how to actually use it? Can it be use with URL from Django. Are there any method else then stated?\nAnswer: Using regular django or plain javascript is not efficient enough for this purpose.
Modern JavaScript frameworks like React, Angular, Vue etc are typically used for these case scenarios.
But still, it's not impossible though to do this without JS frameworks.
One of the approaches that you can use is using AJAX.
You can create a base.html which will be our basic template, and then break the application into little components like home.html, product-list.html, view-product.html. Which we can further include according to our requirements.
{% include 'view-product.html' with product=product_object %}
(something like this)
Make an AJAX request to /view_product/<id>/ which should return a JsonResponse about which component to display. And through JavaScript, we can handle loading that specific component.
But again, THIS IS NOT A GOOD PRACTICE, I would recommend using JavaScript frameworks for Single Page Applications.",0.40816328,0.42627442,0.0003280133823864162
53,"Question\nSo I've been trying to attack this problem for a while but have no idea how to do it efficiently.
I'm given a substring of N (N >= 3) characters, and the substring contains solely of the characters 'A' and 'B'. I have to efficiently find a way to count all the substrings possible, which have only one A or only one B, with the same order given.
For example ABABA:
For three letters, the substrings would be: ABA, BAB, ABA. For this all three count because all three of them contain only one B or only one A.
For four letters, the substrings would be: ABAB, BABA. None of these count because they both don't have only one A or B.
For five letters: ABABA. This doesn't count because it doesn't have only one A or B.
If the string was bigger, then all substring combinations would be checked.
I need to implement this is O(n^2) or even O(nlogn) time, but the best I've been able to do was O(n^3) time, where I loop from 3 to the string's length for the length of the substrings, use a nested for loop to check each substring, then use indexOf and lastIndexOf and seeing for each substring if they match and don't equal -1 (meaning that there is only 1 of the character), for both A and B.
Any ideas how to implement O(n^2) or O(nlogn) time? Thanks!\nAnswer: Effeciently remove single letter substrings from a string

This is completely impossible. Removing a letter is O(n) time already. The right answer is to not remove anything anywhere. You don't need to.
The actual answer is to stop removing letters and making substrings. If you call substring you messed up.

Any ideas how to implement O(n^2) or O(nlogn) time? Thanks!

I have no clue. Also seems kinda silly. But, there's some good news: There's an O(n) algorithm available, why mess about with pointlessly inefficient algorithms?
charAt(i) is efficient. We can use that.
Here's your algorithm, in pseudocode because if I just write it for you, you wouldn't learn",0.81632656,0.138569,0.45935532450675964
54,"Question\nHow can I connect two applications with the scenario below?
Application1:
Our infrastructure was created on AWS with python-django and react, its a private VPC that i can only access via SSH to the EC2 bastion instance (as far as to be able to write codes into)and the way the backend was deployed to create the backend URL api.mywebsite.com (which has multiple endpoints) was through cloudfront and Route53. (www.mywebsite.com was built via s3 and can talk to the backend api.mywebsite.com).
Application2:
(This is a client infrastructure)
At this time i haven't met the client to know what their system is made of but regardless i need to find a way to write some codes on this system when a specific event is triggered to send data to an API endpoint of Application1.
What would be the best way to implement such a logic or API to connect Application1 and Application2?
(Especially considering that Application1 infrastructure is a private VPC)
This is pretty much the same way that someone would use an API like STRIPE...I guess, but i am not sure how to achieve such result...
thank you in advance\nAnswer: If understood correctly App1 own by your, App2 is own by 3rd party.
The way I would solve in a scalable way is to deploy a service on  aws lambda that can get requests from App2(or any other app that you would decide in the future), I would give this service role in order to connect with App1 and would have all logic there.
What you will gain is that your app is secure(you don't expose it to outside 3rd app), the solution is scalable (you don't need to do nothing if you want to have other apps connecting)",0.0,0.30286026,0.09172433614730835
55,"Question\nI'm using the python playsound module with a little app that I'm making, and it errors out if I use version 1.3.0, so I always use version 1.2.2, but I encountered a bug that caused me to uninstall and reinstall a few different versions. Eventually, I somehow managed to get vscode to think it has playsound 1.3.0 installed. And it errors now when I run it. But if I just run the raw file then it works perfectly fine.
Note: even when I uninstall the module it still tries to run in vscode, and the same 1.3.0 bug happens, the first note plays and then I get an error.\nAnswer: Try restarting VSCode. Sometimes it gets hung up. Whenever you install a new module, restart VSCode Becuase that normally fixes it.",0.0,0.40853328,0.1668994426727295
56,"Question\nCould you please tell me how can I create an alarm from a Python script in a Zabbix system?
I have a Python script in which a certain function is processed, and at a certain point I would like to create an alarm in the Zabbix system when a certain condition is created in the script. I also have a mail server. I was thinking of creating a separate mailbox for Zabbix, to this email address I will send a letter from Python, and the Zabbix system will receive this letter, process and create a Problem. Is such functionality possible?\nAnswer: Easiest is to use zabbix for this. To do that you feed values to an item in zabbix in one of the many ways and create a trigger that fires when you want it.
If a script is needed to generate the values you can implement the script as a user parameter if it is short running. If it takes more than a few seconds using zabbix sender might be smarter.",0.40816328,0.23374903,0.03042032942175865
57,"Question\nI want to make a flutter app that, when a button is pressed, it runs a python or a js script that I made and is saved in a.js or a.py file. Essentially, I want to run a.js or a.py file when a button in Flutter is pressed. Is that possible? And if yes, how can it be done? I am making a desktop application that has to work offline.\nAnswer: Yes you can using API, you make API in django or flask backend which run the python script you want after you make a request to your API via clicking the flutter button",0.20408164,0.1456145,0.0034184057731181383
58,"Question\nI want to make a flutter app that, when a button is pressed, it runs a python or a js script that I made and is saved in a.js or a.py file. Essentially, I want to run a.js or a.py file when a button in Flutter is pressed. Is that possible? And if yes, how can it be done? I am making a desktop application that has to work offline.\nAnswer: Yes, this is possible, but it requires Python and JS interpreters to be installed on the device. A way to ensure this is to ship them together with your app, although this will take a lot of extra space. From your app, you just launch the interpreter as a subprocess, for example python3 your_script.py
If your script is hardcoded and not changed later by the app, you can also turn it into a standalone executable, using a tool such as py2exe. But this is basically the same thing, the interpreter just ends up included in your script.",0.20408164,0.29092216,0.007541276980191469
59,"Question\ni'm doing k-mean clustering on an image (fruit tree image) with k=4 clusters. when i display 4 clusters seperately, fruits goes to cluster1, stem goes to cluster 2, leaves goes to clster3 and background goes to cluster4. i'm further interested in fruit clutser only. the probelm is when i change image to another fruit tree image, fruit cluster goes to cluster2 or sometimes to clsuter3 or 4. my wish is to not change the cluster for fruit, means if fruit is in cluster1 it should be in cluster1 in all images of fruit tree. how can i do that? 2ndly if its not possible i want to select that cluster automatically which contains fruit. how can i do that? thanks in advance.\nAnswer: K-means clustering is unsupervised, meaning the algorithm does not know any labels. That is why the clusters are assigned at random to the targets. You can use a heuristic evaluation of the fruit cluster to determine which one it is. For example, based on data about the pixels (color, location, etc), and then assign it a label by hand. In any case, this step will require human intervention of some sort.",0.40816328,0.14244425,0.07060660421848297
60,"Question\nI have a retail dataset that consists of uncleaned mobile phone numbers. I have data like this




Phone Number




03451000000


03451000001


03451010101


03451111111


03459999999


03459090909




Now there is a very high probability that the above phone numbers are fakely entered by cashier. The genuine number looks like this for example 03453485413.
There are two important things:

The length of the string is always fixed 11 characters
The phone number always starts with 03*********

Now how do I eliminate phone numbers based on the rule that, for example, character repetition of more than 5 times eliminated?\nAnswer: You should use regex to find such patters.
For example:
(\d)\1{4,}
This will match a digit and check if it repeats itself 4 more times. This is the case in examples 1, 2, 4 & 5
Another example is: (\d\d)\1{2,}
This will match 2 digits and checks if it repeats itself 2 more times. This is the case in examples 1, 3, 4, 5 & 6",0.0,0.30767655,0.09466486424207687
61,"Question\nI am trying to make a application that lets you pass in a discord webhook (string value) into a different.py file, then I want to make this.py file into a.exe file. I have no problem with making the applications gui, but I have no idea how I can pass in the webhook value, with converting it to a.exe file I also have no problem.
Does anyone know how I could pass in the webhook string permanently without using other file?\nAnswer: just open the file and write the variable lines to it.",-0.71428573,0.14820337,0.7438874840736389
62,"Question\nI need to simulate my vanet project that's coded with python. I want to use ns-3 or omnet++, and SUMO.
please help me, I don't know how to link my vanet project with the simulator (ns-3 or OMNET++).\nAnswer: Options here include pybind11. Check stackoverflow q and a on linking pybind11 and OMNET++.
Other option is using omnetpy. Check for GitHub project on omnetpy.
In both cases you need to learn OMNET++ first.",0.0,0.2889185,0.08347389847040176
63,"Question\nI have so far discovered four discovered width (or height) related methods and properties of widget (some for them also can be used to get other widget properties). What are the differences between all of them?

widget[""width""]
widget.cget(""width"")
widget.winfo_width()
widget.winfo_reqwidth()

(There are also equivalent methods and properties for height).\nAnswer: -the first one(widget[""width""]) is just a wrapping of some of the other commands, I can't tell you which one without decompiling the function.
-the second one(widget. cget(""width"")) is a method to access to variable width using the access to widget object through the getters methods of the object
-the third one(widget.winfo_width()) can be used to know the current width asking to TK's windows manager
-the fourth one (widget.winfo_reqwidth()) can tell you how much the width of the widget was originally when it was opened, or anyway before the widget.update_idletasks () method.
in general, we can say that they all do the same thing but, as you can guess, there are subtle differences that allow you to access the information you are looking for differently, in this case width",0.81632656,0.065797925,0.5632932186126709
64,"Question\nI just started learning how to code ( in python ) and i was wondering how can I randomly ask questions for a quiz without the answer that follows?
For example, I'd want the robot to ask 'What's the capital of France?' but without it saying 'Paris'?
questions = [(""What's the capital of France?"", ""Paris""), (""Who painted the Mona Lisa?"", ""Da Vinci"")]
Ty :)\nAnswer: random.choice will just return a tuple (since those are the items in your list). So you can access just the first element while printing by doing [0].
For example, print(random.choice(questions)[0]).
In the larger program you'd want to assign the tuple to a variable, so that later you fetch the answer for the same question (by using [1]) instead of randomly selecting again.",0.0,0.3811295,0.14525969326496124
65,"Question\nI have a long text. how to get only ""Brandshubs"" from below HTML?
output = Brandshubs3.7\nAnswer: Just use re.search('Brandshubs', s) or findall function in python re library.
But you will get the string Brandshubs always, that's not meaningful so I guess you want to check exists or count the times? For that you can check/count
the results of these functions' return directly",0.20408164,0.25792748,0.002899374347180128
66,"Question\nI have an existing django webapp, I have two model classes in my models.py for different functionalities in my app. I have used django allauth for all of the login/logout/social sign ins. Note: I have not used django rest framework at all so far in creating my app.
Now, I have to do the same for the android version of my webapp using Java. What exactly do I need to do right now to create the rest APIs and then connect them to the android app? please give some suggestions\nAnswer: yes you have to create new rest API for the android apps. Authentication will be token based for the rest API. storing tokens and retrieving data will be handled by the android app.
The stable authentication for Django is Simplejwt",0.0,0.19524652,0.03812120109796524
67,"Question\nI'm very new to python and programming in general, and I'm looking to make a discord bot that has a lot of hand-written chat lines to randomly pick from and send back to the user.  Making a really huge variable full of a list of sentences seems like a bad idea.  Is there a way that I can store the chatlines on a different file and have the bot pick from the lines in that file?  Or is there anything else that would be better, and how would I do it?\nAnswer: I'll interpret this question as ""how large a variable is too large"", to which the answer is pretty simple. A variable is too large when it becomes a problem. So, how can a variable become a problem? The big one is that the machien could possibly run out of memory, and an OOM killer (out-of-memory killer) or similiar will stop your program. How would you know if your variable is causing these issues? Pretty simple, your program crashes.
If the variable is static (with a size fully known at compile-time or prior to interpretation), you can calculate how much RAM it will take. (This is a bit finnicky with Python, so it might be easier to load it up at runtime and figure it out with a profiler.) If it's more than ~500 megabytes, you should be concerned. Over a gigabyte, and you'll probably want to reconsider your approach[^0]. So, what do you do then?
As suggested by @FishballNooodles, you can store your data line-by-line in a file and read the lines to an array. Unfortunately, the code they've provided still reads the entire thing into memory. If you use the code they're providing, you've got a few options, non-exhaustively listed below.

Consume a random number of newlines from the file when you need a line of text. You would look at one character at a time, compare it to \n, and read the line if you've encountered the requested number of newlines. This is O(n) worst case with respect to the number of lines in the file.

Rather than storing the text you need at a given index, store its location in a file. Then, you can seek to the location (which is probably O(1)), and read the text. This requires an O(n) construction cost at the start of the",0.0,0.24598217,0.06050722673535347
68,"Question\nI'm very new to python and programming in general, and I'm looking to make a discord bot that has a lot of hand-written chat lines to randomly pick from and send back to the user.  Making a really huge variable full of a list of sentences seems like a bad idea.  Is there a way that I can store the chatlines on a different file and have the bot pick from the lines in that file?  Or is there anything else that would be better, and how would I do it?\nAnswer: You can store your data in a file, supposedly named response.txt
and retrieve it in the discord bot file as open(""response.txt"").readlines()",0.0,0.22858596,0.05225154012441635
69,"Question\nI wish to use Django to create a Web app, but first I need to create an virtual environment.
Since I have a Windows system, I have used Win+R and cmd to open the teriminal, then the system shows C:\Users\HP>, I tried to create a datalog named learning_log and use terminal to switch to this datalog, so I typed in learning_log$ python -m venv 11_env, but the system shows 'learning_log$' is not recognized as an internal or external command,
operable program or batch file.
May I know how to create a datalog and how to create an virtual working environment?\nAnswer: First you need to install virtualenv library:

pip3 install virtualenv

After installing virtualenv, now create virtual environment directory with any name:

virtualenv myvenv

Now you can see, myvenv folder. now you can activate the virtual environment using that folder location:

.\myvenv\Scripts\activate.bat

Now you can see the virtual environment activated and you can see (myvenv) on your command prompt.",0.0,0.037190378,0.0013831241521984339
70,"Question\nHello I am new to python and I wanted to know how I can load an image from a directory on the computer in a html page using python?\nAnswer: Can you add more details to your question, please? It is unclear what is the aim here.",0.0,0.1034435,0.010700558312237263
71,"Question\nI need to segregate dashboards for different user roles, how can we do it in flask-login, a user can have a single role assigned to it.
And each user of the role will be assigned to a different entity and it needs to access only that.\nAnswer: Just define a many-to-many relationship between user and roles classes. Then using a decorator, check if role in user.roles",0.40816328,0.3037166,0.010909108445048332
72,"Question\nI want to define a 'Game' model in which exactly (and not more) 2 users will compete. Each of this user can compete in another game in parallel or later. I thought about a Manytomanyfield, but I don't know how to restrict the number of users. How to do it?\nAnswer: Just create a model with two user fields (player1, player2).",0.0,0.08432746,0.007111120503395796
73,"Question\nI am developing an app for Android and i have this issue - after i have taken a picture with the camera, i need to crop it. But the coordinates for the rectangle i have dragged over the capturing area start from the screen's 0,0 coordinates aka touch coordinates do not match actual picture's - if i try to crop the image with these using PIL, i get a partial result.
One possible solution would be to take a partial screenshot with these coordinates and get the cropped picture that way.
I tried to use pyscreenshot, but then i found out that it does not work on Android.
Any ideas how to capture a partial screenshot on Kivy?
Thank you\nAnswer: For possible anyone with the same issue - i did not find any good way to grab a screenshot and crop selected portion out of it. I did, however, solve it this way - i move to a new screen, resize picture to device's screen resolution and from then i can crop it with touch without no issues - i did not have to find a clever way to map screen coordinates with image's. After cropping and saving i resize the image back to a fixed resolution. This is not a elegant solution, but still will serve my purpose.
Thank you anyone who thought along with me.",0.0,0.11219317,0.012587307021021843
74,"Question\nI want to schedule a spider from scrapyd-client by giving the command line arguments as well.
e.g: scrapy crawl spider_name -a person=""John"" -a location=""porto"" -o local.csv
Above command works well when running spider directly from scrapy, but it does not work when running it from rest API using scrapyd-client.
Basically the question is how to send scrapy's command line arguments like (-a, -o) in scrapyd-client?\nAnswer: Use the flag -d instead of the -a flag",0.0,0.37964106,0.14412732422351837
75,"Question\nMy Python Notebooks log some data into stdout and when I run this notebooks via UI I can see outputs inside cell.
Now I am running Python Notebooks on Databricks via API (/2.1/jobs/create and then /2.1/jobs/run-now) and would like to get output. I tried both /2.1/jobs/runs/get and /2.1/jobs/runs/get-output however none of the includes stdout of the Notebook.
Is there any way to access stdour of the Notebook via API?
P.S. I am aware of dbutils.notebook.exit() and will use it, if it will not be possible to get stdout.\nAnswer: Yes, it is impossible to get the default return from python code. I saw that on a different cloud provider you can get an output from logs also.
100% solution it is using dbutils.notebook.exit()",0.40816328,0.17404795,0.05480998754501343
76,"Question\nError is: Import ""brownie"" could not be resolvedPylance
I know there are other SO posts that refer to this, but it seems most of them are talking about booting up a new env and installing x package into that virtual env.
However with Brownie, I'm especially confused because the brownie docs say:

pipx installs Brownie into a virtual environment and makes it available directly from the commandline. Once installed, you will never have to activate a virtual environment prior to using Brownie.

I don't want to mess with the virtual env that brownie uses.
Anyways, my code runs fine and the command line tells me that brownie is installed.It's just that this warning is really annoying me. Can anyone tell me how to clear it up? Thanks!\nAnswer: It's happening because we install python with pipx instead of pip. pylance looks in the location our pip files are generally stored, and doesn't see brownie since we installed with pipx (which installed to it's on isolated virtual environment). So you have a few options:

Ignore it
Install brownie with pip in a virtual environment (not recommended)

If there is another suggestion, happy to hear it",0.5442177,0.04236257,0.25185856223106384
77,"Question\nError is: Import ""brownie"" could not be resolvedPylance
I know there are other SO posts that refer to this, but it seems most of them are talking about booting up a new env and installing x package into that virtual env.
However with Brownie, I'm especially confused because the brownie docs say:

pipx installs Brownie into a virtual environment and makes it available directly from the commandline. Once installed, you will never have to activate a virtual environment prior to using Brownie.

I don't want to mess with the virtual env that brownie uses.
Anyways, my code runs fine and the command line tells me that brownie is installed.It's just that this warning is really annoying me. Can anyone tell me how to clear it up? Thanks!\nAnswer: open command pallete cmd+shift+P (on mac)
type python select
click Python: Select Interpreter


Enter interpreter path
~/.local/pipx/venvs/eth-brownie/bin/python


this works for me.",0.6802721,0.15519667,0.27570420503616333
78,"Question\nError is: Import ""brownie"" could not be resolvedPylance
I know there are other SO posts that refer to this, but it seems most of them are talking about booting up a new env and installing x package into that virtual env.
However with Brownie, I'm especially confused because the brownie docs say:

pipx installs Brownie into a virtual environment and makes it available directly from the commandline. Once installed, you will never have to activate a virtual environment prior to using Brownie.

I don't want to mess with the virtual env that brownie uses.
Anyways, my code runs fine and the command line tells me that brownie is installed.It's just that this warning is really annoying me. Can anyone tell me how to clear it up? Thanks!\nAnswer: for Windows:

Ctrl+Shift+P
Type python select
click Python: Select Interpreter
Enter interpreter path ~/.local/pipx/venvs/eth-brownie/Scripts/python
You can also click ""Find"" to search your files for the path if manually inputting it isn't working

This is an addition to @Ali Sıtkı Aslantaş's answer",0.27210885,0.35272932,0.006499659735709429
79,"Question\nI am working on a C++ project which will be compiled to a shared object file. It needs to have access to the python interpreter, which is where I reach my hurdle: upon linking, I receive /usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/9/crtbeginT.o: relocation R_X86_64_32 against hidden symbol '__TMC_END__' can not be used when making a shared object
(The opening quotation on __TMC_END__ was a backtick, but this ruined the formatting so I changed it, if this is useful information).
The obvious solution is to download the Python source and compile it along with my code, but I would rather not complicate my development workflow to this point, thus, my question is: how do I embed the Python interpreter in my shared object file?
I am on Ubuntu 20.04 with a full Python 3.9 install, and access to all development files.
(My apologies if this is the wrong site)\nAnswer: It appears that my problem was solved by adding the python lib to the g++ shared-object-builder command. I previously only added it to the first phase of compilation (convert into an object file).",0.0,0.18154722,0.03295939415693283
80,"Question\nSo I am doing a face recognition project in a Raspberry Pi. It is a django project and when I run the server I get this error: AttributeError: module 'cv2' has no attribute 'face'
I searched for the error and I came up with that I needed to install opencv-contrib-python
The problem is that when I try to install it the download gets to 99% and I get this: 'pip3 install opencv-contrib-pyt…' terminated by signal SIGKILL (Forced quit).
Does anyone know why this happens? how can I fix this? help is much appreciated\nAnswer: I got the same error and fixed it like this.
Try:

sudo apt install python-opencv libopencv-dev

y/n --> y

sudo apt install python python-pip

y/n --> y

pip install opencv-python",-0.71428573,0.3980477,1.2372856140136719
81,"Question\nHow do you take a screenshot of a particular widget? I keep on getting ""how to take screenshot of whole screen"" or ""how to take screenshot of the window"" when searching, but I want to know how to take screenshot of a widget, a Frame, to be exact. I'm afraid that this forum would require me to give them a code snippet, when in reality I don't know what to do. So I'm just asking if this is possible, and how to do it.
PS: I would really appreciate if it is possible to just give a function the widget variable, and then it would go searching for that widget and then take a precise screenshot of it. I know that there is pyautogui which searches for images, I just don't know what I need to do exactly, since this frame isn't an image and it always changes from now and then, so the automation might get confused.\nAnswer: Maybe you can get the coords of that frame and then make a screenshot from that coords. This should work",0.0,0.2998722,0.08992332965135574
82,"Question\nI develop a custom python library that I put in S3 bucket, and now I want to use Zeppelin with pyspark interpreter to interact with it. However, I can't find a way to do it. Anybody knows how to do so?
Things that I have tried:

In glue it is possible to include external python library in S3 by specifying 'Python lib path', which makes me think that in Zeppelin it is possible
There are methods such as using %dep interpreter but it is only for JAR library, while I want to load python library

Any suggestion is appreciated\nAnswer: Never mind I find the answer
When creating the dev endpoint you can actually specify 'Python lib path' the same way as you specify it at the glue job",0.0,0.19655502,0.03863387554883957
83,"Question\nI want to send requests to a deployed app on a cloud run with python, but inside the test file, I don't want to hardcode the endpoint; how can I get the URL of the deployed app with python script inside the test file so that I can send requests to that URL?\nAnswer: You can use gcloud to fetch the url of the service like this

gcloud run services describe  SERVICE_NAME
--format=""value(status.url)""",0.40816328,0.31088996,0.009462098591029644
84,"Question\nI am using fido2 python package and I would like to know how to generate an EC pair (ES256)
public and private key.
and also how to sign a challenge using the private key
so it'll be possible to verify it with the public key
Thanks\nAnswer: The Web Authentication protocol (and FIDO2 CTAP2 protocol built on top of it) have a challenge/response protocol against a device representing authentication of the user called an Authenticator.
The fido2 python library is meant to be used to talk to the authenticator, not to emulate an authenticator itself. The role that talks to the authenticator is called a Relying Party.
Systems typically further divide the Relying Party role into client and server roles - the client communicates with the authenticator, but really it is relaying the communication to and from the server. In WebAuthn, the browser, the site javascript it is running, and any underlying platform support are all considered part of the client. If you have native code talking USB or NFC to an authenticator (on platforms which let you), that native code is considered an authenticator.
The underlying authentication challenge does not have a cryptographic signature from the relying party. Instead, the cryptographic signature is made by the authenticator - the authenticator generates a new key pair on registration, and then supplies a signature from that key to prove possession and thus prove authentication. Since fido2 does not have authenticator support, it has no need to generate key pairs (outside of potential test code).
Note that this gets to an essential of the underlying WebAuthn and FIDO2 platform trust model - the user must trust the client. For this reason, several platforms have locked out low-level access to authenticator hardware (USB, NFC and BLE communication to hardware) and instead provide system API. Native applications must have entitlements to operate on behalf of a particular web origin as a WebAuthn client, and browsers must request special entitlements from the platform in order to represent all web domains.
This does not affect usage of fido2 for implementing server functionality, but I'd advise you to double-check platform support if you plan to use it to implement any client functionality.",-0.35714287,0.39044237,0.5588836669921875
85,"Question\nI'm trying to update a dynamodb table having some million of rows. Tried updating it with a lambda function but it eventually times out after 15mins. I want to update a single attribute value for all the records present in the table with an efficient way. Can someone suggest how to achieve this?\nAnswer: This is just my 5 cents, and it's probably not a official, perfect, or even great way of doing things. But here we go:

Create a SQS queue
Have a lambda listen to messages on that queue
Run the lambda, have it read the first chunk of items from the database.
From the query you get a LastEvaluatedKey, store this in a separate dynamodb item/table
Put the LastEvaluatedKey to the queue
When the Lambda is invoked again, use the LastEvaluatedKey passed in from the queue and pass it to the ExclusiveStartKey  in your query, so that your query returns the next items from DynamoDB that you haven't processed yet

This setup means your lambda will only process small chunks at a time. It also means that when things go wrong, you have your item in DynamoDB with the LastEvaluatedKey stored, which allows you to continue from where you left off.
Hopefully this was useful, good luck :)",0.40816328,0.3284418,0.006355514284223318
86,"Question\nI have created a KVM in Apigee with the name ""test_kvm"" and the key is ""name"" with value ""myname"".
So basically it's like {""name"": ""myname""} and the value is encrypted. In KVM policy also I am using private prefix <Get assignTo private.name>...</>
Now when I am trying to fetch this variable into my python script using PythonScript policy using the script getname = flow.getVariable(""private.name"") this value is getting populated by ""None"". and not by value ""myname"".
Why am I getting None and how to properly fetch the Key value from KVM in Python?\nAnswer: Got it, the thing is we have to deploy kvm in the same environment that we are using to deploy our api proxy.",0.0,0.23731881,0.05632022023200989
87,"Question\nI am trying to write a program that will take msg files and extract data fields from the msg files such as to, cc, bcc, subject, date/time sent etc. Using the Python library extract_msg I have successfully done this.
What I now need to do is add the functionality to extract individual emails from PST files at bulk into individual MSG files.
I've looked around for a python library that will easily achieve this but I am struggling to find anything. Does anyone have any good suggestions on how I might do this?\nAnswer: What I now need to do is add the functionality to extract individual emails from PST files at bulk into individual MSG files.",0.0,0.41987002,0.1762908399105072
88,"Question\nI need to write a python program to create a table and add columns to it but the name and the quantity of tables and its columns will we user defined means I don't know the name and quantity, it will all be taken as an input from user.
any idea how to do it?\nAnswer: I think using the sqllite3 package it is possible. All you need to do is run the queries with a connection to your database.",0.0,0.086467505,0.007476629223674536
89,"Question\nthere is a task: you need to write a registration menu program. it is necessary to do this necessarily on pygame. no other way. how can this be implemented at all? as I know, pygame does not support multi-window. how to do it then?\nAnswer: Instead of making multiple windows, you could make a single window and then create 2 functions, 1 for the game and the other for registration. Then at start, you could call the registration function and then, the game function. You have not given any code or anything, so we can't help you a lot...",0.0,0.031775028,0.001009652391076088
90,"Question\nI'm not very familiar with pytest but try to incorporate it into my project. I already have some tests and understand main ideas.
But I got stuck with test for Excel output. I have a function that makes a report and saves it in Excel file (I use xlsxwriter to save in Excel format). It has some merged cells, different fonts and colors, but first of all I would like to be sure that values in cells are correct.
I would like to have a test that will automatically check content of this file to be sure that function logic isn't broken.
I'm not sure that binary comparison of generated excel file to the correct sample is a good idea (as excel format is rather complex and minor change of xlsxwriter library may make files completely different).
So, I seek an advice how to implement this kind of test. Had someone similar experience? May you give advice?\nAnswer: IMHO a unit test should not touch external things (like file system, database, or network). If your test does this, it is an integration test. These usually run much slower and tend to be brittle because of the external resources.
That said, you have 2 options: unit test it, mocking the xls writing or integration test it, reading the xls file again after writing.
When you mock the xlswriter, you can have your mock check that it receives what should be written. This assumes that you don't want to test the actual xlswriter, which makes sense cause it's not your code, and you usually just test your own code. This makes for a fast test.
In the other scenario you could open the excel file with xslsreader and compare the written file to what is expected. This is probably best if you can avoid the file system and write the xls data to a memory buffer from which you can read again. If you can't do that, try using a tempdir for your test, but with that you're already getting into integration test land. This makes for a slower, more complicated, but also more thorough test.
Personally, I'd write one integration test to see that it works in general, and then a lot of unit tests for the different things you want to write.",0.0,0.14120156,0.019937878474593163
91,"Question\nI have 2 python files main.py and  test.py when I run main.py I want to run test.py and some point of time in new terminal because if I run in same terminal main.py got crashed and closed and program fails.
Any ideas how can I do this.\nAnswer: Main.py must call other files to run them",0.0,-0.04991722,0.0024917288683354855
92,"Question\nI have 2 python files main.py and  test.py when I run main.py I want to run test.py and some point of time in new terminal because if I run in same terminal main.py got crashed and closed and program fails.
Any ideas how can I do this.\nAnswer: I cannot say for sure without reading the code whether it will work, but here's the steps to run two pieces of code simultaneously:

Open File Explorer, and navigate to where you have saved the files.
Write cmd in the address bar of the file explorer.
When the prompt opens up, write the python command that you use to run the first file: py <filename>.py or python <filename.py> or python3 <filename.py>.
Repeat process 2 and 3, but this time, type the name of the second file.",0.0,0.23958027,0.057398706674575806
93,"Question\nI have not managed to install pyserial for use with Spyder 3.9 (launched from Anaconda.Navigator) in Windows 10.
I tried <conda install -c conda-forge pyserial> from the Windows command terminal and some files were installed, but import pyserial from the Spyder console gives the response <ModuleNotFoundError: No module named 'pyserial'>.
Then I tried to install it again from the Spyder console with pip install pyserial, which gave the response <Requirement already satisfied: pyserial in c:\users\father\anaconda3\lib\site-packages (3.4)>
Then I tried pip uninstall pyserial, which locked up the Spyder console altogether.
I'm afraid it looks a bit of a mess.  Can anybody suggest how to clear it up and make pyserial available?\nAnswer: You should juste write import serial instead of import pyserial.",0.0,0.2189768,0.04795083776116371
94,"Question\nFirstly I guess this question might be duplicated, but I couldn't find it.
Question. It's actually in the title.

Can I get Essential Matrix from Fundamental matrix with few matching keypoints, but without intrinsic parameters?

Situation. I am trying to find Essential matrix. I use my phone camera to take photos, and then extract keypoints using SIFT(or ORB). I have 2 images of an object and it's matching points. I could get F, Fundamental Matrix but I have no idea how to get Essential Matrix from this.
I don't have camera intrinsic parameters, such as Fx, Fy, Cx, Cy.
I am stuck to this situation. I googled but couldn't get answers.
Let me know if it's duplicated then I'd delete this.
PLUS: I also don't have camera coordinates or world coordinates.\nAnswer: No you can't, you need to know the metric size of the pixel and the focal because essential matrix is in the real world.",0.0,0.14582247,0.02126419171690941
95,"Question\nHi I've a Lambda function treated as a webhook. The webhook may be called multiple time simultaneously with same data. In the lambda function, I check if the transaction record is present in the DynamoDB or not. If it's present in db the Lambda simply returns otherwise it execute further. The problem arises here that when checking if a record in db the Lambda get called again and that check fails because the previous transaction still not inserted in db. and transaction can get executed multiple times.
My question is how to handle this situation. will SQS be helpful in this situation?\nAnswer: "" if it's present in db the lambda simply return otherwise it execute further"" given that, is it possible to use FIFO queue and use some ""key"" from the data as deduplication id (fifo) and that would mean all duplicate messages would never make it to your logic and then you would also need
dynamodb's ""strongly consistent"" option.",0.0,0.21930838,0.04809616506099701
96,"Question\nPython beginner here. I would like to use Spyder as my Python IDE. Alas the standalone version does not include pip and I want to work with ""Vanilla Python"" rather than Anaconda. So I installed Spyder via
pip install spyder, which works fine. However, when running spyder3 in the command window, nothing happens. I get no error, but Spyder does not launch either. While the Spyder website says custom installation may be tricky, it does not provide a guide on how to get it done. Does someone know how? OS is Windows 10.\nAnswer: Update: Tried again with Python 3.10.4. ""spyder"" now exists in the Scripts folder and does launch when typing spyder in cmd. Works for me now.",0.0,0.14060217,0.0197689700871706
97,"Question\nI recently uninstalled Anaconda Navigator from C Drive and re-installed in D Drive, However each time I open Navigator and try to launch “Jupyterlab” or Notebook etc it only gives the option to ""install ""
After clicking install it first goes to C/ProgramData/Anaconda3 and says conda environment not found and only after clicking okay does it start looking in D/Anaconda where the actual packages/files are present.And this happens every time you close and relaunch the application
So how do I resolve this? … i have searched and there aren’t any files left from the old install anymore but maybe there are some hidden files somewhere causing this.
Also I kept the add 'Anaconda to PATH "" unchecked as per installation recommendations, Could that be an issue?\nAnswer: It is always good practice to keep all the software installations in C drive if you don't want to do extra path correction related work.
You have not updated the system environment path probably. There are residuals of anaconda pkg even after uninstallation. Remove the residual folder from there (C:\anaconda3). And update the environment path (Update the location from C:\anaconda3\... to D:\anaconda3\...).
Anaconda navigator issue should also get resolved after that.",0.0,0.13427371,0.018029429018497467
98,"Question\nWould there be “dilution” of accuracy if I train the same text classification model with multiple training datasets? For example, my end users would be providing (uploading) their own tagged CSVs to train the model and use the trained model in the future. The contexts of datasets would be different - L&D, Technology, Customer Support, etc.
If yes, how do I have a “separate instance or model” for each user?
I am using Python and would possibly use Gradio or Streamlit as the UI. Open to advice.\nAnswer: I ended up using huggingface's zero-shot classification.",0.40816328,0.08375436,0.10524114966392517
99,"Question\nWhen making a drawing in pygame, for example the rectangle, how do I make my other sprites show up ontop of the rectangle? Currently they are underneath it\nAnswer: when you draw all the objects, sprites should be last drawn on screen.
So first rectangles will be draw and sprites on top",0.0,-0.11918026,0.01420393493026495
0,"Question\nFirst off, I might have formulated the question inaccurately, feel free to modify if necessary.
Although I am quite new to docker and all its stuff, yet somehow managed to create an image (v2) and a container (cont) on my Win 11 laptop. And I have a demo.py which requires an.mp4 file as an arg.
Now, if I want to run the demo.py file, 1) I go to the project's folder (where demo.py lives), 2) open cmd and 3) run: docker start -i cont. This starts the container as:
root:834b2342e24c:/project#
Then, I should copy 4) my_video.mp4 from local project folder to container's project/data folder (with another cmd) as follows:
docker cp data/my_video.mp4 cont:project/data/.
Then I run:  5) python demo.py data/my_video.mp4. After a while, it makes two files: my_video_demo.mp4 and my_video.json in the data folder in the container. Similarly, I should copy them back to my local project folder: 6)
docker cp cont:project/data/my_video_demo.mp4 data/, docker cp cont:project/data/my_video_demo.json data/
Only then I can go to my local project/data folder and inspect the files.
I want to be able to just run a particular command that does 4) - 6) all in one.
I have read about -v   option where, in my case, it would be(?) -v /data:project/data, but I don't know how to proceed.
Is it possible to do what I want? If everything is clear, I hope to get your support. Thank you.\nAnswer: Well, I think I've come up with a way of dealing with it.
I have learned that with -v one can create a place that is shared between the local host and the container. All you need to do is that run the docker and provide -v as follows:
docker run --gpus all -it -v C:/Workspace/project/data:/project/data v2:latest python demo.py data/my_video_demo.mp4
--gpus - GPU devices",0.0,0.6229651,0.3880855143070221
1,"Question\nHi how can I get an element by attribute and the attribute value in Python Selenium?
For example I have class=""class1 class2 class3"".
Now I want to get the element with the attribute class what ca.rries the classes ""class1 class2 class3"".
Is this possible?
If I use xpath, I always need to add the element type, input, option,...
I try to avoid the element type since it varies sometimes.\nAnswer: The CSS selectors would be formatted like this:
'[attribute]'
'[attribute=""value""]'
For example, the selector for the input field on google.com would be:
'input[name=""q""]'",0.0,0.117518544,0.01381060853600502
2,"Question\nI am trying to find a way to disable debugger going into built-in functions. Is there a way to accomplish this? For some reason, I do not see a tab for the debugger in the preferences-> iPython console. Should there be one and how can I enable this? Thanks
Arun\nAnswer: Resolved. Downloaded the latest version of Spyder and it has this functionality. It appears my older version did not have this feature.",0.40816328,0.25149524,0.02454487420618534
3,"Question\nI have a simple python script that I would like to run thousands of it's instances on GCP (at the same time). This script is triggered by the $Universe scheduler, something like ""python main.py --date '2022_01'"".
What architecture and technology I have to use to achieve this.
PS: I cannot drop $Universe but I'm not against suggestions to use another technologies.
My solution:

I already have a $Universe server running all the time.
Create Pub/Sub topic
Create permanent Compute Engine that listen to Pub/Sub all the time
$Universe send thousand of events to Pub/Sub
Compute engine trigger the creation of a Python Docker Image on another Compute Engine
Scale the creation of the Docker images (I don't know how to do it)

Is it a good architecture?
How to scale this kind of process?
Thank you :)\nAnswer: It might be very difficult to discuss architecture and design questions, as they usually are heavy dependent on the context, scope, functional and non functional requirements, cost, available skills and knowledge and so on...
Personally I would prefer to stay with entirely server-less approach if possible.
For example, use a Cloud Scheduler (server less cron jobs), which sends messages to a Pub/Sub topic, on the other side of which there is a Cloud Function (or something else), which is triggered by the message.
Should it be a Cloud Function, or something else, what and how should it do - depends on you case.",0.40816328,0.3214233,0.007523825392127037
4,"Question\nI have a simple python script that I would like to run thousands of it's instances on GCP (at the same time). This script is triggered by the $Universe scheduler, something like ""python main.py --date '2022_01'"".
What architecture and technology I have to use to achieve this.
PS: I cannot drop $Universe but I'm not against suggestions to use another technologies.
My solution:

I already have a $Universe server running all the time.
Create Pub/Sub topic
Create permanent Compute Engine that listen to Pub/Sub all the time
$Universe send thousand of events to Pub/Sub
Compute engine trigger the creation of a Python Docker Image on another Compute Engine
Scale the creation of the Docker images (I don't know how to do it)

Is it a good architecture?
How to scale this kind of process?
Thank you :)\nAnswer: As I understand, you will have a lot of simultaneous call on a custom python code trigger by an orchestrator ($Universe) and you want it on GCP platform.
Like @al-dann, I would go to serverless approach in order to reduce the cost.
As I also understand, pub sub seems to be not necessary, you will could easily trigger the function from any HTTP call and will avoid Pub Sub.
PubSub is necessary only to have some guarantee (at least once processing), but you can have the same behaviour if the $Universe validate the http request for every call (look at http response code & body and retry if not match the expected result).
If you want to have exactly once processing, you will need more tooling, you are close to event streaming (that could be a good use case as I also understand). In that case in a full GCP, I will go to pub / sub & Dataflow that can guarantee exactly once, or Kafka & Kafka Streams or Flink.
If at least once processing is fine for you, I will go http version that will be simple to maintain I think. You will have 3 serverless options for that case :

App engine standard: scale to 0, pay for the cpu usage, can be more affordable than below function if the request is constrain to short period (few hours per day since the same hardware will process many request)
Cloud Function: you will pay per request(+ cpu, memory, network",0.40816328,0.10872811,0.08966141939163208
5,"Question\nScenario: Lets say I have a REST API written in Python (using Flask maybe) that has a global variable stored. The API has two endpoints, one that reads the variable and returns it and the other one that writes it. Now, I have two clients that at the same time call both endpoints (one the read, one the write).
I know that in Python multiple threads will not actually run concurrently (due to the GIL), but there are some I/O operations that behave as asynchronously, would this scenario cause any conflict? And how does it behave, I'm assuming that the request that ""wins the race"" will hold the other request (is that right)?\nAnswer: In short: You should overthink your rest api design and implement some kind of fifo queue.
You have to endpoints (W for writing and R for reading). Lets say the global variable has some value V0 in the beginning. If the clients A reads from R while at the same time client B writes to W. Two things can happen.

The read request is faster. Client A will read V0.
The write request is faster. Client A will read V1.

You won't run into an inconsistent memory state due to the GIL you mentioned, but which of the cases from above happens, is completely unpredictable. One time the read request could be slightly faster and the other time the write request could be slightly faster. Much of the request handling is done in your operating system (e.g. address resolution or TCP connection management). Also the requests may traverse other machines like routers or switches in you network. All these things are completly out of your control and could delay the read request slightly more than the write request or the other way around. So it does not matter with how many threads you run your REST server, the return value is almost unpredictable.
If you really need ordered read write interaction, you can make the resource a fifo queue. So each time any client reads, it will pop the first element from the queue. Each time any client writes it will push that element to the end of the queue. If you do this, you are guaranteed to not lose any data due to overwriting and also you read the data in the same order that it is written.",0.40816328,0.38167226,0.0007017739117145538
6,"Question\nI need to implement payment with credit card in a Django Project. But, I don't know how to start, and I can't find any information about do it.

What's the best module to do the integration?
The module is well documented? and it has code examples about the correct implementation.

Please, if you know how to do it, help me. I'm collaborating with an ONG and we need to implement this for donations payments.
Thanks. Regards,\nAnswer: You need a payment provider. A lot of that. The most popular worldwide are Paypal, Stripe, Braintree, others. You can find local providers if you're outside the USA.
You need to check the API documentation of the selected provider. A lot of them have public API docs and links you can find on the landing pages. Usually, this is REST API, so you can use the Requests library to make the integration. Don't forget to write integrational tests!
You need to be compliant with the rules and policies of the selected provider.
You need to have a bank account to make a settlement from the payment provider.
Most providers have a sandbox to test API without having a real production-approved account.

When I said, ""You need..."" I mean you or your company needs to provide such information or create a needed account(s), bank accounts, etc.",0.0,0.27350527,0.0748051330447197
7,"Question\nI'm trying to deploy my Django 4.0.1 application to Google App Engine. But I'm receiving an error:

Could not find a version that satisfies the requirement Django==4.0.1.

On a localhost the app works fine. The same error I have with asgiref==3.5.0
The full text of this error:

ERROR: Could not find a version that satisfies the requirement
Django==4.0.1 (from -r requirements.txt (line 6)) (from versions:
1.1.3, 1.1.4, 1.2, 1.2.1, 1.2.2, 1.2.3, 1.2.4,
1.2.5, 1.2.6, 1.2.7, 1.3, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.3.6, 1.3.7, 1.4, 1.4.1, 1.4.2, 1.4.3, 1.4.4, 1.4.5, 1.4.6, 1.4.7, 1.4.8, 1.4.9, 1.4.10, 1.4.11, 1.4.12, 1.4.13, 1.4.14,
1.4.15, 1.4.16, 1.4.17, 1.4.18, 1.4.19, 1.4.20, 1.4.21, 1.4.22, 1.5, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.5.5, 1.5.6, 1.5.7, 1.5.8, 1.5.9, 1.5.10, 1.5.11, 1.5.12, 1.6, 1.6.1, 1.6.2, 1.6.3, 1.
6.4, 1.6.5, 1.6.6, 1.6.7, 1.6.8, 1.6.9, 1.6.10, 1.6.11, 1.",0.81632656,0.90703964,0.008228863589465618
8,"Question\nI am testing a React Native app using Appium in Python. This is the first time seeing that the app responds too slow with Appium but works completely fine while testing manually. Any idea on how to improve speed of tests?\nAnswer: One way to improve test speed is use a modern device. For your issue, because the app runs well without also running appium doesn't mean the device is not the culprit. Appium does run on the device while also running the app, so the device needs to be able to run both the app and other apps at the same time well to be able to run smooth appium tests.",0.0,0.3228361,0.10422314703464508
9,"Question\nI have a network diagram that is sketched in Visio. I would like to use it as an input for the networkx graph from node2vec python package. The documentation says that there is a function called to_networkx_graph() that takes, as its input, the following types of data:
""any NetworkX graph dict-of-dicts dict-of-lists container (e.g. set, list, tuple) of edges iterator (e.g. itertools.chain) that produces edges generator of edges Pandas DataFrame (row per edge) numpy matrix numpy ndarray scipy sparse matrix pygraphviz agraph""
But, still, not mentioning other formats like Visio, pdf, odg, PowerPoint, etc.
So, how to proceed?\nAnswer: I think you need to create some data in the format referred to in the documentation, not just a network diagram. A Visio diagram will not do the job and I know of no way to do a conversion.",0.81632656,0.14168525,0.4551408886909485
10,"Question\nI am using python 3.9.2 and pip 21.0.1. I've created a virtual environment. In that I installed flask 1.1.2 and Werkzeug 1.0.0.
I tried to install coolname==1.1.0 and wtforms_components==0.10.5, it says successfully installed but it is not importing files.
from coolname I want generate_slug package
and from wtforms_components, I want TimeField
What I have done to install them:
pip install WTForms-Components==0.10.5
pip install coolname
Can you please tell me how can I get these packages?\nAnswer: You might be using the global interpreter instead of your virtual environment.
You need to check that your IDE's interpreter is configured to that virtual environment you created. if you want to run it from the command line you first need to activate it with env_name\Scripts\activate and then run it.",0.40816328,0.07172942,0.11318773776292801
11,"Question\nHow would I go about using BitBlt in python? I know how to use PatBlt, but I was wondering how you use BitBlt. How would I put a image on the GDI?\nAnswer: If you already know how to use PatBlt then you are most of the way there.
BitBlt just transfers pixels from one DC to another. Often one DC has a bitmap selected into it and the other DC is the screen. It can be used to take screen shots, display pictures or do double buffering.
Let's say you have a bitmap loaded from LoadImage and you want to display it in WM_PAINT.

Call BeginPaint to get a DC for your window (the destination).
Call CreateCompatibleDC to create a source DC that is compatible with the destination.
Call SelectObject to select the bitmap (HBITMAP) into the source DC. You should store the return value so it can be restored during cleanup.
Call BitBlt(destination DC,..., source DC,...., SRCCOPY) to copy the bitmap pixels to the window.
Call SelectObject to restore the original value.
Call DeleteDC to destroy the source DC.
Call EndPaint.",0.0,0.33734822,0.11380382627248764
12,"Question\nI'm new to python and I've been seeing import x as _x. I would like to know how this is different from import x. Thanks!\nAnswer: Writing import x simply imports the module. When using any class or function from the module x, you have to define the module name like x.foo(). Adding the as _x makes _x an alias of the imported module's name. So, instead of x.foo() you can now write _x.foo(). Note that with the alias, you cannot write x.foo().
This helps shortening the module names in cases of large module names such as matplotlib.pyplot or scipy.signal etc. It's certainly much easier to write and read plt.plot() than matplotlib.pyplot.plot().",0.20408164,0.3622514,0.025017673149704933
13,"Question\nI've been coding with R for quite a while but I want to start learning and using python more for its machine learning applications. However, I'm quite confused as to how to properly install packages and set up the whole working environment. Unlike R where I suppose most people just use RStudio and directly install packages with install.packages(), there seems to be a variety of ways this can be done in python, including pip install conda install and there is also the issue of doing it in the command prompt or one of the IDEs. I've downloaded python 3.8.5 and anaconda3 and some of my most burning questions right now are:

When to use which command for installing packages? (and also should I always do it in the command prompt aka cmd on windows instead of inside jupyter notebook)
How to navigate the cmd syntax/coding (for example the python documentation for installing packages has this piece of code: py -m pip install ""SomeProject"" but I am completely unfamiliar with this syntax and how to use it - so in the long run do I also have to learn what goes on in the command prompt or does most of the operations occur in the IDE and I mostly don't have to touch the cmd?)
How to set up a working directory of sorts (like setwd() in R) such that my.ipynb files can be saved to other directories or even better if I can just directly start my IDE from another file destination?

I've tried looking at some online resources but they mostly deal with coding basics and the python language instead of these technical aspects of the set up, so I would greatly appreciate some advice on how to navigate and set up the python working environment in general. Thanks a lot!\nAnswer: Going over the technical differences of conda and pip:
So Conda is a packaging tool and installer that aims to do more than what pip does; handle library dependencies outside of the Python packages as well as the Python packages themselves. Both have many similar functionalities as well, you can install packages or create virtual environments with both.
It is generally advisable to generally have both conda and pip installed since there are some packages which might not be available with conda but with pip and vice versa.
The commands to install in both the ways is easy enough, but one thing to keep in mind is that

conda stores packages in the anaconda/pkgs directory
pip stores it in directory",0.13605443,0.46995693,0.1114908829331398
14,"Question\nI'm working on a project where we are using vanilla javascript, Html and css for frontend while designing the backend in the FastAPI. The way backend connects with the frontend is using Templates, by adding the file name for a particular page and also sending the data as jinja template back to the client side. We have five pages and all are declared in the backend as get requests and we are using a specific html file for loading a particular page. So, there are mainly five html files for five pages
Now, I was deciding to switch to reactjs as the frontend, however, react being the single page application and also just having the Routes for multiple pages, I'm wondering how to I connect it to the backend and also send data back to the client, the way I was doing it before. I tried to look for answers, couldn't find one as of now...
And also I'm kind of new to Reactjs\nAnswer: React, Python, and FastAPI are all great technologies that I too use. But they work more effectively when used for their intended purposes. A hammer is a great tool if you need to pound in some nails, but it's a lousy tool for other purposes such as trying to use it as an eating utensil. So you'd be well advised, if you are going to use React and FastAPI, to use them as they were intended.
Jinja templates are used for server side rendering. But React is (in essence) also a server side rendering system. Both of them will not play nicely together, at least not without more work and complexity.
Instead you'd be better off committing to either the Jinja templating or React. If you choose Jinja, you figure out what the page will be (render) before sending it off to the client. If you choose React, you write the user-visible application, send that off to the client, and then the application on the client's browser fetches data through (usually JSON) APIs from the FastAPI back end.",0.0,0.39054507,0.15252545475959778
15,"Question\nI use iTerm2 as my default terminal app in OSX. A would like to ask if is possible to insert text automatically with a hotkey (or selecting it from a menu), with iTerm, like the old macros of some word processing packages. My idea is, for example, if I press cmd + ctrl s, iTerm insert automatically ""sftp -i""
I know that iTerm has support for scripting with AppleScript and Python  but I'm not sure how I do this\nAnswer: You can also use Keyboard shortcuts.
From Preferences -> Keys
Define a new shortcut and use the ""Action"" ""Send Text""
This works on iTerm2 build  3.4.15",0.0,0.17003077,0.02891046367585659
16,"Question\nI have to analyze a set of bgp-update-files using python and the pybgpstream with a given routing table file. My task is to analyze these update files regarding prefix hijacking events. As far as I know, analyzing these files means to look for all ASs that advertise prefixes that do not belong to them and list those events.
My current code just allows me to go through the directory and check all update files for prefixes and as-paths. Since I do not know how to use the routing table file (ground truth) in pybgpstream, i cannot go any further into analyzing the prefix ownership.
Has anybody a idea, how to check, whether a prefix belongs to a specified AS?\nAnswer: Think about what part of AS-Path called origin. Fix research results:

The last AS along the path to the prefix is considered to be the
origin AS
[https://www.cs.colostate.edu/~massey/pubs/conf/massey_imw01.pdf]

Little hint try to use scientific search engines additionaly to google ;)",0.0,0.17577976,0.030898524448275566
17,"Question\nI am working on a project the backend is in python Django and frontend in react-js and database is MySQL. Now the problem is I need to search products from database in my project but don’t know how to get query for frontend to backend. I don’t know the code Moreover I am using rest-framework and Axios library to connect. I am very thankful to you if you help me to create search related queries in my project. Thank you\nAnswer: You should elaborate more. What is the issue? means ""do you have issue about how to implement search at DB?"" or 'how to send data from frontend to backend and vice-versa'",0.0,0.36946198,0.13650216162204742
18,"Question\nI want to fetch all the issues which are currently in 'IN PROGRESS' status and previously in 'OPEN'status.
By using the following code I would get all the issues that are currently in 'IN PROGRESS' status and the issues that are passed through 'OPEN' status once in its transition state.
eg: issues = jira.search_issues(PROJECT=XYZ AND (status in ('IN PROGRESS')) AND (status WAS in ('OPEN')))
But how can I get the issues which are currently in the 'IN PROGRESS' status and previously in 'OPEN' status in JIRA using Jira-Python?\nAnswer: You can run a search using Jira Python. Use the same search you showed us",0.0,-0.10056269,0.010112854652106762
19,"Question\nI am trying to communicate with a vehicle control unit (VCU) over can. I have figured out the commands (index, data and frequency) and can verify the functionality through PCanView on Windows. Now I am using Nvidia Xavier system with python-can library to send the same commands, and I can verify the commands with candump. However when I power the vehicle engine on while sending these commands, the canbus freezes (this is when the VCU starts expecting the can commands I am sending, it goes into fault state if it doesn't receive the data it expects)
I have successfully used python-can in the past to talk to other can devices and I am confident about the correctness of the code itself.
Hardware connection is fine too, because I can receive non-VCU messages from the vehicle. I can also receive VCU messages after I restart the canbus.
What could be causing the bus to freeze? And is there a way to prevent it? (By setting some config in the socket-can layer itself?)
Please note that restarting the bus will not fix the problem as the vehicle cannot recover once it goes into fault without a restart.
Any help will be appreciated!\nAnswer: Ok, it turns out it was a hardware problem. The length of CAN cables was a bit too much. The bus receives a lot of data transmission when the vehicle is turned on and the CAN cable was flooded with data. I still don't know the mechanics of the fault but decreasing the cable length made it all work.",0.0,0.094721675,0.008972195908427238
20,"Question\nI am trying to communicate with a vehicle control unit (VCU) over can. I have figured out the commands (index, data and frequency) and can verify the functionality through PCanView on Windows. Now I am using Nvidia Xavier system with python-can library to send the same commands, and I can verify the commands with candump. However when I power the vehicle engine on while sending these commands, the canbus freezes (this is when the VCU starts expecting the can commands I am sending, it goes into fault state if it doesn't receive the data it expects)
I have successfully used python-can in the past to talk to other can devices and I am confident about the correctness of the code itself.
Hardware connection is fine too, because I can receive non-VCU messages from the vehicle. I can also receive VCU messages after I restart the canbus.
What could be causing the bus to freeze? And is there a way to prevent it? (By setting some config in the socket-can layer itself?)
Please note that restarting the bus will not fix the problem as the vehicle cannot recover once it goes into fault without a restart.
Any help will be appreciated!\nAnswer: The cable length could be the reason, but take care about the bus topology and especially where the CAN terminations are located.",0.0,0.15306973,0.023430343717336655
21,"Question\nSay you have two random words ('yellow' and 'ambient' or 'goose' and 'kettle'). What tech could be used to rate how similar or different they are in meaning as informed by popular usage? For example, from 0 to 1 where antonyms are 0 and synonyms are 1, 'yellow' and 'ambient' might be 0.65 similar.
Note: I'm not talking about how close the two strings are to each other, but rather an approximation of how similar their meanings are.\nAnswer: I do not really understand what you exactly mean with similarity especially if you want to talk about meaning. You would need a dataset to denote meaning unto words. A popular example of this would be sentiment analysis. If you got a lot of textual data, say tweets from twitter, you might want to know if the data is mostly positive or negative. To do this you would find a dataset of similar nature who has labelled the data already into categories. Then you can use this data to classify the texts into categories (e.g with a Naive Bayes classifier). In this way you can denote meaning on texts computationally.
This would allow general evaluations but also evaluations on an input to input basis on how well they scored across different categories of meaning.
I'm not sure if that's what you're looking for in an answer though.",0.0,0.37826383,0.14308352768421173
22,"Question\nI use BashOperator to execute a python file called app.py in Airflow.
I wrote another python script called to_es.py. There is a function called ""df_to_es()"" in it.
The app.py should call df_to_es() by from utils.to_es import df_to_es, but the Airflow throws an error in red words: 'there is no module called ""def_to_es""'.\nAnswer: Finally I come to answer my own question.
Even though Airflow may indicate that there is a DAG import error, but if you use BashOperator to execute your Python script, you import your own python functions, classes and modules in that script, they work smoothly if you don't have some other errors. Just double check if you are using correct Airflow DAG directory.
So just ignore that DAG import error if you are in my situation. This is something that Airflow develop team need to improve. Something like unit test.",0.0,0.18841088,0.03549866005778313
23,"Question\nI have a command-line python package run in the terminal directly (it was not run inside python environment) that often gives me a maximum recursion error. I know the default setting is 1000, and I know you can use the setrecursionlimit() command to change it. However, every time I change it and exit the python environment, the value resets to the default.
My question is, without changing the python package, is there a way I could change the python default recursion depth permanently. I understand this may not be a good practice, so I plan to change it back to 1000 after I finish using the package, which I probably won't need after a week.
Thanks!\nAnswer: I agree with others that using sys.setrecursionlimit is the way to go. In the end, that is the reason why sys is part of the Python Standard Library.
In an interactive session, you could set your PYTHONSTARTUP global variable to a file with the necessary python commands, however this would still run the exact same code but just a bit more 'hidden' from the user.
I think that if you'd really like to set this to another value, your only option is to dive deep into the C header files of the cpython source code and custom compile python for your needs.",0.0,0.24535102,0.060197122395038605
24,"Question\nHi I am using the google calendar API for a school project and my tokens have expired and I don't know how to refresh them. Please help!!!\nAnswer: Generally, when a token expires, make a new one using the same process and replace the old one in your code with the new one.",0.0,0.19243258,0.03703029826283455
25,"Question\nI am using a library django-upgrade to convert the code from django 1.11 to django 3.24
the command is like

django-upgrade --target-version 3.2 example/core/models.py

I want to know that how can I run this command so that it runs on whole django project.\nAnswer: I am clear on the question but if you are asking how to upgrade Django then in your virtual environment run pip sudo pip install --upgrade django==3.2",0.0,0.13413972,0.017993463203310966
26,"Question\nI have an issue with working with the turtle package in python. I have Anaconda installed on my computer. Within Anaconda I have installed the turtle package with pip command (there is no conda install option for turtle as to my knowledge).
When I start a jupyter notebook and import turtle everything works just fine.
When I start vscode from conda and use the same python environment, I get an import error from vscode saying: importerror: cannot import name Turtle.
Why is this happening and how could I start turtle in vscode?
Thank you!\nAnswer: Use pip install PythonTurtle in the vs code terminal",0.0,0.31179696,0.09721734374761581
27,"Question\nI have an issue with working with the turtle package in python. I have Anaconda installed on my computer. Within Anaconda I have installed the turtle package with pip command (there is no conda install option for turtle as to my knowledge).
When I start a jupyter notebook and import turtle everything works just fine.
When I start vscode from conda and use the same python environment, I get an import error from vscode saying: importerror: cannot import name Turtle.
Why is this happening and how could I start turtle in vscode?
Thank you!\nAnswer: Solved! Shayan solved the problem in the comments... I was so stupid to name my file turtle.py... no comment
I renamed the file and works really great!
Thank you Shayan!",0.0,0.24183887,0.05848604068160057
28,"Question\nI have created a Tabular Dataset using Azure ML python API. Data under question is a bunch of parquet files (~10K parquet files each of size of 330 KB) residing in Azure Data Lake Gen 2 spread across multiple partitions. When I try to load the dataset using the API TabularDataset.to_pandas_dataframe(), it continues forever (hangs), if there are empty parquet files included in the Dataset. If the tabular dataset doesn't include those empty parquet files, TabularDataset.to_pandas_dataframe() completes within few minutes.
By empty parquet file, I mean that the if I read the individual parquet file using pandas (pd.read_parquet()), it results in an empty DF (df.empty == True).
I discovered the root cause while working on another issue mentioned [here][1].
My question is how can make TabularDataset.to_pandas_dataframe() work even when there are empty parquet files?
Update
The issue has been fixed in the following version:

azureml-dataprep : 3.0.1
azureml-core :  1.40.0\nAnswer: Thanks for reporting it.
This is a bug in handling of the parquet files with columns but empty row set. This has been fixed already and will be included in next release.
I could not repro the hang on multiple files, though, so if you could provide more info on that would be nice.",0.20408164,0.07875824,0.015705954283475876
29,"Question\nI have created a Tabular Dataset using Azure ML python API. Data under question is a bunch of parquet files (~10K parquet files each of size of 330 KB) residing in Azure Data Lake Gen 2 spread across multiple partitions. When I try to load the dataset using the API TabularDataset.to_pandas_dataframe(), it continues forever (hangs), if there are empty parquet files included in the Dataset. If the tabular dataset doesn't include those empty parquet files, TabularDataset.to_pandas_dataframe() completes within few minutes.
By empty parquet file, I mean that the if I read the individual parquet file using pandas (pd.read_parquet()), it results in an empty DF (df.empty == True).
I discovered the root cause while working on another issue mentioned [here][1].
My question is how can make TabularDataset.to_pandas_dataframe() work even when there are empty parquet files?
Update
The issue has been fixed in the following version:

azureml-dataprep : 3.0.1
azureml-core :  1.40.0\nAnswer: You can use the on_error='null' parameter to handle the null values.
Your statement will look like this:
TabularDataset.to_pandas_dataframe(on_error='null', out_of_range_datetime='null')
Alternatively, you can check the size of the file before passing it to to_pandas_dataframe method. If the filesize is 0, either write some sample data into it using python open keyword or ignore the file, based on your requirement.",0.0,0.20718354,0.042925018817186356
30,"Question\nI am very new to python and I'd like to ask for an advice on how to, where to start, what to learn.
I've got this fantasy name generator (joining randomly picked letters), which every now and then creates a name which is acceptable, what I'd like to do though is to train an AI to generate names which aren't lets say just consonants, ultimately being able to generate human, elvish, dwarfish etc names.
I'd appreciate any advice in this matter.
Edit:
My idea is: I get a string of letters, if they resemble a name, I approve it, if not - reject. It creates a dataset of True/False values, which can be used in machine learning, at least that's what I am hoping for, as I said, I am new to programming.
Again, I don't mind learning, but where do I begin?\nAnswer: Single characters are not really a good fit for this, as there are combinatorial restrictions as to which letters can be combined to larger sequences. It is much easier to not have single letters, but instead move on to bi-grams, tri-grams, or syllables. It doesn't really matter what you choose, as long as they can combine freely.
You need to come up with an inventory of elements which comply with the rules of your language; you can collect those from text samples in the language you are aiming for.
In the simplest case, get a list of names like the ones you want to generate, and collect three-letter sequences from that, preferably with their frequency count. Or simply make some up:
For example, if you have a language with a syllablic structure where you always have a consonant followed by a vowel, then by combining elements which are a consonant followed by a vowel you will always end up with valid names.
Then pick 2 to 5 (or however long you want your names to be) elements randomly from that inventory, perhaps guided by their frequency.
You could also add in a filter to remove those with unsuitable letter combinations (at the element boundaries) afterwards. Or go through the element list and remove invalid ones (eg any ending in 'q' -- either drop them, or add a 'u' to them).
Depending on what inventory you're using, you can simulate different languages/cultures for your names, as languages differ in their phonological structures.",0.0,0.37117076,0.1377677321434021
31,"Question\nI'm making a music bot for my discord server and I want it to run 24/7 on repl.it but when I run it on my computer I add executable=""./ffmpeg.exe"" to the from_probe function. Nevertheless, replit doesn't support executable files so I need to find an other way to make this work. I tried installing ffmpeg package, I also looked up for tutorials how to use ffmpeg-python with youtube_dl. None of these worked. If you need some additional info, just ask me in the comment section.\nAnswer: It's not possible on replit. FFmpeg was working on replit before, it does not work now, you possibly could find another module to play music.",0.0,0.080875754,0.006540887523442507
32,"Question\nBasically; I messed up. I have pickled some data, a pretty massive dictionary, and while my computer was able to create and pickle that dictionary in the first place, it crashes from running out of memory when I try to unpickle it. I need to unpickle it somehow, to get the data back, and then I can write each entry of the dictionary to a separate file that can actually fit in memory. My best guess for how to do that is to unpickle the dictionary entry by entry and then pickle each entry into it's own file, or failing that to unpickle it but somehow leave it as an on-disk object. I can't seem to find any information on how pickled data is actually stored to start writing a program to recover the data.\nAnswer: pickle is a serialization format unique to Python, and there is no user-level documentation for it.
However, there are extensive comments in a standard distribution's ""Lib/pickletools.py"" module, and with enough effort you should be able to use that module's dis() function to produce output you can parse yourself, or modify the source itself. dis() does not execute a pickle (meaning it doesn't build any Python objects from the pickle). Instead it reads the pickle file a few bytes at a time, and prints (to stdout, by default) a more human-readable form of what those bytes ""mean"".",0.40816328,0.4676218,0.0035353160928934813
33,"Question\nUnlike earlier version 3 releases nowadays building Python 3.10 from source does not seem to run the (time-consuming) tests.
I need to build Python 3.10 on an oldish platform (no, I can't change that). I would actually like to run the tests, even if they are time consuming.
Unfortunately, I can't find the way to do it. Googling shows nonsensical results (how to do testing while using Python, unittest etc), while./configure --help doesn't show anything.
Have the tests been removed? If not, how can I enable them?\nAnswer: Building from source make -j 4 prefix=""/usr"" usually does the tests too. At least that's what I've observed.",0.0,0.28047115,0.07866406440734863
34,"Question\nI am working on a script that needs to be run both from a command prompt, such as BASH, and from the Console in Spyder. Running from a command prompt allows the script file name to be followed by several arguments which can then be utilized within the script; >python script1.py dataFile.csv Results outputFile.csv. These arguments are referenced within the script as elements of the list sys.argv.
I've tried using subprocess.run(""python script1.py dataFile.csv Results outputFile.csv"") to enable the console to behave as the command line, but sometimes it works fine and other times it needs certain arguments, like -f between python and the file name, before it will display what is displayed in the command line. Different computers disagree on whether such arguments help or hurt.
I've searched and searched, and found some clever ways to use technicalities of the specific operating system to distinguish, but is there something native to Python I can use?\nAnswer: If you import sys in the console and then call sys.argv, it will show you the value ['']. While running a script within Spyder expands that array to ['script1.py'] (plus the file address), it will still not get larger than one entry.
If, on the other hand, you run the script from the command line the way you showed above, then sys.argv will have a value of ['script1.py', 'dataFile.csv', 'Results', 'outputFile.csv']. You can utilize the differences between these to distinguish between the cases.
What are the best differences to use? You want to distinguish between two possibilities, so an if - else pair would be best in the code. What's true in one case and false in the other? if sys.argv will not work, because in both cases the list contains at least one string, so that will be effectively True in both cases.
if len(sys.argv) > 1 works, and it adds the capability to run from the command line and go with what is coded for the console case.",0.0,0.33106214,0.10960213840007782
35,"Question\nI noticed this weird behavior where checking if paths existed with those paths being in /media mount points that are not mounted (that is: a external removable disc that is not connected) is exponentially slower than checking on local disc paths.
Is this a known bug? And how can i avoid it?
Should i just glob the mount points in '/media' and if a path starts in '/media' but isn't in the glob, skip it?
Is there a way to make this idea portable or is it hopeless to try?\nAnswer: The reason for the'slowness' is that i had fstab file where i automounted the external drive with a too long timeout.
Specifically i had on the options nofail,noatime,x-systemd-automount,x-systemd.device-timeout=1s
Omitting the option is even more disastrous btw, the default timeout is 90 seconds or something.
I ended up removing the automount and using '1ms' instead of '1s' for the timeout.
Removing the automount is not actually necessary, i think, and i might add it back, because it allows mount to happen even if you don't start gnome, and before autostart gnome programs although it conflicts/duplicates mountpoints with the gnome 'autostart' from autofs and can only be removed with the 'Disks' program in gnome, and requires using x-gvfs-hide to hide the ugly duplicated mountpoint.",0.0,0.24443889,0.05975037068128586
36,"Question\nHi I am using LightAutoML on supervised data can someone help me with how to do preprocessing in this framework I am using it for the first time
I have tried to use train split but it says that input contains null values\nAnswer: I found out in the documentation of lightautoml that it takes care of data preprocessing and feature engineering too",0.40816328,-0.047840536,0.20793947577476501
37,"Question\nI am creating a subprocess using this line of code:
p = subprocess.Popen([""doesItemExist.exe"", id], shell=False)
and when I run the script while I have the Task Manager open, I can see that it creates two processes and not one. The issue is that when I go to kill it, it kills one (using p.kill()), but not the other. I've tried looking online but the only examples I find are about shell=True and their solutions don't work for me. I've confirmed that that line only gets called once.
What can I do? Popen is only giving me back the one pid so I don't understand how to get the other so I can kill both.\nAnswer: I ended up being able to deal with this issue by creating a clean up function which just uses the following:
subprocess.run([""taskkill"", ""/IM"", ""doesItemExist.exe"", ""/F""], shell=True)
This will kill any leftover tasks. If anyone uses this, be careful that your exe has a unique name to prevent you from killing anything you don't mean to. If you want to hide the output/errors, just set the stdout and stderr to subprocess.PIPE.
Also, if there is no process to kill it will report that as an error.",0.0,0.19177794,0.036778781563043594
38,"Question\nThere is a case in my job where l have to remove a specific section (Glossary) from thousands of pdf documents.
The text l want to remove has a different font from the other parts:
Example:
""Floor""  the lower surface of a room, on which one may walk.
""exchange"" an act of giving one thing and receiving another (especially of the same type or value) in return.
Can you please suggest a way how to do it faster?\nAnswer: One of the possible ways to solve this problem is to find the section you want to delete using regex. Then using one of the libraries for pdf editing in python to delete this section.",0.81632656,0.093435764,0.5225710868835449
39,"Question\nThe question may not have been clear as there was not much of a better way for me to word it but I will do my best here: Most scripting is done raw in the IDE. When coding with Python it simply runs everything through the ""console."" I was wondering how full-on apps with different screens and all that cool stuff are made with Python as well, other than just being a console with many different commands?\nAnswer: Basically if you want to design cool stuff with colours and nice picture, you can use turtle. Turtle is a cool library which allows you to draw turtles and other cool animations. Useful for beginners like you :)",0.20408164,0.20983347,3.308358645881526e-05
40,"Question\nThis is a typical python coding challenge. Many beginners have a hard time handling it.
For example, we have a test array as:
test = [[1,2],[1,3],[2,4],[2,1],[2],[5,1],[3,4]]
Q1: count the number of pairs in the list.
Q2: count the number of pairs for 1.
I know I can use the least/greatest function in SQL to do the job, but I don't know how to do it in python, especially in 2 dimension arrays.
Expected result for Q1 is 5 ([1,2],[1,3],[2,4],[5,1],[3,4]）
Expected result for Q2 is 3 (2,3,5)\nAnswer: len([k in test if len(k)==2]) and len([k for k in test if 1 in k]) should get you there.",-0.35714287,0.22883487,0.34336990118026733
41,"Question\nI'm currently trying to read from one Kafka topic, doing some transformation and producing the messages to another topic.
However, i am having a lot of issues with the Consumer. First of all, if we set reasonable session timeout/max poll records values (like 10 s), the consumer takes super long, constantly rebalances and sometimes sends duplicated messages. If we increase the params to crazy values like 30 min, the speed increases dramatically. But the problem is once it reaches the 30 min mark, it rebalances and takes around 30 min to start up again.
I have been playing with a lot of different params but still lost on how to fix this. Any ideas? Thanks\nAnswer: This may be due to some configuration issues. Based on your question I would suggest to please check your auto commit property.
Because ideally kafka does rebalance if it does not recieve the acknowledgement of the message read before session timeout happens. If it is set as false then either set it to true or make sure to commit to kafka once you are done processing the message",0.0,0.231148,0.05342939868569374
42,"Question\nI'm trying to run a face detection model in Unity. It gets input from the webcam, then spits out a face. But trying to make this work with C# has been an absolute nightmare. And despite all my suffering, I still haven't been able to make it work!
If I could use python, I'd be able to get it done easily. So, obviously, I want to find a way to get a python script working in Unity. But IronPython is the only thing I've been able to find, and it's outdated.
I need either knowledge of how to make IronPython work in spite of being outdated, or some other method. Please.\nAnswer: Unity not supported python, But you Can write Python Code and run it by Socket programing, Create Server with python and send data,in C# Connect to server and use data sended with python.",0.0,0.1344322,0.018072014674544334
43,"Question\nI'm trying to run a face detection model in Unity. It gets input from the webcam, then spits out a face. But trying to make this work with C# has been an absolute nightmare. And despite all my suffering, I still haven't been able to make it work!
If I could use python, I'd be able to get it done easily. So, obviously, I want to find a way to get a python script working in Unity. But IronPython is the only thing I've been able to find, and it's outdated.
I need either knowledge of how to make IronPython work in spite of being outdated, or some other method. Please.\nAnswer: You can just run your python script on playtime and let it create some data in files. Then read the files using C# and display data in Unity.",-0.23809524,0.15643609,0.15565495193004608
44,"Question\nI am in the process of converting a JSON into a dateframe.  One of the items in the JSON is a date in the form of a string.  I am calling the item with the following entry:
markets_json['events'][i]['periods']['num_0']['cutoff']
i is a position number in a list that is being generated from a ```for`` loop.  The other fields are dictionary keys.  It returns a string that looks something like this:
2022-02-19T21:08:00Z
I would like to turn this into some sort of real usable date that I might even be able to adjust the time zone on.  The best I have been able to do are to use datetime.datetime.strptime  or  parser.isoparse but those just creates datetime objects (datetime.datetime(2022, 2, 19, 21, 8, tzinfo=datetime.timezone.utc)) which are not usable to me in the dataframe or I need to know how  make them usable.
Thank you in advance for your help.\nAnswer: I found my problem.  There were two different time date formats in the JSON.  Some look like 2022-02-19T21:08:00Z and some are like 2022-02-18 07:38:53.110000+00:00.  That was causing an issue with the dataframe conversion.  Thank you for your help.",0.0,0.11616796,0.013494995422661304
45,"Question\nI have a very big data frame with the orders for some products with a reference. This reference has periodical updates, so for the same product there are a lot of rows in the dataframe. I want to choose the last update for each reference, but i dont know why.
For a reference, for example there are 10 updates, for another, 34, so there is not a patron...
Any ideas?\nAnswer: you can use func iget like this :
df['column'].iget(-1);
or
df.iloc[-1:]",0.0,0.28731567,0.0825502946972847
46,"Question\nI am developing an application on a remote server.
I'm using Python and am connected via ssh with the specific extension.
I start debugging and everything seems to proceed normally but suddenly debugging stops and there is no reason (error or warning).
Can anyone suggest me where to look for the reason or how to trace the problem?
The script, executed outside VSCode, runs smoothly.
Thanks in advance.\nAnswer: in the end I found a possible cause and, therefore, the solution.
There is a problem with the sequence of imports and module import for MariaDB.  By moving this import to the head, the problem was solved.",0.0,0.25397122,0.06450138241052628
47,"Question\nI have a script that I run locally on my laptop, which polls a server and if certain criteria are met, it sends an email. I guess like a heartbeat app of sorts.
The server is a Raspberry Pi, it has a public IP address which is all working fine.
Id like to host the python script on Heroku, so that it can run without my laptop having to be on the local network or always running.
Does anyone have experience of Heroku to show me how I can have the script hosted and running constantly?
My other query is that free tiers of Heroku go to sleep after 30 mins, so would essentially stop the script until getting an http request and spin up the instance once again.
Trying to find some form of elegant solution.
Many thanks for any advice you can give,
All the best,
Simon\nAnswer: Different approachs would be:
1- Cloud Functions

Create lambda function in a cloud provider with your python code (free tier elegible)
Trigger that function every once in a while

2- Get VM on Cloud

Go to AWS, GCP, etc
Get a free tier VM
Run server from there",0.20408164,-0.008724451,0.04528643190860748
48,"Question\nI am writing a c++ executable which can call python interface. However, I don't know how to make this executable program find the python interpreter and run it on other people's computers. In other words, in what order does Py_Initialize() find the local python interpreter.\nAnswer: Py_Initialize does not need to find the Python interpreter, because it is (part of) the Python interpreter! If you can call Py_Initialize, that means that you have made the Python interpreter part of your own program, so there is nothing more to find.
(If you linked it as a dynamic library, there is the issue of how dynamic libraries are found at runtime, but that is not specific to Python.)
Executing stuff on another computer is a completely different problem from embedding the interpreter.",0.40816328,0.32331163,0.007199802901595831
49,"Question\nI have a folder that contains several images, I would then like to display the list of images that have the same name and number (eg: myfile_page1.png, myfile_page2.png, myfile_page2.png,) and then display them in the django template show_file.html\nAnswer: You can store file names and paths in databases such sqlite3 or Postgres and then using SQL filter the files path and name you want to fetch. This way you can both show files with similar names and render them accordingly.",0.0,0.32161397,0.10343554615974426
50,"Question\nGood day,
I have a python code and this code has the following line at the beginning.
from parfile_parser import Parfile
I am using ipython (python version 3.7) and the OS is Ubuntu 20.04. The above line gives me an error as below.
"" No module named 'parfile_parser' ""
So, I tried to find this module by googling to install on my desktop, but I could not find it. Do you happen to know how I can install this module and how to solve this problem?
Thanks!\nAnswer: You said you tried to install it on a desktop, did you install it in your virtual environment. If not try to install it in your virtual environment and then check.",0.0,0.08952671,0.008015031926333904
51,"Question\nI have a 0.4 KV electrical network and I need to use particle swarm optimization algorithm on it to find the optimal place and size for DGs but I'm new to optimization subject I tried a lot but I couldn't know how to do it could anyone help me with it please\nAnswer: From the paper ""Prakash, D. B., and C. Lakshminarayana. ""Multiple DG placements in distribution system for power loss reduction using PSO algorithm."" Procedia technology 25 (2016): 785-792"", PSO algorithm is given below
Step 1: Input data such as line impedance, line power.
Step 2: Calculate voltages at each node and total power loss in the distribution network using forward backward sweep method.
Step 3: Initialize population size.
Step 4: Initialize number of particles to be optimized.
Step 5: Set bus count x=2.
Step 6: Set generation count y=0.
Step 7: Generate random position and velocity for each particle.
Step 8: Calculate power loss for each particle using Active power loss minimization.
Step 9: Initialize current position of each particle as ‘Pbest’.
Step 10: Assign ‘Gbest’ as best amont ‘Pbest’.
Step 11: Update velocity and position of each particle using  velocity and position update equations respectively.
Step 12: If generation count reaches maximum limit, go to Step 13 or else increase the counter by one and go to Step 7.
Step 13: If bus count reaches maximum limit, go to Step 14 or else increase the counter by one and go to Step 6.
Step 14: Display the results.",0.0,0.15549874,0.024179859086871147
52,"Question\nI need some help how to write the type hints, and it is not purely technical.
As an example, imagine a get_state() function returning the current state as a string.
The documentation states the initialize() must be called first and the result of get_state() is undefined prior to initialization. Actually it returns None when uninitialized, but that is an implementation detail.
The annotation could be:

get_state() -> str which is correct assuming a proper usage. I find it helpful from the developer's point of view, but mypy complains because it is clear that the return value could be also None.
get_state() -> str|None which matches the reality the most, but may change in the future and it introduces mypy warnings everywhere the return value is used and is obviously expected to be a string there.
get_state() -> Any which exactly matches the documented API, but is useless.

So, who is the main recipient of the information in the annotation? Is it the developer getting additional information when reading the code? Or is it the type checker tool like the mypy that tries to find possible problems?\nAnswer: These annotations are mostly for the developer as they don't affect runtime. That said, the type checker is also there to make your life easier. So if any of these are making it harder for you don't have to use them...
And more practically, can you return an empty sting ('') instead of None?",-0.35714287,0.3628353,0.5183685421943665
53,"Question\nI need some help how to write the type hints, and it is not purely technical.
As an example, imagine a get_state() function returning the current state as a string.
The documentation states the initialize() must be called first and the result of get_state() is undefined prior to initialization. Actually it returns None when uninitialized, but that is an implementation detail.
The annotation could be:

get_state() -> str which is correct assuming a proper usage. I find it helpful from the developer's point of view, but mypy complains because it is clear that the return value could be also None.
get_state() -> str|None which matches the reality the most, but may change in the future and it introduces mypy warnings everywhere the return value is used and is obviously expected to be a string there.
get_state() -> Any which exactly matches the documented API, but is useless.

So, who is the main recipient of the information in the annotation? Is it the developer getting additional information when reading the code? Or is it the type checker tool like the mypy that tries to find possible problems?\nAnswer: Probably str|None is what you want.
In my view, type annotations in Python try to reap some of the same benefits that static type systems bring to languages that have them. Good examples of languages with strong static type systems are Haskell and Rust. In such languages type annotations can never overpromise, like would happen with get_state() -> str. So that possibility is ruled out. get_state() -> str|None happens to be what the code is capable of supporting, so that is one option, and the documentation should probably then reflect that as well. If the developers think that this return type is likely to change or be different on different systems then it could be reasonable to go for a type like Any, but that would also have implications for how this function should be used. If all you know about this function is that it could return Any(thing) then what exactly can you do with this value? You could test whether it is a string and then use it as a string, but is that the way recommended in the documentation? If yes then the Any type is reasonable, if not then not.",0.0,0.45417523,0.20627515017986298
54,"Question\nI have very little experience in manipulating pdfs using python, and my experience is restricted only to reading using 'pdfreader' a python library. I have a pdf, (which in this case is a past exam paper), I want it to split a page when it encounters a question number, let's say 12 for this example (it would be formatted ""12.""), and save the split part containing the number 12. in a new pdf. How do I do this?
I'm not a very good programmer so sorry if my question is stupid, but searching on the internet I could not find how to do this.\nAnswer: The solution at the end was to transform the pdf page into an image, crop it where I want it, then back to a pdf. To get the coordinates I had to use pdf miner, to then get the pixels to modify the image I had to make a proportion between the height of the page in pdf coordinates and the height of the image I wanted to create in pixels, so then I could transform the coordinates of one into the coordinates of the other.",0.0,0.008268833,6.837359978817403e-05
55,"Question\nI am coding kivy app on PC. At the end, I will convert it to.apk on my android phone (redmi note 9 pro). Which window size should I choose? It will be easier to code with this specific window size (add buttons and text on specific place).
And how to do it. Because I heard that using window.size is not good/not working.
Thanks\nAnswer: It's usually a better idea not to code for a specific window size, but to code the gui in such a way that regardless of the window size it tries to do something sensible. For instance, your app might have a navigation bar at the top that is always 40dp high, then the rest of the app is taken up by something else that grows to fill all the available space.
You can code the whole gui to a fixed size if you want of course, but it probably isn't the best solution.",0.0,0.14548904,0.021167060360312462
56,"Question\nI am using coc.nvim in neovim together with Pylint.
If I try to import my own module e.g. src.reverse_linked_list or an installed module like selenium, CoC displays the error message
[pylint E0401] [E] Unable to import 'xxxxx' (import-error)
double checked that init.py is in my directories
Running the code does not lead to any errors
Does anyone know how to fix this?\nAnswer: The python path need to be the same when running pylint inside neovim vs when running the code. The import using src.reverse_linked_list is suspicious in this regard, src is not generally used in the import.",0.0,0.38293076,0.146635964512825
57,"Question\nI am currently writing a python file that is meant to be run through the command line like pip and npm, but I also need to know when the user launches it directly through the file explorer (as in windows). Is this completely impossible (restricted to the program only knowing that it's run with no sys.argv arguments), or is there a way to make the program differentiate if it's being run directly through something like  the file explorer, or if it's being run through the command line? Thanks!\nAnswer: Be sure to capture the user's operating system first before implementing a Windows specific approach.",0.13605443,0.24357647,0.011560989543795586
58,"Question\nI was asked to make an animation using python, and I was thinking to animate an object making a circular path in uniform motion, but it would be really great if the plane of the circular motion also changes constantly over time (like those oversimplified atom animations), so I tried to figure some parametric equation using spherical coordinates, but I landed on differential equations something that I was supposed to learn next year. Does anyone know how to parameterize this kind of motion?\nAnswer: For the orientation of the plane, you could use latitude and longitude of the normal vector, with a third parameter to describe the angle along the circular path. In order to get cartesian coordinates you will need to figure out the transformation (x, y, z) = f(latitude, longitude, rotation). There are no differential equations involved, just a lot of sin() and cos().",0.0,0.18404031,0.03387083485722542
59,"Question\nI have entered a string as:
str_data = 'Hi'
and then I converted the string to binary using format() as:
binarystr = ''.join(format(ord(x),'b') for x in str_data)
Now, I want to convert binary string back to my original string. Is it possible to do so in Python? If yes, then how to do that?
I know chr() gives character corresponding to the ascii value but how to use it with binary string?\nAnswer: 2 approaches: assuming a fix length for your source string (7 binary entries) or allowing a flexible length
fix length (standard alphabet)
Here we assume that all the binary mapped from a chr entry have always 7 entries (quite standard).
split_source = [''.join(x) for x in zip(*[iter(binarystr)]*7)]
str_data_back = ''.join([chr(int(y, 2)) for y in split_source])
flexible length (more general, but maybe useless)
You need to add a separator when you create binarystr
E.g. binarystr = '-'.join(format(ord(x),'b') for x in str_data)
otherwise it is not possible to say what was from where.
By adding the -, you can reconstruct the str_data with:
str_data_back = ''.join([chr(int(x, 2)) for x in binarystr.split('-')])",0.0,0.46241033,0.2138233184814453
60,"Question\nFor my college project, I'm developing an application that analyzes/detects anomalies in videos.
The backend is Python and the frontend is JS/NodeJS. I setup the backend so it runs 4 ML models and writes the outputs to 4 separate.txt files. I created a JS file that reads the.txt files and reformats the data to be saved as 4.json files. Currently, the frontend just reads the.json files and displays the data... I run npm start to display the application in the browser at localhost:3000. Here is my problem:
In the application, the user can upload a.mp4 file and watch the video (react-player). I think I can setup a button interaction to execute a script that runs the backend.py file. However, I don't know how to continue from there. I need to exchange data twice: First, I need to download the video to the.py file local directory as input to the ML models. Once the models have finished running, I need to
transfer the outputs (Python list or.json format... I know how to do either) back to the application so it can be displayed to the user.
I'm reading online that I can execute data transfer using jquery and/or ajax calls? Or maybe I setup a database? I have 0 experience w/ any of that so I'd like advice on the easiest approach. If you can recommend resources that can help me learn more on the topic, that'd also be helpful.\nAnswer: You can use WebSocket to transfer the files, initially after sending the video to the backend there would be quite a bit of delay for the ML model to process it and come up with the output JSON files, WebSocket would make it easier to make this communication.
(I'm not saying it's not possible to use a rest API to do the same, but it would be easier to use WebSockets)",0.0,0.16689152,0.027852777391672134
61,"Question\nI am trying to create a python program for my Casio fx-9750GIII calculator that computes the molar mass of a given chemical formula. To do this I need more methods for strings from curses.ascii such as isalpha. Unfortunately I don't know how modules work on a non-networked device. Do I just need to download a file and put it on the calculator for an import statement to work or is there something else I need to do?\nAnswer: As the commenter Lecdi said, it is a native python string method, I just made the mistake of typing isalpha() without a string beforehand.",0.0,0.15456319,0.023889780044555664
62,"Question\nWhen starting IDA in GUI mode to analyze the binary, it automatically locates and displays the actual main function code (not the entry point encapsulated by the compiler, but the main function corresponding to the source code).
My question is how to get that address in batch mode (without GUI) via idapython script? I don't see the relevant interface in the IDAPython documentation.
For example, _mainCRTStartup --> ___mingw_CRTStartup --> _main is a sequence of function calls, where _mainCRTStartup is the entry point of the binary, but I want to get the address of _main, can it be done?
Any help or direction would be much appreciated.!\nAnswer: Know the answer, it is idaapi.inf_get_main()",0.0,0.2905519,0.08442040532827377
63,"Question\nI have imported data from excel to python and now want to draw multiple plots on a single figure but for that I will need separate variables like 'x' & 'y' etc because we know that plt.plot(x,y), basically I have two datasets in which I am doing Time series analysis. In first data set I have Monthly+Yearly data in which I combined both columns and formed one column having name Year-Month, In second dataset I have Daily+Yearly data in which I formed one column by merging both and named it as Year-Daily. Now the dependent variable in both datasets is the number of sunspots.

Now I want to Plot Daily and Monthly sunspot numbers on a single Graph in Python, so how will I do that?\nAnswer: What is the library that are you using to import the data?",0.0,0.1596424,0.025485696271061897
64,"Question\nDear Stackoverflow community. I need to know how to build a GeoJson map that allows me to draw a polygon and add the coordinates of this drawed polygon to a python list or a json file.
I have been reading all kind of documentation but I didn´t find anything like I´m searching.
If anybody could help me with code or specific documentation, I would be great for me!!!!!
Thanks a lot.
Pablo.\nAnswer: Ok, let me explain you with an example.

You have a common map made it with folium. Now I need to know how to do the next two points:
I need to know how click on the map and “draw” a polygon or shape.
I can ""draw"" the polygon or any shape, but with a python script specifying the coordinates (latitude and longitude). I don't want that, I want to draw the polygon by clicking on the map.
Finally, I want the polygon coordinates to be automatically added to a python list or a json script.

I hope I was clear explaining the problem.",0.0,0.056566596,0.0031997798942029476
65,"Question\nI'm building a basic Instagram bot, and it got me wondering, how could I get to auto-run at a certain time, or when a certain condition is satisfied (eg. when there's a new file in a folder)?
Help much appreciated.\nAnswer: make a while loop and check your condition each time, suggest you to put some kind of sleep (eg. asynco.sleep() or time.sleep()) and if the condition is true then run the bot",0.40816328,0.44859886,0.0016350363148376346
66,"Question\nI have a python script that previously worked but that now throws the error:ImportError: DLL load failed while importing _gdal: The specified module could not be found. I am trying to upload a shapefile using fiona and originally the message read: ImportError: DLL load failed while importing _fiona: The specified module could not be found. I am using anaconda navigator as my IDE on windows 11.
I am aware that this is a question that has been asked before and I have read the answers to those questions. The solutions, however, hove not worked either due to my circumstance or my misinterpretation and action in following through with it. So my question is either how do I fix this, or, if it is not that simple, to better understand the problem.
I have looked inside the DLLs folder within the environment folder that I am using and there is nothing in there with name fiona, gdal or geopandas.
My attempts so far:
1. uninstall and re-install fiona gdal and geopandas (as I believe they are dependent).
2. update all libraries and anaconda to latest verions.
3. download Visual C++ Redistributable for Visual Studio 2015. Ran into issue during download as it was already installed on my computer, likely because it is a windows computer. Is it possible that this would help if i moved it to a different path/folder?
4. Uninstall and re-install anaconda navigator on cumputer. Re-create virtual environemt and import necessary libraries. result: error in line: import geopandas as gpd: ImportError: DLL load failed while importing _datadir: The specified module could not be found.
If there is a fix that I have not mentioned or if you suspect that I attempted one of the above fixed incorrectly because  of my limited understanding of how python libraries are stored please make a suggestion!
Thank you\nAnswer: I was struggling badly with the same problem for the last couple of days. Using conda, I've tried everything I found on the internet such as:
conda update gdal
conda update -n base -c defaults conda   
Creating new environments (over and over again).
Despite it's not recommended I even tried it with     pip install... but no results.
At the end what worked for",0.40816328,0.19085395,0.04722334444522858
67,"Question\nI want to make a calculator for my data.
Basically I have multiple measurements in different.csv files that are named as their physical representation (temperature_1, current_1, voltage_1 ecc.) and I am trying to make a calculator in python that given a certain expression [e.g. (current_1 * voltage_1) + (current_2 * voltage_2)] is able to load the data from each file and evaluates the result of the expression on the dataframes.
I already made simple functions in order to sum, subtract, multiply and divide dataframes but I am stuck on how to handle complex expressions like the sum of many multiplications [e.g. (current_1 * voltage_1) + (current_2 * voltage_2) + (current_3 * voltage_3) ecc.].
I tried to use a parser but still got no result.
Somebody has any idea on how to handle this?
Note: all the.csv have 2 columns, time and measurement, the number of rows are the same and the acquisition time is at the same timestamp.\nAnswer: I have solved the issue. For anybody who will need similar functions i report here the solution in steps

Write your equation with the name of your files. E.g. current_1 * voltage_1 (you need file current_1.csv, voltage_1.csv)

parse your equation with any parser. I used py_expression_eval.

Extract the variables from the equation (variables = parser.parse(equation).variables())

iterate over the variables and at each step:

load the data in a dataframe
insert the column of the measurement in general dataframe
change the name of that column to the name of your file (e.g. current_1)

by doing this you will obtain a dataframe with columns: time, measurement_1, measurement_2 ecc.

Use df.eval('result='+ expression, inplace=True) to evaluate your initial expression using the columns you have added to the general dataframe


Hope this helps somebody",0.0,0.07553372,0.005705342628061771
68,"Question\nI have the following data frame structure:




id_trip
dtm_start_trip
dtm_end_trip
start_station
end_station




1
2018-10-01 10:15:00
2018-10-01 10:17:00
100
200


2
2018-10-01 10:17:00
2018-10-01 10:18:00
200
100


3
2018-10-01 10:19:00
2018-10-01 10:34:00
100
300


4
2018-10-01 10:20:00
2018-10-01 10:22:00
300
100


5
2018-10-01 10:20:00
2018-10-01 10:29:00
400
400




And I would like to check, using python, how often a trip starts and ends in a given season. The idea was to do these average intervals per day, per hour and then in intervals of a few minutes.
What would be the best approach to doing this?
My desired output would be something to inform  eg: for station 100 on 2018-10-01, a travel starts, on average, every 4 minutes\nAnswer: In order to do that you could group your DataFrame by different travels. Firstly, I would make a new column with a travel id, so travels starting and ending in the same stations can be grouped.
Then you can easily group those rows by travel id and get all the information you need.
Please note that your data sample does not include any ""same travel"". Also, consider providing a code sample for your data, it would be easier for us to work with and run tests.",0.0,-0.04897839,0.0023988825269043446
69,"Question\nI am collecting time series data, which can be separated into ""tasks"" based on a particular target value. These tasks can be numbered based on the associated target. However, the lengths of data associated with each task will differ because it may take less time or more time for a ""task"" to be completed. Right now in MATLAB, this data is separated by the target number into a MATLAB cell, which is extremely convenient as the analysis on this time-series data will be the same for each set of data associated with each target, and thus I can complete data analysis simply by using a for loop to go through each cell in the cell array. My knowledge on the closest equivalent of this in Python would be to generate a ragged array. However, through my research on answering this question, I have found that automatic setting of a ragged array has been deprecated, and that if you want to generate a ragged array you must set dtype = object. I have a few questions surrounding this scenario:

Does setting dtype=object for the ragged array come with any inherent limitations on how one will access the data within the array?

Is there a more convenient way of saving these ragged arrays as numpy files besides reducing dimensionality from 3D to 2D and also saving a file of the associated index? This would be fairly inconvenient I think as I have thousands of files for which it would be convenient to save as a ragged array.

Related to 2, is saving the data as a.npz file any different in practice in terms of saving an associated index? More specifically, would I be able to unpack the ragged arrays automatically based on a technically separate.npy file for each one and being able to assume that each set of data associated with each target is stored in the same way for every file?

Most importantly, is using ragged arrays really the best equivalent set-up for my task, or do I get the deprecation warning about setting dtype=object because manipulating data in this way has become redundant and Python3 has a better method for dealing with stacked arrays of varying size?\nAnswer: I have decided to move forward with a known solution to my problem, and it seems to be adapting well.
I organize each set of separate data into it's own array, and then store them in a sequence in a list as I would with cells in MATLAB.
To save this information, when I separated out",0.40816328,-0.18257403,0.3489706218242645
70,"Question\nI want to make a binary classifier that classifies the following:
Class 1. Some images that I already have.
Class 2. Some images that I create from a function, using the images of class 1.
The problem is that instead of pre-creating the two classes, and then loading them, to speed up the process I would like the class 2 images to be created for each batch.
Any ideas on how I can tackle the problem? If I use the DataLoader as usual, I have to enter the images of both classes directly, but if I still don't have the images of the second class I don't know how to do it.
Thanks.\nAnswer: You can tackle the problem in at least two ways.

(Preferred) You create a custom Dataset class, AugDset, such that AugDset.__len__() returns 2 * len(real_dset), and when idx > len(imgset), AugDset.__getitem__(idx) generates the synthetic image from real_dset(idx).
You create your custom collate_fn function, to be passed to DataLoader that, given a batch, it augments it with your synthetic generated images.",0.40816328,0.43940198,0.0009758566739037633
71,"Question\nAfter learning how to code Python I'm starting to learn and figure out how to structure projects and set virtual environments up, but I can't make my mind up on how are packages managed when the virtual environment is activated. To make an example: I want to make a Django project. So, first of all, mkdir the project folder, cd into it and then execute python -m venv [whatever]. cd into Scripts folder and execute 'activate'. Then, pip install Django and pip list and shows Django. At last, I deactivate the virtual environment and make pip list again. Why is Django listed there? Should it?\nAnswer: You might have installed Django both inside and outside the venv (outside being the system python installation). Deactivate the venv and run pip uninstall django then try again.",0.0,0.18802708,0.03535418584942818
72,"Question\nAfter learning how to code Python I'm starting to learn and figure out how to structure projects and set virtual environments up, but I can't make my mind up on how are packages managed when the virtual environment is activated. To make an example: I want to make a Django project. So, first of all, mkdir the project folder, cd into it and then execute python -m venv [whatever]. cd into Scripts folder and execute 'activate'. Then, pip install Django and pip list and shows Django. At last, I deactivate the virtual environment and make pip list again. Why is Django listed there? Should it?\nAnswer: Okay, finally I understood what was happening and I didn't realize. On one hand, virtual environments are completely independent from the global or system, so what is being installed in one side it shouldn't affect the other. On the other hand, what happened was, that for being inside the Scripts folder, when trying to execute Python commands I was actually executing the scripts with the same name, which are copies to use with the virtual environment and which can be used calling the ""activate"" script. There was actually no problem, it was me messed up.
Thanks to all contributors for their help.",0.0,0.1065501,0.011352923698723316
73,"Question\nI have trained a Scikit Learn model in Python environment which i need to use it for inference in GoLang. Could you please help me how can i export/save my model in python and then use it back in GoLang.
I found a solution for Neural Network model where i can save Tensorflow model in ONNX format and load it using Onnx-go in GoLang. But this is specific for Neural Network models. But I am unable to figure it out for scikit-learn models.\nAnswer: You can develop an REST json API service to expose your scikit-learn model and  communicate with go client.",0.0,0.2962758,0.08777934312820435
74,"Question\nI want to train my YOLOv4 detector on 5 classes [Person,Car,Motorcycle,Bus,Truck]. I used around 2000 images for training and 500 for validation.
The dataset I used is from OID or from COCO.
The main problem is that, when the training is over, the detector finds only one class in the image every time. For example, if it's a human in a car, it returns only the Car or the Person bounding box detection.
I saw that the.txt annotation on every image is only for one class.
It's difficult to annotate by myself 10.000 images.
All the tutorials usually detect only one class in the image.
Any ideas on how to train my model on all 5 classes?\nAnswer: i finally found the solution.
The problem was that OID dataset downloads images with one specific class, like person, car etc.
AS Louis Lac mentioned i must train my model on dataset with all relevant classes",0.0,0.19470704,0.0379108302295208
75,"Question\nMy yolov5 model was trained on 416 * 416 images. I need to detect objects on my input image of size 4008 * 2672. I split the image into tiles of size 416 * 416 and fed to the model and it can able to detect objects but at the time of stitching the predicted image tiles to reconstruct original image, I could see some objects at the edge of tiles become split and detecting half in one tile and another half in another tile, can someone tell me how to made that half detections into a single detection in the reconstruction.\nAnswer: Running a second detection after offseting the tiles split would ensure that all previously cut objects would be in a single tile (assuming they are smaller than a tile). Maybe you could then combine the two results to get only the full objects",0.0,0.26821172,0.07193753123283386
76,"Question\nI have made an application using python using some libraries installed as needed on the go. Now I want to make it usable for person who doesn't know how to install dependencies and which ones are needed.
What to do to transform it into an easy to use application and probably make it able to run on Mac too??
Please suggest some resources that might help me know more.\nAnswer: As Wouter K mentioned, you should install Pyinstaller (pip install pyinstaller for pip) and then cd to the directory you want and type pyinstaller --onefile file.py on the terminal. If you cded in the directory your file is, you just type the name and the extension (.py) of your file. Else, you will have to specify the full path of the file. Also, you can't make an mac os executable from a non mac os pc. You will need to do what I mentioned above on a mac.",0.13605443,0.14665198,0.0001123082111007534
77,"Question\nHaving trouble getting Pandas data reader to retrieve price quotes from Yahoo’s API. The most up to date answer seems to be:

""pip install --upgrade pandas pip install --upgrade pandas-datareader

However, for the time being I will be using Google Collab and its Python platform, does anyone know how to update the pandas here? Or has the API truly just been discontinued?\nAnswer: In Colab you need to put a! before pip",0.13605443,0.36425102,0.052073683589696884
78,"Question\nNormally when I would click on ""Run"" button in VSCode for a Python script it would activate the currently selected virtual environment and simply call python <script_name.py> in the terminal and it all worked fine.
Now all of a sudden every time I try to run a script what is does is instead call a subprocess via conda like so:
conda run -n <environment_name> --no-capture-output --live-stream python <script_name.py>
And this new version is causing some issue because for whatever reason conda refuses to recognise some of the packages as having been installed. The code still works fine when I manually type the run command in the terminal but I want the old behaviour to be back.
Anyone knows how to fix this?\nAnswer: Input: Ctrl + Shift + P.
Enter: Terminal: select default profile.
Change the default to CMD.
Maybe this can help you.",0.0,0.19362962,0.037492431700229645
79,"Question\ni am learning how to use coverage package for evaluating which statements in django/python application are covered by unit tests and which are not. I ran command:
coverage run --source ""APP"" manage.py test && coverage report && coverage html
, which created htmlcov folder and multiple html-files inside it. Each file corresponds to the.py file, however in the beginning of each file-name, i see a strange prefix, something like this: ""d_10d11d93a0707d03_example_py.html"" instead of expected ""example_py.html"". I wasn't able to google any explanations to this. Please help if you know why is this happening and how to avoid this prefix if at all possible.
Thanks!\nAnswer: The prefix is there so that same-named files in different directories won't collide.  There isn't a way to avoid the prefix.  You should open the htmlcov/index.html file, and navigate from there.",0.0,0.26923412,0.072487011551857
80,"Question\nI have two questions:

Is it possible to increase somehow size of the icon in tkinter? Because the icon choosen by me is not clear visible when I use root.iconbitmap(..).
I also have another icon, but it's black and is not visible on black taskbar. What can I do?\nAnswer: I don't believe it's possible to change the size of the icon, unfortunately. However, make sure that the icon your using is a.ico file not some other format..ico files show up bigger and clearer than other formats.
As for the black icon problem, the color of the taskbar is determined by your Windows color settings. There isn't much you can do other than to make an icon that works for both dark and light colors. Although you could get rid of the default taskbar and make your own taskbar in tkinter with whatever colors you would like, if that's something you would like to undertake.",0.0,0.29017836,0.08420348167419434
81,"Question\nWhen trying to write script with python, I have a fundamental hole of knowledge.
Update: Thanks to the answers I corrected the word shell to process/subprocess
Nomenclature

Starting with a Bash prompt, lets call this BASH_PROCESS
Then within BASH_PROCESS I run python3 foo.py, the python script runs in say PYTHON_SUBPROCESS
Within foo.py is a call to subprocess.run(...), this subprocess command runs in say `SUBPROCESS_SUBPROCESS
Within foo.py is subprocess.run(..., shell=True), this subprocess command runs in say SUBPROCESS_SUBPROCESS=True

Test for if a process/subprocess is equal
Say SUBPROCESS_A starts SUBPROCESS_B. In the below questions, when I say is SUBPROCESS_A == SUBPROCESS_B, what I means is if SUBPROCESS_B sets an env variable, when it runs to completion, will they env variable be set in SUBPROCESS_A? If one runs eval ""$(ssh-agent -s)"" in SUBPROCESS_B, will SUBPROCESS_A now have an ssh agent too?
Question
Using the above nomenclature and equality tests

Is BASH_PROCESS == PYTHON_SUBPROCESS?
Is PYTHON_SUBPROCESS == SUBPROCESS_SUBPROCESS?
Is PYTHON_SUBPROCESS == SUBPROCESS_SUBPROCESS=True?
If SUBPROCESS_SUBPROCESS=True is not equal to BASH_PROCESS, then how does one alter the executing environment (e.g. eval ""$(ssh-agent -s)"") so that a python script can set up the env for the calller?\nAnswer: None of those equalities are true, and half of those ""shells"" aren't actually shells.
Your bash shell is a shell. When you launch your Python script from that shell, the Python process that runs the script is a child process of the bash shell process. When you launch a subprocess from the Python script, that subprocess is a child process of the Python process",0.40816328,0.6625432,0.0647091343998909
82,"Question\nI'm a python newbie and am using The Python Bible 7 in 1 written by Florian Dedov to learn.
I've been doing well but I've hit a barrier with list functions as described in the book and would like help.
I can't figure out how to use the list(element) function in the book it is described as typecasts element into list.
My attempt and error have been as follows
numbers= [10,22,61,29]
print(numbers(29))
which gives me type error 'list' object is not callable
How would you use this function properly?\nAnswer: print(numbers(29)) needs to be print(numbers[29])
The round brackets (i) are asking the interpreter to call a function with argument iand square brackets [i] is looking for index i in an object with indexes such as a list.",0.20408164,0.128268,0.005747707560658455
83,"Question\nI am quite new in Airflow. However I tried to delete some DAGs in airflow (manually; using just bottom),but after deletion I got message (so the physically DAG do not exist anymore)
Broken DAG: [/usr/local/airflow/dags/reports_general/templates/data_quality_report_airflow__.py] Invalid control character at: line 2 column 116 (char 118)
Anyone has idea how to resolve?\nAnswer: You have to delete the.py dag file from the dag storage volume and then also from the UI. In the version we use the UI only deletes the data for that dag from the metadata db.",0.0,0.118551314,0.014054413884878159
84,"Question\nNot able to figure out how i can get rid of '' character. I have tried few things but it didnt work out for me.
I think postgressql changes json string like that.\nAnswer: I do realize now that the back slash is a representation and its not coming when i am posting the data..
Thanks for taking the time to contribute an answer. It’s because of helpful peers like yourself that we’re able to learn together as a community :)",0.0,0.0812144,0.006595778279006481
85,"Question\nSo you know how all coding languages usually have a terminal command to run it, like this:
python3 main.py
And then it runs whatever is in'main.py'? I'm trying to make something similar to that, except it's for txt files, so when you run:
CUSTOM greeting.txt
It will tell Python to read everything in greeting.txt, so if 'Hello' is in greeting.txt, and you run CUSTOM greeting.txt, it will print out 'Hello' in terminal. Any help is appreciated!\nAnswer: In your example case, alias CUSTOM=cat in your shell to have cat do the heavy lifting, but in general, yeah, just like Python or any other program can read command line arguments, so could your hypothetical interpreter.
If you were to implement your language in Python, I'd tell you to look at sys.argv...",0.0,0.07675564,0.005891428794711828
86,"Question\nI want to regex a list of URLs.
The links format looks like this:
https://sales-office.ae/axcapital/damaclagoons/?cm_id=14981686043_130222322842_553881409427_kwd-1434230410787_m__g_&#038;gclid=Cj0KCQiAxc6PBhCEARIsAH8Hff2k3IHDPpViVTzUfxx4NRD-fSsfWkCDT-ywLPY2C6OrdTP36x431QsaAt2dEALw_wcB
The part I need:
https://sales-office.ae/axcapital/damaclagoons/
I used to use this:
re.findall('://([\w\-\.]+)', URL)
However, it gets me this:
sales-office.ae
Can you help, please?\nAnswer: Based on your example, this looks like it would work:
\w+://\S+\.\w+\/\S+\/",0.0,0.43838277,0.19217945635318756
87,"Question\nI want to create a python form with a textfiled and a button where a user will enter his/her link and it will create a webview flutter apk for that url. I want advice from the community if it is possible or not. If it is possible, please let me know how can I achieve it.\nAnswer: Maybe you can achieve this via using a remote server. You can create a connection between your python program and the server, and you can create the apk file in server and send it to your python program.",0.0,-0.011152029,0.00012436775432433933
88,"Question\nI try to intercept requests by seleniumwire.
If I dont use option --user-data-dir everything is fine. All requests are showed by driver.requests.
But I need parse some sites with authentication. So I provide in --user-data-dir option profile with remembered accounts. But in this case HTTPS requests not intercepted.
Command driver.requests showes only requests to google-ads and some other trash.
So how to intercept HTTPS requests while providing profile?\nAnswer: I should to disable all proxy extensions.
options.add_arguments(""--disable-extensions"")",0.0,0.32713664,0.107018381357193
89,"Question\nI have made a file.xinitrc in the users home directory. In the file I've put
exec python3 /srv/Game/main.py
But the python script doesn't run when launching x with
startx
What am I doing wrong? I thoght this was how you used.xinitrc\nAnswer: I solved it, and it was mostly issued with permissions in both the directory for the python file and for.xinitrc. Used chmod -R to solve it. Thank you for the suggestions.",0.40816328,0.20014721,0.04327068477869034
90,"Question\ndata = [{""name"":""Anne"", ""followers"":[""Brian""]}, {""name"":""Cindy"", ""followers"":[""Brian"",""G
osh"",""Anne""]},{""name"":""Dave"", ""followers"":[]}]
output :  [{""name"": [""Brian""], ""follows"":[""Anne"",""Cindy""]},...] etc...
my code from now :
from operator import itemgetter
data = data = [{""name"":""Anne"", ""followers"":[""Brian""]}, {""name"":""Cindy"", ""followers"":[""Brian"",""Gosh"",""Anne""]},{""name"":""Dave"", ""followers"":[]}]
x = list(map(itemgetter('followers'), data))
y = list(map(itemgetter('name'),data))
print(""name : "" + str(x), "" follow : "" + str(y))
how to get combine same values and get from key?
i think this is a list in dict...sorry if mistake. i am newbie using this language\nAnswer: It looks like there's a couple errors in your code. First off, in the place where you're defining x and y, it needs to be on separate lines. In other words, change
x = list(map(itemgetter('followers'), data)) y = list(map(itemgetter('name'),data))
to
x = list(map(itemgetter('followers'), data))
y = list(map(itemgetter('name'),data))
to be on separate lines. Also, since that is the only error in your code it looks like, what is it that you are trying to achieve with this code? Since when I run it, all it outputs is name : [['Brian'], ['Brian', 'Gosh', 'Anne'], []]  follow : ['Anne', 'Cindy', 'Dave']. Other than that, good luck with your programming adventures!",0.0,0.052369833,0.0027425994630903006
91,"Question\ntried adding it to code preference settings json as an executable path and it still defaulted to python 2.7 instead of 3.9.7 like i wanted it to. anyone know how to fix this im running visual studio code 2020 for mac
here is the path i added to the settings json
""python.pythonpath"": ""/Library/Frameworks/Python.framework/Versions/3.9/bin/python3""\nAnswer: On the MacOs, you need to take python3 instead of python to run the python3 interpreter. And of course, you can rename the python executable of the python2 to something others such as python2 to avoid the problem.",0.0,0.24773026,0.061370279639959335
92,"Question\non Stackoverflow I have found many posts about how to find the prevalent color of an image in python. However, I have not found anything about videos. Maybe because it could be possible to process each frame, but I was wondering if there is any easier method. In particular, I would like to study the dominant color of a whole movie.
Thank you a lot in advance.\nAnswer: A video consists of multiple frames(images). If you would like to get the dominant color, you should get it per image and merge your results.",0.0,0.021684825,0.00047023166553117335
93,"Question\nIn my company intranet, any request to an external website X in Internet will be redirected to an internal page containing a button that I have to click on. Then the external website X in Internet will be loaded.
I want to write a program that automatically clicks this button for me (so I don't have to click it manually). After that, the program will make the browser redirect to a re-configured website Y (not X) for the purpose of security testing.
I don't have much experience with Python. So I would be really thankful if someone can tell me how I can write such a program.
Many thanks\nAnswer: Python has Selenium and BS4 library to help You out, but if You are not experienced with python, You might as well pick up node.js and puppeteer, its far superior in my opinion.",0.0,0.12705147,0.01614207588136196
94,"Question\nI'm trying to connect my python scripts to an MySQL or MariaDB Server on my RaspberryPi4.
My python script right now just contains import mysql.connector. But when I try to start it via sudo python3 startdb.py I just get import mysql.connector ModuleNotFoundError: No module named'mysql' as an error.
I get an other error, when I start the script via sudo python startdb.py:  import mysql.connector ImportError: No module named mysql.connector.
I searched for a solution on many sites or forums. I mostly just found various versions of pip install mysql-connector-python (also with pip3, mysql-connector-python-rf or mysql-connector) to run but none of them worked for me. The only difference I recognized is that I previously got the error ModuleNotFoundError with both sudo python and sudo python3, but now I only get it with sudo python3.
Does anyone know how to solve this?
Could the fact that my script isn't in a sub-directory of /home/pi/, but instead of /home/, be the problem?
Edit: I just tried executing the script via the desktop mode using my mouse and just clicking on run and it worked. But when I'm using the command line in desktop mode or with a SSH session it doesn't work.
Another Edit: It looks like when I'm starting the script without sudo it'll work just fine. Don't actually know why's that, but I'm good for now. But would be very interesting to know and understand why the sudo makes it ""crash"".
Thanks and happy to hear some solutions :D
Cooki\nAnswer: raspbian give user mode in running, just in Desktop gives some permission to user for run app as root to access all necessary attributes, use sudo with all initial steps when you download and install project package's",0.0,0.0040453076,1.636451452213805e-05
95,"Question\nI am trying to use pyglet in a framework I am working on but I keep running into an error when I run it and cant tell if im a idiot or not.
Traceback (most recent call last:
File ""C:\Users\Admin3\Desktop\custom framework\main.py"", line 6, in <module>
import pyglet
ModuleNotFoundError: No module named 'pyglet'
I dont know if I have imported or installed it incorrectly but every time I run the code it comes back with this error, even though I have pyglet installed and have used it before. Does anyone know how to fix this?\nAnswer: Double check that you have python pyglet specifically installed. sometimes you need to have a version thats specific to python. if that doesn't work, try making sure everything is spelled correctly. looks fine to me, but idk.",0.0,0.16150355,0.02608339861035347
96,"Question\nI'm currently trying to use a number of medical codes to find out if a person has a certain disease and would require help as I tried searching for a couple of days but couldn't find any. Hoping someone can help me with this. Considering I've imported excel file 1 into df1 and excel file 2 into df2, how do I use excel file 2 to identify what disease does the patients in excel file 1 have and indicate them with a header? Below is an example of what the data looks like. I'm currently using pandas Jupyter notebook for this.
Excel file 1:




Patient
Primary Diagnosis
Secondary Diagnosis
Secondary Diagnosis 2
Secondary Diagnosis 3





Alex
50322
50111





John
50331
60874
50226
74444



Peter
50226
74444





Peter
50233
88888







Excel File 2:




Primary Diagnosis
Medical Code




Diabetes Type 2
50322


Diabetes Type 2
50331


Diabetes Type 2
50233


Cardiovescular Disease
50226


Hypertension
50111


AIDS
60874


HIV
74444


HIV
88888




Intended output:




Patient
Positive for Diabetes Type 2
Positive for Cardiovascular Disease
Positive for Hypertension
Positive for AIDS
Positive for HIV




Alex
1
1
0
0
0


John
1
1
0
1
1


Peter
1
1
0
0
1\nAnswer: Maybe you could convert your excel file 2 to some form of key value pair and then replace the primary diagnostics column in file 1 with the corresponding disease name, later apply some form of encoding like one-hot or something similar to file 1. Not sure if this approach would definitely help, but just sharing my thoughts.",0.0,-0.09257853,0.008570784702897072
97,"Question\nIm working on Topic Modeling on Twitter Data. I extracted the data and stored it in MySQL table. The columns are Date, Place, UserID, Text, tweetID, likes, weekID(based on date I have assigned what week it belongs to).
I have also taken data of each week and built a LDA model for each week. Im currently using pyLDAvis from Gensim to visualize the topics in each week.
Is there any way I can compare the LDA models which I have for each week. I want to compare them so I can see how a specific topic has been changing over the weeks.
Any ideas is much appreciated.
I have tried to build LDA models of each week and I have saved them into html and LDA model files. I want to see how topics have been changing between the weeks.\nAnswer: Building a separate LDA model per week means the algorithm's definition of different topics will change from week-to-week. That raises the thorny question of whether particular distinct per-week topics are in any way overlaps or evolutions of previous topics.
Such a comparison might be possible, but seems more complicated than an alternative approach: train your LDA model on the entire corpus, without segmentation by weeks. Then you have one model for ""the whole discourse"".
You could then provide the more-simple analysis of comparing how each week's articles reflect a different mix of topics. For example, does a single topic become more or less prevalent over time?",0.0,0.24365401,0.05936727672815323
98,"Question\nI am trying to compile some c++code that uses matplotlibcpp and I am having quite a difficult time. In my makefile, I am following the example on the matplotlibcpp documentation and adding a -lpython3.9 flag (I am using python 3.9 because my macs python2.7 doesnt allow me to link -lpython2.7).
When I try to compile I get an error stating:
""""ld: library not found for -lpython3.9""
I would like to know what the correct library name I need to use is so I can have access to python 3.9.
For context I am using clang++ and python 3.9 installed using homebrew
PS: I searched in my Versions/3.9/lib folder and it has a file called ""libpython3.9.dylib"" which seems like it might be what I want but I dont know how to include it that same way I would -lpython3.9.\nAnswer: What is the equivalent of -lpython2.7 for python 3.9 (ie where is the python3 library?)

It doesn't have much to do either with C++ or with makefiles, but with a conventional Python 3.9 installation, the analogue of -lpython2.7 is -lpython3.9.

When I try to compile I get an error stating:
""""ld: library not found for -lpython3.9""

That is almost certainly a sign that the library is outside the applicable library search path, not that you have an inappropriate -l option.  You would typically resolve that by using an -L/path/to/directory/of/libpython option earlier on the command line.",0.40816328,0.50205815,0.008816246874630451
99,"Question\nI am working on a project to convert Python code that's using the soon-to-be-discontinued Adwords API to the new version Google Ads API v10.
I have a query that needs a few metrics, but when I use the main customer ID that works to connect, I get REQUESTED_METRICS_FOR_MANAGER error saying I need to ""issue separate requests against each client account under the manager account"".
How do I generate a client account to do this? I haven't seen any examples of this step.
Thanks much!\nAnswer: The account ids I needed were in a local MySQL database that a colleague on the project guided me to look at.
So in general: check with your management and colleagues...",0.0,0.0254367,0.0006470256485044956
0,"Question\nI am using plotly to display a line chart and a pie chart using python, but when i run the code it opens the charts in two separate browsers. I was wondering how I can get both charts to open in one single browser.
The code for both works but separately on different browsers. I tried using the plotly.subplots library but this is what it prints:
ValueError: Trace type 'pie' is not compatible with subplot type 'xy'
at grid position (2, 1)
See the docstring for the specs argument to plotly.subplots.make_subplots
for more information on subplot types\nAnswer: Did you try using plotly.graph_objects.Pie?",0.0,0.14794317,0.02188718132674694
1,"Question\nIs there a way to create a nested bucket in minio using python?
I have not found exact documentation on how to do so however, I read some answer that one can create a folder (is that the same as a bucket?) inside a bucket using fput_object command.
I am not sure how to proceed! Super new to minio! Please help!\nAnswer: I got the answer however, I will leave the solution here just in case someone else gets stuck at the same place.
It was a simple detail I missed about put_object.
A folder can be created while adding an object to a bucket!
Here is how,
minioclient.put_object(
bucket_name,
f""folder_name/"" + ""object_name"",
data,
length
)",0.40816328,0.27530354,0.01765170879662037
2,"Question\nhow can I get every artist name from specific country? Is this even possible? I tried and I didn't find it\nAnswer: Artists on Spotify are not publicly tied to a specific country, and are simply ""on the platform"", therefore you can't get all the artists from a country. Spotify may collect details about their country based on publicly known information (such as their Artist bio), Spotify Premium subscription, Spotify for Artists account, or tax details, but none of this data is public (for obvious privacy concerns, primarily for smaller artists).
As mentioned by oda, you may be able to determine what (artist's) songs are available in a market, however this only tells you if you are able to listen to the song in your region. You can achieve this either by using the available_markets response on the Get Track endpoint, or searching by market as oda mentioned.",0.20408164,0.22913498,0.0006276696803979576
3,"Question\nI have made a Ai cursor controller by face land marks by using python open cv, So i need to use this module inside my web browser application that made from.net c# chromium framework. How can i do this, please help me
I need to embed python module inside.net c# project\nAnswer: Rather than trying to use the library directly from C#, you can create a Python based web service and invoke that service from your C# code. Data can be passed back and forth using MessagePack.",0.40816328,0.061353087,0.12027730792760849
4,"Question\nI am a beginner and I want to work with the pygame module. I have already used pip install to install pygame and it says so on my command prompt. On my command prompt it says:
Requirement already satisfied: pygame in c:\users\35192\appdata\local\packages\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\localcache\local-packages\python310\site-packages (2.1.2)
However, when I try to import pygame in vscode, the module won't import and it says: ""ModuleNotFoundError: No module named 'pygame'""
Does anyone know how to fix this?\nAnswer: I solved the problem. I basically just used Ctrl+Shift+P to open up the command palette. From there, I opened ""Python: Select interpreter"" and then I just changed the python interpreter to be the global one (which I assume is the one used by the command prompt)",0.20408164,0.12128097,0.006855951156467199
5,"Question\nIs it possible?
I want to create a command for my discord bot that makes the bot offline for everyone. I want to type that command in any discord server and then the bot just goes offline. This is much easier than going into your files or terminal to press ctrl+c. Is a command like this possible to even make? And if so, can I please know how to add that command? I want to add some admin commands to my bot and this is definitely a great addition to the list. Thanks -\nAnswer: If I'm understanding what you're trying to do here, you can just add a command that exits the bot.py program by making a call to something like sys.exit(). This would just terminate the program running the bot, making the bot go offline.",0.0,0.20020467,0.04008191078901291
6,"Question\nI am doing this project where i need to install a package called Twint.
I want to install this package and use it's commands in my VS Code.
What happends when i for example type this in my Windows CMD?
pip3 install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint
Because i can't type this in my VS code terminal, where i usually install packages with pip.
It will return an error that says ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?''
Now if i run this in my Windows Command it seems that i can't directly import the package in VS code?
Can anyone help me out with this confusion, where does the files get stored and how do i create good habbits around installing packages?
Hope someone understands what im struggeling with here.
Best\nAnswer: It is often the case that computers have more than one version of python installed and that editors like VS code use a different version than pip uses on the command line. pip installs packages where the version of python it is linked to expects them to be, but VScode doesn't know to look there.
It sounds like you have git installed where pip installs things, so you can upgrade from the command line without issue, but there's no installation of git where VScode is looking, so there's nothing to upgrade.
You either need to find where pip installs things and add it to the $PATH VScode uses, or try running a variation of python -m pip install --user git (specifying a specific url, or other things, as needed) from within VScode, which will ensure the package gets installed in a place that VScode looks for packages.",0.0,0.20858908,0.04350940138101578
7,"Question\nI am trying to create a script in Python to be used with Notepad++'s PythonScript plugin. So far I've gotten Pandas working correctly in the environment and I've read in a CSV and sorted it the way I desired, the end goal is to remove all of the text in Notepad (which I know how to work with at this point) and write in the text from my Pandas sorted CSV.
The issue is that when I write the text from that CSV to the console to check it, Pandas has reformated my CSV to make it easier to look at, it removes all of the quotes from the fields and adjusts the tab sizes (my files are tab delimited, with some tabs having different length). I need my CSV to be the exactly the same just sorted differently, If anyone can help it would be greatly appreciated.
Some statements I'm using:
(csv is a String containing all of the text in my CSV file)
panda_csv = pd.read_csv(csv, sep=""\t"")
sorted = panda_csv.sort_values(by=[""Name""], ascending=True, inplace=False)
console.write(sorted.to_string())\nAnswer: Since your original file seems to be tab-delimited, you can use the following to write output with a tab separator.
sorted.to_csv('output.csv', sep ='\t')",0.0,0.07040578,0.004956974182277918
8,"Question\nas the title says really. I've managed to implement a very simple LSTM model that takes an input sequence and outputs a given float value.
Firstly, I'd like to know if it's possible to get answers taken from a set of possible answers. E.g. if I know the answer should be in [1,2,3] to output the answer as being 1.
Secondly, if possible I'd like the output to be a probability distribution on the possible answers, e.g. [0.5,0.3,0.2].
I've implemented my simple LSTM model in Python using the various keras packages. Any pointers to the right direction to learn about how to implement this would be great!\nAnswer: LSTM is basically one type of recurrent neural network which provide many to many functionality. For that you need to add final dense layer with same number of input class in softmax layer, so you will get exact probability for each input class.",0.81632656,0.05390787,0.5812822580337524
9,"Question\nLet's say I want to create a blueprint of an online store that I can sell to potential clients. However, the idea is to make it so that the client chooses features that they want their store to have (from a pre-designated list) and pay only for those features, that way avoiding paying for something they don't need.
So for example, one client wants to have and online store with a Cart, Purchase History, Graphs, Reports. Another one only wants simple registration and cart (but maybe in future they will want me to ""enable"" other features as well).
My question is: what, in your opinion, is the best way to tackle this problem from a Django developer perspective?
My thoughts:

Create every feature as a separate app and deploy the whole thing commenting out the features that client did not pay for. And when a client wants additional features, I simply go to settings.py and uncomment the app they requested. Can I even make changes on the settings.py file on a live website without restarting it?
Create an admin panel for myself (the superuser) that will have an ability to turn features on or off. So when the client pays for a new feature, I simply go to my admin panel and put a tick mark next to it, and voila! That way I don't have to make changes in settings.py file (and I don't need access to the hosting server, just the django admin panel). Thought I'm not sure how to do this programmatically. I think I need a separate Model with a lot of BooleanFields(?)

What are pros and cons of each approach and what other ways to achieve this can you think of?
Edit 1.
To clarify, this will not be a shopify-style service, but simply a website that I will create separately for each client. But I want to have a blueprint to quickly create one and add/remove features on the fly.\nAnswer: I would recommend the admin panel approach.  If you plan to launch a service online that many people can access to generate a store front, you don't want to push code changes for a simple thing like adding a feature.  That would get cumbersome and is not a good practice.  Also modifying the settings.py file would typically need to have the web service (i.e. Apache) restarted so the file is reinitialized.
With the admin panel approach, then all you",0.40816328,-0.07419817,0.2326725572347641
10,"Question\nI used 3 different algorithms (Linear Regression, Logistics Regression, Decision Tree) to solve the same prediction problem and I have to compare their error measures. The problem at first was that the MAE, MSE, and RMSE values kept changing with each run, it was really problematic for me. The suggested solution was to use random_state.
The ""random_state"" argument works for Logistic Regression and Decision Tree but Linear Regression doesn't take this argument. In that case, how do I keep the error measure values from changing? Is there any alternative to ""random_state"" for Linear Regression?\nAnswer: The answer is simple : you don't need it since there is no local optima to stuck in with different random seeds
because generally in logistic regression problems; there is a global optimum.",0.0,0.23219591,0.05391494184732437
11,"Question\nI was able to use selenium to log into a scheduling website and click to the list of clients. Every client can be clicked on, to gather info about how many appointments they have left. What I want to do now is loop through all the clients, clicking on them, getting whatever info I need in an array or whatever (problem for later).
As of right now my main question would actually just be clicking on one client and then clicking on the next one until the list is complete. I can figure out the rest later.
How do I go about doing this? In previous questions I see that many people already have the list of URLs ready, here I obviously don't.\nAnswer: You can first fetch all the links you would want to click on by using
findElements method.
Then you will need a loop using foreach.
pseduo code will be
foreach(linkwebelement in listoflinks){
link.click
do your work
go back to page
}
you may come across in here stale element excpetion, if you do, you will need page handle again.
hope this helps.",0.40816328,0.058733582,0.12210111320018768
12,"Question\nwith other projects I've developed I have been able to hit 'run' to launch my application, and then shift+F10 from then on.
It seems excessively cumbersome to have to launch the terminal and type python manage.py runserver every time I want to test something...
Is there a better way? can I write a python file like run.py with some script that will launch the app, or some other solution?
Edit:
I worked out to add 'runserver' as an argument to the run config for manage.py.... is there a better way?
I've searched for ages, and all I can find is a thousand tutorials telling me how to start a project, rather than how to launch it more easily.
thanks\nAnswer: it' so simple. i've been trying all manner of hacky approaches to make a script, but i finally found 'django server' in the run configurations, complete with a launch browser option.
awesome.",0.0,-0.18786898,0.035294756293296814
13,"Question\nAbout 4 days ago, while I was doing my schoolwork I noticed that the variable colors don't turn blue like they used to, and it does not show me problems in the code anymore. I am a beginner in coding, so the ""not showing problem"" thing is a big issue for me. Would anyone know how can I get them back?
Also, this problem is in all of my Visual Studio Code tabs, so its not just a specific code doing it.
[EDIT: nothing that i tried fixed the issue, since uninstalling or changing files is not an option for me since i need administrator rights to do so [which i do not have], but it is alright, i have installed another linter and now everything should be fine]\nAnswer: uninstall Visual Studio Code
and delete user\AppData\Roaming\Code (windows)
and the install it again,
may be it's due to a buggy extension or some setting",0.0,0.18950856,0.03591349348425865
14,"Question\nAbout 4 days ago, while I was doing my schoolwork I noticed that the variable colors don't turn blue like they used to, and it does not show me problems in the code anymore. I am a beginner in coding, so the ""not showing problem"" thing is a big issue for me. Would anyone know how can I get them back?
Also, this problem is in all of my Visual Studio Code tabs, so its not just a specific code doing it.
[EDIT: nothing that i tried fixed the issue, since uninstalling or changing files is not an option for me since i need administrator rights to do so [which i do not have], but it is alright, i have installed another linter and now everything should be fine]\nAnswer: Maybe you need to install a vs code plug-in called pylance and make sure that it is not disabled in your workspace.",0.0,-0.06337422,0.004016291815787554
15,"Question\nI am currently working with cx_oracle
Here with SELECT statements I am able to use the fetchall() function to get rows.
But how to get the outputs for queries that fall under Data Definition Language (DDL) category.
For example, after executing a GRANT statement with cursor.execute(), the expected output assuming the query is valid would be,
""GRANT executed successfully""
But how do I get this with cx_oracle, Python.\nAnswer: The answer is that you have print it yourself, which is what SQL*Plus does.
DDL statements are statements not queries because they do not return data.  They return a success or error condition to the tool that executed them, which can then print any message.  In your case the tool is cx_Oracle.  There isn't a way to get the type (GRANT, CREATE etc) of the statement automatically in cx_Oracle.  Your application can either print a generic message like'statement executed successfully', or you can extract the first keyword(s) from the SQL statement so you can print a message like SQL*Plus does.",0.40816328,0.5268653,0.01409017015248537
16,"Question\ni want to control every pixel of my tkinter program but when i search how, i just found out that the only solution is to use 'canvas' and even if it allows us to draw lines, i think i didnt find out how to change for exemple the color of the pixel (x=200, y=478)
and i m also afraid that when i use my code on a really big size of pixels, the program becomes slow with tkinter.. i m still a beginner for those who havent noticed it yet.. so i m curious to know if there is no tool in tinker that actually is used like it takes the x, y of the pixel and the color you wanna apply! i guess it should be possible bcs how either tkinter is supposed to be programmed.. (i m still a beginner ^^) anyways if this tool exists pls tell me about it and if it is supposed to be as slow as canvas i can actually just try a better code..\nAnswer: so i m curious to know if there is no tool in tinker that actually is used like it takes the x, y of the pixel and the color you wanna apply!

There is no such tool. Tkinter simply isn't designed to do this. If you need to control individual pixels in a GUI, tkinter is the wrong tool to use.",0.0,0.21950471,0.04818231984972954
17,"Question\nI have clustered the pixels of an image into clusters of different sizes and shapes. I want to max pool each cluster as fast as possible because the max pooling happens in one layer of my CNN.
To clarify:
Input is a batch of images with the following shape [batch_size, height of image, width of image, number of channels]. I have clustered each image before I start training my CNN. So for each image I have a ndarray of labels with shape [height of image, width of image].
How can I max pool over all pixels of an image that have the same label for all labels? I understand how to do it with a of for loop but that is painstakingly slow. I am searching for a fast solution that ideally can max pool over every cluster of each image in less than a second.
For implementation, I use Python3.7 and PyTorch.\nAnswer: I figured it out. torch_scatter. scatter_max(img, cluster_labels) outputs the max element from each cluster and removes the for loop from my code.",0.0,0.08969021,0.008044333197176456
18,"Question\nI'm still new to coding and I'm asked to create a Patients part in a hospital management system
It should include:
1- Add Patients Records (Name Age, Gender, ID, Insurance, Phone Number)  (write it in file)
2- Add Health Condition (write it in file)
3- Add Appointments (write it in file)
4- Cancel Appointments (clear or remove whatever in file)
5- Get Appointments (read it from file)
6- Get Patients Records (read it from file)
I wanna make sure to use proper classes, methods, inheritance, overriding, polymorphism, reading and writing from files, and exception handling
The thing is I don't know how to start this and since this is a hospital management system, there will be a class named hospital and under it will be staff, patients, pharmacy, and etc
patients are considered what? should they be an individual class?
and how can I fill all the requirements?
So how this code should look like?\nAnswer: This question is too broad to be answered, but I will try to provide you with answers to your questions.

Will there be a class named hospital and under it will be staff,...

hospital would not have to be an object unless your system encompasses multiple hospitals (and each of them specialises in an area, etc...). By what I understand you don't need something like that, so you can ignore it.

staff, patients, pharmacy, and etc patients are considered what?

They would be each considered an object.

should they be an individual class?

Yes.
An example of where you can use polymorphism and inheritance would be between staff and patients. They are both people with common attributes such as name, gender, etc... So you could, create a superclass called person and have both staff and patient inherit it. Then create member variables and methods that are common to all people in the person class, while things such as health condition which are specific to patients can be kept in the patient class.
That was just an example. If you were to get a thorough understanding of the OOP concepts you mentioned yourself, everything would start to fit into place. It is also best if you created a class diagram, or some sort of overview diagram and look through the architecture of your system before you start implementing it.",0.40816328,0.26181293,0.02141842618584633
19,"Question\nI am working on a project which will require the user to enter the UI and create the table name on their own. Inputting table name and columns (column name, type, and other info).
Although it's easy to parametrize standard queries (i.e. insert/replace/update), I couldn't find ANY resource on how to parametrize DDL statements such as CREATE. Nor libraries that can handle that easily.
I was planning to apply (1) controls on the UI and (2) controls on the API I am going to call to run this DDL. But do you have any better idea/resource on how to get a CREATE statement from i.e. a JSON input? I am working on redshift.. Cheers!\nAnswer: I’ve used jinja2 templates and json config for this type of process.  It integrates with python and can be used standalone.  Just template your create table statements and apply the json config.",0.0,-0.0044056177,1.9409468222875148e-05
20,"Question\nskeleton for the code
The functions you need to write are as follows:
def create grid(grid):
""""""create a 4x4 array of zeroes within grid""""""
def print grid (grid):
""""""print out a 4x4 grid in 5-width columns within a box""""""
def check lost (grid):
""""""return True if there are no 0 values and there are no
adjacent values that are equal; otherwise False""""""
def check won (grid):
""""""return True if a value>=32 is found in the grid; otherwise
False""""""
def copy grid (grid):
""""""return a copy of the given grid""""""
def grid equal (grid1, grid2):
""""""check if 2 grids are equal - return boolean value""""""\nAnswer: If your util.py is just responsible for editing grid you can write code as you mentioned. But if your util.py has more functionalities, for instance opening dataset, multi dimension grid and... you can use classes. Put your function which related each other to the classes and import classes from util.py wherever you want. You can read single responsibility design pattern.",0.0,0.3475294,0.1207766905426979
21,"Question\nI came across everything related to iterators today and there are still some things I don't understand. I'm still a Python beginner but I'll try to explain the best I can.

I understand the difference between an iterable and an iterator. However, couldn't we just simply implement the __next__ method on a list and somehow make it to go back to list[0] when StopIteration was raised? Wouldn't that free up some memory as iterators also allocate space in memory? What's really the reason for iterators to exist?
I also understand the difference between a generator and a list, for example. Also, there is a type of iterator for each object (e.g range_iterator). How are generator_iterators different from other iterator objects? Are they added values on the fly?\nAnswer: Iterators (objects representing streams of data) are created because
iterables (which iterators can be created from) couldn't keep the index
of the last element returned, because otherwise, you would only be
able to use it one at a time. So for that reason, each time you iterate over the same object, a new iterator is created. From the Python Wiki:


Iterators will typically need to maintain some kind of position state information (e.g., the index of the last element returned). If the iterable maintained that state itself, it would become inherently non-reentrant (meaning you could use it only one loop at a time).


Generator iterators (which in turn are returned by generator
functions) are also streams of data but generated on the fly. From
the Python Documentation:


generator iterator
An object created by a generator function.
Each yield temporarily suspends processing, remembering the location execution state (including local variables and pending try-statements). When the generator iterator resumes, it picks up where it left off (in contrast to functions which start fresh on every invocation).",0.40816328,0.47914445,0.005038327071815729
22,"Question\nI want to change the title text to other names but i don't know the command.
Like from C:\windows\py.exe to a better looking title like Python but I cannot find how to do it.
I need the Python 3.10 version of Windows command 'title...'
Thanks for your help!\nAnswer: You can use os module system command os.system(""title your title"").",0.0,0.049262524,0.0024267961271107197
23,"Question\nI am new to RASA. I gone through updated documentation Rasa 3 but I don't know how to pre-process the message of the user before nlu-model.
e.g., if user enter hi, so i want to read that text before any action taken by rasa like tokenization etc.
If anyone can please guide me for this.
EDIT: I want to capture user text in rasa itself, before any other pipeline action, so that I can do my own processing. (for learning purpose)\nAnswer: In such scenario, you can handle the user message from front end (chatbot widget), specifically from JS script.",0.0,0.24393004,0.05950186401605606
24,"Question\nI have python code that depends on specific libraries like selenium and interaction with google chrome to extract data from the web.
my code works fine but i need a lot of records to do analysis, so i can't leave my computer on, to run the script for a month.
That's why I thought of running the script in a cloud service like aws but I don't have a clear idea of ​​how to do it, because I need the script to not stop
and I would rather not have to pay for it (or at least not that much money)
That said, my code opens a website, looks for a specific text data and saves it in a csv document.
I thank you in advance for the help\nAnswer: You will have to check the terms of each cloud service as many do have downtime/restarts on their free tiers.
The kind of task you're describing shouldn't be very resource hungry, so you may be better off setting up your own server using a Raspberry Pi or similar.",0.40816328,0.07510114,0.11093039065599442
25,"Question\nI want to remove backslash from a string,
I tried result.replace('\\','') but nothing changed.
Anyone has an idea how I can remove it?
result = '[(\'company_ids\', \'in\', company_id), (\'warehouse_ids\', \'in\', warehouse_id)]'\nAnswer: it's no a part of the string but you can use result.replace(""\"","""") to remove it",0.0,-0.005995631,3.594759255065583e-05
26,"Question\nI am a designer and I code only a little bit. I would like to know how I could use the output of the Twitter API (which I master) to use them on Unity. Which programming language would be the best? The goal is for me to do real-time creation on Unity.
Thanks in advance for your help! :)\nAnswer: I believe you can have your twitter bot communicate with unity by using files. So you can write in a file then read the content with C# during runtime.",0.20408164,0.059415936,0.020928164944052696
27,"Question\nI am trying to use wand on python on my Mac Monterrey. When I run the command on python  from wand.image import Image as Img I get the error ImportError: MagickWand shared library not found. You probably had not installed ImageMagick library.
This is what I had done, following diverse guidelines. Any suggestions as to how to make wand find imagemagick?
I installed imagemagick via homebrew. I can confirm that I have that the following items exist in my computer: /opt/homebrew/Cellar/imagemagick and /opt/homebrew/Cellar/imagemagick@6.
I also did a brew unlink imagemagick && brew link imagemagick
I added the following line to the end of my.zshrc:
export MAGICK_HOME=""/opt/homebrew/Cellar/imagemagick""
I installed Wand via pip install in my local environment\nAnswer: That was the wrong path to put in the.zshrc. The correct path was export MAGICK_HOME=""/opt/homebrew",0.0,0.48666954,0.23684723675251007
28,"Question\nI have 3 scripts at the end of each script i have a dataframe results and i want to run this 3 scrips from one script and to show results (3 dataframes) that i will regroupe in one dataframe.
If you know how to run this 3 scripts at the same time and get results in one file (Dataframe)\nAnswer: In scripts make sure you run them inside if __name__ == __main__: block(so you don't run then while importing). Turn those scripts into functions (or classes, depending on the structure of your code) and then import them to the main python file. Then write their results to one file inside the main script.",0.0,0.18654364,0.03479853272438049
29,"Question\nI am using the tf_Agents library for contextual bandits usecase.
In this usecase predictions (daily range between 20k and 30k predictions, 1 for each user) are made daily  (multiple times a day) and training only happens on all the predicted data from 4 days ago (Since the labels for predictions takes 3 days to observe).
The driver seems to replay only the batch_size number of experience (Since max_step length is 1 for contextual bandits). Also the replay buffer has the same constraint only handling batch size number of experiences.
I wanted to use checkpointer and save all the predictions (experience from driver which are saved in replay buffer) from the past 4 days and train only on the first of the 4 days saved on each given day.
I am unsure how to do the following and any help is greatly appreciate.

How to (run the driver) save replay buffer using checkpoints for the entire day (a day contains, say, 3 predictions runs and each prediction will be made on 30,000 observations [say batch size of 16]). So in this case I need multiple saves for each day
How to save the replay buffers for past 4 days (12 prediction runs ) and only retrieve the first 3 prediction runs (replay buffer and the driver run) to train for each day.
Unsure how to handle the driver, replay buffer and checkpointer configurations given the above #1, #2 above\nAnswer: On the Replay Buffer I don't think there is any way to get that working without implementing your own RB class (which I wouldn't necessarily recommend). Seems to me like the most straight forward solution for this is to take the memory inefficiency hit and have two RB with a different size of max_length. One of the two is the one given to the driver to store episodes and then rb.as_dataset(single_determinsitic_pass=true) is used to get the appropriate items to place in the memory of the second one used for training. The only thing you need to checkpoint of course is the first one.
Note: I'm not sure off-the-top-of-my head how exactly single_determinsitic_pass works, you may want to check that in order to determine which portion of the returned dataset corresponds to the day you want to train from. I also have the suspicion that probably the portion corresponding to the last day shifts, because if I don't remember wrong",0.40816328,-0.08369812,0.2419276386499405
30,"Question\nI'm working on Program, api application to python of telegram, in version 2, filter edited has replaced with a decorator of his own, @clinet.on_edited_message,
the problem is that I want to invert is action, and get only not edited messages, with filter is easy, add  ~ in the beginning, but how do I do that in a decorator? thanks\nAnswer: As the Release Notes for Version 2 describe, @app.on_edited_message() only filters edited messages. If you want non-edited messages, use @app.on_message() instead.",0.0,0.2719338,0.07394798845052719
31,"Question\nAnyone know how to actually enable auto imports? I've tried in the pref's auto-import settings, cleared my project cache and restarted but I never get any auto import prompts!
On mac, running latest pyCharm CE version.
this is not simply a matter of enabling the setting; that setting is enabled in the preferences but still I get no suggestions for auto-imports
ie
import requests
gets me nothing no popup, no suggestion to import it etc.\nAnswer: You can simply follow these steps:

In the Settings/Preferences dialog (Ctrl+Alt+S), click Editor | General | Auto Import.
In the Python section, configure automatic imports:
Select Show import popup to automatically display an import popup when tying the name of a class that lacks an import statement.Select one of the Preferred import style options to define the way an import statement to be generated.

Let me know how it goes!",0.0,0.2626562,0.06898828595876694
32,"Question\nI've been wanting to make a program that hides a folder that has a lot of my sensitive files on it.  I use python as my main programming language.
I need a way to hide the folder from Windows Explorer, even if the ""Show Hidden Files"" option is checked. I know how to hide it normally, right-clicking on the folder and checking the ""hidden"" option, but I need users to not be able to see it at all. I also need to be able to unhide it using the same program.
If anyone knows a solution, please let me know!
Thank you!\nAnswer: Theoretically, you could make a program that loads your folder into the RAM on startup while deleting it and re-saving the folder on shutdown, however this would be very risky when you press ""shutdown anyways"". If a certain condition is met, you could show the folder(e.g. saving it to file system).
Another Idea would be to try to remake it as the very infuriating folder in C:\Program Files\WindowsApps which you can only access in command-line
Also if you are able to complete this project, I would be very interested in using it too.",0.0,0.07053757,0.004975548479706049
33,"Question\nI've been wanting to make a program that hides a folder that has a lot of my sensitive files on it.  I use python as my main programming language.
I need a way to hide the folder from Windows Explorer, even if the ""Show Hidden Files"" option is checked. I know how to hide it normally, right-clicking on the folder and checking the ""hidden"" option, but I need users to not be able to see it at all. I also need to be able to unhide it using the same program.
If anyone knows a solution, please let me know!
Thank you!\nAnswer: I have opted for a secured, hidden folder marked as hidden, system reserved and encrypted, and requiring a python script to unlock. Using the attrib DOS command, but it does the trick. I'm also using EFS encryption on the folder, so even if the system and hidden attributes are cleared, people still can't access it.",0.0,0.09965438,0.009930994361639023
34,"Question\nI want my python code to be run in external terminal (window).
Obviously i have to edit 'launch.json' file, where I should
change the option 'console:internalConsole' to 'console:externalTerminal'.
Problem is, I don't find a 'launch.json' file. I guess I have
to set up one, but I'm not sure how to do it.
Seems that the extension 'code runner' could do this trick,
but the extension breaks down.
I tried to make changes in the settings menu, I chose code to be run
in external terminal, but it still uses the internal one.
May be you can give me a direction?
Thanks\nAnswer: it's done.Created launch.json file and 'console: externalTerminal'",0.0,0.0006522536,4.2543479139567353e-07
35,"Question\nmacos monterey 12.3.1
python -V: python3
which python3: /usr/local/bin/python3
alias python=""/usr/bin/python3""
path: /Users/Bill/Public/browser/depot_tools /usr/local/bin /usr/bin /bin /usr/sbin /sbin
proxy: proxychains4 + tor socks5 127.0.0.1 9150
cloned dedepot_tools in git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git
run gclient working good
run: fetch v8
run: gclient sync working good
run: tools/dev/gm.py x64.release
show:
env: python: No such file or directory
how to fix it?
should install python-is-python3?
brew info python-is-python3
Error: No available formula with the name ""python-is-python3"". Did you mean python-tk@3.9?\nAnswer: As a short-term workaround, you have two options:
(1) Create a symlink python in your $PATH that references python3. This can be done in one of several ways:

Some Linux distros have a python-is-python3 package for that
Some distros have other ""official"" ways to do it
Or you can just sudo ln -s /usr/local/bin/python3 /usr/local/bin/python (or whatever the correct paths are on your system).

(2) You can call gm.py with an explicit Python binary:
python3 tools/dev/gm.py
Medium-term, gm.py should be updated to require python3 directly. The fact that it doesn't do so yet is an artifact of the Python2-to-Python3 migration.",0.0,0.4794438,0.22986634075641632
36,"Question\nI installed WSL2 and use Visual Studio (VS) 2022 on the windows. I want to configure my VS 2022, so I can develop Python projects in WLS2. All tutorials I find online only talks about visual studio code. I don't want to install another IDE. Does anybody know how to do that or point me to some material?\nAnswer: Unfortunately not at this time, no.  The VSCode integration with WSL is done through the use of an extension that creates an interop server between Windows VSCode and the Linux side of WSL.  There isn't an extension like that for Visual Studio, however.
The only WSL integration that I'm aware of in Visual Studio 2022 is the inclusion of a WSL toolchain that allows you to target WSL/Linux in C++ projects.  But nothing that I'm aware of for Python, no.
I believe you'll need to install an additional IDE to get Python/WSL support, sorry.",0.0,0.22657222,0.05133496969938278
37,"Question\nI have a question, how do I refresh the page where I am currently in django? I am new with all this, add a favorite button, but I am doing a redirect to a page, it is wrong, since I only need the page where I am currently to be refreshed, could someone help me?\nAnswer: greeted, I just had to do a redirect to a request.META.get['HTTP_REFERER']",0.0,0.08961129,0.008030183613300323
38,"Question\nI want to create MS Teams chat-bot without using MS Bot Framework.
However, in the official document, there was only an example of using the MS Bot Framework.
I want to develop this process through message processing through FastAPI and my own AI logic.
Is there a guide for proper usage?\nAnswer: If you're only looking to send the odd notification to a user you can do this via Graph as Conrad said (there are reasons why this might not be the best solution and taking the bot approach is usually the better option - being able to send a notification from an ""app"" rather than a user with rich adaptive cards, for example)
The main thing to remember here is that a ""bot"" is just an api endpoint and the ""bot framework sdk"" largely exists to simplify the process of parsing and processing the messages that are sent on that endpoint (and some additional complexities around auth with the Azure bot service, etc). The interaction between Teams and your bot is also not request/response it's actually request/request and the SDK does a reasonable job of abstracting this so you don't have to worry about it.
Having said that, as long as you have an api endpoint that will accept the messages being sent from Teams (and proxied through the bot service) you don't have to use the sdk, plus it's all open source so you can inspect the framework to see what it's doing... I'd highly recommend using it though as it really does make your life a lot easier and some of those message structures aren't very well documented... the bottom line is that it's not trivial, but it is possible!",0.20408164,0.35855716,0.0238626878708601
39,"Question\nI have two large datasets. Let's say few thousands rows for V dataset with 18 columns. I would need to find correlations between individual rows (e.g., row V125 is similar to row V569 across the 18 columns). But since it's large I don't know how to filter it after. Another problem is that I have B dataset (different information on my 18 columns) and I would like to find similar pattern between the two datasets (e.g., row V55 and row B985 are similar, V3 is present only if B45 is present, etc...). Is there a way to find out? I'm open to any solutions. PS: this is my first question so let me know if it needs to be edited or I'm not clear. Thank you for any help.\nAnswer: Row V125 is a value, perhaps you meant row 125. If two rows are the same, you can use the duplicate function for pandas or find from the home menu in excel.
For the second question, this can be done using bash or the windows terminal for large datasets, but the simplest would be to merge the two datasets. For datasets of few thousand rows, this is very quick. If you are using a pandas dataframe, you can then use the append function to merge them and find the duplicates.",0.0,0.14591098,0.021290013566613197
40,"Question\nI want to implement the Cognito Hosted UI for some users in my Django application.
I have successfully been able to redirect the users to the desired url for authentication using the following:
return redirect(https://....amazoncognito.com/oauth2/authorize?client_id=....redirect_uri=localhost).
I am able to successfully authenticate and redirect back to my localhost where the url in the browser is localhost/?code=xyz. I do not understand how I can retrieve this code xyz back in python to perform next steps? I see that in the Django Terminal that it reads the required code. This is what the terminal shows:
[04/May/2022 16:08:15] ""POST /login HTTP/1.1"" 302 0
[04/May/2022 12:09:04] ""GET /?code=xyz HTTP/1.1"" 200 8737
I just do not know how to get this code xyz in my views.py so that I can continue the login. I tried variations of request.GET that did not work.
Any help is appreciated!!\nAnswer: I just figured it out 5 days later (what 5 days of not looking at your code can do!)
request.GET.get(‘code’) gives back the 'xyz' that shows up in the url in the browser.",0.0,0.07027787,0.004938979167491198
41,"Question\nI am trying to edit some docs on github, here are my instructions:

Clone the repo
Create a virtual environment
Install dependencies
Start a local server

I am able to 1 and 2 no problem, and 3 installs: mkdocs and mkdocs-material
I run into an issue on #4 with an error:
""ERROR    -  Config value: 'pages'. Error: The configuration option 'pages' was removed from MkDocs. Use 'nav' instead.
Aborted with 1 Configuration Errors!""
The MkDocs website does state this:
""Breaking change: the pages config option that was deprecated for a very long time now causes an error when used (#2652)
To fix the error, just change from pages to nav.""
I am not a dev, so not sure what changing pages to nav means. There are some files named pages, but changing to nav would be nav(2).
How would I fix this?
TIA!\nAnswer: So MkDocs no longer supports the 'pages' tag on generating the site. The replacement for it is called 'nav'.
So all you have to do generally is replace 'pages' in your mkdocs with 'nav' and that should fix it",0.0,0.33179128,0.11008545756340027
42,"Question\ncontext: I have a python program which relies on multiple libraries implemented in C++, exposed via pybinder.
My impression is: when python import xxx, it loads shared libraries (.so) into virtual memory. This is something I remember I read from book or webpage but cannot find it now.
I want to know how I can verify it in Linux? I tried to strace with event open, the only shared libraries opened are something like py/__init__.so (all python internal libraries), which doesn't look correct to me.
In the code base, I have multiple libcurl with multiple versions, all of them are introduced by 3-rd parties libraries. I want to know if I have a way to enforce when running python applications, they don't intervene?
For example: python program A relies on C++ shared library B and C.
B relies on libcurl (V1), statically linked.
C also relies on libcurl (V2), also statically linked.
Bad things could happen if B invokes libcurl (V2), since there might be conflict.\nAnswer: On Linux systems, you can see which dynamic libraries are loaded into a process by running cat /proc/pid/maps where pid is the relevant process ID.
About your second question: If the libraries are statically linked to libcurl then they cannot use the wrong library version: the linking is static.
If the linking is dynamic then only one libcurl can be loaded. However libcurl seems to be relatively API/ABI stable so this usually wouldn't be a problem.",0.0,0.43452752,0.1888141632080078
43,"Question\nI have a file that holds a JSON which is read and written when devices log onto a server I have.  Each device spawns its own process, and I have run into the issue where it looks like somehow two processes have written to the file concurrently. I assumed that using json.dump(object,open(filename,'w')) would be fast enough to prevent this issue, but clearly not.
Is there a way to block write access to an open file?  I didn't see a way to do this in the open() documentation.  I could control it with some sort of control file, but this seems like a problem that has already been solved.  I just don't know where to look.\nAnswer: While it is not the best way to do it, I personally would create a second file that contains a flag or a single integer or some text and use it as a marker that a process is using the file. Just remember to reset the file when the process is done writing to avoid locking the main text file forever.
But I would highly recommend trying felipecrp answer first.",0.0,0.11469549,0.013155055232346058
44,"Question\nI'm trying to debug pytest tests in Visual Studio 2022.  I have the tests showing up in test explorer and when I run them they execute, however when I set a breakpoint and try to debug the test the breakpoints show the warning symbol and when I hover over it, it informs me that the breakpoint will not be hit because symbols are not loaded.  The test does actually run, I just can't figure out how to get it to load whatever it needs to in order for me to debug.
It's python so there's no module to inspect in the modules window to tell it to load symbols for and nothing is compiled so I don't know where symbols would even be.  Is there something that needs to be configured in the pyproject (besides setting the test framework to pytest)?  Debugging in my other python projects just worked out of the box.  Is there a setting for loading symbols for your own python code?  If so, does anyone know where or why it would be disabled for pytest tests while testing?\nAnswer: It turns out this was a bug in VS.  Simply updating to the latest version resulted in everything magically working as expected.",0.0,0.2722497,0.07411989569664001
45,"Question\nI am following along an Edureka video from 2020 about random forest models in Python in preparation for a data science internship.   I am trying to follow the speaker's code, but I am running into an issue when I copied his
import pydotplus
code.   Here is the code I tried instead after some searching.   It is the first block that was not giving me a syntax error, but it seems to be taking an indefinite amount of time to run.
import sys
!{sys.executable} - m pip install pydotplus
Any ideas on how I can improve the code?\nAnswer: Edit
I think I found the issue.
pip install pydotplus
should work.   I think I just needed to refresh the kernel and run everything again!",0.0,0.22117567,0.048918675631284714
46,"Question\nimg_height,img_width=180,100 batch_size=32 train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir1,validation_split=0.01,subset=""training"",seed=123,image_size=(img_height, img_width),batch_size=batch_size)
Output: Found 1376 files belonging to 4 classes.
Using 1363 files for training.
how can I get the total number of classes in a variable?\nAnswer: label_map = (train.ds.class_indices)",0.0,0.23836637,0.05681852251291275
47,"Question\nI have a question about working with huge amount of data. I am working with Google Big Query (i don't think it is a problem of this DB) and need to SELECT data from one table, change it (using python) and then INSERT to another table. Could you tell me, how can i speed up these operations. I use the for loop for each row of my SELECT command. And working with only 15k rows is very long-time process. Maybe multithreading or some libraries could help me to do EXACTLY the same function to all of my >15k rows in DB. Thanks.\nAnswer: Missing some details about the process (which DB server is it?)
Anyway, The best approach would be:

Fetch by buffering: dbChunk = ""DB Cursor"".fetchmany(buffer_size)
Change the data in Python  Data structures (LIST) ==> dbChunk2
Load into second table, using ""DB Cursor"".executemany(InsertString, dsChunk2)

dsChunk2 is the updated LIST item where data was fetched into ([ (...), (...),... ])



you can tune the buffer_size to get the best results. (start with 1000,I think)
Note: InserString should be included Columns, Values and bind variables - match to the Select statement.",0.0,0.21239448,0.045111414045095444
48,"Question\nI'm just learning learning python and don't know many things. At the moment I'm building a telegram bot that can help you to find appropriate text to read in foreign lang.
The core function I want to implement is:
The bot suggest you the text to read based on you your vocabulary. (when you mark a text as ""read"", all the words are added to your dictionary. like that bot collects info)
For example you are user A, you know 500 words, and you want to get the text from the bot database where you know at least 75% of words or at least 90% of words.
Right now I have the database of user words and texts. How should I approach indexing, that whould tell me how many words user know from each text?
Obviously, I can compare the list of user words with the list of words from each text at every bot start. But I'm not sure if it is the most efficient way. Each time indexing 100+ texts feels like a strange idea.
Could you please suggest me where can I read about similar problems? Or how can i search it? I don't even know how to google it...\nAnswer: You don't need to process every text at every bot start.
Process every text once.
Then write the results of all the processing to a file. When the bot starts, recover the data by reading that data file.",0.0,0.10079265,0.01015915721654892
49,"Question\nI'm just learning learning python and don't know many things. At the moment I'm building a telegram bot that can help you to find appropriate text to read in foreign lang.
The core function I want to implement is:
The bot suggest you the text to read based on you your vocabulary. (when you mark a text as ""read"", all the words are added to your dictionary. like that bot collects info)
For example you are user A, you know 500 words, and you want to get the text from the bot database where you know at least 75% of words or at least 90% of words.
Right now I have the database of user words and texts. How should I approach indexing, that whould tell me how many words user know from each text?
Obviously, I can compare the list of user words with the list of words from each text at every bot start. But I'm not sure if it is the most efficient way. Each time indexing 100+ texts feels like a strange idea.
Could you please suggest me where can I read about similar problems? Or how can i search it? I don't even know how to google it...\nAnswer: You can use database like Elasticsearch that allows you to do full-text search. And when you'll query with user words, it will also give you confidence value, with which you can decide which text has better matching.",0.0,0.1449759,0.021018011495471
50,"Question\nI have a text file with lines like this one:
Cubo: 100% (left_x:  744   top_y:  395   width:  167   height:  181)
I would like to assign the appropiate int for each one of the variables, something like: left_x = 744, top_y = 395, width = 167, height = 181 but without having to do it manually.\nAnswer: You can open a file by using the open method. For example, you could do
external_code = open(""src/external_path.txt"")
... and get the text inside of the file by doing...
text = external_code.read()
Because it returns the string, you will need to parse the data from it.
For the parsing, I am unsure what would the best route for it. You could split the string different keywords and set your variable based on what number resides with the keyword...
Also, you may want to include more information next time you post; people tend to get a little annoyed with it. :/",0.0,0.13477534,0.018164392560720444
51,"Question\nWe use opnerp (odoo) on a linux server (debian), I want to locate the python interpreter used by the odoo daemon,
So the question is how I can change the path to my new python interpreter.
In other words, how does odoo choose its interpreter to run the modules?\nAnswer: In odoo-bin its called out like #!/usr/bin/env python3",0.20408164,0.099829435,0.010868522338569164
52,"Question\nMany answers about using the Youtube Data API v3 to get the thumbnail of a playlist; and many answers for how to get or set the thumbnail of a video.
But none about how to set the thumbnail of a playlist. The documentation shows nothing and no searches into the API documentation or Stack Overflow find the question or its answers.
I tried using the thumbnails().set() method for setting thumbnails to videos, but that returns a permission denied error indicating that using playlist IDs in its request for video IDs is not a good monkey patch.
Please help.\nAnswer: My original question is invalid.
YouTube allows users to set thumbnails of videos to be any properly formatted image. YouTube then only allows users to set a specific video thumbnail as the playlist thumbnail. It is not possible to set an image as the thumbnail of the playlist unless it is a thumbnail of a video in that playlist; and in that use case the video is associated to the playlist for the representative thumbnail.",0.0,0.3449781,0.11900988221168518
53,"Question\nI am doing a little project where i have prepared the back end in python and the graphic user interface woth HTML, CSS & JS.
The python script doesn't require any external librariesand the only thing that it does is opening a JSON file with the data processed, since it's a game the results are casually generated, and because of this I don't have to pass any parameters to the script.
The problem is that I don't know how to trigger the script so it generates the json that i can access trough JabaScript.
the scheme of the project is this:
index.html
pages (folder)

newGame.html
loadGame.html
rules.html

python (folder)

python_script1.py
python_script2.py
python_script3.py
main.py

specifically i have to trigger the script once the user has loaded the newGame page or the loadGame one.
(obiuvsly the js isn't node.js is actual client-side JavaScript)
I obiuvsly did some research and i found the pyodyde open source project does what i want, the fact is that i can't figure out how to connect the interface file with the back-end one with this resource.\nAnswer: But than, once i typed in the command and created the html file from the python script, how do I connect it to the ""newLevel"" page?",0.0,0.035174906,0.0012372740311548114
54,"Question\nI am researching uses for Javacards and smartcards utilizing
different ATRs. I want to change these card's ATRs via python to be
different from the OEM ATR that comes on these cards as default. I
figured out a way to change the card's atr, however, it is utilizing a
script online - which really doesn't help.
Below is the script I found to change the ATR using PyResMan
script mode, however, it is only set to one atr, which is a bank and I
have no use for it.
The Script I found Online: {
(Beginning numbers show line numbers)

00A4040010********************************

00F00000

C0D6029A02F807

C0D601240108

C0D601470108

C0D601260403600000

C0D601490403600000

C0D6012201FE

C0D601360E0D80318065B0893501F183009000

C0D601590E0D80318065B0893501F183009000

C0D603010101

C0D6030510404142434445464748494A4B4C4D4E4F

C0D6031d0101

C0D6032110404142434445464748494A4B4C4D4E4F

C0D603390101

C0D6033D10404142434445464748494A4B4C4D4E4F


}
The problem with this script is that I only know what the first 2
lines do, and that lines 9&10 are the lines that actually contain the
data for the ATR changing process. I need to know how I can change the
ATR using the ATR in regular expression form.
If anyone knows a way to decode this script, and be able to edit it,
or knows an easier way to change the ATR of an Unfused or non
pre-presonialized javacard, please let me know! I've been researching
those for over a month now, and cannot find an answer that actually
works.\nAnswer: Setting the ATR for a",0.0,0.28529727,0.08139453828334808
55,"Question\nLooking for a way to be able to find the color data of a pixel(s) on a Canvas. I'm sort of a beginner to coding, just gauging if this is possible/how to go about this. This is for a project I've been working on, and for part of it, it's necessary to compare the color data of a Canvas to an Image. I know how to find the per pixel color data of an Image, but I haven't found anything for a canvas. I'm using a Tkinter Canvas at the moment, but if it's not possible on that library, I'm open to switching to another library. Any ideas? Thanks guys!\nAnswer: Looking for a way to be able to find the color data of a pixel(s) on a Canvas.

Tkinter doesn't provide the ability to do this.",0.0,0.3152145,0.09936019033193588
56,"Question\nI have installed miniforge on my mac, in that using a 'local env' on jupyter notebook. I can't change the numpy version (need to downgrade to 1.19.5) on this kernel, have tried:
(1)pip install numpy==1.19.5 &
(2)conda install -c conda-forge numpy=1.19.5.
numpy version seems to be changing easily on conda-ipython3 kernel, but my project is running on 'local env'
very new to all this, still learning. Please help\nAnswer: first make sure that your local environment is activated by running:.../{your venv folder path}/Scripts/activate. Because if you install numpy on the wrong virtual environment then it won't work.
Then uninstall numpy by running pip uninstall numpy. Then install the numpy version you want.",0.0,0.1901815,0.03616899996995926
57,"Question\nThere probably isn't one ""right answer"" to this question. I'm interested in thoughts and opinions.
We have a couple hundred RHEL7/Centos7/Rocky8  nodes. Many of them have python modules installed via pip/pip3.
I've been searching for a best practices on routine/monthly patching these modules...so I far haven't found any. Obviously things installed with rpm/yum/dnf are pretty easy to deal with.
From the pip man page:
pip install --upgrade SomePackage
Great!
But how do you update all of them?
Sure. It is possible to do a ""pip list/freeze"" pipe that to awk...etc..
Surely, there's a better way. Ideally, one that captures things like ""boto3 V1.2 replaced with boto3 V1.3""
Right now it feels like I'm the only one thinking about this. Maybe I am and it is stupid. I'm ok with that response as well (but please tell me why).\nAnswer: A common solution is to deploy the application code inside a Docker container - the container image contains its own version of Python and all the dependency modules, so you don't have to update each module on all the host machines individually.  It also means that the combination of OS, Python and modules that you deploy can be tested and then ""frozen"" into an immutable image which is then deployed the same everywhere.

Right now it feels like I'm the only one thinking about this.

I realise the above answer is probably not helpful in your situation as you already have a fairly large system deployed...  but it might help to explain why not many people are developing solutions to your problem!",0.40816328,0.25421762,0.023699264973402023
58,"Question\nI need reflection, vision and documentation on my problem.
I wrote a python script to calculate something from an API and export the result in a CSV file. Then, I use a JavaScript script to display the data from this CSV file on a building website.
I need to have the latest data available for my website, so I opened a VM instance in Google Cloud Platform (Google Compute Engine) and set a Crontab job to run automatically my python script. The calculation is now executed every day and the result is exported to the CSV file, but stored in this VM instance.
Here is my goal: How can I get my CSV file on my website? The CSV is always on the virtual machine and I do not know how to communicate with my JavaScript script to the VM. Do I have to communicate directly with the VM? Do I have to go through another step before (server, API, etc.)?
I cannot find a specific solution for my problem on the internet.
Thanks in advance.\nAnswer: How can I get my CSV file on my website?

By making your python script output the CSV into your website's root folder.
Example, if you're running apache, chances are your root folder is somewhere in /var/www/html/...
If the script is generated from another machine (not the one with your website), then I would host it and make the server hosting your website fetch it via cronjob.
Basically:
If your CSV is generated from the same machine as the website that will use it - simply output it to the website's folder
If your CSV is generated from another machine, make it publicly accessible and have your website's machine cronjob fetch that CSV a few minute after it's generated.",0.0,0.33714134,0.11366427689790726
59,"Question\nI’m trying to figure out how to get a list of all substrings within a given range of lengths. For example, I have a string of n= 123456789 and I need to get all substrings between mmin and mmax. Let’s define mmin=7 and mmax=9.
How would you loop over the range of 7 to 9 to produce a list of substrings from n with a length that is within the range? The function should be dynamic to handle any range as long as it doesn’t exceed the length of the initial string. I’m expecting substrings with a length of 7,8,9\nAnswer: if mmax < len(n):
sub_string = []
for i in range(mmin, mmax):
sub_string.append(n[i])",-0.35714287,0.21373594,0.3259026110172272
60,"Question\nMy project is the conception and realization of a web application for the detection of ransomwares based on machine learning.
For the realization, I made a web application with python, Django, HTML and CSS.
On the other hand i have to create a machine learning code that makes the detection of these ransomware viruses.
Now what I have to do is deploy the machine learning code in my web app.
I'll explain a little how it works, the user first registers and then he chooses a csv file to scan, he uploads it and then he chooses the machine learning model he wants  use and then he clicks on scan, when he clicks on scan the file will be scanned by the model he has chosen and a result will be returned to him which is either 1: the file is a ransomware or 0: it is not a ransomware.
So,
I built the web app, I built the model
Now my problem is how to pass user input to the model
And than take the model's out put to the user.
Need help please.\nAnswer: I've managed to do something like this using a deep learning model.
It's fairly simple everything you're doing in python can be done in the django project you just have to create views and forms to handle the user data input.
So you're either new to django or want the whole code snippets?",0.0,-0.011070043,0.0001225458545377478
61,"Question\nI have a series of lists with 7 elements (but could be any number). I am trying to find the lists that have all positive elements, all negative elements, and those that change sign at ANY point,
for example [3, 6, 7, -2, -5, -1, -1] or [3, 1, -1, -2, -1, -5, -1].
Note that, though I used integers in my example, the lists are actually made of floats.
I can easily find all lists that are always positive or negative, but how do I find those that change sign (it could go from positive to negative as in example or from negative to positive).\nAnswer: Use the for loop to iterate from 0th element to penultimate element and compare the signs or check if the product of 2 consecutive numbers is less than 0.
If yes, then append the elements in the list.",0.13605443,0.08523387,0.0025827293284237385
62,"Question\nI have 2 Python servers which are cyclically receiving data from 2 different sensor systems. Both servers are running fine individually.
I would now like to run both servers simultaneously and process the received data together in one application which receives the data from one of the servers and then requests the data from the other server. However, I currently have no approach how I could implement this in an efficient way.
Does anyone here have an approach, a similar problem or an idea how this could be implemented efficiently?
Thanks in advance! :)\nAnswer: if you are working with IOT system, i think you should use mqtt broker",0.0,0.26209873,0.06869574636220932
63,"Question\nI have a file that is only local, and I wanted to see if there is a way to show the changes or last saves made to the file.\nAnswer: If you keep vscode up to date in recent update devs introduced local history that is independent of git. You can find it by searching for Local History in command pallet (you can access it with CTRL+Shift+P by default)",0.0,0.41628355,0.17329199612140656
64,"Question\nI have a problem at work where I need to perform series of sequential tasks on different devices. These devices do not need to interact with each other and also each of the sequential tasks can be performed on each of the devices independently.
Assuming I have Tasks (A->B->C->D)(Ex: End of A triggers B and end of B triggers C and so on), Devices(dev1, dev2) can execute these tasks independent of each other.
How can I design a centralized system that executes each task on each device. I cannot use Threading or Multiprocessing due to Infra limitations.
I'm looking for some design suggestions(Classes) and How I can go about designing it.
First approach I thought about was brute force where I blindly use loops to loop over devices and perform each task.
Second approach I was reading about State Design Pattern and I was not sure how I can implement it.
EDIT: I have implemented the answer I have provided below. However I would like to know the correct way to transfer information between states. I know states needs to be mutually exclusive but each task needs to access certain resources that are common amongst all the resources. How can I structure this?\nAnswer: I probably try something with flask for super simple api and a client app on devices that ""pool"" data from center api and post results so center server know the progress and what is current used. client app would be super simple loop with sleep so it wont 100% cpu without needed.",0.0,0.10725069,0.011502710171043873
65,"Question\nI have running python file ""cepusender/main.py"" (and another python files), how can i restart/kill only main.py file?\nAnswer: kill is the command to send signals to processes.
You can use kill -9 PID to kill your python process, where 9 is the number for SIGKILL and PID is the python Process ID.",0.0,0.30411434,0.09248553216457367
66,"Question\nI need help trying to find out how to get the X position of the right side of the text, not to be confused with anchors or alignments, I haven't seen this be answered anywhere so if you could help then that would be great!\nAnswer: Sovled it on my own:
What you need to do is:
Provide the text e.g: test = ""hello""
then get mask and box e.g: test_width = FontNameHere.getmask(test).getbbox()[2]",0.0,0.13182771,0.0173785462975502
67,"Question\nI am currently trying to develop an employee scheduling tool in order to reduce the daily workload. I am using pyomo for setting up the model but unfortunately stuck on one of the constraint setting.
Here is the simplified background:

4 shifts are available for assignation - RDO (Regular Day Off), M (Morning), D (Day) and N (Night). All of them are 8-hrs shift
Every employee will get 1 RDO per week and constant RDO is preferred (say staff A better to have Monday as day off constantly but this can be violate)
Same working shift (M / D / N) is preferred for every staff week by week (the constraint that I stuck on)
a. Example 1 (RDO at Monday): The shift of Tuesday to Sunday should be / have better to be the same
b. Example 2 (RDO at Thursday): The shift of Mon to Wed should be same as the last working day of prior week, while the shift of Fri to Sun this week also need to be same but not limit to be which shift

Since the RDO day (Mon - Sun) is different among employees, the constraint of point 3 also require to be changed people by people conditionally (say if RDO == ""Mon"" then do A ; else if RDO == ""Tue"" then do B), I have no idea how can it be reflected on the constraint as IF / ELSE statement cant really work on solver.
Appreciate if you can give me some hints or direction. Thanks very much!\nAnswer: The constraints you are trying to create could be moderately complicated, and are very dependent on how you set up the problem, how many time periods you look at in the model, etc. etc. and are probably beyond the scope of 1 answer.  Are you taking an LP course in school?  If so, you might want to bounce your framework off of your instructor for ideas.
That aside, you might want to tackle the ROD by assigning each person a cost table based on their preferences and then putting in a small penalty in the objective based on their ""costs"" to influence the solver to give them their ""pick"" -- assumes the ""picks"" are relatively well distributed and not everybody wants Friday off, etc.
You could probably do the same with the shifts, essentially making a parameter that is indexed by [employee, shift] with ""costs"" and using that in the",0.20408164,0.30836445,0.010874904692173004
68,"Question\nI deleted.idea on pycharm, how can I regenerate it?
I tried looking for project structure but can't find it.
I searched in Google ""how to regenerate.idea"" and found no explanation.
How can I do it on Mac, and how can I do it on Windows?\nAnswer: Re-opening the directory/folder from within PyCharm will create a new.idea directory structure.",0.40816328,0.3586077,0.0024557544384151697
69,"Question\nAm building a model using K nearest neighbours. The model predicts if someone has cancer or not. 1 for true, 0 for false. I need the model other than predicting presence of cancer or not giving a 0 or 1,how can i make the model also show the probability of the prediction being 1?
Edit:Am doing a project and it specifies i use the K nearest Neighbour classifier\nAnswer: You must use a regressor instead of a classifier, meaning that a regression model can give you a probability of someone having a concern or not and the probability between the two values of 0 and 1, 0~1 (0~100%).",0.0,0.12746954,0.016248483210802078
70,"Question\nI hope this isn’t off topic, I am not really sure which forum to use for a question like this:
I have a series of datapoints of about an hour in time from a sensor that retrieves data 20 times per second. Along with it I receive timestamps of a periodic event in this data in the format of %Y-%m-d %H:%M:%S.%f, which looks e.g. like this 2019-05-23 17:50:34.346000.
I now created a method to calculate these periodic events myself and was wondering how I could evalute my methods accuracy. My calculations are sometimes bigger and sometimes smaller by a few milliseconds compared to the actual timestamp. But when I run my own calculated timestamp against the actual timestamp by using pythons scipy.stats.pearsonr(x,y) method, I always receive a correlation of nearly 1. I assume that‘s because these small differences in the order of millisenconds don‘t seem relevant in an hour of data. But how could I evaluate the accuracy of two timestamps a reasonable way? Are there better metrics to use than the correlation?\nAnswer: It seems that you are trying to compute a linear statistical correlation (pearson) for something that is, by nature, a timeseries data. This will not tell you much and drawing a conclusion based on the results is dangerous.
It so happens that your two vectors x and y are growing linearly in the same direction which is not surprising given that they are timestamps.
Let's take an example for stationary data and time series data:
Time series data:
Your sensor starts giving measurements at time t1 and continues to do so until time t2 is reached. You compute the periodic event's timestamp using your own method then compare it to the actual timestamp. However, there is no reliable way using linear statistical correlations to see if the two are related and how related are they.
Stationary data :
Now consider the same sensor giving measurements, but now instead of computing your periodic events all at once, take a single event and compute it multiple times using your empirical data using different measurements (so forget about any notion of time at this point (i.e. repeat the measurement multiple times). The result can be averaged and an error on the mean can be computed (see info on standard error). This, now, can be",0.40816328,0.39694148,0.00012592870916705579
71,"Question\nI need advice. I want make 3 types of user:

member (only nickname/password/email)
company (nickname/password/email/company/regon + permision for edit model)
admin

My question is aobut how make model for this users:

make big model for member and company together but field which are only for comapny make empty for member. Next to by admin panel i can make group and add ""comapny"" ppl
make 2 type of user (here i will need advice like what i should use on it) and seperate website register for member and company and login should be the same form

Thank you for answer\nAnswer: You can achieve this simply by creating a Boolean field for each type of user (member, company, admin) in your User model. Save the different type of users from different URLs, when save just change the Boolean flag into True. Then, you can handle the user in view like if user_type.member: and so on. It also can be achieved by using Enum (choice) field.",0.0,0.23915702,0.057196080684661865
72,"Question\nI want to run a Python script in my terminal (mac) that takes a csv file as input. At the beginning of the Pyton script, a package named cvxpy is imported. When running the code with data in the terminal I get the error:
ImportError: No module named cvxpy.
I'm feeling it's a directory fault, but I don't know how to fix this (e.g. how to get the Python script and python packaga in the same directory)
Somebody got a clue?
Thanks.\nAnswer: You need to have the module installed.
To install it, type : pip3 install cvxpy
If you already have it installed, please double check by typing pip3 list",0.0,0.38864762,0.1510469764471054
73,"Question\nIm trying to convert pine script to python. I use security function for requesting data in pine script. Can someone help me how the exact implementation would be?
security(tickerid, timeframe, close, gaps=barmerge.gaps_off, lookahead=barmerge.lookahead_on)
Also im using lookahead_on (this is basically used for backword compatibility for pine script version i guess)
I tried to find security function lib code but couldn't find it.\nAnswer: For example if running a strategy looking at the 4h timeframe candle stick. You can have a timer running on an hourly timeframe and look a the 4h timeframe candle stick for the candle which hasn't yet closed and do your calculations there.
One drawdown from making calculations before the candle stick is close is that the trend may change and produce a different outcome by the time the candle is closed.
It may give you early signals but it may also give false signals. Pick your poison.",0.0,0.19450152,0.037830840796232224
74,"Question\nWhen preforming image co-registration of multiple subjects, how should we select the reference image?

Can a randomly selected image form one dataset could be the reference image for an image from the other dataset?
If we do that, should all the images belonging to the reference image dataset be co-registered with the reference image as well?

I couldn't find any material in this area. Could someone please advice?\nAnswer: I'm not sure exactly what you mean by the term ""dataset"", but I will assume you are asking about co-registering multiple images from different patients (i.e. multiple 3D images per subject).
To answer your questions:

If there are no obvious choices about which image is best, then a random choice is fine. If you have e.g. a CT and an MRI for each subject, then co-registration using the CT images is likely going to give you better results because of intrinsic image characteristics (e.g. less distortion, image value linked to physical quantity).
I suppose that depends on what you want to do, but if it is important to have all imaging data in the same co-registered reference space then yes.

Another option is to try and generate an average image, and then use that as a reference to register other images to. Without more information about what you are trying to achieve it's hard to give any more specific advice.",0.40816328,0.16292995,0.060139384120702744
75,"Question\nI recently came across the Google Coral dev board mini in search of a ML microcontroller platform for a robotics and speech recognition project. I realized that there are minimal tutorials on creating projects from scratch for the dev board mini but a ton of example projects. The problem with these example projects is that it gets imported through a git clone through the Mendel linux terminal, which doesn't really tell me how to create my own project and where to compile and code it. To make things more clear I will use the ESP32 dev board as an example:
To write a program (C++) on a ESP32 dev board that controls the I/O pins, I used PlatfromIO to compile and flash the microcontroller. What IDE can be used to perform the same functionality on the Google-Coral dev board mini? Does there exist an article about this?
Sorry if my question seems obvious, but I feel that I spent too much time searching for the solution. Thanks in advance! :)\nAnswer: The Google Coral dev board mini is a single board computer running a full Linux operating system, whereas the ESP32 is a microcontroller which does not run an OS, it is only flashed with a single C++ program. The difference is the same as a Raspberry Pi and an Arduino. Since the Coral dev board has a full OS, you can plug a monitor, mouse, and keyboard into it and develop code directly on it. Or you can use your PC to ssh into the coral board to copy files over and remotely run commands. Through these ways you can use any IDE you want.",0.0,0.2646122,0.07001961767673492
76,"Question\nim trying to learn how to fetch API using python-flask with heroku as the cloud service
in the app, there's a key to communicate with the source server and i set the config variable (config vars) like
X_App_APIKeys = {key strings} 
how can i use the variable as an object in my code?
i.e : myapikeys = X_App_APIKeys\nAnswer: Simply import os
Then myapikeys = os.environ['X_App_APIKeys']
Notice that this only works on the live app deployed on Heroku when the config has been set up. To run your local instance, replace it with your real key instead.",0.0,0.2789172,0.07779479771852493
77,"Question\nI have a Python file on a Unix server, that will run 18 hours a day. The file will be run with Cron each day. The file has A while True loop with sleep. Assuming there is no uncaught exception, can this file run for this long? or how long can it run?
I have a background in PHP, and know there is a maximum execution time, is there a such thing in Python?\nAnswer: Yeah, no problem. Python files are interpreted without such configuration in mind, so the process will run as long as you need it to.",0.0,0.2569431,0.06601975858211517
78,"Question\nI'm writing a simple mod manager which uses the nexus website to download the mod. When on the website, there is an option to download a given mod as a file or via a mod manager, the latter being an ""nxm"" link. I figured out how to associate a program with the nxm link through registry editor, but now I don't know how to get any information from that link within the python program. Is there a module that will do it? Essentially, I want to know how to open a python program via a link and have the link as an object which I can use in the program.\nAnswer: Try using py-script and javascript alongside.",0.0,0.20922428,0.0437748022377491
79,"Question\nI'm using django to create an webapp. I have been asked if I can look at creating a sort of double log in system.
For example: A nurse on the ward will open up my webapp and will be met with a login page for their ward. So all nurses on that ward will have that generic log in.
Once they've logged in they will be met with a second login page where they can enter their normal user credentials that only they know.
I'm quite stuck at how to achieve this currently - or if someone could suggest an alternative method I'd be grateful!\nAnswer: I would recommend authenticating the user normally, then use some property of that user (maybe a permissions group etc.) to represent the ward they belong too.  Then it follows to restrict that user to information/pages relevant to their ward",0.0,0.084935665,0.007214067038148642
80,"Question\nI have been looking for a while at large number generation through seeds and I’ve struggled to find what I am looking for due to how specific it is. I think the main thing that is causing the problems is the size of numbers I want to generate, for example... I want to have it so that I can generate every single number that is 5000 digits long through some form of seed or selective input process. The other problem being that I would like to be able to reverse the process to know how I got there. If I have this number I want to know what inputs or what seed would generate this. Hope this makes sense and I understand this is a extremely specific question.\nAnswer: For a problem like this I would use an encryption. Since encryption is a one-to-one process provided you use unique inputs, and keep the same key, then the outputs are guaranteed unique. for unique inputs, encrypt the numbers 0, 1, 2, 3,... You will need to extend the inputs by adding leading zeros so they are the size you need, enough for the binary equivalent of 5,000 decimal digits.
The output from the encryption will be the same size. Just convert the binary output back into the equivalent decimal number. There may be a few boundary cases just over the limit where the decimal and binary boundaries; in those cases just re-encrypt until the result is within the set limit. This is the cycle walking technique from Format Preserving encryption.",0.40816328,0.25025177,0.024936044588685036
81,"Question\nI am training a CNN model to classify simple images (squares and crosses) and everything works just fine when I use the cpu but when I use the gpu everything works until the training starts and i get this error:
2022-06-15 04:25:49.158944: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401
And then the program just stops.
Does anyone have an idea how to fix this?\nAnswer: if you use pycharm, you can select the ""Emulate terminal in output console"" option to print detailed error information.
Run->Edit Configration->Execution ->Emulate terminal in output console
On windows, maybe CUDA is missing zlibwapi.dll file, and you can download it and  move it to bin of cuda.",0.81632656,0.22591281,0.34858840703964233
82,"Question\nI want to save my codes and i want to restrict it that no one can change it even myself.
I don't know how to do it. is it possible?
I know how to share without edit permission. but how to do it for myself\nAnswer: There's a Playground Mode in Colab (check File menu), it still allows you to run and change everything but nothing will be saved, that applies to output too. You can also create links to Playground mode Colab Notebooks yourself by appending #forceEdit=true&sandboxMode=true to the URL, i.e colab.research.google.com/drive/1R4O...?hl=en#forceEdit=true&sandboxMode=true  and perhaps collect those links into single Notebook and access those files from there.
But even if you accidentally do mess up your Notebook, in most cases you can still roll back to pervious revision through Revision history, you can name revisions and manually save & pin your Notebook.
Colab also integrates with Github and you can conveniently save the copy there as well for proper version control - File >> Save a Copy in Github
Works with private repositories and Github Gists too. And by default the ""Open In Colab""  button is added to those files, so no need to manually re-import anything to Colab or GDrive.",0.0,0.38590264,0.14892084896564484
83,"Question\nI am having issues with running yolov5 on colab. I was able to run the code fine when I had  I had more classes, and a slightly smaller dataset, but now I have decreased the amount of classes and 70 instances when the overall one has 3400 instances. Now I am getting this error.
terminate called after throwing an instance of 'c10::CUDAError'
Other times I will get
 cuda assertion index >= -sizes[i] && index < sizes[i] && index out of bounds
any idea what could be causing this and how to fix it?\nAnswer: The issue was that I was not outputting any labels that I said existed. So for example if I claimed that there would be labels ""0"",""1"",""2"" in the training dataset. There was no instances of label ""2"".",0.0,0.27706861,0.07676701992750168
84,"Question\nI am using Pandas ExcelWriter to create an excel file from a dataframe. I have also applied formatting on the excel file like Font size, font colour etc
Now I am trying to convert the excel to CSV using to_csv method.
After conversion, the CSV file is not retaining any formatting done previously.
My question is how do I retain formatting in CSV?\nAnswer: CSV cannot store formatting. If you want that, save as an excel file. (Or of course other outputs that save formatting - including HTML - but have other feature drawbacks - it depends on what you need.)",0.81632656,0.121210456,0.48318639397621155
85,"Question\nI have a column in a dataset that has string and digits, (Column 2),
I need to extract digits with 10 or more. as (Column 3) / output.
any idea how to do this?




Column1
Column2




A
ghjy 123456677777 rttt 123.987 rtdggd


ABC
90999888877 asrteg 12.98 tggff 12300004


B
thdhdjdj  123 jsjsjjsjl  tehshshs  126666555533333


DLT
1.2897 thhhsskkkk 456633388899000022


XYZ
tteerr 12.34




Expected output:
|Column3|
|-------|
|123456677777|
|90999888877|
|126666555533333|
|456633388899000000|
| |
I tried a few codes, regex, lambda function, apply, map, but is taking the entire column as one string. didnt want to split it because real dataset has so many words and digits on it.\nAnswer: Maybe this works:

Take the value of the Column 2
Split the values
for loop the values
Check if the value is numeric and if the length is equal or greater than 10
Get the value if the previous validation is true
Set the value to the Column 3",0.0,-0.19200301,0.03686515614390373
86,"Question\nI'm trying to get the input from a tkinter text widget, but it is coming up with the error:
_tkinter.TclError: invalid command name "".!text""
Here is where I define the textbox: textbox = tkinter.Text(main_root)
And this is how I call the get() function: textbox.get(""1.0"", ""end-1c"")
Anyone got any ideas what is going on? Usually this works fine.\nAnswer: I was calling textbox.destroy() before calling the get() function. This is what caused the error to be raised.",0.0,0.30373096,0.09225250035524368
87,"Question\nI'm trying to change the positions of spellers to a simple format. By changing RW to forward or CM to midfielder. Only there are several values ​​in a cell. How do I combine or drop the other values ​​in the cell?




player
player_positions




messi
RW, ST, CF


Ronaldo
ST,LW




how do i change RW, ST, CF just simple to Forward?
Am trying:
df.replace(to_replace=r'^RW', value='Forward', regex=True)
but then i get:




player
player_positions




messi
Forward, ST, CF


Ronaldo
ST,LW\nAnswer: You can add everything in the replace statement.
df = df.replace(to_replace=r'^RW, ST, CF', value='Forward', regex=True)
or
df = df.replace(to_replace=r'^RW\D*', value='Forward', regex=True)",0.40816328,0.1410647,0.07134164869785309
88,"Question\nI have been encountering this message after trying to import numpy, pandas, matplotlib, and seaborn all by themselves. I am not sure how to fix this. Any suggestions?
I am using Python 3.8.8, matplotlib 3.3.4, pandas 1.2.4, numpy 1.20.1, seaborn 0.11.1.
I have recently updated my Anaconda navigator to 2.1.0. Would this possibly have caused any issues?
In the shell command, after trying to import each of those packages individually, I see this message:
Intel MKL FATAL ERROR: Cannot load libmkl_intel_thread.1.dylib.\nAnswer: Solution: I reinstalled Anaconda Navigator.",0.20408164,0.25883728,0.002998180454596877
89,"Question\nI have a streamlit app that is set to run by doubleclicking a *.bat file that contains the command to start the app: streamlit run myApp.py. Does anyone know how to force that app to run in specific python version? My base python is 3.9, I created an environment (conda env create...) based on python 3.6 where I installed streamlit and created the app and I want it to run there. I open sypder using that environment, double-checked that the env is active (!python --version in the spyder console) but when I run the app using the *bat file, it shows that is running in python 3.8!\nAnswer: Although Streamlit is a Python library (and used to be a stand-alone company), streamlit run myApp.py in that context is a reference to an executable streamlit. In cases where there are multiple, you can specify the exact one you want to use:
/path/to/conda/env/streamlit run myApp.py",0.0,0.2938378,0.08634064346551895
90,"Question\nI have a streamlit app that is set to run by doubleclicking a *.bat file that contains the command to start the app: streamlit run myApp.py. Does anyone know how to force that app to run in specific python version? My base python is 3.9, I created an environment (conda env create...) based on python 3.6 where I installed streamlit and created the app and I want it to run there. I open sypder using that environment, double-checked that the env is active (!python --version in the spyder console) but when I run the app using the *bat file, it shows that is running in python 3.8!\nAnswer: /path/to/your/python -m streamlit run",0.0,0.46052206,0.21208056807518005
91,"Question\nI am developing a program to be constantly listening for data on ignition, and when they send the information, I would add the value to a tag or some property. And the serial data may be sent multiple times. So I have to wait till the window is closed, to stop that process.
I thought of trying to have a background asynchronous thread to loop or waiting for interrupts in the background constantly, but that doesn't seem to be feasible with the way ignition is structured (to my understanding, I learned ignition 2 weeks ago).
I tried taking a look at system.serial. But I don't see any way to constantly listen for data.
If anyone has any idea on how I would implement this?\nAnswer: Look into adding a TCP device in the OPC server.  You will be able put the IP and port of the device.
With this device, you can add an OPC tag into your tags.  You can then put in a change script to do an action every time the tag changes, which will change with data is detected on that device.",0.0,0.21885061,0.04789559170603752
92,"Question\nI know how to add leading zeros for all values in pandas column. But my pandas column 'id' involves both numeric character like '83948', '848439' and Alphanumeric character like 'dy348dn', '494rd7f'. What I want is only add zeros for the numeric character until it reaches to 10, how can we do that?\nAnswer: I understand that you want to apply padding only on ids that are completely numeric. In this case, you can use isnumeric() on a string (for example, mystring.isnumeric()) in order to check if the string contains only numbers. If the condition is satisfied, you can apply your padding rule.",0.20408164,0.2636953,0.003553788410499692
93,"Question\nPS C:\Users\gwill\OneDrive\Documents\new luno\Luno10> pip install dotenv
Fatal error in launcher: Unable to create process using '""C:\Users\gwill\AppData\Local\Programs\Python\Python310\python.exe""  ""C:\Users\gwill\AppData\Local\Programs\Python\Python310\Scripts\pip.exe"" install dotenv': The system cannot find the file specified.\nAnswer: It should be pip install python-dotenv.",0.0,0.3060038,0.0936383306980133
94,"Question\nHave a few questions regarding SnowPark with Python.

Why do we need Snowpark when we already have Snowflake python connector(freely) that can use to connect to Python jupyter with Snowflake DW?

If we use snowpark and connect with Local jupyter file to run ML model. Is it use our local machine computing power or Snowflake computing power?If its our local machine computing power how can we use Snowflake computing power to run the ml model?\nAnswer: Using the existing Snowflake Python Connector you bring the Snowflake data to the system that is executing the Python program, limiting you to the compute and memory of that system. With Snowpark for Python, you are bringing your Python code to Snowflake to leverage the compute and memory of the cloud platform.",0.1632653,0.2453115,0.0067315781489014626
95,"Question\nI have to develop a android application which will capable of doing predictions on device meaning I have to perform every action on the device itself
The application has to extract features from audio and feed them to tensorflow lite model for prediction
For training the model, I extracted the features from audio using Librosa, but I am not able to find a suitable framework which can help me extract features from audio like librosa and make prediction using tflite model
I found out that I can do something using Ironpython or python.net in unity but I am still confused about how to achieve it.
So my question is whether there is way to run the python script written on android device with unity.
Also if there are other frameworks, that can help me achieve my goal of on-device prediction,,I will welcome those suggestions\nAnswer: It was not feasible to accomplish the task using unity effectively.
I solved the problem using chaquopy plugin for android studio, this enables you to use python with java or you can code the whole android application in python using chaquopy.",0.40816328,0.1189754,0.08362963050603867
96,"Question\nI'm using a basic Ubuntu 18.04 VPS and I've been using gunicorn3 for a while. I've decided to upgrade to python3.8, however, it's unclear how to make gunicorn3 run python3.8 applications. Is there any way to do so?
Thanks!\nAnswer: It seems that there's a package for this called gunicorn3 (this was tested on ubuntu)
sudo apt-get install gunicorn3
then running the following command should work and run gunicorn with python3:
gunicorn3 --log-level debug --bind 0.0.0.0:30443 server:app",0.0,0.40267944,0.16215074062347412
97,"Question\nI tried installing Rasterio library using command- pip install rasterio and also used conda commands in anaconda cmd, both are didn't worked and it's taking too long to install so I aborted the process. Let me know in detail how to install raserio library for python?
I tried with installing GDAL and raterio whl file by using pip commands. It got installed, but when I tried to import module- it's giving me an error saying No module found. Please help to fix this.
Thanks in advance\nAnswer: I just had the similar problem, and tl:dr the issue for me was multiple environments, like Ameya said. Here's a longer version of Ameya's spot-on answer with steps to diagnose and fix.
Without realizing it, I had two environments going on: my jupyter notebook was running python 3.10 and my global python was running 3.9, and each was looking for site-packages in different locations (/opt/homebrew/Cellar/jupyterlab/3.4.7/libexec/lib/python3.10/site-packages vs /Users//Library/Python/3.9/lib/site-packages).
This happened because I had trouble with getting python and specifically jupyterlab running on Monterey and the only fix for me was using homebrew to manage the packages. Anything I installed with brew from the command line, went into /opt/homebrew/Cellar... etc and could be seen by my jupyter notebook. Anything I used pip install to get from within an open notebook also went onto this path that my notebook could see. But anything I used pip install from the command line to get, went to the path of the global environment's site packages. Then, my jupyter notebook couldn't see them.
You don't mention that you are using jupyter notebook but perhaps something analogous could happen between multiple environments for you.
You can check if this is the case for you by doing the following:

start python from the command line
import sys
run sys.path
start jupyter notebook, or start python from your other environment
same thing, import sys, run sys.path

Are they the same? If not, probably pip is putting your rasterio in other python env site-packages.
To fix, you can either pip install from",0.0,0.71774304,0.5151550769805908
98,"Question\nSo, I am trying to build an automation web app where I can upload pdf files, and then a python code will run which extracts certain data from the uploaded pdf files.
I know how to make a python code that takes a pdf from my local system and extracts the required data, but how do I make the code use the uploaded pdf files on the web app?
I plan to deploy this website where the python code can be run on any system using the website, so how exactly do I pass the uploaded file as input to the python code?
Also, will using Flask make it easier to approach this?\nAnswer: sounds like you want to deploy one website, then the end user can access the web page and upload pdf file, web service accept the request and run python script to parse data from uploaded pdf, right?
if above is correct, seems you can save the pdf on the serverside, and deploy another cronjob to monitor the path, if new pdf file reached, can trigger python script running.",0.40816328,0.08035773,0.10745647549629211
99,"Question\nI'm in my script using the os.startfile(""program_path"") command and it opens the program without any problem in visual studio.
However as soon as I close visual studio, the program I started also closes.
Is there a way to keep the program running even though I've closed visual studio?\nAnswer: you could start the script in cmd (or terminal if you are on linux) instead of visual studio using python <filename.py> or python3 <filename.py if you have python 2 and python 3 installed",0.40816328,0.19688195,0.04463979974389076
0,"Question\nI've been trying to find a way to convert POST data from a request
when i print the data this is what i get b'message=sdfa&arent=gfsdgd&btn3=pressed'
I've tried different ways to get it ive used replace but i realized that wasnt gonna work i saw that flask does something like this but id have to rewrite all my code using it
i am using http.server and i was wondering if i could have the output formatted like json so were i could use something like value[""message""] and it would give me what is inside the message
this is my first time using stack overflow so im sorry if i dont make sense or look stupid\nAnswer: question was answered by a comment
There is a library that formats a link into practically a json file
the library is called urllib
big thanks to metatoaster",0.0,-0.10805459,0.011675795540213585
1,"Question\nI have plotted a 3D radiation plot using Python, with theta on the x-axis and phi on the y-axis and magnitudes along z.  I initially used numpy.meshgrid to create the 2d array for thetas and phis. Now how can I find the peak points( main lobe and side lobes) from this graph?
find_peak function of the scipy.signal library seems to deal with 1d array only.\nAnswer: Try to use maximum_filter from scipy.ndimage.filters, or even just a simple thresholding could do the trick, provided prior smoothing/transformations like erosion/dilation.",0.0,0.26162046,0.06844526529312134
2,"Question\nLooking on tips how to get the data of the latest row of a sheet. I've seen solution to get all the data and then taking the length of that.
But this is of course a waste of all that fetching. Wondering if there is a smart way to do it, since you can already append data to the last row+1 with worksheet.append_rows([some_data])\nAnswer: I used the solution @buran metnion. If you init the worksheet with
add_worksheet(title=""title"", rows=1, cols=10)
and only append new data via
worksheet.append_rows([some_array])
Then @buran's suggestion is brilliant to simply use
worksheet.row_count",0.0,0.18852353,0.035541120916604996
3,"Question\nWhen I use Spyder, I have fairly frequently accidentally closed the wrong consoles. Usually by a cmd+W. Most applications ask if I want to save before close/confirm I definitely want to close. Spyder just closes, so I may lose training that's been occurring for an hour, say, by a single click or keyboard typo, and I cannot find a way to get it to ask before it does close a console.
Anyone know how to fix this?\nAnswer: You could use hist -g in the console to get your history back.
And you could do this:
Tools > Preferences > General > Advanced Settings
check Prompt when exiting",0.0,0.23164892,0.053661223500967026
4,"Question\nI am working with a dataset. As a precautionary measure, I created a back-up copy using the following command.
Orig. Dataframe = df
df_copy = df.copy(deep = True)
Now, I dropped few columns from original dataframe (df) by mistake using inplace = True.
I tried to undo the operation, but no use.
So, the question is how to get my original dataframe (df) from copied dataframe (df_copy)?\nAnswer: Yoy cannot restore it. Code like below dosen't work.
df = df_copy.copy(deep = True)
Every variables which reference original df keep reference after operation above.",0.0,0.1496067,0.022382166236639023
5,"Question\nI wonder how I can install a python 3.7 virtual environment on a Linux machine that currently has python 2.7? I also do not have sudo privilege.\nAnswer: If you want to install python without sudo privileges, then just download a compressed version of python 3, decompress it, run.",-0.35714287,0.12773293,0.2351045459508896
6,"Question\nAs far as I understand, Gitpod creates a pod which runs a Linux server container that includes my pre-configured IDE.
I am not too familiar with pods and containers, so please correct me if I'm wrong.
To my understanding, a container is essentially a VM, or are there any differences between the two?
Now, moving on to Python, which I am also just getting to know.
So a common practice is to set up a virtual environment, so that packages will be installed in there, and not globally, on the computer running the project.

Note - I will be abbreviating with ""venv"", but I will not be specifically referring to venv/virtualenv/etc.

Comparing it to NodeJS with which I am more familiar, that would be the same as using project/.node_modules vs usr/local/lib/.node_modules
By default npm i installs on the repository, and only installs on the machine when using the -g option.
Is that correct so far?*

So, with that information in mind, since Gitpod is running a container specifically built to serve as a single project IDE - Do I even need to run a venv?

I can see a use case in which I may be testing how my code runs against Python3 and Python2.8, thus using two different venvs
(I now wonder how that would look code-wise, how would the scripts import the correct libraries, but that question is way off-topic 😅)

Anyway, my question assumes:

If I were to use it on a local machine, I'd only need one venv
In terms of Python version and package collisions:

Either the default Gitpod image satisfies my needs
Or, well, I can edit the image myself anyway




* Footnote - I apologise for asking many questions beside the main one.
I hope it is not against the rules, I simply want to know if I understand correctly\nAnswer: In an attempt to answer this objectively, I think the shortest argument for using both boils down to:

The Gitpod provides your development environment: editors, IDEs, Python installations that a virtual environment can use, etc.

The virtual environment provides your application's execution environment: the exact Python version and packages it needs to run.


Your preferred tooling and workflow, however, might make this distinction moot.",0.40816328,0.4206658,0.00015631303540430963
7,"Question\nI am writing an experimental code that generates random tokens using os.urandom, I was wondering how do I generate a 66 bit random token.? 66 bit is 8.25 bytes and it does not take floating numbers. I specifically need 66 bit how do i do it?\nAnswer: You can generate 9 bytes and take the first 66 bits using a bitmask operation. on the last byte.
Here's a beautiful one liner:
"""".join([bin(c).replace(""0b"", """").zfill(8) for c in us.urandom(9)])[:-6]
This will generate 9 bytes of random as a byte string, them iterate the 9 bytes in the byte string, convert each one into a binary string notation (i.e. 0b110001, remove the 0b from the front, zero pad where needed, join into a single string and strip the excess 6 bits from the end.
convert ever",0.10204082,-0.38992673,0.24203208088874817
8,"Question\nI am writing an experimental code that generates random tokens using os.urandom, I was wondering how do I generate a 66 bit random token.? 66 bit is 8.25 bytes and it does not take floating numbers. I specifically need 66 bit how do i do it?\nAnswer: simple solution: you get three 32-bit random values (os.urandom(96)), and simply ignore 30 bits of the last 32 bits you get
better solution: you get a 33 random 32-bit values, put them in a buffer, take the lowest 66 bit (using bitwise operations, &), and then shift your index into the buffer. For the next random value, you take the next 66 bit from your buffer through bitwise operations.

Also make sure you actually need all your randomness to come from the operating system. This is usually not the case. In most practical applications, you seed your pseudo-random number generator once with random values from your OS, and then have a very cheap way of generating sufficiently random/secure random numbers. It depends on what you build!
But especially for things like session tokens, you don't need incredibly cryptographically secure values - take a (properly) seeded PRNG, get sufficiently many bits, append a secret salt to the end, run a SHA-256 hash on it, get a session token that cannot be guessed, and knowledge of one would not allow you to infer any other valid tokens.",0.10204082,0.26866806,0.027764633297920227
9,"Question\nToday I come with this question probably to someone who has large experience in this.
Basically what the title indicates. We have that app and we have to migrate it to microservices.
We didn't find any solid approach (or we felt it like that) about this. What we ended up doing is creating 1 project per microservice (a single functionality related to a module app, in general) but then we had some problems because we already had a database to work with since this is already a functioning app.
We had problems communicating with the existing models, so basically what we did was to point in every settings.py of the projects to the existing DB, and with python3 manage.py inspectdb, we grabbed the existing models. This approach ended up working, but we feel that is not the best approach. We had a lot of problems with circular imports and more.
Is out there good practices about microservices with Django, and how to properly do it, like in most of the cases that we want to create something with the framework?
If someone knows or has something we would really appreciate it!\nAnswer: you can use Django for Microservices. in this case you have only one few apps, and you start every service on own port:
first Django project
pdf generator + views generate small pdf
second Django project.
pdf generator + views generate big pdf (code can inherit other project)
orchestra:
third Django project: Autorization + call big or small pdf generator service
settings
settings.py for first and second is very easy and allows only call from internal ip's. here we don't need middleaware, template, cache, admin and other settings.
settings.py for orchestra is also very easy and used only auth and made call by internal ip ant send response to user. Here we don't need much middlaware, and don't need many other settings.
gains:

All is independent. if one server fall, other can work.
Updates are easy. One small server update is always easy than monolith update.
development is easy: three small teams works on own small projects.
Units testing is easy and fast
For complex business goals the whole system is faster.

pains:

after 100 micro-services it is completely complex to work with that all.
code style from many teams is always different. Don't matter how strict you define styleguide or",0.0,0.3631343,0.13186651468276978
10,"Question\nI want to use name from for an argument of a function. But from is a reserved keyword, so python will throw an exception if I decide to use this name for a variable.
How programmers in python deal with it? I think from is a common name for variables, in python as well. Is there any standards in python for how to rename the argument/variable in this case? Do they use prefixes/suffixes like from_?\nAnswer: Many languages have these challenges. Some ideas:

As mentioned in the comments, tack an underscore on the end: from_
Add context or type (also mentioned in comments): from => from_list
Mangle the spelling (from => frm, e.g. class => klass or cls)
Use a synonym: from => source

Each approach has its advantages and disadvantages.",0.0,0.39371306,0.15500997006893158
11,"Question\nI have an AES encrypted file in python. Lets call it encrypted.exe.
I have the key for the encryption. I can then use python to decrypt my file (Cipher module in python) and obtain decrypted.exe. I then save the file (using write). Later I can run the file by clicking on it or through a cmd or powershell.
What I want to do is be able to run the decrypted.exe without saving it. For instance I thought about, decrypting with my AES key then loading it in the RAM and run it from there.
I don't know how to do it nor if it is possible with python.\nAnswer: I would say you can, but it’s more than overhead. It’s really complex task, so I would recommend to save file to hard drive, than use subprocess or os library to execute it, then delete it.",0.0,0.045496285,0.0020699119195342064
12,"Question\nI would like to simply access the number of enveloppes (per user and per period) for billing purposes. Any idea how to do it in a fast way?
Thank you!\nAnswer: Probably best is to set up an account-level webhook via the Connect feature.
Your software can then be notified anytime a user in your account sends an envelope. You can add the information to your own BI (Business Intelligence) system and create derivative reports such as envelopes per user per week, etc.",0.0,0.23722535,0.0562758669257164
13,"Question\nI have a dataset that looks like this:




Date
Value




1871-01
4.5


1871-02
10.7


1871-03
8.9


1871-04
1.3




all the way to 2021-12.
how do I get the average value for each year in Python? For example, the 1871 average would be the average of all of the values from 1871-01 to 1871-1 and I would like it for all years from 1871-2021.
...           ...\nAnswer: Depends on the what format the data is being given to you.  Is it json? csv? If you already know how to import and read the data with python.. you just need to assign the years to variables and average them.  (x1 + x2 + x3) / (number of averaged variables)",0.0,-0.054430246,0.002962651662528515
14,"Question\nI'm working on a python-django project.
If a have a class where one of its attributes have

max_length=100

how can I change to

max_length=5000

? for instance...
Thank you!\nAnswer: Basically when you edit a field on a Model you need to makemigrations and migrate for changes to take effect in database.  
python manage.py makemigrations <optional_appname(If not provided it affects all apps.)> 
python manage.py migrate <optional_appname>",0.0,0.40382004,0.16307061910629272
15,"Question\nCan somebody explain me why Why print(5 ** 2 ** 0 ** 1) = 5 in Python?
I am trying to learn the language and somehow I am not quite sure how this math is done.
Thank you in advance.\nAnswer: ** is exponentiation.
0 raised to the power of 1 is 0. So, we could re-write the statement as print(5**2**0) without changing the result.
2 raised to the power of 0 is 1. So, we can re-write the statement as print(5**1) without changing the result.
5 raised to the power of 1 is 5. So, we can rewrite the statement as print(5) without changing the result.",0.20408164,0.38275385,0.03192375972867012
16,"Question\nI am new to python and trying to figure out a problem that gui doesn't launch when clicking on the.pyw file.
On my windows 10 machine, I have installed python 3.5, and environment path is set.
I was given a set of python files (.py) and some looks like shortcut file (.pyw). And I was told to double click the file then a gui will launch.
Some.pyw works well, gui launch.
However there are some failed. After double click, there was a quick cmd terminal open and closed automatically. Then no gui pop out.
I want to know what is the cause and how do I debug it.
From the properties of.pyw file, is is pointing to one of the.py file.
Let me know if posting the.py file helps. I will then post here.\nAnswer: Managed to resolve it by selecting the correct pylauncher to open the file.",0.0,0.08729386,0.007620218675583601
17,"Question\nI have virtualenvwrapper-win installed from the command line. When I try to do virtualenv env (env is the name of my virtual environment I'd like to set up) I get this:

'virtualenv' is not recognized as an internal or external command, operable program or batch file

I'm not sure what's wrong. Anyone know how to fix?\nAnswer: Try these commands :
To create virtual env - mkvirtualenv venv_name
To view all virtual env - lsvirtualenv
To activate any virtual env -  workon venv_name",0.0,0.46720314,0.21827878057956696
18,"Question\nMy problem is as follows. I have a 2d image of some tissue and a 3d stack of the same region of the tissue and plus more tissue that does not go into my 2d image. Now, the 3d stack is slightly rotated with respect to the 2d image, but also has some local deformation, so I can't simply apply a rigid rotation transformation. I can scroll through the 3d stack and find individual features that are common to the 2d image. I want to apply a nonlinear transformation such that in the end I can find my source 2d image as a flat plane in the 2d stack.
My intuition is that I should use thin plate spline for this, may the scipy RBF interpolator, but my brain stops working when I try to implement it. I would use as input arguments let's say 3 points (x1, y1, 0), (x2, y2, 0) and (x3, y3, 0) with some landmarks on the 2d image and then (x1', y1', z1'), (x2', y2', z2') and (x3', y3', z3') for the corresponding points into the 3d stack. And then I get a transformation but how do I actually apply this to an image? The bit that confuses me is that I'm working with a 3D matrix of intensities, not a meshgrid.\nAnswer: scipy RBF is designed to interpolate scattered data, it's just a spline interpolator. To warp a domain, however, you need to find another library or write TPS (thin plate spline) yourself; scipy doesn't do it. I recommend you check VTK, for example. You feed your landmark information of the reference image and the target image to a vtkThinPlateSplineTransform object. Then you can get the transformation matrix and feed it to a vtkImageReslice object, which warps your image accordingly.",0.0,0.13117844,0.017207782715559006
19,"Question\nIs it possible to run python script, which I will upload on Heroku or DG Ocean droplet (depending on which of them is most comfortable for what I am trying to do), from external website button?
My scenario: I have scrapper and I want to run it when user clicks on button on my webpage, so scrapper will scrape current data and will show it to user. Is it possible to do? or if we have any other way?\nAnswer: For others, who will have same things in the feature:  So, i am going to make FLASK app to use DG Droplet or Heroku, after i will import Flask app using Iframe in my Web project and use only BUTTON, like this i will use my scraper on my Webpage.
Thanks",0.0,-0.010215104,0.00010434834257466719
20,"Question\nI have strings in the database that are the names of countries and cities for example like this:
Italy-Milan OR France-Paris.
How can i select only the city part, like select what comes only after the '-' using python?\nAnswer: If data is the variable having you city value and other info than to get only the city name
data.split(""-"")[1]",0.0,0.2488324,0.061917565762996674
21,"Question\nI first downloaded python x64. Now, I have both python x64 and python x32. How can I create a python file for just the x32, and not the x64?
Edit:
I thought this was a duplicate, but it was not, the other answer does not have a response that works.\nAnswer: Python is an interpreted language, so Python files can always run in both 32-bit and 64-bit environments, with the same behaviour and output (in most cases).
If you want to control which Python instance your code runs with, you should start the program by giving the direct path to the version of Python which you intend to run.",0.20408164,0.18135494,0.0005165028851479292
22,"Question\nThere is an excel file named test.xlsx, which has 3 sheets: ['Sheet1', 'Sheet2', 'Sheet3'], how do I use Python to reorder the sheets as: ['Sheet3', 'Sheet1', 'Sheet2']\nAnswer: Workbooks have the move_sheet() method.",0.0,0.31439716,0.09884557127952576
23,"Question\nusing pydiffmap, I could find a nice low dimension Manifold in my data, and extract what seems to be meaningful low dimension components.
I would like now to reverse the operator, and project my data back to my original high dimensional space keeping only these few important dimensions I could identify.
First, is this mathematically possible? And if so how to do it?
Thanks a lot!\nAnswer: I just went into the Diffusion algorithm behind the package, and realized that there is no guarantee that you can go from a vector in the diffusion space back into the data space.
This is because the diffusion space represent the distances to the original data points. So if at least two points are different, the null vector in the diffusion space (at distance 0 of all original data points in the data space) will have no equivalent in the data space.
Hope this can help someone else!",0.0,0.16068423,0.02581942081451416
24,"Question\nI used Forge API to post projects in BI360, now I am trying to add few users using the data frame of the projects created. According to the Forge Documentation you can post users to one project, looking for  an advise how to post users to several projects
TIA\nAnswer: I checked, and all endpoints take only one project id, so you have to loop through multiple projects and add the user(s) to all projects, currently not possible in one call.",0.0,0.20315272,0.04127102717757225
25,"Question\nI want to train my custom license plate using Yolov5, but I have a problem.
My problem is that my dataset is separated for each character and I have no idea how to make  annotation file suitable for Yolo which is as follows.
Because what I've seen so far, for triainig, you definitely need the entire license plate, which can be used to label each of the characters.
And my question is, if I train these images, can I achieve a license plate recognition system?\nAnswer: With Yolov5, you can achieve a good licence plate detection system, Yolov5 wont be reach the succes to recognite the licence plates itself. After the detection(with Yolov5) you can extract the information from bounding boxes and use it for recognition.",0.0,0.26509058,0.0702730193734169
26,"Question\nI'm using SQL transform of apache_beam python and deploy to Dataflow by Flex Template. The pipeline show the error: Java must be installed on this system to use. I know the SQL transform of beam python using Java, I researched the way to add Java to pipeline but all failed.
Can you give any advice on how to fix this error? Thank a lot.\nAnswer: You need to have either Java8 or Java11 installed locally to start a Java expansion service to expand your SqlTransforms into Java SDK transforms. This is pipeline construction different from later pipeline execution and could be where your issue occurred.",0.0,0.08846557,0.007826156914234161
27,"Question\nI am currently building a tool in Django for managing the design information within an engineering department. The idea is to have a common catalogue of items accessible to all projects. However, the projects would be restricted based on user groups.
For each project, you can import items from the catalogue and change them within the project. There is a requirement that each project must be linked to a different database.
I am not entirely sure how to approach this problem. From what I read, the solution I came up with is to have multiple django apps. One represents the common catalogue of items (linked to its own database) and then an app for each project(which can write and read from its own database but it can additionally read also from the common items catalogue database). In this way, I can restrict what user can access what database/project. However, the problem with this solution is that it is not DRY. All projects look the same: same models, same forms, same templates. They are just linked to different database and I do not know how to do this in a smart way (without copy-pasting entire files cause I think managing this would be a pain).
I was thinking that this could be avoided by changing the database label when doing queries (employing the using attribute) depending on the group of the authenticated user. The problem with this is that an user can have access to multiple projects. So, I am again at a loss.\nAnswer: It looks for me that all you need is a single application that will manage its access properly.
If the requirement is to have separate DBs then I will not argue that, but... there is always small chance that separate tables in 1 DB is what they will accept",0.0,0.32049263,0.10271552205085754
28,"Question\nI am currently building a tool in Django for managing the design information within an engineering department. The idea is to have a common catalogue of items accessible to all projects. However, the projects would be restricted based on user groups.
For each project, you can import items from the catalogue and change them within the project. There is a requirement that each project must be linked to a different database.
I am not entirely sure how to approach this problem. From what I read, the solution I came up with is to have multiple django apps. One represents the common catalogue of items (linked to its own database) and then an app for each project(which can write and read from its own database but it can additionally read also from the common items catalogue database). In this way, I can restrict what user can access what database/project. However, the problem with this solution is that it is not DRY. All projects look the same: same models, same forms, same templates. They are just linked to different database and I do not know how to do this in a smart way (without copy-pasting entire files cause I think managing this would be a pain).
I was thinking that this could be avoided by changing the database label when doing queries (employing the using attribute) depending on the group of the authenticated user. The problem with this is that an user can have access to multiple projects. So, I am again at a loss.\nAnswer: Django apps don't segregate objects, they are a way of structuring your code base. The idea is that an app can be re-used in other projects. Having a separate app for your catalogue of items and your projects is a good idea, but having them together in one is not a problem if you have a small codebase.
If I have understood your post correctly, what you want is for the databases of different departments to be separate. This is essentially a multi-tenancy question which is a big topic in itself, there are a few options:

Code separation - all of your projects/departments exist in a single database and schema but are separate by code that filters departments depending on who the end user is (literally by using Django.filters()). This is easy to do but there is a risk that data could be leaked to the wrong user if you get your code wrong. I would recommend this one for your use-case.

Schema separation - you are still using a single database but",0.0,0.20157605,0.040632907301187515
29,"Question\nAs the title suggests, I need a way to get the resolution of the media running in the Gstreamer pipeline. I know it has something to do with the caps in the pipeline. But how do I access it?
Also, let's say I somehow got it and it is of the type Gst.caps, how do I get the actual width and height from this?\nAnswer: If running your pipeline by gst-launch-1.0, you can actually see the caps negotiation by putting GST_DEBUG=7 before gst-launch-1.0.  However, GST_DEBUG=5 may get you the information you need without the huge amount of trace information given by GST_DEBUG=7.",0.0,0.6598604,0.43541571497917175
30,"Question\nMy hello message is not visible...
I'm writing a 3D video game using Python (3.10) language with Panda3d (1.10) as 3D game engine.
Some graphical parts of the game are not 3D, there are just 2D elements:

spell bar,
player resume
quest panel
skill tree
etc
spells/attacks book
and others

I see on the web that pictures are used to styling Panda3D Direct* components.
However, in my opinion, I think use statics pictures (.png for exemple) is not efficient when game design must be evolved. That's why I prefer use HTML/CSS to render 2D elements, especially since CSS is very powerful if you know how to use it. I prefere to use SVG into the scene but I don't understand how create SVG file.
So, my question is:
Can I use HTML/CSS inside a Panda3d application?
Thank to all!
Have a nice day!\nAnswer: I think I've found a workaround (temporary).

I create HTML/CSS file that styling my 2D component that must be injected inside the Panda3D scene
I create a Python script that convert an HTML part as PNG picture using RGBA for transparency
I use the generated PNG picture inside my Panda3D scene on the target component.

It's not pure HTML/CSS, but it can works.",0.0,0.1904971,0.036289144307374954
31,"Question\nwhen I enter the youtube URL in my browser I want the URL to automatically change and visit a different website (invidio.us). Can someone provide any pointers as to how to achieve this with Python?\nAnswer: I don't think you can solve this in python. You will need a browser plugin for this one, and the add-ons aren't written in python.
Try JavaScript.",0.0,0.1260612,0.01589142717421055
32,"Question\nI'm running vscode-server to develop on a remote machine via ssh. This machine has no connection to the internet and runs Python 3.6.5.
I would like to use pylint in vscode for the linting. Problem is that I cannot install it the normal way, since I don't have an internet connection.
What I tried so far:

Use pip download pylint, tar the resulting folder, move it via scp and install it on the remote machine. This didn't work since my local mchine has a different python version from the remote (local: 3.10.x and remote: 3.6.5).
Use the Install on remote: ssh button in the vscode marketplace. This succeeds but when I write code, a message pops up that says: Linter pylint is not installed. When I click on install, it just tries to execute pip install pylint on the remote, which will obviously fail...

Any suggestions on how to proceed here?\nAnswer: This didn't work since my local machine has a different python version from the remote (local: 3.10.x and remote: 3.6.5).

I don't know if it's ultimately going to work, but you can download the latest pylint compatible with python 3.6.5 explicitly, it's pylint 2.13.9 afaik so pip download ""pylint==2.13.9"".",0.0,0.2320453,0.05384501814842224
33,"Question\nI am creating a web-app w/ Flask + Flask-WTF that has CRM-like features as a project. My current database (MongoDB) structure is I have:

Users who can login,
People who are assigned to users, and
Records who are assigned to people.

People have various fields to be filled out (Name, Phone Number, Email, etc).
I want Users to be able to create custom fields for people. I am trying to plan out how to implement this from a database design perspective. My initial thoughts are to:

For each field created, add a new field without a value for each People assigned to the user.
Use a for-loop to dynamically create the form class by looping through each field-value in my database and excluding non-required ones.
Use a for-loop to dynamically output the web form by looping through each field-value in my database and excluding non-required ones.

Another idea I have is:

For each field created, add the custom field, with a parentRecord equal to the User ID to a new MongoDB collection.
Use a for-loop to create the form class & web form dynamically, but I wouldn't need to exclude non-required ones as the only fields in the collection would be the custom ones, and wouldn't include any special data points that don't get displayed.

So my questions are:

Will my ideas above work?
Which one is better?
Is there a better way?\nAnswer: I decided to create a customfields MongoDB collection that has a parentRecord as the User.
I faced a few challenges:

I had to dynamically create a form via flask WTF. I ended up using wtforms_dynamic_fields to accomplish this. I then used a for-loop to dynamically generate each form in Jinja. I simply queried the customfields DB where the parentRecord matched the logged in User, and then created custom fields based upon the values saved in customfields.

The second issue I faced was getting the data from the submitted form and then building a MongoDB-friendly list to insert a new record with when creating new record. This was accomplished by using request.form.items() and iterating through them and using list.update() to add all of my required fields to a list.",0.0,-0.07861996,0.006181097589433193
34,"Question\nI made a blackjack game and a calculator following an online tutorial, however the only tutorial used replit module (from replit import clear -> clear() ) (replit being the website/online interpreter). I am now trying the same program in vscode:

is there a way to import replit to vscode?
is there a similar module to get the same result?
I have seen some people make suggestions to clear console but they all seem to depend on which OS I am running, since my desktop is windows and laptop is mac I want something that can work on both.\nAnswer: Did you try ""import os"" at the beginning of code and using ""os.system('clear')""?
This should work on any OS or code editor/IDE.",-0.71428573,0.09792435,0.6596851944923401
35,"Question\nWhen I run poetry show - most of my packages are blue but a few are red? What do these two colors mean?
I think red means the package is not @latest?\nAnswer: Yes, red indicates that you have an outdated package and blue is up to date",-0.35714287,0.28146878,0.40782487392425537
36,"Question\nWhen I run poetry show - most of my packages are blue but a few are red? What do these two colors mean?
I think red means the package is not @latest?\nAnswer: Black: Not required package
Red: Not installed / It needs an immediate semver-compliant upgrade
Yellow: It needs an upgrade but has potential BC breaks so is not urgent
Green: Already latest",0.40816328,0.2048378,0.041341252624988556
37,"Question\nI was trying to automate the process off adding things to a list by clicking the add button and cant figure out how to get selenium to click on every button that has the text ""add"" on it but not the other buttons.
My end goal is to click add on every anime on the page from my anime list and after every click click the submit button, then once a page is finished go to the next then next letter.\nAnswer: findElements in Selenium returns you the list of web elements that match the locator value, unlike findElement, which returns only a single web element. If there are no matching elements within the web page, findElements returns an empty list. After that, you can iterate through your list and do the actions.",0.0,0.3849424,0.14818066358566284
38,"Question\nSo, I would like to know if there is a way to delete an line already plotted using matplotlib. But here is the thing:
I'm plotting within a for loop, and I would like to check before plotting if the line that I'm about to draw is already on the figure, and if so, I would like to delete it.
I was told an idea about plotting it anyways but with the same color of the background, but again, to do this I would have to check if the line already exists. Any idea how to do this?\nAnswer: Any idea how to do this?


During iteration, before making a new line

check if x and y coordinates of the new line are the same as any of the lines already contained in the plot's Axes..",0.0,0.09469235,0.008966641500592232
39,"Question\nI'm working on some modules that email all data that should save in the database
with the attached file. I don't know how to save them as drafts in Django, Help me to solve this problem. I am new to Django.
Thanks in advance.\nAnswer: I think you need to just create a normal save and then create a function to prevent everyone except yourself from viewing the post.",0.0,0.00096195936,9.253658390662167e-07
40,"Question\nI have been developing in django for sometime now, and have developed a neat website having functionality such as writing blogs, posting questions, sharing content etc. However there is still one thing that is missing and i.e. creating notification for users.
What I want to do is to inform users in their profiles, whenever somebody comments on their posts, or if they are following a particular post and there is an update on it, then inform the user of that update. I have looked around many applications but I am still very confused about how to do it.
In case of using django-notification I seem to have an impression(which can be wrong) that I can use this only to inform the user via email, i.e. I cannot show these notifications in the user profile, just like we have on facebook.
Firstly I would like to know if I am wrong, and then I really need some proper tutorial or guidance on how to go about doing it. I know how to register a notification and send it on proper signal but there is no documentation on how to show these notices in a template, if this can be done.
Any guidance/tutorial/getting started doc will be deeply appreciated.\nAnswer: Websockets are simple and very useful in terms of bi-directional communication between clients and servers. If your app's features list grows it will be possible that you will need additional events in your app. WS will help you to scale.",0.0,0.20356077,0.04143698513507843
41,"Question\nMy idea is to create a function that takes an input of a number and outputs a string of that length, which consists of ""keyboard-like"" spam.
I know how to generate a completely random string of characters, however, I'm trying to make it look as if it's real ""spam"" from someone typing on a keyboard.
For example:

a key closer in proximity to one previously pressed is more likely to be pressed next
keys on the home row may have a higher chance to be pressed
areas where the fingers rest are more likely to be pressed

I'm new to StackOverflow and newish to python so forgive me if there's any errors with how I'm asking this.
I don't have any code yet because I have 0 idea how I would go about doing this.
The best idea I have would be to randomly assign values to each key, and somehow tell the program where each key is physically located in proximity to each other, but this seems inefficient.
I'm not looking for a way to input this into a textbox or on a website somewhere, I only want to generate the random text in a string through a function.
You don't have to write the whole thing but I would greatly appreciate any help or ideas how to do this, thank you.\nAnswer: I would just open a file and spam your keyboard for 10 minutes or so. This will likely generate a huge data set that perfectly matches what you want.
Next to generate random like strings you can select random short(ish) chunks from the file and concatenate them together to achieve the desired spam strings.
That might look something like
"""".join([example_string[i:i+random.randrange(3,5)] for i in [random.randrange(0,n) for _ in range(10)]])",0.0,0.32757998,0.10730864107608795
42,"Question\nMy idea is to create a function that takes an input of a number and outputs a string of that length, which consists of ""keyboard-like"" spam.
I know how to generate a completely random string of characters, however, I'm trying to make it look as if it's real ""spam"" from someone typing on a keyboard.
For example:

a key closer in proximity to one previously pressed is more likely to be pressed next
keys on the home row may have a higher chance to be pressed
areas where the fingers rest are more likely to be pressed

I'm new to StackOverflow and newish to python so forgive me if there's any errors with how I'm asking this.
I don't have any code yet because I have 0 idea how I would go about doing this.
The best idea I have would be to randomly assign values to each key, and somehow tell the program where each key is physically located in proximity to each other, but this seems inefficient.
I'm not looking for a way to input this into a textbox or on a website somewhere, I only want to generate the random text in a string through a function.
You don't have to write the whole thing but I would greatly appreciate any help or ideas how to do this, thank you.\nAnswer: It sounds like you want some sort of implicit or explicit Markov chain.  For each key, you assign a set of probabilities for what the next key is.  Start with a random key, and then move to the next key according to the assigned probabilities.",0.0,0.1883201,0.035464461892843246
43,"Question\nI installed Anaconda, but it did not include the GUI app, Anaconda-Navigator app in the Applications folder. What do I need to do to get the GUI app?
Details:
Computer: 2021 14-inch MacBook Pro, M1 Max
OS: macOS Monterey 12.5
A month ago, I had the full Intel version of Anaconda, including Anaconda-Navigator, running fine.
I decided I wanted the M1 version, so I uninstalled it using the method detailed on the Anaconda website (anaconda-clean), rm -rf ~/opt/anaconda3, and remove conda section of.zshrc. I also deleted Anaconda-Navigator from the Applications folder and removed all ~/Library/Receipts, and restarted the laptop.
I the used the GUI installer for the M1 version, which set up conda and seemingly the complete ~/anaconda3 folder, but it didn't install the Anaconda-Navigator app.
I repeated the full uninstall and used the shell installer, getting the same result - no Anaconda-Navigator.
Any suggestions on how I can get Anaconda-Navigator GUI?
Thanks!!
Mike\nAnswer: run -> conda install anaconda-navigator
Worked for my on my Mac mini M1. Was looking for ages to find this. Hope this helps someone looking for it as well.",0.30612245,0.13614458,0.028892477974295616
44,"Question\nI installed Anaconda, but it did not include the GUI app, Anaconda-Navigator app in the Applications folder. What do I need to do to get the GUI app?
Details:
Computer: 2021 14-inch MacBook Pro, M1 Max
OS: macOS Monterey 12.5
A month ago, I had the full Intel version of Anaconda, including Anaconda-Navigator, running fine.
I decided I wanted the M1 version, so I uninstalled it using the method detailed on the Anaconda website (anaconda-clean), rm -rf ~/opt/anaconda3, and remove conda section of.zshrc. I also deleted Anaconda-Navigator from the Applications folder and removed all ~/Library/Receipts, and restarted the laptop.
I the used the GUI installer for the M1 version, which set up conda and seemingly the complete ~/anaconda3 folder, but it didn't install the Anaconda-Navigator app.
I repeated the full uninstall and used the shell installer, getting the same result - no Anaconda-Navigator.
Any suggestions on how I can get Anaconda-Navigator GUI?
Thanks!!
Mike\nAnswer: I had the same issue. After installing with conda install anaconda-navigator run it from terminal with anaconda-navigator and lock it to the  Dock (right click -> options -> keep in dock)",0.10204082,0.19488543,0.008620122447609901
45,"Question\nI am currently working on a multi-layer 1d-CNN. Recently I shifted my work over to an HPC server to train on both CPU and GPU (NVIDIA).
My code runs beautifully (albeit slowly) on my own laptop with TensorFlow 2.7.3. The HPC server I am using has a newer version of python (3.9.0) and TensorFlow installed.
Onto my problem: The Keras callback function ""Earlystopping"" no longer works as it should on the server. If I set the patience to 5, it will only run for 5 epochs despite specifying epochs = 50 in model.fit(). It seems as if the function is assuming that the val_loss of the first epoch is the lowest value and then runs from there.
I don't know how to fix this. The function would reach lowest val_loss at 15 epochs and run to 20 epochs on my own laptop. On the server, training time and epochs is not sufficient, with very low accuracy (~40%) on test dataset.
Please help.\nAnswer: For some reason, reducing my batch_size from 16 to 8 in model.fit() allowed the EarlyStopping callback to work properly. I don't know why this is though. But it works now.",0.0,0.22245872,0.0494878813624382
46,"Question\nWhile trying to activate a virtual environment called ""dat1triviabot_env"" on Windows, I get this error:
dat1triviabot-env\Scripts\activate.bat: The module 'dat1triviabot-env' could not be loaded. For more information, run 'Import-Module dat1triviabot-env'.
I have the virtualenv package installed, version 20.16.2, and I used the command python3 -m venv dat1triviabot-env to create the virtual environment.
Does anyone know how to fix this?\nAnswer: If you are in PowerShell, try just running dat1triviabot-env/Scripts/activate.",0.0,0.06846559,0.004687536973506212
47,"Question\nMOST OF US PROBABLY KNOW BUT MOBIZEN IS AN MOBILE SCREEN RECORDING APP
The part that i want to know is how,
mobizen logo stays on screen and,
toolbar appears whenever you click
the logo stays on screen even while phone is on another app,
where i should research to make something like this.

(I'm using 'kivy' library in PYTHON)

-PurpleLime
REGARDS\nAnswer: i've found, its named ""Floating Application"" guys check documentation of it",0.0,0.26243606,0.06887269020080566
48,"Question\nFor type datetime.datetime, how do we check whether the timestamp points to the beginning of the day?
The beginning of the day means the time 00:00:00.000000 of the day, while the date part can be anyone.\nAnswer: Something like this will take care of it. In this example I am comparing with the current time. datetime.time() with no arguments is initialised to (0,0).
datetime.time() == datetime.datetime.now().time()",0.0,0.30103242,0.09062051773071289
49,"Question\nmatplotlib.pyplot.gcf()
Gets the current figure.
But if you have multiple figures, how do you know which one is the current one?\nAnswer: According to the documentations, gcf() will get the current figure from pyplot figure stack. As stack works in LIFO(Last in first out) manner. The current figure will be that figure which you have made most recently.",0.40816328,0.43262482,0.0005983668379485607
50,"Question\nI have a hex string as follows:
\x00\x00\x00\xa5v\xe6A\x0033\x0033\x00\x00\x00\x00\x00\x00
So I know \x means to use hexadecimal, so for characters like \x00 I know that means a null character. However for \xa5v how do I go about converting that? Is it a different format to hex?\nAnswer: You can convert characters that are not hexadecimal, like ""v"" letter with the python function ord(). It returns an integer of that unicode character. So make something like this: hex(ord(""v"")) it returns the hex value of letter ""v"" in unicode format.",0.0,0.13143998,0.017276469618082047
51,"Question\nExample i have 4 column in my dataframe,
i want to use jaro similarity for col: A,B  vs col: C,D containing strings
Currently i am using it between 2 columns using
df.apply(lambda x: textdistance.jaro(x[A], x[C]),axis = 1))
Currently i was comparing with names
|A|C |result|
|--| --- | --- |
|Kevin| kenny |0.67|
|Danny |Danny|1|
|Aiofa |Avril|0.75|
I have records over 100K in my dataframe
COLUMN A -contains strings of person name
COLUMN B -contains strings of city
COLUMN C -contains strings of person name (to compare with)
COLUMN D -contains strings of city (to compare with)
Expected Output
|A|B|C|D |result|
|--|--|---| --- | --- |
|Kevin|London| kenny|Leeds |0.4|
|Danny |Dublin|Danny|dublin|1|
|Aiofa|Madrid |Avril|Male|0.65|\nAnswer: df.apply(lambda x: textdistance.jaro(x['A']  + x['B'], x['C'] + x['D']),axis = 1))
thank you DarrylG",0.0,0.60644305,0.367773175239563
52,"Question\nI have already installed a package on my Linux machine via conda install, but I found an error when running it on Jupyter Notebook. I'm able to make a simple modification to the package's python script, but the modified code isn't recognized when I called the package again. Do I need to update the package so that the changes would be recognized, and if so, how do I do that?\nAnswer: The library's code does not get imported directly. Instead, it is compiled the first time you import it and put into a pycache folder in the corresponding site-packages directory. In this folder, locate the.pyc file corresponding to the one that you modified and delete it.
Now, when you import, the source code is compiled anew and your changes will have taken effect.
If the package has a GitHub page, it might be worthwhile to make a pull request and ask that they also update the conda package",0.40816328,0.27906126,0.016667332500219345
53,"Question\nI understand that to drop a column you use df.drop('column name', axis=1, inplace  =True)
The file format is in.csv file
I want to use above syntax for large data sets and more in robust way
suppose I have 500 columns and I want to keep column no 100 to 140 using column name not by indices and rest want to drop, how would I write  above syntax so that I can achieve my goal and also in 100 to 140 column, I want to drop column no 105, 108,110 by column name\nAnswer: Instead of using a string parameter for the column name, use a list of strings refering to the column names you want to delete.",0.0,0.12543017,0.015732726082205772
54,"Question\nOrient, please, on a following question.
I created an ML model in Jupiter notebook (python) based on the internal data of the company where I work. The management suggests ""wrapping"" this model, making something like a finished boxed product for use outside the company.
Can you tell me how this can be technically implemented?
And is it possible to make a ready-made boxed product out of Jupiter at all?
I have no ideas besides sharing the file on GitHub.\nAnswer: You can export your model using joblib.
Now, something I like to use for easy deployment is pipedream. So you can upload the file generated from joblib anywhere, call it using pipedream and then you can use webhooks to trigger the workflow.
Also there is other many options, heroku, pythonanywhere, Azure, GC, AWS, where you can store this file and make it run with new data.
There are libraries where you can deploy easier like gradio or plotly if you are in a hurry.",0.0,0.27637565,0.07638350129127502
55,"Question\nI was starting my coding journey but hit a wall before even starting.
When I am trying to run a guizero code from pycharm I am getting this error
""ModuleNotFoundError: No module named 'guizero' ""
But it works fine If i run it via python command prompt. So with my little knowladge I feel pycharm is not able to see guizero libraries and I have no idea how to make it happen.
So tldr is HALP! Thanks in advance for any response ^^\nAnswer: Well opening an account here made me fluffier eneough to solve my own issue.
For those who experience same just download zip version of guizero. Copy the guizero folder in guizero-master and paste it to  blabla/PyCharm/Blabla/Lib so pycharm can see it as external library ^^",0.0,0.4877058,0.2378569394350052
56,"Question\nI create an env of meep.
'$ conda create -n mp -c conda-forge pymeep pymeep-extras'
then
'$ conda activate mp'
I can import meep module in python.
However, I want to use run it in spyder5. But in the env. mp, I have no spyder. I am confused how to use spyder in mp env.\nAnswer: I also had the problem and solved it. The solution is quite simple, that is installing meep and spyder at the same time with this command:
conda create -n meep -c conda-forge pymeep pymeep-extras spyder",0.0,0.47303945,0.22376632690429688
57,"Question\nI have created a game bot with SIKULIX, is it possible to create a web panel or dashboard that controls the bot I created from SIKULIX? if yes, how to do that\nAnswer: Yes, that is possible you could make a page in your webpanel '/bot' and the only output that page has is the action you want your bot to do and control that with a web panel. your bot could get that action through a http request",0.0,-0.026681125,0.0007118824287317693
58,"Question\nMy issue requires some backstory.
I was having some troubles with pip, so I reinstalled Python.  After the reinstall pip began to work, but Pycharm, my IDE, could no longer find Python.  When I reinstalled Python it created a new folder for itself (Python310), but Pycharm kept looking in the old folder (Python39).  I couldn't figure out how to get Pycharm to look in the new folder.  Even deleting and reinstalling it did nothing.
So, I renamed Python310 to Python39 and changed the PATH.  Now Pycharm can find Python.  But pip has developed a new and exciting error.  When I try to use it I get the following message:
Fatal error in launcher: Unable to create process using '""C:\Users\user\AppData\Local\Programs\Python\Python310\python.exe""  ""C:\Users\user\AppData\Local\Programs\Python\Python39\Scripts\pip.exe"" install numpy': The system cannot find the file specified.
If I read this correctly pip is still trying to look in Python310.  Would you please tell me what I need to do to get pip to looking in the right place?\nAnswer: Option 1:
delete and reinstall again. and then when creating a project it should prompt you to pick a basic interpreter, choose python310 or whatever version you're using.
Option 2:
use a different IDE.",-0.23809524,0.10283446,0.11623305082321167
59,"Question\nMy issue requires some backstory.
I was having some troubles with pip, so I reinstalled Python.  After the reinstall pip began to work, but Pycharm, my IDE, could no longer find Python.  When I reinstalled Python it created a new folder for itself (Python310), but Pycharm kept looking in the old folder (Python39).  I couldn't figure out how to get Pycharm to look in the new folder.  Even deleting and reinstalling it did nothing.
So, I renamed Python310 to Python39 and changed the PATH.  Now Pycharm can find Python.  But pip has developed a new and exciting error.  When I try to use it I get the following message:
Fatal error in launcher: Unable to create process using '""C:\Users\user\AppData\Local\Programs\Python\Python310\python.exe""  ""C:\Users\user\AppData\Local\Programs\Python\Python39\Scripts\pip.exe"" install numpy': The system cannot find the file specified.
If I read this correctly pip is still trying to look in Python310.  Would you please tell me what I need to do to get pip to looking in the right place?\nAnswer: try to uninstall all of the existing python versions. and install it again. using any of application allow you to delete most of files, so to prevent error when re-install.",-0.23809524,0.1099838,0.12115900963544846
60,"Question\nI tried duplicating a the same window but it just went. I made this server.py file that needs to be run twice if we need 2 players. Anyone know how to duplicate windows in vscode?\nAnswer: You could open up a new terminal window with cntrl + shift + backtick, and run your python server there (probably on a different port).",0.0,0.21821171,0.04761635139584541
61,"Question\non my hosting provider, I try to make a cron job running a.py-file.
The cron-job starts but I always get this error message ""ImportError: No module named mysql.connector"".
When I run exactly the same script via CLI, it runs smoothly. It connects to my db, it updates, inserts,...  So, there is no issue with having this module (yes I installed).
So, how can I get this cron job to work?
This is how I start that job: python /home2/******/public_html/Test/cron_job_test.py
(when I run this via CLI it works)
Many thanks,
Peter\nAnswer: After hours of trying, I still wasn't able to get it working.
So time for an ugly bypass solution. Instead of a python script, I made a php script to do the work>
And run this cron job 'php /home2/******/public_html/Test/cron.php' and it works.
But I still want to know how to get the mysql.connector to work via crontab ;-)",0.0,0.40120244,0.16096340119838715
62,"Question\nI have a function which send images to the email
The requirements is, i have only two images the first one i need to send as a attachments and another one in the body.
Using alternative in the MIMEmultipart it sending the both images as a documents and i have tried using two multipart that is also not helping. let me know how to approach the issue and also let me know whether it is possible or not
Any idea would be appreciated\nAnswer: You can use HTML to embed the wanted image in the body.
Some e-mail clients interpret this still as an attachment. Maybe check, how other clients interpret your solution.",0.0,0.003122151,9.747825970407575e-06
63,"Question\nI was trying to do semantic segmentation using Detectron2, but some tricky errors occurred when I ran my program. It seems there might be some problems in my environment.
Does anyone know how to fix it?

ImportError: cannot import name 'is_fx_tracing' from 'torch.fx._symbolic_trace' (/home/eric/anaconda3/envs/detectron_env/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py)\nAnswer: @Mohan Ponduri's solution worked for me. Thanks. Seems to be the problem with new Detectron2 installation. Also I was able to use detectron2 on conda environment, just in case any one wants to know",0.40816328,-0.25515407,0.4399898648262024
64,"Question\nI have data that looks like this for IP addresses: for security reasons I am writing made up numbers here.




Subnet 1
Subnet 2
Site




5.22.128.0
17
Texas


5.22.0.0
17
Boston


etc
etc
etc




Question: Can I write a VBA or python code to do the below:
to take each Subnet 1 and:  if the third octet is 128 then add 127 rows below it and fill them as such:




Subnet 1
Subnet 2
Site




5.22.128.0
17
Texas


5.22.129.0
17
Texas


5.22.130.0
17
Texas




.... all the way to:




Subnet 1
Subnet 2
Site




5.22.255.0
17
Texas




And if the third octet is 0 then do the same thing but from 0 to 127. while keeping the other data intact (Site and Subnet 2) the same.
I didn't really know where to begin so I don't have code but my thinking was:
either:
A. Change the decimals to commas to represent figures in millions then add a summation calc until it reaches certain numbers.
B.Create two lists one from 0-127 and one from 128-255 and then append them to the values on the columns but I still don't know how to get multiple rows for it.
I am fairly new but if there is anything wrong with the way the question is presented please let me know. - don't care if it is done through VBA or python as I can write both - Just need a direction as to how to start.\nAnswer: Question: Can I write a VBA or python code to do the below

Answer Well, I don´t know if you can write it, but it's writeable :)
If you want to have the address in one column and work only with that column you will have to do some string manipulation in your code, having as reference the dots in the strings.
Or you can have a column with each one of the octect and then concatenate them with the dots in another column. This way you won't have to do string manipulation or even code at all, maybe you",0.0,0.16476226,0.027146602049469948
65,"Question\nI have a Python script that should have slightly different behaviour if it's running inside a Kubernetes pod.
But how can I find out that whether I'm running inside Kubernetes -- or not?\nAnswer: An easy way I use (and it's not Python specific), is to check whether kubernetes.default.svc.cluster.local resolves to an IP (you don't need to try to access, just see if it resolves successfully)
If it does, the script/program is running inside a cluster. If it doesn't, proceed on the assumption it's not running inside a cluster.",0.20408164,0.43814278,0.054784614592790604
66,"Question\nI want to install third-party Python modules from the Python interactive shell rather than the Terminal or Command Prompt. What Python instructions do I run to do this so I can then import the installed module and begin using it?
(For those asking why I want to do this: I need reliable instructions for installing third-party modules for users who don't know how to navigate the command-line, don't know how to set their PATH environment variable, may or may not have multiple versions of Python installed, and can also be on Windows, Mac, or Linux and therefore the instructions would be completely different. This is a unique setup, and ""just use the terminal window"" isn't a viable option in this particular case.)\nAnswer: From the interactive shell (which has the >>> prompt) run the following, replacing MODULE_NAME with the name of the module you want to install:
import sys, subprocess; subprocess.run([sys.executable, '-m', 'pip', 'install', '--user', 'MODULE_NAME'])
You'll see the output from pip. You can verify that the module installed successfully by trying to import it:
import MODULE_NAME
If no error shows up when you import it, the module was successfully installed.",1.0,0.42055547,0.33575597405433655
67,"Question\nI have a star power-up in my game, and when it collides with the player I want an 'invincibility timer' that starts up. This timer would basically turn off all collisions for 5 seconds, and after the 5 seconds are over, they would turn on again. Is there a better way to accomplish this, and if not, how can I write this in pygame?\nAnswer: Use a boolean => invisibility = false
Set the boolean to true once the player collides with the star power-up.
Then in your if-else statements or loops, state that if invisibility == true, no collisions will take place.",0.0,0.28659123,0.08213453739881516
68,"Question\nI have installed numpy using
pip install numpy
In the current directory.
But when I try to import it in my jupyter notebook, it gives an error.
ModuleNotFoundError                       Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_17100/2172125874.py in 
----> 1 import numpy
ModuleNotFoundError: No module named 'numpy'
Please help me resolve this problem. I have tried uninstalling numpy and re-installing, yet it does not work at all. I am using a Windows 10 system.\nAnswer: First, pip install numpy will install NumPy package in python/site-package, not the current directory. You can type pip show numpy in the terminal to check its information(path, version...).
Secondly, Maybe the interpreter you choose in Jupyter notebook is not the same as the one you installed numpy on. You might need to check that.
To check Whether it has numpy. You might use pip list to check that, in case pip corresponds to the interpreter you wanna check.
Hope this will help you.",0.40816328,0.16047299,0.06135047972202301
69,"Question\nhow to fix 404 error in github.io in new repositories? I tried the new, public, changed mode, nothing helps. there is a python script in the repositories to collect information, maybe there are some restrictions on github for python scripts?\nAnswer: There is one possible reason for it, GitHub pages isn't enabled. To solve this, Repo Settings -> Github Pages section -> Select the main branch. If it is already there, then wait for some time and refresh your site again.",0.0,0.34111458,0.1163591593503952
70,"Question\nI have two different dataframes and I need to add informatio of one dataframe into another basede on a column where they share the same values.
Something like this:
DF1:




Invoices
Client
Product
Product type




00000001
AAAAAA
A1a1





DF2:




Product
Product type
Product description




A1a1
Type A1
description of the product




The first Dataframe is a list of all invoices over the last year, which has one row for each product in that invoice, I need to add the ""Product type"" from DF2 on DF1 for each product.
I've tried to use the merge function but it adds the column and that's not what I need to do.
I need to compare the ""Product"" columns on both DFs and when the value is the same populate DF1 ""Product"" with DF2 ""Product"" value.\nAnswer: df3 = pd.merge(df1, df2, how =""left"", on ""Product"")",0.0,0.07396293,0.005470514763146639
71,"Question\nI'am trying to make a web bot with Python in IntelliJ, i found a plugin called selenium that makes life a lot better. the only thing is that you can only download the selenium plugin with the IntelliJ Ultimate version. Does anybody know how to use the selenium plugin with the IntelliJ community edition.\nAnswer: For coding with Python they have PyCharm. IntelliJ is designed for other languages like Java",0.20408164,0.18516356,0.0003578938194550574
72,"Question\nI'am trying to make a web bot with Python in IntelliJ, i found a plugin called selenium that makes life a lot better. the only thing is that you can only download the selenium plugin with the IntelliJ Ultimate version. Does anybody know how to use the selenium plugin with the IntelliJ community edition.\nAnswer: Currently i installed Selenium and its working. First i installed Pycharm and installed selenium in the terminal of pycharm by typing ""pip install selenium"", and after that i installed PyPI source archive by typing ""python3 setup.py install"". Then i needed the right webdriver voor my Google Chrome.
Its running perfectly.",0.0,0.045384586,0.0020597607363015413
73,"Question\nI've created a widget and I've applied several form layouts to tabs within the widget. I would like to make some tabs available based on some boolean flags.
I've tried tab_my_tab.setEnabled(False) which disables the content of the tab, but it doesn't hide the tab. I'd like it if the tab is completely hidden.\nAnswer: I've found the solution to my question. I was able to hide one or more tabs by using the setTabVisible(index, bool) method on the QTabWidget().
To hide multiple tabs you have to start hiding the highest index number tab first before hiding the lower index number tabs.",0.0,0.23878556,0.057018544524908066
74,"Question\nI am currently using the subprocess module to play.mp3 files. It works just fine getting them to play, but once I do subprocess.call([""afplay"", ""../music/songname.mp3""]), that's all the code will do until the entire duration of the song has finished playing. I want to make things happen while the song is playing. I don't know how easy this is but I've struggled to find people online asking about the same thing. Is it possible to use a different command with the same subprocess module and achieve this result? Is there a completely separate way to achieve this? I'm open to anything, but keep in mind I'm very new to this.
I have created a loop track so that, after a certain time in the song has been reached, it will instead play an identical track which has set beginning and end times that, when played back-to-back, create a perfect music loop. Once that first problem is solved, how can I rig this track to repeat infinitely in the background?
I'm very new to this.\nAnswer: subprocess forks then execs a process, but it waits for the process to finish before returning.  you will need to do this in a separate thread, use an asyncio suprocess interface or do an initial a fork yourself.  it is a bit tricky.  what is also complicated, is that you will likely not get a perfect gapless playback by looping subprocess commands, as python will likely take a millisecond or two to re-execute the command.",-0.23809524,0.019287348,0.06624578684568405
75,"Question\nI am using PyCaret for comparison of regression models on my data.
After blending of the top 3 models, how can I get the final regression equation(model) of the blender?
Please help!!\nAnswer: lets say lr is the classifier and equation you want is mx+b, you can get:
m=lr.Coef_
b=lr.Intercept_",0.0,-0.09744233,0.009495007805526257
76,"Question\nI am working on a sentiment analysis project, where the backbone is ofc a model. This was developed using sklearn's off the shelf solutions (MLP) trained with my data. I would like to ""save"" this model and use it again in JavaScript.
Adam
I have looked at pickle for python but I'm not sure how i could use this for JS. This is a chrome extension I am developing so I would rather not set up and server. I should add this is course work, so spending money is a no!\nAnswer: After some research I pretty much determined its not possible using sklearn  in JS. My solution was to use keras and use tensorflow JS.
Alternatively, I have learnt the maths behind the network and ""raw"" code it using no libraries. This took a lot longer than just converting everything to keras although.",0.0,0.26460928,0.07001806795597076
77,"Question\nI am defining a python project using a pyproject.toml file. No setup.py, no setup.cfg.
The project has dependencies on an alternate repository: https://artifactory.mypypy.com, how do I specify it?\nAnswer: The dependency is independent on where it is hosted, the dependency is on the package not the repository.
The correct way to remediate the problem is to change your pip configuration to look in multiple repositories using the extra-index-url setting. This can be done either in your pip.conf or by specifying --extra-index-url on the pip command line.",0.81632656,0.34540737,0.22176489233970642
78,"Question\nI developped an application using youtube api and python. This application launchs live streams on my channel.
To do so I went to google console, and created ""consent screen"", ""credentials""  downloaded ""json auth file""....etc then developped my code in python, run it, everything works fine in my machine and on my channel
Now I want that this application to be used by some users (friends), so they can launch lives in their channels
Shall they create also consent screen... because this too heavy, as consent screen asks for many details about developper profile. But the final users are not developpers
So please any exlanation on how to be able to use a youtube api app elsewhere\nAnswer: When you give your script to others they will need to create their own project up on google cloud console. You can not share your client id and client secrete with others. Unless you can compile your python into an executable.  I believe that is possible.
If they don't want to create their own projects then you should host your script some place like a web page where they can then click on it authorize the app and create their live streams.  this will mean to switching to web application credentials, i am assuming you created a desktop app / native app currently.
If you go with option one the users are going to have to verify their apps themselves.  If you go with option two your going to have to verify the application run by others.",0.40816328,0.21359527,0.03785670921206474
79,"Question\ni have a MILP with ~3000 binaries, 300000 continuous variables and ~1MM constraints. I am trying to solve this on the VM how long could it potentially take on a 16 core 128 gig machine? also what are the general limits of creating problems using  pulp that cplex solver can handle on such a machine? any insights would be appreciated\nAnswer: It is impossible to answer either question sensibly. There are some problems with only a few thousand variables that are still unsolved 'hard' problems and others with millions of variables that can be solved quite easily. Solution time depends hugely on the structure and numerical details of your problem and many other non-trivial factors.",0.0,0.13643003,0.01861315220594406
80,"Question\ni have a MILP with ~3000 binaries, 300000 continuous variables and ~1MM constraints. I am trying to solve this on the VM how long could it potentially take on a 16 core 128 gig machine? also what are the general limits of creating problems using  pulp that cplex solver can handle on such a machine? any insights would be appreciated\nAnswer: The solution time is not just a function of the number of variables and equations. Basically, you just have to try it out. No one can predict how much time is needed to solve your problem.",0.0,0.052719116,0.002779305214062333
81,"Question\nI have to convert the object to DateTime. However, it shows a year, month, and day at the front. So how can I display only time?
f1['Time'] = pd.to_datetime(f1['Time'], format = '%H:%M:%S.%f')
f1['Time']
It shows:
0    1900-01-01 01:32:03.897
1    1900-01-01 02:02:34.598
2    1900-01-01 01:34:31.421
What I want is time only, like this:
0    01:32:03.897
1    02:02:34.598
2    01:34:31.421\nAnswer: It seems that you're successfully converting the data to datetime objects. What is being shown in the output is the string representation of the date.
The format string you passed to pd.to_datetime is just for parsing the dates. It does not affect their internal data. If you only need strings, you should call.strftime(""%H:%M:%S.%f"") for each date object in your collection.",0.0,0.029421091,0.000865600595716387
82,"Question\n[GAUSS-51400] : Failed to execute the command: python3 '/soft/openGauss/script/local/PreInstallUtility.py' -t create_cluster_paths -u omm -g dbgrp -X '/soft/openGauss/clusterconfig.xml' -l '/gaussdb/log/omm/om/gs_local.log'.Error:
[GAUSS-50202] : The /gaussdb must be empty. Or user [omm] has write permission to directory /gaussdb. Because it will create symbolic link [/gaussdb/app] to install path [/gaussdb/app_78689da9] in gs_install process with this user.\nAnswer: Under the OMM user, execute vi~/. bashrc, clear the environment variables, and re execute the initialization script.",0.0,0.3690225,0.13617759943008423
83,"Question\nIn my webapp, I'd like to allow users who want to deploy an instance to write their own templates. Specifically, I would like to include a template for a data protection declaration using the include tag and have this point to a location which users can define in their settings.
However, this would not be translatable, as all translated strings have to be in django.po and that file is in version control.
Is there a way to extend django.po, e.g. use an include statement to point it to a second, user generated translations file, similar to how I can include templates within other templates?\nAnswer: Not entirely sure if this is possible, but you best bet is probably to use some other mechanism for translation. For example, you could create a template-tag user_translation and make it fetch the translation from the database or settings.",0.0,0.38905442,0.151363343000412
84,"Question\nWe currently have a working application that is ready to post and get data from an API which displays results of predicted disease (purpose of the ML model). Right now we don't have an exact idea on how to make the.ipynb communicate with the application provided we have large data for training the model.
We have 2.ipynb files Model.py and Predict.py. One performing the required pre-processing, split (for train, test and validation), train and save the model. Predict uses the saved model and classifies the user input.
The main concern is how do we send the data from User's end-point(Flutter Application) to Predict.py and get the result data back to the user on the application.
We have considered the idea of hosting the model with prediction somewhere, but do not know on how to proceed further.
This is my first encounter with handling Deep Learning with Flutter Application. Any kind of information on proceeding forward will be very helpful.\nAnswer: First,.ipynb files are Jupyter Notebooks.
Second, do you have your API ready? Is there a server dedicated to it or is there only the flutter app, that is ony the front of your application.
If you do not have an API, you have to create one (using whatever framework you want).
To facilitate things, create it in Python, and you can directly import your model as a Python module and use it.",0.0,0.21872354,0.04783998429775238
85,"Question\nI have a python script that is using
while:
try:
except:
I am wondering how to stop it running (not exit script) and then return to the beginning.\nAnswer: I think I have this now. when using except, just call the main function again.",0.0,0.061031938,0.00372489751316607
86,"Question\nQ1:
When I have a GRPC connection with one server(S) and several clients(C1 and C2)(Using Response-streaming RPC).
I wonder how frames S sends to C1 and C2?
For example, there's 10 frames that server needs to response. What will C1 and C2 receive separately and Why?

C1 gets 5 frames and C2 gets another 5(I tried my program and seems it acts like this way)
C1 gets all 10 frames C2 gets the same all 10 copies.

And is there a way to choose from 1 or 2?
Q2:
a GRPC connection with only one server(S) and only one client(C) this time(Using Response-streaming RPC still).
I forcely stop C(i.e. ctrl+c) and restart the program(C_second).But this time C_second still only gets parts of frames that S sends.
Seems the connection between S and C(forcely stopped) still be alive?\nAnswer: gRPC is point-to-point. The connection C1 -> S and C2 -> S are separate. Any message that S sends will be directed at either C1 or C2. If duplication is required between clients, the server application logic will need to manage this itself.

Great question. This is a point that people often miss in designing their protocol. gRPC itself is stateless. If a channel dies, the client will not attempt to download previously sent messages. Instead, the RPC needs to be made idempotent or stateful. That is, if checkpointing needs to happen in a streaming API, the application needs to manage that itself.",0.81632656,0.30158597,0.26495787501335144
87,"Question\nI'm not used to the TextVectorization Encoder Layer. I created my vocabulary manually before. I was wondering how one can save a Keras Model which uses the TextVectorization layer. When I tried to do it with simply model.save() and later models.load_model() I was prompted with this error:
AssertionError: Found 1 Python objects that were not bound to checkpointed values, likely due to changes in the Python program. Showing 1 of 1 unmatched objects: [<tensorflow.python.ops.lookup_ops.MutableHashTable object at 0x7fb9602df7c0>]\nAnswer: I've solved my problem by using another version of Keras. If someone faces a similar issue I can recommend to use a different (most of the time newer) version of Keras.
As I already said in my comment. I can't really recommend Keras and or Tensorflow right now. I've started a big NLP project some time ago (half a year). And since then Keras had multiple updates. Their documents changed like 2 times. And the old examples are not there anymore. The new way to create Text Tokens is quite nice but their example uses Masking_zero=True. Which basically means that It will pad the sequences for you and following layers will ignore the zero. That sounds nice but masking is not compatible with Cuda which makes training larger models a time consuming job because it's not hardware accelerated with the GPU. And most NLP models are quite large.",0.0,0.24912804,0.06206478178501129
88,"Question\nThere is exactly one module in the ""pinax"" contributed library – which is essential to my project – that I wish to override.  And that one module is site-packages/pinax/templatetags/templatetags/shorttimesince_tag.py, which references the django.utils.tzinfo module which no longer exists in current Django.  (Having been replaced by django.utils.timezone.
I would like to ""override"" this particular module by arranging things so that Python will encounter it first.  Well, I tried unsuccessfully to put this into the wsgi file of the application:

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), ""overrides"")))

I then created an overrides/pinax/templatetags/templatetags directory tree with an __init__.py file in each.
I don't think that Django gets this far, before it dies with an import error.  (Because a ""print()"" statement didn't produce any output.)
But, I feel that something along these lines ought to work.  Because I really don't want to have to absorb this entire contributed library into my application, literally just to change one line of source.
Any ideas?  Am I just barking up the wrong tree? *(P.S.: I am also using another library, the ""machina"" forum, which does include a very nice built-in ""override"" mechanism. But I don't know exactly how it works, and I haven't yet ""dumpster-dived"" to find out...)\nAnswer: Tested it with local files, and it seems, when you override a package by creating a package in override with same name and __init__.py inside, it overrides the whole original package, not just the files you've created. So you have to copy all the files from that package.
And if you don't specify __init__.py, python doesn't recognize your package and imports the original.
I think, the best way here is to fork the original package's repo, change it, and use the fork in your project. Also consider creating a PR to the original repo if you think your changes might be useful to others.",0.40816328,0.31176615,0.00929240696132183
89,"Question\nMy code was working 2 days ago and then chrome updated. I have no idea how to address the below issue, though the specific problem is very clear.

selenium.common.exceptions.SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version 104
Current browser version is 106.0.5249.91 with binary path C:\Program Files (x86)\Google\Chrome\Application\chrome.exe

Can you help?\nAnswer: I solved this by two steps

checked the verison of chrome by going to settings > about chrome > updating version
Check the chrome driver download version  matching to chrome version

unzip the downloaded file and read to go",0.20408164,0.12993741,0.005497366655617952
90,"Question\nI'm a bit new to MicroPython, Scripting languages, etc. So currently, I'm working on a project in which I'm using NUCLEO-G431RB(128kb flash and 32kb ram). STM32G43RB is a low-memory microcontroller. Hence, officially, no MicroPython file firmware file is available for this board.
As Micropython is an open-source platform, the code files are available on the website. Therefore, I wanted to know how I could compile Micropython source code with only selected modules (basic hardware peripheral modules) and eliminate all unnecessary modules (Bluetooth, network, etc.).
My overall goal is to have a bare minimum stack (which I can upload on the low-memory controller as well) of Micropython so that I can run a basic code dealing with hardware peripherals and like that. Any lead, hint or link would be helpful and much appreciated.\nAnswer: The key to the answer is py/mpconfig.h. That file lists all PP symbols which can conditionally enable the core code. In your own port's config file, normally named mpconfigport.h, you then #define MICROPY_SOME_FEATURE 0 to disable the feature.",0.40816328,0.23623097,0.029560718685388565
91,"Question\nI am creating a bot for a VIP channel, and I have a question. How can I implement a kick through time?
The bot has subscriptions for 1, 3, 12 months, how can I make the user get kicked from the group in a month?\nAnswer: Make a database with users, their subscription plans, when they subscribed.
Then loop trough the database every day and if (current_date > (when_they_subsribed + their_subscription_plan)): kick(user).
If you need any help in coding that then write to me on discord. RobertK#6151",0.0,0.25157872,0.06329185515642166
92,"Question\nI am working on a project, in which we have requirement that we have to start our primary key by 1001. The framework used is Django here.
So, we have to create a migration having id as alter field. But I don't know how to alter sequence of primary key. From some articles, I use auto_increment but it gives error of undefined.
Thanks\nAnswer: One thing you could do is in you migrations file create a RunPython script, and in there create 1000 records of whatever you need to start at pk 1001, and after that you delete all of these objects. Then the next object you will create for that model will have pk 1001. Not really pretty, but it would work.",-0.35714287,0.15986127,0.2672932744026184
93,"Question\nI am using mailgun to send an email to notify users that their requested download for the data has been completed, with a link to download the file, the file is stored on AWS.
Your data is ready for download <a href=""aws-link"" download>here</a>
Something like this.
Is there a way to tell the browser to download the file, instead of opening it in a new tab? I guess it's the problem with MIME type, but how do I set it up\nAnswer: Okay, so the file is automatically downloaded if I set the Content-Type to binary/octet-stream when uploading the file in AWS Bucket.
I was looking into the opposite side of the puzzle.",0.0,0.06392884,0.004086896777153015
94,"Question\nI need to install Python and Selenium webdriver on a PC that does not have access to the internet, and don't how to accomplished that.
Sofar, I managed to download Python onto a thumb drive and was able to install it on the PC. But when it comes to Selenium webdriver, I don't know what to do.
Please help.
Regards,
Kiet\nAnswer: You can create a virtual environment with all dependencies that you need and transfer the venv folder.",0.0,0.30420434,0.09254028648138046
95,"Question\nI am writing a LoadTestShape Class. I want to be able to set number of transaction per minute. If I try to set it by specifying user_count it doesn't work because the RPS will vary cpu to cpu.
If say I want to set 1000 transaction per minute, how can we achieve that in locust?\nAnswer: I ended up using constant_throughput to control the number of requests per second.
For different time of the day I'd use a different throughput value.
Simply set the wait_time = constant_throughput(0.1). Based on RPS you want you can either set the value low for less number of requests and more for more RPS.",0.0,0.18051755,0.032586585730314255
96,"Question\nI am new in programming in Python 3. When I was installing Anaconda, after choosing the folder I got the following warning: ""Warning: 'Destination Folder' contains 1 space. This can cause problems with several Conda packages. Please consider removing the space."" I am running on Windows 10 64 bit. Can anyone tell me what is that space and how could I remove it? Any suggestions please? Thanks\nAnswer: Typically in programming you want to avoid directories which contain spaces. Windows environments are prone to a lot of'spacy' directories. I suggest you input cd in your windows shell (or pwd for unix) to get your current directory path. If there's any spaces in there, you should rename them to remove empty spaces.",0.40816328,0.19895256,0.04376912862062454
97,"Question\nI have deployed a service in cloud run that takes about 40/45 minutes to run.
It should run everyday but I don't know how to cron it.
I have tried with cloud scheduler but I received a status DEADLINE_EXCEEDED message.
Can you recommend another service?\nAnswer: Sadly, there is no serverless service with a timeout above 30 minutes for now (Cloud Workflows, Cloud Task, Scheduler are limited to 30 minutes max, 10 minutes for PubSub).
However, even if your Cloud Scheduler log in error the invocation (because no answer has been received in the 30 minutes), your process continue to run. You simply doesn't have the retry features (retries on error, timeout,....)
If you want to implement retries, you have complex architecture to build based on Cloud Logging, sinks, PubSub and Cloud Functions/Cloud Run to be able to re invoke your long running service (in mode ""fire and forget"")",0.0,0.37385243,0.1397656351327896
98,"Question\nGiven a numpy array 'a', a[0,0] and a[0][0] return the same result, so how do I choose them and what is the difference between them?\nAnswer: Assuming that a is 2D, a[0] will return the first row. You can then index into that by column, which is what you're doing with a[0][0]. Both options return the upper left element. The single indexer call (aka, [0,0]) is likely more performant, if that's all you're doing, but it can be convenient to iterate through the rows and work with them individually.",0.0,0.44217157,0.1955157071352005
99,"Question\nI have a problem which works to a degree in Excel using solver. However, I do not know how it can be done within Power BI*
What I essentially want to do is distribute/reallocate volume across several categories.
Example:
I have 5 warehouses all working at different productivity efficiencies.

Warehouse A = 90 throughput rate per hour - Max Capacity 1,200,000
Warehouse B = 85 throughput rate per hour - Max Capacity 800,000
Warehouse C = 100 throughput rate per hour - Max Capacity 2,200,00
Warehouse D = 75 throughput rate per hour - Max Capacity 1,000,000
Warehouse E = 95 throughput rate per hour - Max Capacity 1,100,000

I have 5,000,000 units of volume that I need to distribute to these 5 warehouses. The idea is to reduce cost so the most efficient warehouse should get as much of this volume until it hits its maximum capacity after which it moves to the warehouse with the second best throughput rate.
So in this example Warehouse C will get allocated as much volume as it can take before they hit their max capacity. Then it will move to warehouse E, then Warehouse A. The algorithm should stop as soon as the 5,000,000 units are used up.
(Cost is calculated using the cost per labor hour £15. i.e Volume (units) / Throughput rate = Hours ; Hours * £15 = £ Cost)
Is there a way to do this?\nAnswer: You can run R scripts from Power BI. There are R packages to model and solve problems like this. I would start by developing a mathematical model for this problem. That makes it easier to select a suitable package.",0.0,0.08749223,0.007654889952391386
0,"Question\nI have not been able to come up with a better title, it's a really simple issue though, I just don't know what to call it exactly.
I have a database of horses simplified here:




horse_name
stable_name




Horse1
Stable1




I am only interested in further analyzing records which feature stables that own many horses so I wanted to filter out the small stables (ones with less than 10 horses).
What I've tried:
Attempt 1:
Step 1: df['Stable'].value_counts() > 10  -> gives me boolean values, I inteded to use this to only query the part of the database that satisfied this condition.
Step 2: df[df['Stable'].value_counts() > 10] -> I wrap this in another df, hoping I get the result that I want, but I don't, I get a key error.
Attempt 2:
Step 1: df['Stable'].value_counts().sort_values(ascending=False).head(21)  -> a little clunky, but by trial and error, I figured out there are 21 stables with more than 10 horses, and this query returned just those stables. All I needed now is to filter the database out using this result.
Step 2: df[df['Stable'].value_counts().sort_values(ascending=False).head(21)] -> same issue, returns a key error.
I also tried: df[df['Stable'] in df['Stable'].value_counts() > 10] again, that didn't work, and I don't think I'll sleep today.
Can anyone explain why this is happening in a way that I can understand? And how should this be done instead?\nAnswer:.value_counts() returns a series where it counts the unique values of the values in the column.
Try this:
df[df['Stable'] > 10]",0.0,0.31499243,0.09922023117542267
1,"Question\nI'm using XGBoost model to predict attacks, But I get 100% accuracy, I tried Random Forest as well, and same, I get 100%. How can I handle this ovrefitting problem?
The steps I followed are:
Data cleaning
Data splitting
Feature scaling
Feature selection
I even tried to change this order, but still get the same thing.
Do you have any idea how to handle this? Thanks\nAnswer: Overfitting occurs when your model becomes too complex for its task. Simply said instead of learning patterns in your data, the model will be able to learn every case it is presented in the training set by heart.
To avoid this, you will have to choose a model that is less complex, in your case reduce the depth of your trees. Split your data in separate train, validation and test sets, then train different models of different complexities. When you evaluate these models, you will notice that its predictive capabilities on the training set will increase with complexity. Initially its capabilities on the validation set will follow until a point is reached where no more increase on the validation set can be achieved. On the contrary, it will likely decrease beyond this point, because you are starting to overfit.
Use this data to find a suitable model, then finally evaluate the model you decided to use by using the test set you have kept aside until now.",0.40816328,0.18983006,0.047669392079114914
2,"Question\nMy final project in CS50 is a salary slip generator in pdf format. I got these functions with me but I don't know to test them.

create_pdf() - function that opens my data file (.xlsx), iterates over its data, puts them into variables which will then be called by fpdf to put them into the pdf file. This function will generate as much pdf's depending on the number of data inside the data file.

merge_pdf() - function that merges all the previously generated pdf's into one pdf. This function I might try to check if it outputs the merged pdf or not but still not quite clear to me how to implement it.

get_print_date() - this function only I created just for the sake of adding extra functions to my project hoping that I can test it. It takes datetime.now() and returns the string value of the current date and time. But how can I assert also the return value if the return value changes over time?\nAnswer: Mine is a generic answer regardless of the language used.
Generally, when I have to test some method or function that has side effects or does not return any data, I check for some basic functions called within this function, and I mock them.
These core features are features that I assume are working and do not need to be further tested, such as:

Files management;
Access to the database;
etc..

I therefore suggest you find some libraries to allow you to make mocks of the services used within your functions and change the architecture of your software accordingly.
I hope I was clear.",0.81632656,0.14073408,0.45642518997192383
3,"Question\nI have a coded an AI Python program and now I want to implement it in a mobile application. How can I add the python program in the mobile application, and which programming language should I use for the application to implement the python program?\nAnswer: since python has no built-in mobile app development capability. But you can use other packages to create some sort of mobile application for this you can use packages like Kivy, PyQt, or even Beeware's Toga library.",0.40816328,0.09228736,0.09977759420871735
4,"Question\nI am trying to delete the contents of a column but would like to keep the column.
For instance I have a table like.




Numbers1
Numbers2
Numbers3
Numbers4
Numbers5




five
four
three
two
two


six
seven
eight
nine
ten


nine
seven
four
two
two


seven
six
five
three
one




I would like to remove all the contents of column b but I want to keep column Numbers2
the desired output be like




Numbers1
Numbers2
Numbers3
Numbers4
Numbers5




five

three
two
two


six

eight
nine
ten


nine

four
two
two


seven

five
three
one




kindly help
Thankyou\nAnswer: First, you could delete the column with df = df.drop('Numbers2', axis=1)
Second, replace the column with df['Numbers2'] = """"",0.0,-0.1282795,0.01645563170313835
5,"Question\nFor the past few days, i've been trying to set up a FastAPI & Uvicorn server on my Windows machine (although the main idea is to set it up on an old Android device that i dont use anymore.)
The server (in both my android and my pc) can be reached by any machine in my local network. The problem comes when I try to access it through my public IP. So far I have done everything i can think of to try solv it:

Port forwarding TCP port 8000 (and others) to my PC's and Android's IP.
Allow incoming and outgoing connections in Windows's Firewall (not sure how to do this in Android though)

Some things to have in mind:

I don't want to host the server in the cloud as it needs to interact with local devices. For example, to send WOL packages to my laptop.
I have tried to use reverse proxy services like Ngrok, but the link changes every X amount of time, so it's not reliable enough.
My android device,which is a Sony E2053, currently runs rooted 4.4.4 kitkat, without the possibility to install a custom ROM (locked bootloader).

I barely have knowledge about networking so I'm really messed up with this. Any help is aprecciated.\nAnswer: This question is way to broad for StackOverflow, and is highly dependent on the specifics of your situation. However, to get you going here is a highlevel overview of how it would normally work.
Situation: you have a house, in which there is a internet connection to which your computer is connected. There are two networks at play: your internal network (LAN) and your internet (WAN). Your modem/router connects these two networks together. Your computer has a internal IP (10.x.x.x or 192.x.x.x typically), so has your modem/router. Your modem/router also has a public IP address. You can find that public IP when going to websites as whatismyip.com.
In order for your local computer to be reachable via the internet, you need to forward the traffic meant for you computer to your computer on your modemrouter. Typically this is done by portforwarding on the modemrouter. On your modemrouter there would be a setting that let you forward all incoming traffic on port 8000 to :8000.",0.0,0.36651242,0.1343313455581665
6,"Question\nI have overridden django_tenants.utils.tenant_context to activate a timezone specific to the tenant.
class NewTenantContext(tenant_context):...
I can use the new context like this
with ALSTenantContext(tenant):...
All this works fine for a specific process to start for a tenant.
But how do I make sure NewTenantContext is used when tenant is set from Admin or API?\nAnswer: I am able to resolve this by overriding middleware  django_tenants.middleware.TenantMainMiddleware",0.0,0.48636508,0.23655098676681519
7,"Question\nI am doing notification window and I want to do when massage pop up is shows for some time and then it clears it self.
I know how to do that it waits with time.sleep() but I dont know how to clear screen.blip().\nAnswer: I think the best thing for you to do is to draw the whole window again, removing the pop up. So when the condition to send the message is met, redraw the window with the message, and then after a few seconds, redraw it again without that pop up. The window is redrawn all the time when your screen refreshes anyway, so you're not doing anything drastic.",0.0,0.13279545,0.01763463206589222
8,"Question\nvalue = 12343211122321
print(value.count(1))
The count() function does not tell me how many time the number 1 is in the value variable.
What do I have to do?\nAnswer: Try something like print(str(value).count('1')) as count() is a method of str or list objects (and not int)",0.81632656,0.16127121,0.4290975034236908
9,"Question\nI have a large dataframe which combines data from multiple excel (xlsx) files. The problem is every column with decimal values is seperated with a dot.I need to replace every dot with a comma. I have already tried using the replace function, but the issue some columns also contains string values. So my question is, how do I replace dot with comma on each column in my dataframe and also keep the string values?
Example:
Column a: 
14.01 -> 14,01 
No data (keep)\nAnswer: This is probably your default language setting for Office tool is US or UK where. is used a decimal denoter where as in languages like German it is a,. If  you are using Libre Office, you can go to Tools -> Language -> For all text -> More and change the default decimal separator key. If you are using Microsoft excel, there should be something similar. Afterwards save the excel and then open it back in pandas. Voila.",0.0,0.21321023,0.0454585999250412
10,"Question\nI need to find the minimum distance from a point (X,Y) to a curve defined by four coefficients C0, C1, C2, C3 like y = C0 + C1X + C2X^2 + C3X^3
I have used a numerical approach using np.linspace and np.polyval to generate discrete (X,Y)  for the curve and then the shapely's Point, MultiPoint and nearest_points to find the nearest points, and finally np.linalg.norm to find the distance.
This is a numerical approach by discretizing the curve.
My question is how can I find the distance by analytical methods and code it?\nAnswer: You need to differentiate (x - X)² + (C0 + C1 x + C2 x² + C3 x³ - Y)² and find the roots. But this is a quintic polynomial (fifth degree) with general coefficients so the Abel-Ruffini theorem fully applies, meaning that there is no solution in radicals.
There is a known solution anyway, by reducing the equation (via a lengthy substitution process) to the form x^5 - x + t = 0 known as the Bring–Jerrard normal form, and getting the solutions (called ultraradicals) by means of the elliptic functions of Hermite or evaluation of the roots by Taylor.

Personal note:
This approach is virtually foolish, as there exist ready-made numerical polynomial root-finders, and the ultraradical function is uneasy to evaluate.
Anyway, looking at the plot of x^5 - x, one can see that it is intersected once or three times by and horizontal, and finding an interval with a change of sign is easy. With that, you can obtain an accurate root by dichotomy (and far from the extrema, Newton will easily converge).
After having found this root, you can deflate the polynomial to a quartic, for which explicit formulas by radicals are known.",0.20408164,0.13236785,0.005142867565155029
11,"Question\nI've got a database with production data in multiple tables. I want to analyze the history of the units produced and create a timeline. I am doing this in Python (jupyter lab notebook) and using a cloud based MySQL 8.0 database. Neither of the IDs (both strings and integers) is the primary ID in the database and the IDs cannot be assumed to be sequential. My current strategy is to

First get the IDs from the first event.
Do a new query with a WHERE IN [previous IDs] cluase.
Extract  ID's from 2.
Repeat 2-4 until the final stage.

The IDs are not primary keys in any table. This strategy isn't working as in one stage I have over 800 000 IDs that goes into WHERE IN clause and I can't execute it. Bonus question: Should it work, or is there a limitation in how the query can be formed (such as number of characters or length etc.)?
What I wonder is how to execute this? Is there a way to perform this in a better SQL query or should I split this into multiple queries? Can I use some Python tricks to kind of stream the data in multiple parts?\nAnswer: I have over 800 000 IDs that goes into WHERE IN clause

That's way, way too many for IN.
The best way to handle this kind of volume is to use a temporary table with CREATE TEMPORARY TABLE and join the tables instead of using IN.  A temporary table can have an index so that can help speed things up for the join.
This may seem like a very heavy operation but actually it's not; mysql is very good at this kind of thing.",1.0,0.17073792,0.6876755952835083
12,"Question\nI'm using Django on Ubuntu 18.04.
I've got everything set up. And I type python manage.py run_huey in the server (through an SSH connection) to start huey, and it works.
However this is done through the command line through SSH and it will shut off when I close the SSH connection.
How do I keep run_huey running so that it will stay active at all times? Furthermore, after a system reboot, how do I get run_huey to automatically start?\nAnswer: You may explore supervisorctl utility for ubuntu, it keeps process running and can log into file and any other features. Google it.",0.0,0.22544318,0.05082463100552559
13,"Question\nhow to use pretrained model on 512x512 images on real images with different sizes like Yolo object detection on real image?
CNNs require fixed image sizes, so how do you manage to use the models on real images larger than the inputs?\nAnswer: If it is just about the image size, you could resize your image to have same size as model input. When you receive the output, assuming that you have bounding boxes or locations etc, you can always rescale them back to original image size. Many ML/DL frameworks provide this functionality",0.0,0.04465425,0.0019940021447837353
14,"Question\nOk, no code here, more trying to get some directions.
I'm working on my home automation using tuya objects. Till now I was able to create a websocket (using python websockets and asyncio) that gets a message and turn on my devices. I created a flask website to configure passwords, keys etc. Now what I'm trying to achieve is using a NFC tag(scanned by my phone) call the websocket sending a message. I bought some NFC tags, got a an android app called NFC Tools to record data into the NFC tag.
Problem is NFC tools doesnt give me too many options I can add text, and URLs but I dont know how to call my websocket from there. Can I call it using its URL like ws://something.go? Can I make the phone not open a browser when I scam the tag? Should I create a page on flask for that and put the page address?
Anyway, I'm kind of lost. Can you guys point me in the right direction?\nAnswer: End up creating a get route on flask and using NFC Tools Pro with http get to access it. The route just send a message to my websocket and turn on or off my devices. Thx Andrew for the tip I will have that in mind if this goes anywhere far from a personal project.",0.0,0.04865426,0.0023672368843108416
15,"Question\nI am want to connect/know if there are ways to get Bloomberg data to Python. I see we can connect through blpapi/pdblp package.
So wanted to check what is the pricing for this. Appreciate if anyone can help me here?
Getting ways to connect to Python to get Bloomberg data\nAnswer: Bloomberg has a number of products, which support the real-time API known as the BLP API. This API is a microservice based API. They have microservices for streaming marketdata (//blp/mktdata), requesting static reference (//blp/refdata), contributing OTC pricing (//firm/c-gdco), submitting orders (//blp/emsx), etc etc. The API supports a number of languages including Python, Perl, C++,.NET, etc. The API pattern requires setting up a session where you 'target'/connect to a delivery point. There are several flavours of delivery points depending on what Bloomberg products you buy. For the Bloomberg (Professional) Terminal, you have something called Desktop API (DAPI), they have something called the Server (SAPI), they have something called B-PIPE, another is EMSX. They all present delivery points. They all support the same BLP API.
The Bloomberg Terminal's delivery point is localhost:8194. No Bloomberg Terminal, no localhost delivery point. However, maybe your organisation has bought an Enterprise B-PIPE product, in which case you don't need a Bloomberg Terminal, and the delivery point will sit on at least two servers (IPs), again on port 8194.
So, bottom line, the API library is available and you can develop against it. Problem is, the first few lines of creating a session object and connecting to the end point will fail unless you have a Bloomberg product. There's no sandbox, sadly.
Pricing depends on product, and unfortunately you'll also need to consider your application use-case. As an example, if you're writing a systematic trading application, then the licensing of the Bloomberg (Professional) Terminal will not permit that, however, a B-PIPE will include a licence that will permit that (plus hefty exchange fees if not OTC).
Good luck.",0.40816328,0.29718667,0.012315806932747364
16,"Question\nI am relatively new to web development and very new to using Web2py. The application I am currently working on is intended to take in a CSV upload from a user, then generate a PDF file based on the contents of the CSV, then allow the user to download that PDF. As part of this process I need to generate and access several intermediate files that are specific to each individual user (these files would be images, other pdfs, and some text files). I don't need to store these files in a database since they can be deleted after the session ends, but I am not sure the best way or place to store these files and keep them separate based on each session. I thought that maybe the subfolders in the sessions folder would make sense, but I do not know how to dynamically get the path to the correct folder for the current session. Any suggestions pointing me in the right direction are appreciated!\nAnswer: If the information is not confidential in similar circumstances, I directly write the temporary files under /tmp.",0.0,0.1848768,0.034179430454969406
17,"Question\nI am relatively new to web development and very new to using Web2py. The application I am currently working on is intended to take in a CSV upload from a user, then generate a PDF file based on the contents of the CSV, then allow the user to download that PDF. As part of this process I need to generate and access several intermediate files that are specific to each individual user (these files would be images, other pdfs, and some text files). I don't need to store these files in a database since they can be deleted after the session ends, but I am not sure the best way or place to store these files and keep them separate based on each session. I thought that maybe the subfolders in the sessions folder would make sense, but I do not know how to dynamically get the path to the correct folder for the current session. Any suggestions pointing me in the right direction are appreciated!\nAnswer: I was having this error ""TypeError: expected string or Unicode object, NoneType found"" and I had to store just a  link in the session to the uploaded document in the db or maybe the upload folder in your case. I would store it to upload to proceed normally, and then clear out the values and the file if not 'approved'?",0.0,0.048143387,0.0023177857510745525
18,"Question\nCan anyone please tell me how to stop my server? I did like the first few things you need to do in order to commence a django webpage, url pattern, request, HttpResponce etc. and I ran my server but that rocket is still showing on my screen despite trying to kill, pkill, ctrl+pause, ctrl+C.
I'm so done with this...
I looked up on The Internet how to stop my django server. Nothing worked. On top of that when I ran it I got a ""ModuleNotFoundError"" but the rocket is still showing when I type in the numbers...\nAnswer: Without a screenshot, we can only guess the issue.
I see you've tried some of the basic keybinds, have you tried CTRL+F4?
You could also kill the process.
ps auxw | grep runserver
This will return a process and the associated PID
de        7956  1.8  0.6 540204 55212?        Sl   13:27   0:09 /home/de/Development/sampleproject/bin/python./manage.py runserver
then kill it with
kill 7956
Please make sure there are no &'s in your command as it may send it to the background.",0.0,0.08555031,0.0073188552632927895
19,"Question\ni want to know how to set default page when installing new module.
Like example, My main module is about restaurant ERP, by it's default when i open my module the default page is Restaurant Overview and then i installed reporting sub module with Report Dashboard on it. How to set default page from Restaurant Overview to Report Dashboard when i open restaurant module?\nAnswer: Odoo permits to set an initial action for every user.
From Settings -> Users and Companies -> Users pick one user.
Move to Preferences tab, and pick desired action in Initial page action field.
Repeat this for every user.",0.0,0.2524057,0.06370864063501358
20,"Question\nI am having a trouble with connecting MSSQL to python using Configparser, and one of DB's password has included a character '#'.
Since we don't use escape character "" in INI file, then how should I use a character #? (byw, I can't change the password :( )\nAnswer: The # character is used to start a comment in an INI file. Anything after the # on the same line is ignored by the INI parser.",0.0,0.3093195,0.09567855298519135
21,"Question\nIf I have a function that is quadratic complexity, i.e. the runtime is directly proportional to the square of the input size.  How would I identify its best or worse case runtime, specifically how do I determine if the best and worst case are the same?\nAnswer: This really depends on your algorithm and your input data. For example
a search algorithm can have a typically runtime of O(x^2) but in certain situations this differs:

Typically runtime is for statistically random data (normal case)

If the input data is already sorted (best case), then it depends on the algorithm if it still has normal runtime or is faster e.g. O(1)

If the input data is shaped explicitly in the worst order that there is possible for this specific algorithm (but not very likely), then it may have a bigger worst-case O-notation


Which algorithm you choose then depends on your use case:

If the data is most likely already sorted or partially sorted: You may go with an algorithm that has better best case runtime
If you want fastest results in most cases (with unsorted data): You go for best normal case
If your algorithm may kill someone if it takes to long (e.g. dealing with malicious user input): You will choose one which as equal normal/worst case runtime",0.81632656,0.49424297,0.10373783856630325
22,"Question\nCreate a row that sums the rows that do not have a data in all the columns.
I'm working on a project that keeps throwing dataframes like this:




1
2
3
4
5





108.864

INTERCAMBIADORES DE
1123.60      210.08     166.71     1333.68





CALOR 8419500300




147.420       5.000
PZ
1A0181810000
81039.25       15149.52    19237.754880        96188.77



147.420

INTERCAMBIADORES DE
3882.25      725.75     921.60     4608.00





CALOR 8419500300




566.093      12.000
PZ
1A0183660000
66187.40       12374.29     6546.806709        78561.68



566.093

INTERCAMBIADORES DE
3170.76      592.80     313.63     3763.56





CALOR 8419500300




3.645       1.000
PZ
1A0185890000
836.64          159.69      996.330339          996.33



3.645

INTERCAMBIADORES DE
40.08        7.65      47.73       47.73





CALOR 8419500300




131.998       3.000
PZ
1A0190390000
32819.41        6135.17    12984.858315 ",0.0,0.02670306,0.0007130533922463655
23,"Question\nThe case is really simple. I am using Anaconda and have registered it as the default Python. As it seems, Anaconda has some issues with confluent_kafka library, therefore I need to install and use Python alone for a specific case.
I ran the installation (Python 3.10), added Python to path as well (so both Python are added to the path) but I don't have any clue on how to ""point"" and use the standalone Python explicitly in the desired case and Anaconda otherwise.
I have searched for the answer but came up with proposed solution on for Linux.\nAnswer: Use the full path of the executable, like c:/python3.10/python script.py
EDIT:
It can be located in another directory, check that with where python and adapt the first command",0.20408164,0.48683792,0.07995110750198364
24,"Question\nThe case is really simple. I am using Anaconda and have registered it as the default Python. As it seems, Anaconda has some issues with confluent_kafka library, therefore I need to install and use Python alone for a specific case.
I ran the installation (Python 3.10), added Python to path as well (so both Python are added to the path) but I don't have any clue on how to ""point"" and use the standalone Python explicitly in the desired case and Anaconda otherwise.
I have searched for the answer but came up with proposed solution on for Linux.\nAnswer: Since I cannot comment on other answers, here is how you could install packages for different python versions
c:/python3.10/python.exe -m pip install package_name",0.20408164,0.4563916,0.06366032361984253
25,"Question\nI was learning how to make graphs with python pandas. But I couldn't understand how this code works.
fig, ax = plt.subplots( ) ax = tips[['total_bill','tip']].plot.hist(alpha=0.5, bins=20, ax=ax)
I couldn't understand why the code words only when there is fig infront of ax.
Also I have no idea what 'ax=ax' means.
I found everywhere but I couldn't find the answer...\nAnswer: Pandas is using the library matplotlib to do the plotting. Try to read up a bit about how matploltib works, it will help you understand this code a bit.
Generally, plotting with matplotlib involves a figure and one or more axes. A figure can be thought of as a frame where multiple plots can be created inside. Each plot consists of an axes object which contains your x- and y-axis and so on.
With the command plt.subplots(), you create in a single function a figure object and one or more axes objects. If you pass no parameters to the function, just a single axes object will get created that is placed on the figure object. The figure and axes are returned as a tuple by the function in the form of (figure, axes). You are unpacking that tuple with the first line into the variable fig and ax.
Then, when you call the plotting function on your pandas data, you tell the function on which axes object to do the plotting. This is what the parameter ax means in that function. So you are telling the function to use your axes object that your variable ax is assigned to by setting the parameter ax to ax (ax = ax).
Doing ax = tips[['total_bill','tip']].plot... is redundant. The plotting function returns the axes object on which the plotting was performed by pandas. However, you are just overwriting your already existing axes with the returned axes, which in this case are the same object. This would only be needed if you don't pass the ax parameter to the plotting function, in which case pandas would create a brandnew figure and axes object for you and return the axes object in case you want to do any further tweaks to it.",0.0,0.44888663,0.20149920880794525
26,"Question\nTake the first four characters of your surname - this is your dog name. Now, using the mapping suggested in Lecture 8 (slides 18 and 19), work out your collar number.
For example
""My dog name is LEVI and my collar number is 214873""
I want to know the method how it find.\nAnswer: There is not enough information to answer this. What is ""Lecture 8 slide 18 and 19""? If you ask a question at least provide the full context. As is, nobody will be able to answer this for you.",0.0,0.07468265,0.005577498581260443
27,"Question\nThanks to everyone reading this.
I'm a beginner to pytorch. I now have a.pt file and I wanna print the parameter's shape of this module. As I can see, it's a MLP model and the size of input layer is 168, hidden layer is 32 and output layer is 12.
I tried torch.load() but it returned a dict and I don't know how to deal with it. Also, I wanna print the weight of input layer to hidden layer(that maybe a 168*32 matrix) but I don't know how to do that. Thanks for helping me!\nAnswer: The state dictionary of does not contain any information about the structure of forward logic of its corresponding nn.Module. Without prior knowledge about it's content, you can't get which key of the dict contains the first layer of the module... it's possibly the first one but this method is rather limited if you want to beyond just the first layer. You can inspect the content of the nn.Module but you won't be able to extract much more from it, without having the actual nn.Module class at your disposal.",0.0,0.11405617,0.01300880964845419
28,"Question\nI am trying to trigger an Azure Function from Logic Apps. Running the Azure function takes more than 2 minutes as it is reading a file from a location, converts it to another format and then writes it to a different location. The problem is that the Logic Apps is creating a request, waits for 2 minutes to get a response, but this response doesn't come because the function is not finishing that fast. So the logic app assumes there is an error and recreates the request.
I read in the documentation that there is no way to increase the timeout period. I tried creating two threads in the azure function. One returns 202 http status code to the logic app, and the other one would remain as a daemon and keeps running. But the file doesn't seem to be copied.
Does anyone have any idea how could this be achieved?\nAnswer: Continue the work on another logic app.
Just change your logic app to return Accepted/OK response and calls the function.
The function does the work and after it finishes (or fails) it calls another logic app where it continues the work (or deal with the error).",0.0,0.19427413,0.037742435932159424
29,"Question\nOn hovering the mouse pointer over a function, whether inbuilt or custom, the definition and docstring of the function shows as expected, however it shows twice.  I have had a search for what might be causing this and looked around my settings, but have yet to find a reason for it.  If it makes a difference, am using Python on VSCode.
Perhaps someone knows what might be causing this and how to resolve?  I have not attached any images, since I think the issue is quite clear, but happy to provide if needed for clarification.
Thanks!\nAnswer: Not sure still what caused the issue, but disabling and then re-enabling the Pylance extension has solved this.  Thanks.",0.0,0.33782762,0.1141275018453598
30,"Question\nI'm using the RSA encryption/decryption system, and I have the modulus n (which is a 2048 bit number) and I need to find p and q, which satisfy n = p*q and both are prime numbers. The clue that is given to me is that p is equal to q but with its bits inverted as I say in the title of this post (concretely r and s have the same bits so we could say that p and q have their halves inverted). I don't find the way to take advantage of this so I would be very grateful if someone could help me
I have tried to traverse the number n to find the number p that satisfies that p * p_halfs_inverted = n but logically n is too huge and it is not viable to do it in this way.\nAnswer: OK here's how you can solve this problem.
Start by representing p and q in terms of two k-bit numbers r and s as follows (for your example, k=512):

p = 2kr + s
q = 2ks + r

The value of n is the product of these two numbers:

n   =   pq   =   (2kr + s)(2ks + r)   =   22krs + 2k(r2 + s2) + rs

The first two terms on the right are both multiples of 2k, so the k lowest bits of n are exactly equal to the k lowest bits of rs. Furthermore, since rs is typically a 2k-bit number and r2 + s2 is typically a (2k+1)-bit number, the k highest bits of n are also mostly equal to the k highest bits of rs, but perhaps slightly larger due to the carry generated when adding the 2k(r2 + s2) term.
If n◁ and n▷ are numbers representing the top k bits and bottom k bits of n, then we can generate a candidate value for rs by calculating 2kn◁ + n▷. If this value is correct, we can subtract (22k + 1)rs from n to obtain the value of 2k(r2 + s2). Divide this result by 2k and add 2rs to obtain r2 + 2rs + s2, then calculate the square root of this value to obtain the value of r + s. (If the number isn't a perfect",0.40816328,0.39693877,0.00012598957982845604
31,"Question\nI have an emergency door that individuals should only be exiting from, so I'm trying to think of ways to use computer vision with python to identify if someone entered through it. I've found posted discussing tracking individuals and object detection, but I can't find anything on entering or exiting a door. Any suggestions/guidance would be greatly appreciated.\nAnswer: It can be done in different ways. I'll give you few suggestions and you pick up as per your need

Fix the camera in a way like only those people, who exit the room will be recorded
If you want to save and record those data, you can have ID card detection with matching face

If you explain it some more deep I'll suggest some other ways",0.40816328,0.3403488,0.004598802421241999
32,"Question\nI would like to create a CNN model in Python and I have organized my data in such a way that I have 100 csv files with different sizes (all of them have 141 colunms but some have 33 rows and others have 70 rows). All of those files can be categorized in 6 different categories. All the examples that I have seen so far for buiding a CNN model are using either just one dataset in pandas or using several images of the same size. So the question would be, Can I use my data for creating a CNN model in this fashion? If yes, Can anyone give me some tricks or/and tips of how to?
Thanks a lot in advance!
I have seen some Tensorflow or PyTorch examples but I dont know how to use them with my data\nAnswer: It depends on the reason that the data are separated in different files in the first place and what you want to achieve.
If each file contains observations for a different entity AND you want to predict observations about EACH specific known entity, you can build a model for each entity. In this case, the entities with more training data will of course have better results.
Still, if the difference between those entities can be described with numerical values, depending on the exact problem, you can also try adding those to the training data and then concatenating everything. In this case, the added features will make the final classification to better classify the observations of each different entity without building 100 models. Note however that this could work only if the ""qualities"" of the entities actually affect the observations in some degree, otherwise (if the observations are randomly distributed amongst the entities) the results will probably be worse.
If however the observations of different entities are needed to train a model that works for any entity (including unknown ones), the data can be concatenated to a single table (pandas DataFrames were mentioned in the question) and then train your model with this combined dataset.",0.0,0.3136571,0.09838078171014786
33,"Question\nWould anybody know how to change the version I use in Py-Script? Currently my Py-Script is using python 3.10, but I would like to be able to use python 3.6.
I had python 3.10 and 3.6 installed, so i tried removing 3.10, but that didn't work, as I also expected, but other than that, I have no clue how to and have had no luck finding any information on the topic online.
I'm quite new to programming, so any help would be greatly appreciated!\nAnswer: You cannot easily change the Python version. Python is included with Pyodide which PyScript loads. Changing the version would require rebuilding Pyodide.
Note: I am not sure if it would be possible to use vanilla Python 3.6 with the current version of Pyodide.
Improve your code to work with Pyodide's bundled version of Python.",0.81632656,0.33074713,0.23578737676143646
34,"Question\nI have a django project, that works similar to Jupyter Notebook, in terms of Being a program launched offline in localhost on a web browser, moreover my webapp has an opencv webcam pop-up, that will be launched when you press a button.
I want to deploy my django project, so it can be launched by just clicking a file in Windows.
According to what I read, There are two possible solutions:

Install Python Interpreter with Dependencies on client computer first, and using a bat file, to launch the django server.
Containerizing the Django project with Python and its dependencies, either using Docker or perhaps an exe file?

Which solution is better? I would prefer the second one personally, but I’m confused how to do so.
Can it be done as simple as using pyinstaller or not?
Here are my dependencies for reference:
Django
pillow
django-object-actions
django_user_agents
django-cleanup
opencv-python
imutils
cmake
dlib
face-recognition\nAnswer: I think that the best practise would be to use containers like e.g. docker. After that you have the following benefits:

Dependencies inside the container machine (automatically with pip install from requirements file)
Multiplatform possibility
Versioning with tags
You can run database in a second container if needed (combined with docker compose)
Click and run with docker desktop

fyi: There a lots of tutorials on how to deploy django in docker containers :)",0.40816328,0.21530381,0.03719477728009224
35,"Question\nI'm new in programming, actually I use it for Machine Learning.
I have installed python and anaconda (I don't know if that is right, or I have to install only anaconda?).
And I can see in start menu: (Anaconda powershell, Jupyter, Spyder, Anaconda navigator, Anaconda prompt).
So my question is: Do I still have to use vscode as IDE, or one of the listed programs that come with anaconda? If the answer is the second choice, I will ask, which one of them?
Thanks.
I'm using python just because I have a project in ML, So I must to set the necessary things for ML, like libraries, dataset, and algorithms. Then I have to learn how to use them.
Any help will be very apprecheated.\nAnswer: If I were you I would use vcscode. Anaconda is only a python with extra features, but to code you would appreciate the VSCOde. It comes with many features and you can install extensions to burst your experience. Go for it.",0.0,0.06106305,0.003728696145117283
36,"Question\nI need to add different types of files (CSV, XML, xlsx, etc.) to the database (Postgresql). I know how I can read it via pandas, but I have some issues with adding this to the database.
What libraries do I need to use? And does it need to convert them into one format?\nAnswer: Read files with pandas:
csv_df = pd.read_csv('file.csv')
xml_df = pd.read_xml('file.xml')
xlsx_df = pd.read_excel('file.xlsx')

Add tables in db with columns like in your file

Add files to db
xlsx_df.to_sql('table_name', engine, if_exists='replace', index=False)",0.40816328,0.3982876,9.752915502758697e-05
37,"Question\nGood evening, I'm trying to do some tests with selenium in Firefox, but I'm stuck, I can not click on a button, because I got the message accepting cookies and that does not allow me to continue with the test, I do not know how to make selenium accept cookies.
This is the message it gave me:
An exception occurred: ElementClickInterceptedException Message: Element <select id=""tramiteGrupo[1]"" class=""mf-input__l"" name=""tramiteGrupo[1]""> is not clickable at point (470,571) because another element <a class=""small cli-plugin-button cli-plugin-main-button"" href=""#""> obscures it
I want to get selenium to accept cookies and be able to continue entering parameters.\nAnswer: Get Selenium to pause(check docs on how to pause, something like Thread.sleep(2000);) for a minute just before it clicks the ""Accept cookies"" button and try click the button yourself when it's paused. So you will be able to see what element is blocking it.
Then use ""page scroll up""/""page scroll down""/x/y/whatever to move it so it will not be blocked when trying to click the button.",0.0,0.1773578,0.031455785036087036
38,"Question\nI have used widely used packages(installed via pip) for a while in Jupyter notebook without any issues. I tried to do Python coding in VScode,but it somehow cannot load those packages.
I have tried changing python interpreter, but it did solve the issue. Does anyone know how to resolve this issue?\nAnswer: Hi you can use terminal for installation.
otherwise you can anaconda iDE its very good tool and user friendly.",-0.35714287,-0.02113223,0.11290314793586731
39,"Question\nI'm trying to develop a code that open an image where you can select a point quit the mouse and drag to form a rectangle until you don't release the left button.
Then from python I should receive the starting coordinates and the height and width in pixel of the rectangle, how can I do it?
I saw that the packages argparse and cv2 can be used, but I don't really know how to approach it.\nAnswer: I won't do the job for you but I'm willing to help.
You will need 2 blocks of code:

an image displayer
a mouse-event listener

To start, you may forget about the image displayer. You may concentrate on the mouse listener while you draw your rectangle anywhere on the screen.
Select a mouse listener library. There are many on pypi.org.
I propose pynput because it is easy to work with and is well documented.

read documentation (focus on ""on_click"")
write your code to implement your mouse listener. It's simple (less than 10 lines). At the end of your program, add a statement:
input("">"")
run your program. Click anywhere on the screen and drag to another point. Release.
your on_click() function will be called twice (once for button press and once for button release). Record the two sets of X-Y coordinates (unit is pixels).
once the button is released, compute the size of the rectangle (in pixels).
press any key on the keyboard to end the program.

Once your program is working you may work on the imager. If the image is large, you may have to use a scaling factor to reduce it. You will have to introduce the scaling factor in your sizing equations.
When a program skeleton will exist, do not hesitate to ask questions.
Asking for help when there is no visible sweat will not bring you  many answers.",0.0,0.2599224,0.06755964457988739
40,"Question\nI have a directory full of other directories with thousands of text files and I don't know how to parse every file to look for matches. Is there any way in python?
I tried the read file module but I have to specify a directory and I don't know how to open every file, not only the ones I specified.\nAnswer: If you have a character that separates each directory, you can use that to split the text.
Search about the split function in Python.
' txt.split('') '
If you put the text it's more easy to explain.",0.0,0.15025187,0.0225756224244833
41,"Question\nI have two data frames that I want to merge on a same column name but the values can have different variations of a values.
Examples. Variations of a value :




Variations




USA


US


United States


United States of America


The United States of America




And let's suppose the data frames as below:
df1 =




country
column B




India
Cell 2


China
Cell 4


United States
Cell 2


UK
Cell 4




df2 =




Country
clm




USA
val1


CH
val2


IN
val3




Now how do I merge such that the United States is merged with USA?
I have tried DataFrame merge but it merges only on the matched values of the column name.
Is there a way to match the variations and merge the dataframes?\nAnswer: Use.count to count how many times United States is stated in the list and then make an if command to see if united stated is listed more than once in the list. Do it to all of the other options and make a final if command to check if either any of them are in the list to output the value that you want.",0.0,0.030535161,0.0009323960985057056
42,"Question\nI have 2 windows, first is the main window (window1) and another window with opencv (window2). I have a button on window1 that opens window2. Whenever I open window2 on window1 the camera won't show on the GUI. But if I open window2 individually which is on a different file, the camera is showing.
I tried to put it on a single python file, and it still doesn't work, the camera is still not showing.\nAnswer: I found the answer.
I fixed it by removing the main loop on window2 since camera window2 also runs in tkinter.",0.0,0.17561245,0.03083973191678524
43,"Question\nI have a data file that I'm cleaning, and the source uses '--' to indicate missing data.
I ultimately need to have this data field be either an integer or float.  But I am not sure how to remove the string.
I specified the types in a type_dict statement before importing the csv file.
6 of my 8 variables correctly came in as an integer or float.  Of course, the two that are still objects are the ones I need to fix.
I've tried using the df = df.var.str.replace('--', '')
I've tried using the df.var.fillna(df.var.mode().values[0], inplace=True)
(and I wonder if I need to just change the values '0' to '--')
My presumption is that if I can empty those cells in some fashion, I can define the variable as an int/float.
I'm sure I'm missing something really simple, have walked away and come back, but am just not figuring it out.\nAnswer: OK, we figured out two options to make this work:
solution 1:
df = df.replace(r'^--$', np.nan, regex=True)
solution 2 (a simplified version of #1):
df = df.replace(r'--', np.nan)
Both gave the expected output of empty cells when I exported the csv into a spreadsheet.  And then when I reimported that intermediate file, I had floats instead of strings as expected.",0.0,0.28675133,0.08222632855176926
44,"Question\nI have a DynamoDB with hashes as UserIDs and set as partition key.
I want to know whether an Item exists in the table or not.
I gonna pass an array of User-Hashes. Each Hash in this array should be checked whether it exists or not.
I already found a solution with GetItem. But that would mean, that i have to loop over all the User-Hashes in the array, right?
Does anybody has a solution how to do this without looping? Looping takes too much of the performance.\nAnswer: There is no shortcut here. You could do parallel (multi-threaded client) calls to reduce the overall latency.",0.81632656,0.17540455,0.41078102588653564
45,"Question\nI'm deploying django app to pythonanywhere where i used APScheduler for automatically send expire mail whenever subscription end date exceed.
I don't know how to enable threads, so that my web app runs perfectly on pythonanywhere.\nAnswer: On hosting platforms like PythonAnywhere, there might be multiple copies of your site running at different times, in order to serve the traffic that you get.  So you should not use an in-process scheduler to perform periodic tasks; instead, you should use the platform's built-in scheduled tasks function.",0.40816328,0.33434433,0.005449237767606974
46,"Question\nI'm restricted to a PostgreSQL as'model storage' for the models itself or respective components (coefficients,..). Obviously, PostgreSQL is far from being a fully-fledged model storage, so I can't rule out that I have to implement the whole model training process in Java [...].
I couldn't find a solution that involves a PostgreSQL database as intermediate storage for the models. Writing files directly to the disk/other storages isn't really an option for me. I considered calling Python code from within the Java application but I don't know whether this would be an efficient solution for subsequent inference tasks and beyond [...]. Are there ways to serialize PMML or other formats that can be loaded via Java implementations of the algorithms? Or ways to use the model definitions/parameters directly for reproducing the model [...]?\nAnswer: Using PostgreSQL as dummy model storage:

Train a model in Python.
Establish PostgreSQL connection, dump your model in Pickle data format to the ""models"" table. Obviously, the data type of the main column should be BLOB.
Anytime you want to use the model for some application, unpickle it from the ""models"" table.

The ""models"" table may have extra columns for storing the model in alternative data formats such as PMML. Assuming you've used correct Python-to-PMML conversion tools, you can assume that the Pickle representation and the PMML representation of the same model will be functionally identical (ie. making the same prediction when given the same input). Using PMML in Java/JVM applications is easy.",0.0,0.37725323,0.14232000708580017
47,"Question\nI have two series of stock prices (containing date, ticker, open, high, low, close) and I'd like to know how to combine them to create a dataframe just like the way Yahoo!Finance does. Is it possible?
""Join and merge"" don't seem to work\nAnswer: Use pd.concat([sr1, sr2], axis=1) if neither one of join and merge work.",1.0,-0.085847855,1.1790655851364136
48,"Question\nWhen trying to make an animation in Ursina Engine you can call a frameanimation3d function but it requires an obj file for each frame.
So if there are 100.obj files to load, the game will be slower. Is there maybe a way to load all these files faster?
( Or maybe use panda3d actor function, but doesn't it support a certain type of file? )\nAnswer: It is true, you can load animations faster with panda3d or ursina. If you are using panda3d, use their.egg file. You can improve performance by instancing and using LoD. Check the examples in Panda's Manual. Don't forget, ursina is built above panda3d, so you can use Panda's code in ursina.
P.S.: I'm not sure if Panda's actor supports LoD.
I wrote this in case if you are using the same model",0.0,0.22133723,0.048990167677402496
49,"Question\nI am working on an application similar to a quiz model. My requirement is,

The web socket server should be able to send time updates - say each question has 30 sec time, i need a scheduled job which will send 30,29,28.....0 to the web socket client.
Once the time is up, i need one more functionality to send the next question to the web socket client(i.e., it should be 30 sec interval job).

Note: I am usign AsyncWebSocketConsumer as my base consumer.
Can anyone help me with good design on how i can implement this functionality and some code snippets/tutorials in doing so?\nAnswer: I think the count should be done on the quizz page, and one finished the answer is sent to the server.
On the server you can also control that the question has been answer in 30s.",0.0,0.00030833483,9.507036224931653e-08
50,"Question\nFor example, the following list has x number of items each (2 in the one below, item1 and item2 in each sublist).
alist = [[1,2],[4,1],[3,2]...]
...
I want to get the average of item1 and the average of item2 throughout all sublists.
Output should be something like: [('item1', item1's avg), ('item2', item2's avg), … ]
I'm having trouble finding out how I can do this with 2 unknowns, the number of items, and the number of sublists. Is there a pythonic way to do this?\nAnswer: averages = [sum(sublist)/len(sublist) for sublist in alist]",0.20408164,0.12393695,0.006423171143978834
51,"Question\nI recently installed a newer version of python version 3.10.8. I still have the old version running version 3.9.9.
I would like to install pandas on the new python installation using pip install pandas command. I don't know how to specify in the terminal so that it installs on the python 3.10.8 version installation.\nAnswer: The comments above were useful. I will explain how i resolved it in case someone else has a similar issue.
There was no installation folder for the python 3.10.8 in the programs folder, i want to believe it was because of the installed 3.9.9 version which had a folder Python39 in the default installation location.
So I uninstalled and reinstalled the newer version (3.10.8) to a custom location/folder. I opened the Python\Scripts folder of the new installation in the command line. I was able to install pandas with
pip install pandas",0.0,0.23309374,0.05433269217610359
52,"Question\nBy using python I need to know how to find a substring in a text file.
I tried using in and not in function in python to find a substring from a text file but i am not clear about it\nAnswer: Finding the index of the string in the text file using readline() In this method, we are using the readline() function, and checking with the find() function, this method returns -1 if the value is not found and if found it returns 0.",0.0,0.26320416,0.06927642971277237
53,"Question\nBy using python I need to know how to find a substring in a text file.
I tried using in and not in function in python to find a substring from a text file but i am not clear about it\nAnswer: finding the index of the string in the text file using readline()In this method,we are using the readline()function,and checking with the find()function,this method returns-1 if the values is not found and if found it returns o",0.0,-0.63587356,0.4043351709842682
54,"Question\nI have an embedded linux system that I need to run a python script whenever it boots. The python script needs to have a terminal interface so the user can interact and see outputs. The script also spawns another process to transfer large amounts of data over SPI, this was written in C.
I've managed to get the script to start on launch and have terminal access by adding
@reboot /usr/bin/screen -d -m python3 /scripts/my_script.py
to the crontab. I can then do ""screen -r"" and interact with the script. However if launched in this way the script fails to start the external SPI script. In python I launch the script with subprocess.Popen
proc=subprocess.Popen([""./spi_newpins,""-o"",""/media/SD/""+ latest_file""])
and this works perfectly whenever I manually launch the script, even within screen. Just not when it is launched by crontab. Does anyone have any ideas on how to get the spi subprocess to also work from crontab?\nAnswer: Fixed now, I had to add an absolute path to the spi_newpins function call
proc=subprocess.Popen([""/scripts/./spi_newpins"",""-o"",""/media/SD/""+ latest_file""])",0.0,0.32032454,0.10260780900716782
55,"Question\nadmin SDK. I want to know how to set the Channel Id on the Notification? I saw in Legacy Http Server Protocol has a parameter ""android_channel_id"", but in the SDK, Notification, or Android_Config, I don't see the way to set the channel id. Please help\nAnswer: I found that I have to use AndroidConfig and AndroidNotification Objects to construct the message. On the Android Notification Object that is an attribute channel_id to set the channel info.",0.0,0.41404825,0.17143595218658447
56,"Question\nI wish to call http api to change password in roundcube mail client using python code. How can this be done? Where is the exact location in the roundcube configuration and how is it invoked?\nAnswer: Ensure the httpapi is available in your install

In the main roundcube configuration file config.inc.php set the variables:
$config['password_httpapi_url']         =
'http://host:5000/change_user_password'; // required
$config['password_httpapi_method']      = 'GET'; // default
$config['password_httpapi_var_user']    = 'username'; // optional
$config['password_httpapi_var_curpass'] = 'curpass'; // optional
$config['password_httpapi_var_newpass'] = 'newpass'; // optional


Important NOTE: If you use the GET method you can pass the variables as parameters using  query string values eg. the request.args.get('username')
If you use the POST method you need to use the form fields. eg. request.form['username']

Pass the http api driver name in plugins/password/config.inc.php:
$config['password_driver'] = 'httpapi';

Reload the web server.",0.0,0.29145122,0.08494380861520767
57,"Question\nI'm using the BeautifulSoup module in Python 3.10 to get HTML data off a web page.
The way this web page is structured, there's a bunch of <h1> tags which section the web page. Some of these will look like <h1 class=""title""><img attributes/> Text</h1>, and some will lack the img element but are otherwise identical.
Suppose I start with a string called name and a BeautifulSoup object called soup. This BeautifulSoup object contains several <h1> tags as described above, each of which is followed by more HTML code. Suppose further that no two <h1> elements contain identical text.
I'd like to compile a function which does the following:

Searches the BeautifulSoup object for a <h1> element which contains a string that, excluding the <img> content, exactly matches the input string name.
If it's not the last <h1> tag in the BeautifulSoup object, return everything from that <h1> tag until the next <h1> tag. The latter tag shouldn't be included in the return, but the former tag can be optionally included or excluded. If it is the last <h1> tag, return everything from that tag to the end of the object.

I'm only just learning BeautifulSoup. I know how to use.find() or.find_all() to track down which <h1> tag matches, but I don't know how to return all the following blocks as well.\nAnswer: Actually, when you crawl data using BeautifulSoup. All HTML tags will be written down. Just use a loop to find that checks for your image's existence or not.",0.0,0.21453375,0.04602472856640816
58,"Question\nSo here's my situation:
I am working on chess software and to make it work I had to change one line of MIT-licensed library which I am using in my project. Modified library is in my virtual environment which is not included in my GitHub repository because I've read that is a bad practice to include venv into GitHub repo. Someday I'd like to share with my project to the chess world and I don't know how to handle this modified library. Normally without any changes I'd just include requirements.txt so a user could simply install it with pip. But here I can't do that because it'd download a library without necessary change. I have some ideas how to handle that but none of them seems optimal. Thanks in advance for any help!
PS. I know it's not strictly programming related question but honestly I don't know a better place to ask it.\nAnswer: You can just fork the library and point your requirements.txt to e.g. git+https://github.com/yourusername/librarything.git@yourbranch for the time being.
(Via my comment.)",0.40816328,0.1829927,0.05070178955793381
59,"Question\nRecently, I was working on my HTML project and I wanted to add some functionality and I had already written code for that in Python. But, I faced a problem I couldn't add that Python code to my HTML code.
So, I wanted to know that how I can add my Python code to my HTML code.\nAnswer: you can use PyScript it is a new framework and It uses python in HTML code to develop apps.",0.0,0.080622494,0.006499986629933119
60,"Question\nIn several Lambda functions and Elastic Beanstalk instances for a project, they all use the same helper functions and constants.
I am trying to follow the DRY method and not hard code these parameters into each Lambda/EB application, but instead have the modules that contain them just import into each Lambda/EB application.
I was ideally hoping to

put all these modules in a separate GitHub repo
create a codepipeline to an S3 bucket
import them into EB/Lambdas wherever needed

I have the first 2 steps done, but can't figure out how to import the modules from S3.
Does anyone have any suggestions on how to do this?\nAnswer: The best way to track changes in code is using a repo but if you need to use an s3 as a repo you can consider enabling versioning in the s3 bucket/repo and define some s3 event source to trigger your pipeline.
For using those dependencies I think it's best to consider using layer for lambda functions or shared EFS volumes in instances for Beanstalk if these dependencies are very important in size.",0.20408164,0.34365964,0.019482018426060677
61,"Question\nA long time ago I read about the closest true value to zero, like zero = 0.000000001, something like that. In the article they mentioned about this value in Python and how to achieve it. Does anyone knows about this? I have look up here in SO but all the answers are about the closest value to zero of an array and that's not my point.\nAnswer: The minimum positive denormalized value in Python3.9 and up is given by math.ulp(0.0) which returns 5e-324, or 4.940656e-324 when printed with format(math.ulp(0.0), '.7').",0.81632656,0.20653683,0.37184351682662964
62,"Question\nThis is a simple issue. I use jupyter notebook for python and usually deal with pdfs using pymupdf.
I usually define pdf = fitz.open('dir/to/file.pdf') but somethimes I forget to close the file before i redefine pdf = fitz.open('dir/to/other_file.pdf')
Sometimes I need to (for example) move or delete file.pdf (the original file) but I can't because python is using it.
Not being an expert, I don't know how to close this file when I have redefined the variable pdf, as obviously pdf.close() would close 'other_file.pdf' and I end up reeinitializing my.ipynb file, which feels dumb.
How can I access an object which variable name has been redefined?\nAnswer: Writting this issue made me think about globals()
Browsing throughout its keys I found that the objects which variables have been reused are stored with dummy names (don't know the term used for them). I found the object I was looking for and I was able to 'close' it.
If there's a better - more elegant solution, I'd be glad to hear about it.",0.0,0.19124913,0.03657623007893562
63,"Question\nPlease i am trying to get the value of the last index in a list
the list is 3855 long
i try using len(Atr[""ADX""]) - 1 
but it's giving me 3829, which is not correct
so please guys how do get the last index
`\nAnswer: you can get last index with this code
a = {5,6,4,1,2,3}
a[-1]
its an example for you
you can type index -1 and python gave you the last index",0.0,0.29343265,0.08610272407531738
64,"Question\nI am running a command os.system(""unit run"" + directoryPath + "" urun shell""), which opens the shell prompt of the unit. How should I run commands on the shell prompt that is a whole new prompt getting open up with Python?
I tried executing the command os.system(""unit run"" + directoryPath + "" urun shell /c command""), but that didn't worked as I was expecting that the command should have ran on the shell prompt.\nAnswer: As far as I know you can just call os.system() again with your shell-command.",0.0,0.1834042,0.03363710269331932
65,"Question\nWhen I run this code on google colab.
from google.cloud import aiplatform
The following error occurred
ImportError: cannot import name 'WKBWriter' from'shapely.geos' (/usr/local/lib/python3.8/dist-packages/shapely/geos.py)
Does anyone know how to solve this problem?
I was working fine on 2022/12/16, but today it is not working.\nAnswer: Running into a similiar issue. So far, I am able to tell that google.cloud actions will not run if I have shapley files installed. When I delete the shapley files on my computer I am able to run google.cloud methods",0.0,0.26503915,0.07024575024843216
66,"Question\nI want to create a Student Register and Login Api without using serializer in django Rest Framework.
So I want to know how I make CRUD operation for those api using ApiView
Any one please solve this\nAnswer: If you are using DRF then you must need to create the serializers. But you can create the API without using DRF and serialzers.",0.0,0.220294,0.04852944612503052
67,"Question\nI want to create a Student Register and Login Api without using serializer in django Rest Framework.
So I want to know how I make CRUD operation for those api using ApiView
Any one please solve this\nAnswer: I don't think you can, or even should but first, you need to understand what a serializer actually does:

Serializers allow complex data such as querysets and model instances to be converted to native Python datatypes that can then be easily rendered into JSON, XML, or other content types. Serializers also provide deserialization, allowing parsed data to be converted back into complex types, after first validating the incoming data

So don't be scared by serializers. Just take your time and learn how to use them using the many tutorials available online.",0.0,0.2836901,0.08048006892204285
68,"Question\nI am using Jina ai for my neural search project, and setting up the jina in my pycharm. What will be the yaml configurations and json schema will be?
I am trying to find setting up resources, unable to get the proper setup guide for pycharm.\nAnswer: Click menu Preferences -> JSON Schema mappings;

Add a new schema, in the Schema File or URL write https://api.jina.ai/schemas/latest.json; select JSON Schema Version 7;

Add a file path pattern and link it to *.jaml or *.jina.yml or any suffix you commonly used for Jina Flow’s YAML.",0.40816328,0.48079753,0.005275734234601259
69,"Question\nI have quite some STL files to render as turntable in Blender.
I need to apply to each model the same setting(scale, lights,shading, etc..)
Is it somehow possible to do a ""batch process"" of all the models with the same setting?
I know I can use python to re-create the same scene/setting but I wonder is there is something easier.\nAnswer: I don't see a way around using the integrated Python Interface, which amongst others was made to facilitate exactly such problems.
However, if programmatically setting all the ""outer"" scene parameters (such as lighting) is too much work, you may consider providing a standard scene that is set up using the classic ""click-by-click""-way. Using such default template, you then could think of selectively loading and unloading your STL files (e.g. using a simple loop) into it, where you may have only little additional steps to take (e.g. scaling, triggering the final render). Loading all STLs at once and toggling their visibility in the renders is also a way to go, yet with potentially higher memory loads than integrating and deleting them one by one.",0.0,0.27428567,0.07523263245820999
70,"Question\nSo, I have been working in the jupyter lab and I have created a code that uses numpy.
In the jupyter I had no problem using numpy. I installed the library, and I used the command import to use such module.
However, I had to save my code as a py file. When I try to run my py file using my cmd, I receive this message:
ModuleNotFoundError: No module named 'numpy'
If i type in my cmd ""pip show numpy"" it shows me that I have numpy installed.
I also have tried to add to my code a line with:

pip install numpy


pip3 install numpy


python -m pip install numpy


python3 -m pip install numpy


conda install -c anaconda numpy


conda install numpy

and for every option I received a syntax error:
SyntaxError: invalid syntax
Can someone help me?\nAnswer: As mentioned in the comments section, it seems like your Python script (.py) was not using the correct environment. Please ensure that correct virtual environment is selected by doing conda activate name_of_the_kernel, where name_of_the_kernel should match the kernel used in your Jupyter notebook.",0.0,0.12605453,0.015889743342995644
71,"Question\nI am trying to merge multiple dataframes to a master dataframe based on the columns in the master dataframes. For Example:
MASTER DF:




PO ID
Sales year
Name
Acc year




10
1934
xyz
1834


11
1942
abc
1842




SLAVE DF:




PO ID
Yr
Amount
Year




12
1935
365.2
1839


13
1966
253.9
1855




RESULTANT DF:




PO ID
Sales Year
Acc Year




10
1934
1834


11
1942
1842


12
1935
1839


13
1966
1855




Notice how I have manually mapped columns (Sales Year-->Yr and Acc Year-->Year) since I know they are the same quantity, only the column names are different.
I am trying to write some logic which can map them automatically based on some criteria (be it column names or the data type of that column) so that user does not need to map them manually.
If I map them by column name, both the columns have different names (Sales Year, Yr) and (Acc Year, Year). So to which column should the fourth column (Year) in the SLAVE DF be mapped in the MASTER DF?
Another way would be to map them based on their column values but again they are the same so cannot do that.
The logic should be able to map Yr to Sales Year and map Year to Acc Year automatically.
Any idea/logic would be helpful.
Thanks in advance!\nAnswer: Generally this is impossible as there is no solid/consistent factor by which we can map the columns.
That being said what one can do is use cosine similarity to calculate how similar one string (in this case the column name) is to other strings in another dataframe.
So in your case, we'll get 4 vectors for the first dataframe and 4 for the other one. Now calculate the cosine similarity between the first vector(PO ID) from the first dataframe and first vector from second dataframe (PO ID). This will return 100% as both the strings are same.
For each and every column, you'll get 4 confidence scores. Just pick the highest and map them",0.0,0.25972793,0.06745859235525131
72,"Question\nLike the title says, I would like to know if python's heapq.heapify() will work faster on a list that is close to a heap or does it do the entire operation element by element on every list?
I'm debating on how often to use heapify().\nAnswer: The obvious answer is yes. If you supply a sorted array to heapify it won't have to perform any swaps at all. If you supply a reverse-sorted array it will have to perform the maximum number of swaps.
That said, there is no benefit to pre-sorting the array before passing it to heapify because the total time (i.e. analyzing and arranging the array, plus heapify time) will exceed the maximum time required for heapify to do its work on even the worst-case arrangement.
That said, you shouldn't have to call heapify more than once. That is, you call heapify to construct the heap. Then you call the heappush and heappop methods to add and remove items.
I suppose, if you have to add a large number of items to an existing heap, you could append them to an existing heap and then call heapify to re-build the heap. Hard to say the exact circumstances under which that would be useful. I'd certainly give any such code a big ol' WTF if I were to see it in a code review.",0.0,0.24908113,0.06204141303896904
73,"Question\nFor some reason I can do almost everything on Django but access to model.objects, like, it doesn't even appear, I cannot delete nor find anything without that (or I do not know how), I'm learning Django from a basic course.
I use Python 3.11, Django 4.1.4, on Pycharm 2022.3.
PD:
If you know of another way of doing the Create, Read, Update and Delete of objects I would appreciate that.
Sorry for the English.
I tried searching on the web about this problem but since I'm a noob on Django and trainee on Python most things are like white noise to me, like, I don't understand.\nAnswer: It does work, it's just that Pycharm does not detect the.objects functions, I do not know why though.",0.0,0.14837378,0.022014779970049858
74,"Question\nNewbie alert
I have code that takes a picture with the camera on the ESP32 but then that picture is in the flash memory and I don't know how to retrieve it. How do I get it onto my computer using micropython or circuitpython?\nAnswer: If you wrote your code in Circuit Python you can make the esp32-cam connected to a wifi hotspot and transfer...  via http or mqtt as you whish.
But if you manage to get a picture from the camera WITH circuit python then you need to share how you achieved that!",0.0,0.18002552,0.0324091874063015
75,"Question\nI have been looking to communicate two or multiple microservices in django and need to make them communicate with each. I've reasearched about it and didnt get proper info about it. what i understood is each microservices application completely dont depend on one another including the database. Now how to communicate each microservices with one another. there are 2 methods **synchronous and asynchronous ** method.
i dont want to use synchronous. how to communicate the endpoints of api in asynchronous way? i found some message brokers like rabbitMQ, kafka, gRPc... Which is the best brokers. and how to communicate using those service? i didnt get any proper guidance. I'm willing to learn, can anybody please explain with some example? It will be huge boost for my work.\nAnswer: There are a few different ways to communicate between microservices in a Django Rest Framework (DRF) application. Here are a few options:
Use HTTP requests: One option is to use HTTP requests to send data between microservices. This can be done using the requests library in Python or using JavaScript's fetch API in the frontend.
Use a message queue: Another option is to use a message queue, such as RabbitMQ or Kafka, to send messages between microservices. This can be useful if you need to decouple the services and handle asynchronous communication.
Use a database: You can also use a database, such as PostgreSQL or MongoDB, to store data that is shared between microservices. This can be done using Django's ORM or a database driver in your preferred language.
Which method you choose will depend on your specific requirements and the nature of the communication between your microservices.",0.0,0.32167852,0.10347706824541092
76,"Question\nI have a string 'ABCAPITAL23JAN140CE'. This is the symbol for an option traded on stock exchange. ABCAPITAL part of the string is the company name. 23 is year 2023. JAN is for month. 140 is the strike price and CE is the type of the option.
All these components can vary for different options.
I need a function such that pieces_of_string = splitstring('ABCAPITAL23JAN140CE')
where
pieces_of_string = ['ABCAPITAL', 23, 'JAN', 140, 'CE'] is returned
how do I do that?\nAnswer: import re
print(re.findall(r""[A-Z]+|\d+"", ""ABCAPITAL23JAN140CE""))",-0.17857143,0.16624886,0.11890102922916412
77,"Question\nI'm new with vim and working on Mac with python, and want to know how to comment/uncomment a single line with vim. Most of the IDEs that I've worked with use Cmd+/ it there something similar?  I tried to find the answer but everything I found was about multiple lines!\nAnswer: Comments in Python are prefixed with #
Working in vim you will need to insert # at some point where you think it's appropriate.
Go to that point then type: i#ESC
...where ESC is the escape key
To delete a comment you'll want to search for #
Type: /#
This will take you to the first occurrence of # from your current position. Once you've established that you've found the comment that you want to delete then...
Type: d",0.20408164,-0.2706377,0.22535845637321472
78,"Question\nMy Python import could not be found after i changed the directory and restarted VSCode.
I installed the package via cmd (pip install ) and it was found in Vscode.
I restarted VSCode because i changed the file location to a other directory. The package wasnt found since then. I uninstalled the package and installed it via Powershell but it wouldnt work. Updated the pip installer.
Created a new file with in the directory where it has been before and installed the package again.
VSCode doesnt recognize the package anymore.
Import """" could not be Resolved (Pylance(reportMissingImports))
Does anybody know why this behavious appears and how to fix it?
I havent found a proper solution on here or another forum\nAnswer: In the bottom right of your VS code instance, you'll see something like 3.11.0 64-bit, which indicates the version of Python that VS code is referring to when running and linting your code. The problem is you installed the package with a different version of Python. If you click on the aforementioned button (that says 3.11.0 64-bit) you should see a list of options show up for the different Python versions installed. You need to change to the one that you installed the package on.",0.0,0.13971078,0.019519103690981865
79,"Question\nI'm using Christofides algorithm to calculate a solution for a Traveling Salesman Problem. The implementation is the one integrated in networkx library for Python.
The algorithm accepts an undirected networkx graph and returns a list of nodes in the order of the TSP solution. I'm not sure if I understand the algorithm correctly yet, so I don't really know yet how it determines the starting node for the calculated solution.
So, my assumption is: the solution is considered circular so that the Salesman returns to his starting node once he visited all nodes. end is now considered the node the Salesman visits last before returning to the start node. The start node of the returned solution is random.
Hence, I understand (correct me if I'm wrong) that for each TSP solution (order of list of nodes) with N nodes that is considered circular like that, there are N actual solutions where each node could be the starting node with the following route left unchanged.
A-B-C-D-E-F-G-H->A could also be D-E-F-G-H-A-B-C->D and would still be a valid route and basically the same solution only with a different starting node.
I need to find that one particular solution of all possible starting nodes of the returned order that has the greatest distance between end and start - assuming that that isn't already guaranteed to be the solution that networkx.algorithms.approximation.christofides returns.\nAnswer: After reading up a bit more on Christofides, it seems like, due to the minimum spanning tree that's generated as first step, the desired result of the first and last node visited being those along the path that are the furthest apart, is already the case.",0.0,0.14653617,0.02147284895181656
80,"Question\nCurrently, we have a table containing a varchar2 column with 4000 characters, however, it became a limitation as the size of the 'text' being inserted can grow bigger than 4000 characters, therefore we decided to use CLOB as the data type for this specific column, what happens now is that both the insertions and selections are way too slow compared to the previous varchar2(4000) data type.
We are using Python combined with SqlAlchemy to do both the insertions and the retrieval of the data. In simple words, the implementation itself did not change at all, only the column data type in the database.
Does anyone have any idea on how to tweak the performance?\nAnswer: You could also ask your DBA if possible to upgrade the DB to max_string_size=EXTENDED, then the max VARCHAR2 size would be 32K.",-0.23809524,0.44079244,0.46088847517967224
81,"Question\nI have a Python script that downloads some excel spreadsheets from a website, and then uploads these spreadsheets to a folder on OneDrive, at the moment I have to run this script on my machine every day, I would like to know if there is a way to run this script on a server or something, so I don't have to keep my computer on all the time.
I thought about uploading the script to Heroku and using the platform's scheduling service, but I don't know how to integrate with OneDrive\nAnswer: Yes, it is possible to schedule a python script to run without using your local machine. There are a few options for doing this:
Use a cloud-based computing service, such as Amazon Web Services (AWS) or Google Cloud Platform (GCP). These services allow you to set up virtual machines and run your python scripts on them.
Use a scheduling service, such as Cron or Windows Task Scheduler. These services allow you to set up a schedule for your python script to run at specific intervals.
Use a remote server or virtual private server (VPS). These allow you to access a machine remotely and run your python scripts on it.",0.0,0.4181865,0.17487993836402893
82,"Question\nI have dataset for indoor localization.the dataset contain columns for wireless access point about 520 column with the RSSI value for each one.the problem is each row of the dataset has values of one scan for a signals that can be captured by a device and the maximum number of wireless access point that can be captured about only 20 ( the signal can be from 0dbm which is when the device near the access point and minus 100dbm  when the device far from the access point but it can capture the signal) the rest of access points which are out of the range of the device scan they have been compensated with a default value of 100 positive.these value (100 dbm ) about 500 column in each row and have different columns when ever the location differ.the question is how to deal with them?\nAnswer: One option to deal with this issue, you could try to impute (change) the values that are out of range with a more reasonable value. There are several approaches you could take to do this:

Replacing the out-of-range values with the mean or median of the in-range values
Using linear interpolation to estimate the missing values based on the surrounding values

The choice will depend on the goal of your machine learning model and what you want to achieve.",0.0,0.26168007,0.06847646087408066
83,"Question\nI have a pyautogui code that repeats a order to click on a webpage but sometimes that webpage freezes and does not load, how could i detect that.

the webpage in not on selenium and chrome has been opened by pyautogui too

Update 1:
I have just realised that the website will realise that i have been on the website for a long time so it will not load certain elements. This usually happens evry 20 minutes.\nAnswer: I finally solved the problem by simply reloading the page every 20 minutes which solved the problem.",0.40816328,0.091418386,0.10032732784748077
84,"Question\nI have a Lambda function which calls a Python script, which in turn gives results in json format.
There is a possibility for the results of the script to tend to infinity, and we end up with ""inf"" values in the json. When this happens, the script can run locally, but encounters an error when run in Lambda:
botocore.errorfactory.InvalidRequestContentException: An error occurred (InvalidRequestContentException) when calling the Invoke operation: Could not parse request body into json: Could not parse payload into json: Non-standard token 'Infinity': enable JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS to allow at... 
among the results when run locally, I do see:
0.008559854691925183, inf, inf, inf, 0.0011680872601948522
It seems to be telling me to enable this feature of the json parser.... I have no idea how to do that. I have checked around and I see people running into a similar json error in different contexts,but found no examples for AWS/Python. Are there a couple lines I can add to my Lambda function to ignore the error?
Alternatively, maybe ""inf"" should just be replaced by the largest possible float value, or something like that?
The plotting of these results is handled by a separate lambda function, so it would be enough to shuttle the results along and handle the infinite value there, but the mere presence of this non-numeric value seems to throw a wrench in the gears. How would you handle this? Thanks.\nAnswer: The answer for me was to use the Numpy function np.nan_to_num() on the data before returning it from the python script",0.40816328,0.104454696,0.0922389030456543
85,"Question\nI can't find how to set up or change the Webhook through API.
Is it possible to change it, set it when I am buying a number, or select one Webhook URL for all numbers?
I tried to find this info in the documentation but there was helpful to me\nAnswer: You will have to log into your Twilio console.
From the Develop tab, select Phone Numbers, then Manage > Active Numbers.
You can set the default Webhook (and back-up alternate Webhook) by clicking on the desired number and entering it under the respective Phone or (if available) SMS fields. You will likely have to set the Webhook (takes 2 seconds) for each phone number purchased as the default is the Twilio Demo URL (replies back with Hi or something)
The nature of a Webhook should allow any change in functionality to be done externally (on your end) through your Webhook script's functionality and thus dynamically changing the Webhook URL through the API on a case-by-case basis is discouraged and frankly should not be necessary. Someone may correct me if mistaken.",0.0,0.18594277,0.0345747135579586
86,"Question\nI have a Orange python widget that I created.  I would like to make it a standard widget in Orange canvas.  I have reviewed several tutorials to do this so I understand the code that must be created but after that how do you import that code into the widget library in Canvas?
No problems at this point looking for a complete tutorial on widget creation and import into Orange Canvas.
Reviewed several tutorials both text and video but they fall short of successful importing the code into Canvas.
When I followed the widget creation on Orange and ran the install command ""pip install -e."" from the setup directory the command executed successfully but when I open Orange Canvas the Demo OWDataSampler was not present.  Not sure how the setup tool knows how to update the Orange application to recognize where the application is installed.
Any help would be appreciated.\nAnswer: I was able to get it to work properly. Need to open the Orange command prompt, navigate to the directory that has the setup.py file, and run the pip install -e. command. The widget is listed properly in canvas.",0.0,0.13349438,0.017820749431848526
87,"Question\nI have a python script which is executed from terminal as
script.py 0001
where 0001 indicates the subcase to be run. If I have to run different subcases, then I use
script.py 0001 0002
Question is how to specify a range as input? Lets say I want to run 0001..0008. I got to know seq -w 0001 0008 outputs what I desire. How to pipe this to Python as input from terminal? Or is there a different way to get this done?\nAnswer: Tried the following already but did not work earlier as I did not have the subcases pulled in the script repo. The following works:
script.py 000{1..8}",0.0,0.24033278,0.0577598474919796
88,"Question\nwhen using pip install pandas
An error occurs as follows:
Collecting pandas
Using cached pandas-1.5.2.tar.gz (5.2 MB)
Installing build dependencies... done
Getting requirements to build wheel... error
error: subprocess-exited-with-error
× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─> [28 lines of output]
Traceback (most recent call last):
File ""d:\py\lib\site-packages\pip_vendor\pep517\in_process_in_process.py"", line 351, in 
main()
File ""d:\py\lib\site-packages\pip_vendor\pep517\in_process_in_process.py"", line 333, in main
json_out['return_val'] = hook(**hook_input['kwargs'])
File ""d:\py\lib\site-packages\pip_vendor\pep517\in_process_in_process.py"", line 112, in get_requires_for_build_wheel
backend = _build_backend()
File ""d:\py\lib\site-packages\pip_vendor\pep517\in_process_in_process.py"", line 77, in build_backend
obj = import_module(mod_path)
File ""d:\py\lib\importlib_init.py"", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
File """", line 1030, in _gcd_import
File """", line 1007, in _find_and_load
File """", line 972, in _find_and_load_unlocked
File """", line 228, in _call_with_frames_removed
File """", line 1030, in _gcd_import
File """", line 1007, in _find_and_load
File """", line 986, in _find_and_load_unlocked
File """", line 680, in _load_unlocked
File """", line 790, in exec_module
File """", line 228, in call_with_frames_removed",0.0,0.03352666,0.0011240368476137519
89,"Question\nSo I know how to create topics on Confluent Cloud with the confluent_kafka AdminClient instance but I’m not sure how to set the topic’s message schema programmatically? To clarify, I have the schema I want to use saved locally in an avro schema file(.avsc)\nAnswer: Use the AdminClient to create the topic and then use the SchemaRegistryClient to register the schema for the topic.",0.0,0.2676645,0.07164428383111954
90,"Question\nI am using robot framework as the main test framework with selenium (+ python libraries) to test web ui. I'm having issues with continuous integration in Jenkins and need to change the screenshot name (that is being assigned automatically with index (selenium-screenshot-{index}.png) to more unique name for several test cases eq.:
${TEST NAME}-screen-{index}.png

I know how to access automatic varibales, but how do I set the automatic generation name to something other than selenium-screenshot on Suite Setup/ beggining of the tests level?

Tried using Set Screenshot Directory to make it more unique for test suites but filenames are still the issue. Also using keyword to capture screenshot and setting the name there is not enough, as some keywords make screenshots on failure and they are still being named with selenium-screenshot convention.\nAnswer: Also using keyword to capture screenshot and setting the name there is not enough, as some keywords make screenshots on failure and they are still being named with selenium-screenshot convention.

You could create your own custom keyword that would handle naming and run on failure. You could use Register Keyword To Run On Failure in Suite Setup to specify which keyword to run on failure.",0.81632656,0.4063394,0.16808946430683136
91,"Question\nthere may very well be an answer to this question, but it's really hard to google for.
you can add commands to gdb by writing them in python. I am interested in debugging one of those python scripts that's running in gdb session.
my best guess is to run gdb on gdb and execute the user added command and somehow magically break on the python program code?
has anybody done anything like this before? I don't know the mechanism by which gdb calls python code, so if it's not in the same process space as the gdb that's calling it, I don't see how I'd be able to set breakpoints in the python program.
or do I somehow get pdb to run in gdb? I guess I can put pdb.set_trace() in the python program, but here's the extra catch: I'd like to be able to do all this from vscode.
so I guess my question is: what order of what things do I need to run to be able to vscode debug a python script that was initiated by gdb?
anybody have any idea?
thanks.\nAnswer: so I figured it out. it's kinda neat.
you run gdb to debug your program as normal, then in another window you attach to a running python program.
in this case the running python program is the gdb process.
once you attach, you can set breakpoints in the python program, and then when you run commands in the first window where the gdb session is, if it hits a breakpoint in the python code, it will pop up in the second window.
the tipoff was that when you run gdb there does not appear to be any other python process that's a child of gdb or related anywhere, so I figured gdb must dynamically link to some python library so that the python compiler/interpreter must be running in the gdb process space, so I figured I'd try attaching to that, and it worked.",0.0,0.16728586,0.027984559535980225
92,"Question\nI am making an algorithm that performs certain edits to a PDF using the fitz module of PyMuPDF, more precisely inside widgets. The font size 0 has a weird behaviour, not fitting in the widget, so I thought of calculating the distance myself.
But searching how to do so only led me to innate/library functions in other programming languages.
Is there a way in PyMuPDF to get the optimal/maximal font size given a rectangle, the text and the font?\nAnswer: As @Seon wrote, there is rc = page.insert_textbox(), which does nothing if the text does not fit. This is indicated by a negative float rc - the rectangle height deficite.
If positive however, the text has been written and it is too late for optimizing the font size.
You can of course create a Font object for your font and check text length beforehand using tl = font.text_length(text, fontsize=fs). Dividing tl / rect.width gives you an approximate number of lines in the rectangle, which you can compare with the rectangle height: rect.height / (fs * factor) in turn is a good estimate for the number of available lines in the rect.
The fontsize fs alone does not take the actual line height into account: the ""natural"" line height of a font is computed using its ascender and decender values lh = (font.ascender - font.descender) * fs. So the above computation should better be rect.height / lh for the number of fitting lines.
.insert_textbox() has a lineheight parameter: a factor overriding the default (font.ascender - font.descender).
Decent visual appearances can usually be achieved by setting lineheight=1.2.
To get a good fit for your text to fit in a rectangle in one line, choose fs = rect.width / font.text_length(text, fontsize=1) for the fontsize.
All this however is no guarantee for how a specific PDF viewer will react WRT text form fields. They have their own idea about necessary text borders, so you will need some experimenting.",0.40816328,0.17984813,0.05212780460715294
93,"Question\nWorking on a data transfer program, to move data from an oracle database to another
application that I cant see or change. I have to create several text files described below and drop them off on sftp site.
I am converting from a 20+ year old SQR report. (yes SQR) :(
I have to create text files that have a format as such    an_alpa_code:2343,34533,4442,333335,.....can be thousands or numbers separated by comma.
The file may have only 1 line, but the file might be 48k in size.
There is no choice on the file format, it is required this way.
Tried using Oracle UTL_FILE, but that cannot deal with a line over 32k in length, so looking for an alterative. Python is a language my company has approved for use, so I am hoping it could do this\nAnswer: This gave me one long line
file_obj = open(""writing.txt"", ""w"")
for i in range(0,10000):
file_obj.write(""mystuff""+str(i)+"","")
# file_obj.write('\n')
file_obj.close()",0.0,0.12188208,0.014855241402983665
94,"Question\nI have a list of expressions (+ - *):
[""2 + 3"", ""5 - 1"", ""3 * 4"",...]
and I need to convert every expresion to expression = answer like this 2 + 3 = 5.
I tried just doing print(listt[0]) but it outputs 2 + 3, not 5. So how do i get the answer of this expression? I know that there is a long way by doing.split() with  every expression, but is there any other faster way of doing this?
UPD: I need to use only built-in functions\nAnswer: Use eval() function. The eval() function evaluates the specified expression, if the expression is a legal Python statement, it will be executed.",0.0,0.057027698,0.003252158174291253
95,"Question\nI have a question regarding Python/cx-Oracle.
The Oracle SQLcl and SQL*Developer tools, both support proxy server connections (not to be confused with proxy users).
For example, on SQLcl their is a command line option, ""--proxy"", which is nothing to do with proxy users.
I can't say that I know exactly how they work, but the options are there, and I assume that there is an option in an API in there to support it.
Is this something which cx-Oracle supports?
Thanks,
Clive
I tried looking at the cx-Oracle docs, but couldn't spot anything which might help.\nAnswer: I had another through the docs and it appears that you are expected to make changes to oracle config files (sqlnet.ora and testament.ora). That said, it also appears that newer EZconnect string syntax supports the proxy server requirement.",0.0,0.20387667,0.04156569764018059
96,"Question\nI have 10 virtual enviroments for 10 diferent projects, but they have in common many requirements.
For example: let say all use pandas.
Question 1: this mean I have 10 pandas pip downloaded occuping storage?
Question 2: for this commonly used requierments wouldn't be a better solution to install at system level (""base"")? how do you do this?
Question 3: Is it a crazy idea to create a virtual enviroment with this libraries and use it as a ""base"" and then set include-system-site-packages = true in pyvenv.cfg?
What are the good practices?
I'm on macOS and use hombrew python.
Thanks in advance for your insights and experience.\nAnswer: How about you create a docker container for your common packages, and then you just need to apply whatever you are doing on top of that image. Yes you will have 10 images, but only 1 image with pandas for example.
So to answer your questions...
Question 1, yes.
Question 2, check docker and start using it.
Question 3, in line with question 2 if you use docker.",0.0,0.21979553,0.04831007122993469
97,"Question\nUsed Pymupdf faced the problem of getting information about the text in the pdf file
I asked in the library's discord channel about the possibility of obtaining information about intervals, but they told me that the library does not know how to work with them
Perhaps there are other libraries that can do this?
I tried to look in other libraries but did not find it. Maybe I missed something....\nAnswer: i solved my problem by pdfminer.six and pymupdf by getting line  and character position
thx all of you",0.0,-0.50725377,0.25730639696121216
98,"Question\nI have somehow managed to mess up my pip indexes for a local virtual env.
pip config list returns the following
:env:.index-url='https://***/private-pypi/simple/' global.index-url='https://pypi.python.org/simple'
This makes pip to always default to searching the private pypi index first. Any idea how I can remove the env specific index? It does not appear in the pip.conf file and running pip config unset env.index-url does not work either or I can't get the right syntax.
Thanks!\nAnswer: You can remove the environment-specific index by directly editing the environment's pip.ini file or pip.conf file. The file should be located in the environment's lib/pythonX.X/site-packages/pip/ directory. Simply delete the line with the ""index-url"" value and the default global index will be used.",0.0,0.28875536,0.08337965607643127
99,"Question\nAs an example, I can cross validation when I do hyperparameter tuning (GridSearchCV). I can select the best estimator from there and do RFECV. and I can perform cross validate again. But this is a time-consuming task. I'm new to data science and still learning things. Can an expert help me lean how to use these things properly in machine learning model building?
I have time series data. I'm trying to do hyperparameter tuning and cross validation in a prediction model. But it is taking a long time run. I need to learn the most efficient way to do these things during the model building process.\nAnswer: Cross-validation is a tool in order to evaluate model performance. Specifically avoid over-fitting. When we put all the data in training side, your Model will get over-fitting by ignoring generalisation of the data.
The concept of turning parameter should not based on cross-validation because hyper-parameter should be changed based on model performance, for example the depth of tree in a tree algorithm….
When you do a 10-fold cv, you will be similar to training 10 model, of cause it will have time cost. You could tune the hyper-parameter based on the cv result as cv-> model is a result of the model. However it does not make sense when putting the tuning and do cv to check again because the parameter already optimised based on the first model result.
P.s. if you are new to data science, you could learn something call regularization/dimension reduction to lower the dimension of your data in order to reduce time cost.",0.0,0.28108996,0.07901156693696976
